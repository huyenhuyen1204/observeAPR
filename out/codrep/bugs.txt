null-------------
@@ -210,7 +210,7 @@
             SimpleOrderedMap grpMap = (SimpleOrderedMap) oGrp;
             Object sGroupValue = grpMap.get( "groupValue");
             SolrDocumentList doclist = (SolrDocumentList) grpMap.get( "doclist");
-            Group group = new Group(sGroupValue.toString(), doclist) ;
+Group group = new Group(sGroupValue != null ? sGroupValue.toString() : null, doclist) ;
             groupedCommand.add(group);
           }
 
---------------
-------------
@@ -430,7 +430,7 @@
     checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect);
     // now test with multiple threads: note we do the EXACT same thing we did before in each thread,
     // so this should only really fail from another thread if its an actual thread problem
-    int numThreads = _TestUtil.nextInt(random, 4, 8);
+int numThreads = _TestUtil.nextInt(random, 2, 4);
     AnalysisThread threads[] = new AnalysisThread[numThreads];
     for (int i = 0; i < threads.length; i++) {
       threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect);
---------------
-------------
@@ -255,7 +255,7 @@
 
   @Override
   public void write(TextResponseWriter writer, String name, IndexableField field) throws IOException {
-    writer.writeStr(name, field.stringValue(), false);
+writer.writeStr(name, field.stringValue(), true);
   }
 
   public ExchangeRateProvider getProvider() {
---------------
-------------
@@ -67,7 +67,7 @@
     writer.addDocument(d1);
     writer.addDocument(d2);
     IndexReader reader = writer.getReader();
-    writer.close();
+writer.shutdown();
 
     IndexSearcher searcher = newSearcher(reader);
     searcher.setSimilarity(new SimpleSimilarity());
---------------
-------------
@@ -215,7 +215,7 @@
     }
 
     @Override
-    protected boolean score(Collector collector, int max, int firstDocID)
+public boolean score(Collector collector, int max, int firstDocID)
         throws IOException {
       throw new UnsupportedOperationException(UNSUPPORTED_MSG);
     }
---------------
-------------
@@ -288,7 +288,7 @@
   }
 
   public void assertThreadSafe(final Analyzer analyzer) throws Exception {
-    int numTestPoints = 1000;
+int numTestPoints = 100;
     int numThreads = _TestUtil.nextInt(random, 3, 5);
     final HashMap<String,String> map = new HashMap<String,String>();
     
---------------
-------------
@@ -966,7 +966,7 @@
                 }
                 System.gc();
             }
-        });
+}).start();
     }
 
     /**
---------------
-------------
@@ -1263,7 +1263,7 @@
       reader = (AtomicReader) readerIn;
     } else {
       // Composite reader: lookup sub-reader and re-base docID:
-      List<AtomicReaderContext> leaves = readerIn.getTopReaderContext().leaves();
+List<AtomicReaderContext> leaves = readerIn.leaves();
       int subIndex = ReaderUtil.subIndex(docID, leaves);
       reader = leaves.get(subIndex).reader();
       docID -= leaves.get(subIndex).docBase;
---------------
-------------
@@ -39,7 +39,7 @@
     RuleChain.outerRule(new SystemPropertiesRestoreRule());
 
   public TestSameRandomnessLocalePassedOrNot() {
-    super(false);
+super(true);
   }
   
   public static class Nested extends WithNestedTests.AbstractNestedTest {
---------------
-------------
@@ -367,7 +367,7 @@
 						// Columns is really a Storable[]
 						for (int i = 0; i < columns.length; i++)
 						{
-							if (! (columns[0] instanceof Storable))
+if (! (columns[i] instanceof Storable))
 							{
 								SanityManager.THROWASSERT(
 								"columns[" + i + "] expected to be Storable, not " +
---------------
-------------
@@ -79,7 +79,7 @@
   public CharsRef indexedToReadable(BytesRef input, CharsRef charsRef) {
     // TODO: this could be more efficient, but the sortable types should be deprecated instead
     final char[] indexedToReadable = indexedToReadable(input.utf8ToChars(charsRef).toString()).toCharArray();
-    charsRef.copy(indexedToReadable, 0, indexedToReadable.length);
+charsRef.copyChars(indexedToReadable, 0, indexedToReadable.length);
     return charsRef;
   }
 
---------------
-------------
@@ -106,7 +106,7 @@
             /* Do read repair if header of the message says so */
             if (message.getHeader(ReadCommand.DO_REPAIR) != null)
             {
-                List<InetAddress> endpoints = StorageService.instance.getLiveNaturalEndpoints(command.key);
+List<InetAddress> endpoints = StorageService.instance.getLiveNaturalEndpoints(command.table, command.key);
                 /* Remove the local storage endpoint from the list. */
                 endpoints.remove(FBUtilities.getLocalAddress());
                 if (endpoints.size() > 0 && DatabaseDescriptor.getConsistencyCheck())
---------------
-------------
@@ -23,7 +23,7 @@
 
 public class KeyGenerator {
     private static String randomKey(Random r) {
-        StringBuffer buffer = new StringBuffer();
+StringBuilder buffer = new StringBuilder();
         for (int j = 0; j < 16; j++) {
             buffer.append((char)r.nextInt());
         }
---------------
-------------
@@ -36,7 +36,7 @@
 
 import org.apache.derby.iapi.sql.execute.RunTimeStatistics;
 
-import org.apache.derby.impl.sql.execute.rts.ResultSetStatistics;
+import org.apache.derby.iapi.sql.execute.ResultSetStatistics;
 
 import java.util.Properties;
 
---------------
-------------
@@ -107,7 +107,7 @@
 	 * and then it is turned off while we process the query underlying the view
 	 * v1.             
 	 */
-	boolean isPrivilegeCollectionRequired = true;
+private boolean isPrivilegeCollectionRequired = true;
 
     QueryTreeNode(ContextManager cm) {
         this.cm = cm;
---------------
-------------
@@ -117,7 +117,7 @@
       String s = stemmer.stem(token.termText());
       // If not stemmed, dont waste the time creating a new token.
       if ((s != null) && !s.equals(token.termText())) {
-        return new Token(s, 0, s.length(), token.type());
+return new Token(s, token.startOffset(), token.endOffset(), token.type());
       }
       return token;
     }
---------------
-------------
@@ -129,7 +129,7 @@
             FileOutputFormat.setOutputPath(job, new Path(OUTPUT_PATH_PREFIX + i));
 
             ConfigHelper.setThriftContact(conf, "localhost",  9160);
-            ConfigHelper.setColumnFamily(job.getConfiguration(), KEYSPACE, COLUMN_FAMILY, "BytesType", "RandomPartitioner");
+ConfigHelper.setColumnFamily(job.getConfiguration(), KEYSPACE, COLUMN_FAMILY);
             SlicePredicate predicate = new SlicePredicate().setColumn_names(Arrays.asList(columnName.getBytes()));
             ConfigHelper.setSlicePredicate(job.getConfiguration(), predicate);
 
---------------
-------------
@@ -63,7 +63,7 @@
 
       // check that it is a format we can understand
       if (format < TermInfosWriter.FORMAT_CURRENT)
-        throw new CorruptIndexException("Unknown format version:" + format + " expected " + TermInfosWriter.FORMAT_CURRENT + " or higher");
+throw new IndexFormatTooNewException(null, format, -1, TermInfosWriter.FORMAT_CURRENT);
 
       size = input.readLong();                    // read the size
       
---------------
-------------
@@ -114,7 +114,7 @@
     Random r = new java.util.Random(17);
 
     public MockIndexWriter(Directory dir, Analyzer a, boolean create, MaxFieldLength mfl) throws IOException {
-      super(dir, false, a, create, mfl);
+super(dir, a, create, mfl);
     }
 
     boolean testPoint(String name) {
---------------
-------------
@@ -53,7 +53,7 @@
  * int is <= 3, else 2*baseBlockSize.
  */
 
-public class MockVariableIntBlockPostingsFormat extends PostingsFormat {
+public final class MockVariableIntBlockPostingsFormat extends PostingsFormat {
   private final int baseBlockSize;
   
   public MockVariableIntBlockPostingsFormat() {
---------------
-------------
@@ -46,7 +46,7 @@
     return this.parent == parent;
   }
 
-  public MultiDocsAndPositionsEnum reset(final EnumWithSlice[] subs, final int numSubs) throws IOException {
+public MultiDocsAndPositionsEnum reset(final EnumWithSlice[] subs, final int numSubs) {
     this.numSubs = numSubs;
     this.subs = new EnumWithSlice[subs.length];
     for(int i=0;i<subs.length;i++) {
---------------
-------------
@@ -85,7 +85,7 @@
     // Also, remove any non-alphanumeric characters, so that the lock file will
     // be created for sure on all systems.
     String randomLockName = "lucene-"
-        + ManagementFactory.getRuntimeMXBean().getName().replaceAll("[^a..zA..Z0..9]+","") + "-"
++ ManagementFactory.getRuntimeMXBean().getName().replaceAll("[^a-zA-Z0-9]+","") + "-"
         + Long.toString(new Random().nextInt(), Character.MAX_RADIX)
         + "-test.lock";
     
---------------
-------------
@@ -345,7 +345,7 @@
       throw new IllegalArgumentException("cannot change value type from " + fieldsData.getClass().getSimpleName() + " to BytesRef");
     }
     if (type.indexed()) {
-      throw new IllegalArgumentException("cannot set a Reader value on an indexed field");
+throw new IllegalArgumentException("cannot set a BytesRef value on an indexed field");
     }
     fieldsData = value;
   }
---------------
-------------
@@ -413,7 +413,7 @@
     }
 
     // save last input
-    lastInput.copy(input);
+lastInput.copyInts(input);
 
     //System.out.println("  count[0]=" + frontier[0].inputCount);
   }
---------------
-------------
@@ -192,7 +192,7 @@
       if (docValue < value) {
         return -1;
       } else if (docValue > value) {
-        return -1;
+return 1;
       } else {
         return 0;
       }
---------------
-------------
@@ -192,7 +192,7 @@
 	{
 		InternalDriver id = InternalDriver.activeDriver();
 		if (id != null) { 
-			Connection conn = id.connect("jdbc:default:connection", null);
+Connection conn = id.connect( "jdbc:default:connection", null, 0 );
 			if (conn != null)
 				return conn;
 		}
---------------
-------------
@@ -46,7 +46,7 @@
     if(coreContainer.getZkController() != null) {
       this.cloudDesc = new CloudDescriptor();
       // cloud collection defaults to core name
-      cloudDesc.setCollectionName(name == "" ? coreContainer.getDefaultCoreName() : name);
+cloudDesc.setCollectionName(name.isEmpty() ? coreContainer.getDefaultCoreName() : name);
       this.cloudDesc.setShardId(coreContainer.getZkController().getNodeName() + "_" + name);
     }
     
---------------
-------------
@@ -53,7 +53,7 @@
         ByteArrayOutputStream bos = new ByteArrayOutputStream();
         DataOutputStream dos = new DataOutputStream( bos );
         BootstrapMetadataMessage.serializer().serialize(bsMetadataMessage, dos);
-        return new Message( StorageService.getLocalStorageEndPoint(), "", StorageService.bsMetadataVerbHandler_, new Object[]{bos.toByteArray()} );            
+return new Message( StorageService.getLocalStorageEndPoint(), "", StorageService.bsMetadataVerbHandler_, bos.toByteArray() );
     }        
     
     protected BootstrapMetadata[] bsMetadata_ = new BootstrapMetadata[0];
---------------
-------------
@@ -481,7 +481,7 @@
 	public void boot(boolean create, Properties startParams) 
 			throws StandardException
 	{
-		softwareVersion = new DD_Version(this, DataDictionary.DD_VERSION_DERBY_10_5);
+softwareVersion = new DD_Version(this, DataDictionary.DD_VERSION_DERBY_10_6);
 
 		startupParameters = startParams;
 
---------------
-------------
@@ -95,7 +95,7 @@
     }
     if (ids.size() == 0) return;
 
-    StringBuffer sb = new StringBuffer("id:(");
+StringBuilder sb = new StringBuilder("id:(");
     for (String id : ids) {
       sb.append(id).append(' ');
       model.remove(id);
---------------
-------------
@@ -220,7 +220,7 @@
 
                     if (op.type == OperationType.MINUS)
                     {
-                        value *= -1;
+if (value > 0) value *= -1;
                     }
                 }
                 catch (NumberFormatException e)
---------------
-------------
@@ -78,7 +78,7 @@
      * @return a array of string with the first index being the data center and the second being the rack
      */
     public String[] getEndpointInfo(InetAddress endpoint) {
-        String key = endpoint.toString();
+String key = endpoint.getHostAddress();
         String value = hostProperties.getProperty(key);
         if (value == null)
         {
---------------
-------------
@@ -24,7 +24,7 @@
 import java.util.Map;
 import java.util.TreeMap;
 
-import org.apache.lucene.codecs.DocValuesReaderBase;
+import org.apache.lucene.codecs.lucene40.values.DocValuesReaderBase;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentReadState;
---------------
-------------
@@ -186,7 +186,7 @@
 
   @Override
   public synchronized void rollbackIndexWriter(SolrCore core) throws IOException {
-    newIndexWriter(core, true, true);
+newIndexWriter(core, true, false);
   }
   
   protected SolrIndexWriter createMainIndexWriter(SolrCore core, String name, boolean forceNewDirectory) throws IOException {
---------------
-------------
@@ -518,7 +518,7 @@
       if (segmentInfos.size() > 0 || newSegment != null) {
         final FrozenBufferedDeletes packet = new FrozenBufferedDeletes(pendingDeletes, delGen);
         if (infoStream != null) {
-          message("flush: push buffered deletes");
+message("flush: push buffered deletes startSize=" + pendingDeletes.bytesUsed.get() + " frozenSize=" + packet.bytesUsed);
         }
         bufferedDeletesStream.push(packet);
         if (infoStream != null) {
---------------
-------------
@@ -57,7 +57,7 @@
 
     public CassandraServer()
     {
-        storageService = StorageService.instance();
+storageService = StorageService.instance;
     }
 
     protected Map<String, ColumnFamily> readColumnFamily(List<ReadCommand> commands, ConsistencyLevel consistency_level)
---------------
-------------
@@ -92,7 +92,7 @@
       Random random = LuceneTestCase.random();
       try {
         Document doc = new Document();
-        doc.add(new Field("id", "1", TextField.TYPE_UNSTORED));
+doc.add(new TextField("id", "1", Field.Store.NO));
         writer.addDocument(doc);
         holder.reader = currentReader = writer.getReader(true);
         Term term = new Term("id");
---------------
-------------
@@ -85,7 +85,7 @@
       return new VariableIntBlockIndexInput(in) {
 
         @Override
-        protected BlockReader getBlockReader(final IndexInput in, final int[] buffer) throws IOException {
+protected BlockReader getBlockReader(final IndexInput in, final int[] buffer) {
           return new BlockReader() {
             public void seek(long pos) {}
             public int readBlock() throws IOException {
---------------
-------------
@@ -82,7 +82,7 @@
     document = new String[]{"ff"};
     result = classifier.classify(model, document, "unknown");
     assertTrue("category is null and it shouldn't be", result != null);
-    assertTrue(result + " is not equal to " + "unknown", result.getLabel().equals("unknown"));
+assertTrue(result + " is not equal to " + "d", result.getLabel().equals("d"));//GSI: was unknown, but we now just pick the first cat
 
     document = new String[]{"cc"};
     result = classifier.classify(model, document, "unknown");
---------------
-------------
@@ -191,7 +191,7 @@
       }
 
       // ... then all shards:
-      final Weight w = query.weight(searcher);
+final Weight w = searcher.createNormalizedWeight(query);
 
       final TopDocs[] shardHits = new TopDocs[subSearchers.length];
       for(int shardIDX=0;shardIDX<subSearchers.length;shardIDX++) {
---------------
-------------
@@ -1067,7 +1067,7 @@
       hash.get(sort[i], expected);
       docValues.lookupOrd(i, actual);
       assertEquals(expected.utf8ToString(), actual.utf8ToString());
-      int ord = docValues.lookupTerm(expected, actual);
+int ord = docValues.lookupTerm(expected);
       assertEquals(i, ord);
     }
     AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);
---------------
-------------
@@ -79,7 +79,7 @@
         if (expectedRows == null)
             JDBC.assertEmpty(rs);
         else
-            JDBC.assertFullResultSet(rs, expectedRows);
+JDBC.assertUnorderedResultSet(rs, expectedRows);
         rs.close();
     }
     
---------------
-------------
@@ -47,7 +47,7 @@
  * same order to the other indexes. <em>Failure to do so will result in
  * undefined behavior</em>.
  */
-public final class ParallelAtomicReader extends AtomicReader {
+public class ParallelAtomicReader extends AtomicReader {
   private final FieldInfos fieldInfos;
   private final ParallelFields fields = new ParallelFields();
   private final AtomicReader[] parallelReaders, storedFieldsReaders;
---------------
-------------
@@ -56,7 +56,7 @@
      */
     public static Test suite() {
         if (JDBC.vmSupportsJSR169()) {
-            return new TestSuite("empty: client not supported on JSR169");
+return new TestSuite("empty: client not supported on JSR169; procs use DriverMgr");
         }
         else {
             return TestConfiguration.defaultSuite(
---------------
-------------
@@ -304,7 +304,7 @@
         // compact so we have a big row with more than the minimum index count
         if (cfStore.getSSTables().size() > 1)
         {
-            cfStore.doCompaction(cfStore.getSSTables().size());
+cfStore.doCompaction(2, cfStore.getSSTables().size());
         }
         SSTableReader sstable = cfStore.getSSTables().iterator().next();
         long position = sstable.getPosition(key);
---------------
-------------
@@ -398,7 +398,7 @@
       } else {
         // pick valid term
         loc = random.nextInt(validTerms.length);
-        t = new BytesRef(validTerms[loc]);
+t = BytesRef.deepCopyOf(validTerms[loc]);
         if (VERBOSE) {
           System.out.println("\nTEST: valid term=" + t.utf8ToString());
         }
---------------
-------------
@@ -81,7 +81,7 @@
   public boolean equals(Object other) {
     if (other instanceof IndexCommit) {
       IndexCommit otherCommit = (IndexCommit) other;
-      return otherCommit.getDirectory().equals(getDirectory()) && otherCommit.getSegmentsFileName().equals(getSegmentsFileName());
+return otherCommit.getDirectory().equals(getDirectory()) && otherCommit.getVersion() == getVersion();
     } else
       return false;
   }
---------------
-------------
@@ -139,7 +139,7 @@
 		ijTokMgr = new ijTokenManager(charStream);
 		ijParser = new ij(ijTokMgr, this);
 		this.out = out;
-		this.ignoreErrors = ignoreErrors;
+if ( ignoreErrors != null ) { this.ignoreErrors = (Hashtable) ignoreErrors.clone(); }
 		
 		showErrorCode = 
 			Boolean.valueOf(
---------------
-------------
@@ -105,7 +105,7 @@
         LinkedHashMap<PendingFile, PendingFile> mapping = new LinkedHashMap<PendingFile, PendingFile>();
         for (PendingFile remote : remoteFiles)
         {
-            Descriptor remotedesc = remote.getDescriptor();
+Descriptor remotedesc = remote.desc;
 
             // new local sstable
             Table table = Table.open(remotedesc.ksname);
---------------
-------------
@@ -2078,7 +2078,7 @@
                     RawColumnDefinition rcd = new RawColumnDefinition();
                     rcd.index_name = cd.getIndexName();
                     rcd.index_type = cd.getIndexType();
-                    rcd.name = ByteBufferUtil.string(cd.name, Charsets.UTF_8);
+rcd.name = ByteBufferUtil.string(cd.name);
                     rcd.validator_class = cd.validator.getClass().getName();
                     rcf.column_metadata[j++] = rcd;
                 }
---------------
-------------
@@ -33,7 +33,7 @@
   }
 
   @Override
-  public FieldConfig getFieldConfig(CharSequence fieldName) {
+public FieldConfig getFieldConfig(String fieldName) {
 
     // there is no field configuration, always return null
     return null;
---------------
-------------
@@ -175,7 +175,7 @@
     target.close();
   }
   
-  private void checkOverrideMethods(Class<?> clazz) throws Exception {
+private void checkOverrideMethods(Class<?> clazz) {
     boolean fail = false;
     for (Method m : clazz.getMethods()) {
       int mods = m.getModifiers();
---------------
-------------
@@ -805,7 +805,7 @@
     // the upgrade test. But in 10.9 and higher, that won't happen
     // because LOB is never read into memory for the trigger being
     // used by this test.
-    public void testTriggersWithLOBcolumns() throws Exception
+public void atestTriggersWithLOBcolumns() throws Exception
     {
         Statement s = createStatement();
         ResultSet rs;
---------------
-------------
@@ -618,7 +618,7 @@
             for (InetAddress endpoint: entry.getValue())
             {
                 if (endpoint.equals(FBUtilities.getLocalAddress()))
-                    rpcaddrs.add(DatabaseDescriptor.getRpcAddress().toString());
+rpcaddrs.add(DatabaseDescriptor.getRpcAddress().getHostAddress());
                 else
                     rpcaddrs.add(Gossiper.instance.getEndpointStateForEndpoint(endpoint).getApplicationState(ApplicationState.RPC_ADDRESS).value);
             }
---------------
-------------
@@ -46,7 +46,7 @@
         ByteArrayOutputStream bos = new ByteArrayOutputStream();
         DataOutputStream dos = new DataOutputStream(bos);
         serializer_.serialize(cdMessage, dos);
-        Message message = new Message(StorageService.getLocalStorageEndPoint(), "", StorageService.calloutDeployVerbHandler_, new Object[]{bos.toByteArray()});
+Message message = new Message(StorageService.getLocalStorageEndPoint(), "", StorageService.calloutDeployVerbHandler_, bos.toByteArray());
         return message;
     }
     
---------------
-------------
@@ -50,7 +50,7 @@
       String typeString;
       if (index != -1) {
         fieldName = field.substring(0, index);
-        typeString = field.substring(index, field.length());
+typeString = field.substring(1+index, field.length());
       } else {
         typeString = "auto";
         fieldName = field;
---------------
-------------
@@ -107,7 +107,7 @@
 	 */
 	@Deprecated // TODO remove in 3.2
 	public void setExclusionTable( Map<?,?> exclusiontable ) {
-		exclusions = new HashSet(exclusiontable.keySet());
+exclusions = exclusiontable.keySet();
 	}
 }
 
---------------
-------------
@@ -129,7 +129,7 @@
 
     @Override
     public void setNextReader(AtomicReaderContext context) throws IOException {
-      fromDocTerms = FieldCache.DEFAULT.getTerms(context.reader(), field);
+fromDocTerms = FieldCache.DEFAULT.getTerms(context.reader(), field, false);
     }
 
     static class Avg extends SV {
---------------
-------------
@@ -95,7 +95,7 @@
                          optKey(MinhashOptionCreator.NUM_HASH_FUNCTIONS), String.valueOf(numHashFunctions),
                          optKey(MinhashOptionCreator.KEY_GROUPS), String.valueOf(keyGroups),
                          optKey(MinhashOptionCreator.NUM_REDUCERS), "1",
-                         optKey(MinhashOptionCreator.DEBUG_OUTPUT), "true"};
+optKey(MinhashOptionCreator.DEBUG_OUTPUT)};
   }
   
   private static Set<Integer> getValues(Vector vector) {
---------------
-------------
@@ -43,7 +43,7 @@
   }
 
   @Override
-  protected boolean score(Collector collector, int max, int firstDocID) throws IOException {
+public boolean score(Collector collector, int max, int firstDocID) throws IOException {
     return scorer.score(collector, max, firstDocID);
   }
 
---------------
-------------
@@ -115,7 +115,7 @@
 
     // for back compat, a shards param with URLs like localhost:8983/solr will mean that this
     // search is distributed.
-    boolean hasShardURL = shards != null && shards.charAt('/') > 0;
+boolean hasShardURL = shards != null && shards.indexOf('/') > 0;
     rb.isDistrib = hasShardURL | rb.isDistrib;  
 
     if (rb.isDistrib) {
---------------
-------------
@@ -49,7 +49,7 @@
         try
         {
             DataInputStream is = new DataInputStream(buffer);
-            CounterMutation cm = CounterMutation.serializer().deserialize(is);
+CounterMutation cm = CounterMutation.serializer().deserialize(is, message.getVersion());
             if (logger.isDebugEnabled())
               logger.debug("Applying forwarded " + cm);
 
---------------
-------------
@@ -250,7 +250,7 @@
       }
 
       @Override
-      public int lookupTerm(BytesRef key, BytesRef spare) {
+public int lookupTerm(BytesRef key) {
         try {
           InputOutput<Long> o = fstEnum.seekCeil(key);
           if (o == null) {
---------------
-------------
@@ -347,7 +347,7 @@
                 if (failed.get()) break;
                 for(int j=0;j<10;j++) {
                   final String s = finalI + "_" + String.valueOf(count++);
-                  f.setValue(s);
+f.setStringValue(s);
                   w.addDocument(doc);
                   w.commit();
                   DirectoryReader r2 = DirectoryReader.openIfChanged(r);
---------------
-------------
@@ -562,6 +562,6 @@
   }
 
   private void checkHits(Query query, int[] results) throws IOException {
-    CheckHits.checkHits(query, "field", searcher, results);
+CheckHits.checkHits(random, query, "field", searcher, results);
   }
 }
---------------
-------------
@@ -186,7 +186,7 @@
 
   @Override
   public synchronized void rollbackIndexWriter(SolrCore core) throws IOException {
-    newIndexWriter(core, true, true);
+newIndexWriter(core, true, false);
   }
   
   protected SolrIndexWriter createMainIndexWriter(SolrCore core, String name, boolean forceNewDirectory) throws IOException {
---------------
-------------
@@ -427,7 +427,7 @@
     
     FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(CP_A, NUM_CHILDREN_CP_A), 
         new CountFacetRequest(CP_B, NUM_CHILDREN_CP_B));
-    FacetsCollector fc = new CountingFacetsCollector(fsp , taxoReader, new FacetArrays(taxoReader.getSize()), true);
+FacetsCollector fc = new CountingFacetsCollector(fsp , taxoReader, new FacetArrays(taxoReader.getSize()));
     searcher.search(new MatchAllDocsQuery(), fc);
     
     List<FacetResult> facetResults = fc.getFacetResults();
---------------
-------------
@@ -40,7 +40,7 @@
     @Override
     protected int getLevelForDistance(double degrees) {
       GeohashPrefixTree grid = new GeohashPrefixTree(ctx, GeohashPrefixTree.getMaxLevelsPossible());
-      return grid.getLevelForDistance(degrees) + 1;//returns 1 greater
+return grid.getLevelForDistance(degrees);
     }
 
     @Override
---------------
-------------
@@ -224,7 +224,7 @@
   private Map<SegmentInfoPerCommit,Boolean> segmentsToMerge = new HashMap<SegmentInfoPerCommit,Boolean>();
   private int mergeMaxNumSegments;
 
-  private Lock writeLock;
+protected Lock writeLock;
 
   private volatile boolean closed;
   private volatile boolean closing;
---------------
-------------
@@ -2223,7 +2223,7 @@
             else
             {
                 sb.append(stub ? 'D' : 'C');
-                sb.append(containerId.getContainerId());
+sb.append(Long.toHexString(containerId.getContainerId()));
                 sb.append(".DAT");
             }
             return storageFactory.newStorageFile( sb.toString());
---------------
-------------
@@ -193,7 +193,7 @@
             assert container != null;
             IColumn reduced = container.iterator().next();
             ColumnFamily purged = shouldPurge ? ColumnFamilyStore.removeDeleted(container, controller.gcBefore) : container;
-            if (purged != null && purged.metadata().getDefaultValidator().isCommutative())
+if (shouldPurge && purged != null && purged.metadata().getDefaultValidator().isCommutative())
             {
                 CounterColumn.removeOldShards(purged, controller.gcBefore);
             }
---------------
-------------
@@ -42,7 +42,7 @@
 
   @Override
   public String toString() {
-    return commandName;
+return commandName + ":{flags="+flags+", version="+version;
   }
 
   public long getVersion() {
---------------
-------------
@@ -602,7 +602,7 @@
         // This is mostly due to the fact that the current implementation for
         // the on-disk back end doesn't handle logDevice when dropping.
         // Security is another concern.
-        if (!name.startsWith(PersistentService.INMEMORY)) {
+if (!name.startsWith(PersistentService.INMEMORY + ":")) {
             throw StandardException.newException(
                     SQLState.SERVICE_DIRECTORY_REMOVE_ERROR, name);
         }
---------------
-------------
@@ -482,7 +482,7 @@
       path = path.substring(0, path.length() - 2);
     }
     
-    int splits = path.split(File.separator).length;
+int splits = path.split("\\" + File.separator).length;
     
     StringBuilder p = new StringBuilder();
     for (int i = 0; i < splits - 2; i++) {
---------------
-------------
@@ -1194,7 +1194,7 @@
     public byte[] norms(String fieldName) {
       byte[] norms = cachedNorms;
       SimilarityProvider sim = getSimilarityProvider();
-      if (fieldName != cachedFieldName || sim != cachedSimilarity) { // not cached?
+if (!fieldName.equals(cachedFieldName) || sim != cachedSimilarity) { // not cached?
         Info info = getInfo(fieldName);
         Similarity fieldSim = sim.get(fieldName);
         int numTokens = info != null ? info.numTokens : 0;
---------------
-------------
@@ -398,7 +398,7 @@
     MergePolicy.OneMerge runningMerge;
     private volatile boolean done;
 
-    public MergeThread(IndexWriter writer, MergePolicy.OneMerge startMerge) throws IOException {
+public MergeThread(IndexWriter writer, MergePolicy.OneMerge startMerge) {
       this.tWriter = writer;
       this.startMerge = startMerge;
     }
---------------
-------------
@@ -356,7 +356,7 @@
     segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));
 
     synchronized (directory) {			  // in- & inter-process sync
-      new Lock.With(directory.makeLock("commit.lock")) {
+new Lock.With(directory.makeLock("commit.lock"), COMMIT_LOCK_TIMEOUT) {
 	  public Object doBody() throws IOException {
 	    segmentInfos.write(directory);	  // commit changes
 	    return null;
---------------
-------------
@@ -34,7 +34,7 @@
   private String fieldName = "text";
 
   public AnalyzerTransformer() {
-    this(new StandardAnalyzer(Version.LUCENE_41), "text");
+this(new StandardAnalyzer(Version.LUCENE_42), "text");
   }
 
   public AnalyzerTransformer(Analyzer analyzer) {
---------------
-------------
@@ -152,7 +152,7 @@
 	}
 
 	/**
-		@exception StandardException Standard Cloudscape error policy
+@exception StandardException Standard Derby error policy
 	*/
 	public void open() throws StandardException
 	{
---------------
-------------
@@ -122,7 +122,7 @@
       }
 
       reader = w.getReader();
-      w.close();
+w.shutdown();
     }
 
     // NOTE: sometimes reader has just one segment, which is
---------------
-------------
@@ -113,7 +113,7 @@
         // Throw exception if any of the DC doesn't have livenodes to accept write.
         for (String dc: strategy.getDatacenters())
         {
-        	if (dcEndpoints.get(dc).get() != responses.get(dc).get())
+if (dcEndpoints.get(dc).get() < responses.get(dc).get())
                 throw new UnavailableException();
         }
     }
---------------
-------------
@@ -29,7 +29,7 @@
  * Base class for JDBC JUnit test decorators.
  */
 public abstract class BaseJDBCTestSetup
-    extends TestSetup {
+extends BaseTestSetup {
 	
 	public BaseJDBCTestSetup(Test test) {
 		super(test);
---------------
-------------
@@ -56,7 +56,7 @@
         value = clusterWritable.getValue();
         valueClass = value.getClass();
       }
-      log.info("Read 1 Cluster from {}", clusterPath);
+log.debug("Read 1 Cluster from {}", clusterPath);
       
       if (valueClass.equals(Kluster.class)) {
         // get the cluster info
---------------
-------------
@@ -180,7 +180,7 @@
                 // If we can't create the directory the exception will occur 
                 // when trying to create the trace file.
                 File traceDirectory = new File(fileName).getParentFile();
-                if (!PrivilegedFileOps.exists(traceDirectory))
+if (traceDirectory != null)
                 {
                     PrivilegedFileOps.mkdirs(traceDirectory);
                 }
---------------
-------------
@@ -58,7 +58,7 @@
   public void setUp() throws Exception {
     super.setUp();
 
-    conf = new Configuration();
+conf = getConfiguration();
 
     inputFile = getTestTempFile("trainingInstances.seq");
     outputDir = getTestTempDir("output");
---------------
-------------
@@ -25,7 +25,7 @@
 {
     public int getCapacity();
     public void setCapacity(int capacity);
-    public int size();
+public int getSize();
 
     /** total request count since cache creation */
     public long getRequests();
---------------
-------------
@@ -65,7 +65,7 @@
                 }
             }
         };
-        new Thread(runnable).start();
+new Thread(runnable, "COMMIT-LOG-WRITER").start();
 
         MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
         try
---------------
-------------
@@ -153,7 +153,7 @@
 	private void deactivate() {
 		if (!isActive())
 			return;
-		new StopAction(subsystems.getRootSubsystem(), true, false).run();
+new StopAction(subsystems.getRootSubsystem(), subsystems.getRootSubsystem(), true).run();
 		for (ServiceRegistration<?> registration : registrations) {
 			try {
 				registration.unregister();
---------------
-------------
@@ -175,7 +175,7 @@
             ColumnFamilyStore cfs = table.getColumnFamilyStore(cf.id());
             for (IColumn column : cf_.getColumnsMap().values())
             {
-                cf.addColumn(column.localCopy(null)); // TODO fix this
+cf.addColumn(column.localCopy(cfs));
             }
             rm.add(cf);
         }
---------------
-------------
@@ -1715,7 +1715,7 @@
                 sessionState.out.printf("      Columns sorted by: %s%s%n", cf_def.comparator_type, cf_def.column_type.equals("Super") ? "/" + cf_def.subcomparator_type : "");
                 sessionState.out.printf("      Row cache size / save period in seconds: %s/%s%n", cf_def.row_cache_size, cf_def.row_cache_save_period_in_seconds);
                 sessionState.out.printf("      Key cache size / save period in seconds: %s/%s%n", cf_def.key_cache_size, cf_def.key_cache_save_period_in_seconds);
-                sessionState.out.printf("      Memtable thresholds: %s/%s/%s (millions of ops/MB/minutes)%n",
+sessionState.out.printf("      Memtable thresholds: %s/%s/%s (millions of ops/minutes/MB)%n",
                                 cf_def.memtable_operations_in_millions, cf_def.memtable_flush_after_mins, cf_def.memtable_throughput_in_mb);
                 sessionState.out.printf("      GC grace seconds: %s%n", cf_def.gc_grace_seconds);
                 sessionState.out.printf("      Compaction min/max thresholds: %s/%s%n", cf_def.min_compaction_threshold, cf_def.max_compaction_threshold);
---------------
-------------
@@ -656,7 +656,7 @@
 
                 if( i > 20 )
                     throw StandardException.newException(
-                        SQLState.LOG_FULL, sfe, null);
+SQLState.LOG_FULL, sfe);
             }
         }
     }
---------------
-------------
@@ -234,7 +234,7 @@
         rm.add(new QueryPath(SCHEMA_CF,
                              null,
                              DefsTable.DEFINITION_SCHEMA_COLUMN_NAME),
-                             ByteBuffer.wrap(org.apache.cassandra.avro.KsDef.SCHEMA$.toString().getBytes(UTF_8)),
+ByteBuffer.wrap(org.apache.cassandra.db.migration.avro.KsDef.SCHEMA$.toString().getBytes(UTF_8)),
                              now);
         return rm;
     }
---------------
-------------
@@ -597,7 +597,7 @@
     private static boolean randomlyReadRepair(ReadCommand command)
     {
         CFMetaData cfmd = DatabaseDescriptor.getTableMetaData(command.table).get(command.getColumnFamilyName());
-        return cfmd.readRepairChance > random.nextDouble();
+return cfmd.getReadRepairChance() > random.nextDouble();
     }
 
     public long getReadOperations()
---------------
-------------
@@ -209,7 +209,7 @@
 	}
 
 	public boolean jdbcCompliant() {
-		return false;
+return true;
 	}
 
 	/*
---------------
-------------
@@ -185,7 +185,7 @@
         		{"XJ004","Database '{0}' not found.","40000"},
         		{"XJ015","Derby system shutdown.","50000"},
         		{"XJ028","The URL '{0}' is not properly formed.","40000"},
-        		{"XJ040","Failed to start database '{0}', see the next exception for details.","40000"},
+{"XJ040","Failed to start database '{0}' with class loader {1}, see the next exception for details.","40000"},
         		{"XJ041","Failed to create database '{0}', see the next exception for details.","40000"},
         		{"XJ049","Conflicting create attributes specified.","40000"},
         		{"XJ05B","JDBC attribute '{0}' has an invalid value '{1}', valid values are '{2}'.","40000"},
---------------
-------------
@@ -289,7 +289,7 @@
     final Codec codec;
     int randomVal = random.nextInt(10);
     
-    if ("Lucene3x".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) && randomVal <= 3)) { // preflex-only setup
+if ("Lucene3x".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) && randomVal < 2)) { // preflex-only setup
       codec = new PreFlexRWCodec();
       PREFLEX_IMPERSONATION_IS_ACTIVE = true;
     } else if ("SimpleText".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) && randomVal == 9)) {
---------------
-------------
@@ -62,7 +62,7 @@
   
   static {
     // no ssl currently because distrib updates read scheme from zk and no zk in this test
-    sslConfig = null;
+ALLOW_SSL = false;
   }
   
   @BeforeClass
---------------
-------------
@@ -68,7 +68,7 @@
 
   public SimpleTextFieldsReader(SegmentReadState state) throws IOException {
     fieldInfos = state.fieldInfos;
-    in = state.dir.openInput(SimpleTextPostingsFormat.getPostingsFileName(state.segmentInfo.name, state.segmentSuffix), state.context);
+in = state.directory.openInput(SimpleTextPostingsFormat.getPostingsFileName(state.segmentInfo.name, state.segmentSuffix), state.context);
     boolean success = false;
     try {
       fields = readFields(in.clone());
---------------
-------------
@@ -51,7 +51,7 @@
 {
     private static Logger logger = Logger.getLogger(Streaming.class);
     private static String TABLE_NAME = "STREAMING-TABLE-NAME";
-    public static final long RING_DELAY = 30 * 1000; // delay after which we assume ring has stablized
+public static final int RING_DELAY = 30 * 1000; // delay after which we assume ring has stablized
 
     /**
      * Split out files for all tables on disk locally for each range and then stream them to the target endpoint.
---------------
-------------
@@ -448,7 +448,7 @@
     BytesRef term0 = new BytesRef();
     NumericUtils.intToPrefixCoded(0, 0, term0);
     writer.deleteDocuments(new Term("id_int", term0));
-    writer.close();
+writer.shutdown();
 
     IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher search = newSearcher(reader);
---------------
-------------
@@ -292,7 +292,7 @@
       return null;
     }
     
-    System.out.println("num active:" + numActive + " for " + slice);
+//System.out.println("num active:" + numActive + " for " + slice);
     
     int chance = random.nextInt(10);
     JettySolrRunner jetty;
---------------
-------------
@@ -47,7 +47,7 @@
   final static BytesRef PAYLOAD = new BytesRef("        payload ");
 
   public SimpleTextFieldsWriter(SegmentWriteState state) throws IOException {
-    final String fileName = SimpleTextPostingsFormat.getPostingsFileName(state.segmentName, state.formatId);
+final String fileName = SimpleTextPostingsFormat.getPostingsFileName(state.segmentName, state.segmentSuffix);
     out = state.directory.createOutput(fileName, state.context);
   }
 
---------------
-------------
@@ -98,7 +98,7 @@
   
   /** check the expDocNrs first, then check the query (and the explanations) */
   public void qtest(Query q, int[] expDocNrs) throws Exception {
-    CheckHits.checkHitCollector(q, FIELD, searcher, expDocNrs);
+CheckHits.checkHitCollector(random, q, FIELD, searcher, expDocNrs);
   }
 
   /**
---------------
-------------
@@ -114,7 +114,7 @@
   public void fullRankWide() {
     Matrix x = matrix().transpose();
     QRDecomposition qr = new QRDecomposition(x);
-    assertFalse(qr.hasFullRank());
+assertTrue(qr.hasFullRank());
     Matrix rActual = qr.getR();
 
     Matrix rRef = reshape(new double[]{
---------------
-------------
@@ -404,7 +404,7 @@
                     QuorumResponseHandler<Row> quorumResponseHandlerRepair = new QuorumResponseHandler<Row>(
                             DatabaseDescriptor.getQuorum(),
                             readResponseResolverRepair);
-                    logger.info("DigestMismatchException: " + command.key);
+logger.info("DigestMismatchException: " + ex.getMessage());
                     Message messageRepair = command.makeReadMessage();
                     MessagingService.getMessagingInstance().sendRR(messageRepair, commandEndPoints.get(commandIndex), quorumResponseHandlerRepair);
                     try
---------------
-------------
@@ -56,7 +56,7 @@
     private ServiceNamingEnumeration(BundleContext context, ServiceReference[] theRefs, ThingManager<T> manager)
     {
       ctx = context;
-      refs = theRefs;
+refs = (theRefs != null) ? theRefs : new ServiceReference[0];
       mgr = manager;
     }
     
---------------
-------------
@@ -31,7 +31,7 @@
   /** we will manually instantiate preflex-rw here */
   @BeforeClass
   public static void beforeClass3xPostingsFormat() {
-    LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE = true;
+LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true;
   }
 
   @Override
---------------
-------------
@@ -254,7 +254,7 @@
     System.clearProperty("solrcloud.update.delay");
   }
   
-  private void printLayout(String zkHost) throws Exception {
+static void printLayout(String zkHost) throws Exception {
     SolrZkClient zkClient = new SolrZkClient(
         zkHost, AbstractZkTestCase.TIMEOUT);
     zkClient.printLayoutToStdOut();
---------------
-------------
@@ -39,7 +39,7 @@
    *
    */
   public void init(NamedList args) {
-    Integer v = (Integer)args.get("setTermIndexInterval");
+Integer v = (Integer)args.get("setTermIndexDivisor");
     if (v != null) {
       termInfosIndexDivisor = v.intValue();
     }
---------------
-------------
@@ -44,7 +44,7 @@
 {
     private static Logger logger = LoggerFactory.getLogger(StreamCompletionHandler.class);
 
-    public void onStreamCompletion(InetAddress host, PendingFile pendingFile, CompletedFileStatus streamStatus) throws IOException
+public void onStreamCompletion(InetAddress host, PendingFile pendingFile, FileStatus streamStatus) throws IOException
     {
         /* Parse the stream context and the file to the list of SSTables in the associated Column Family Store. */
         if (pendingFile.getFilename().contains("-Data.db"))
---------------
-------------
@@ -24,5 +24,5 @@
 
 public interface IMessageSink
 {
-    public Message handleMessage(Message message, InetAddress to);
+public Message handleMessage(Message message, String id, InetAddress to);
 }
---------------
-------------
@@ -392,7 +392,7 @@
 		try {
 			return fr.getAsStream(JarDDL.mkExternalName(schemaName, sqlName, fr.getSeparatorChar()), generationId);
 		} catch (IOException ioe) {
-			throw StandardException.newException(SQLState.LANG_FILE_ERROR, ioe.toString(),ioe);	
+throw StandardException.newException(SQLState.LANG_FILE_ERROR, ioe, ioe.toString());
 		}
 	}
 
---------------
-------------
@@ -32,7 +32,7 @@
       }
 
       @Override
-      protected Reader initReader(Reader reader) {
+protected Reader initReader(String fieldName, Reader reader) {
         return new MockCharFilter(CharReader.get(reader), 7);
       }
     };
---------------
-------------
@@ -47,7 +47,7 @@
    
     Date dateOfBirth;
     
-	dateOfBirth = (dob == null || "".equals(dob)) ? null : new SimpleDateFormat("dd-mm-yyyy").parse(dob);
+dateOfBirth = (dob == null || "".equals(dob)) ? null : new SimpleDateFormat("yyyy-MM-dd").parse(dob);
 	
     persistenceService.createAuthor(email, dateOfBirth, name, displayName, bio);
   }
---------------
-------------
@@ -66,7 +66,7 @@
       }
     }
     writer.commit();
-    writer.close();
+writer.shutdown();
     AtomicReader open = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
     NumericDocValues norms = open.getNormValues(floatTestField);
     assertNotNull(norms);
---------------
-------------
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.assertVocabulary;
+import static org.apache.lucene.analysis.VocabularyAssert.assertVocabulary;
 
 import java.io.Reader;
 
---------------
-------------
@@ -347,7 +347,7 @@
       /** Input must not be null */
       public static LastModFrom parse(final String s) {
         try {
-          return valueOf(s.toUpperCase());
+return valueOf(s.toUpperCase(Locale.ENGLISH));
         } catch (Exception e) {
           log.warn( "Unrecognized value for lastModFrom: " + s, e);
           return BOGUS;
---------------
-------------
@@ -180,7 +180,7 @@
 		if (isSameNodeType(o)) 
 		{
 			BaseColumnNode other = (BaseColumnNode)o;
-			return other.tableName.equals(other.tableName) 
+return other.tableName.equals(tableName)
 			&& other.columnName.equals(columnName);
 		} 
 		return false;
---------------
-------------
@@ -849,7 +849,7 @@
 			resultSet.generate(acb, mb);
 
 			// arg 2 generate code to evaluate generation clauses
-			generateGenerationClauses( resultColumnList, resultSet.getResultSetNumber(), acb, mb );
+generateGenerationClauses( resultColumnList, resultSet.getResultSetNumber(), false, acb, mb );
 
 			// arg 3 generate code to evaluate CHECK CONSTRAINTS
 			generateCheckConstraints( checkConstraints, acb, mb );
---------------
-------------
@@ -48,7 +48,7 @@
         ByteArrayInputStream bufIn = new ByteArrayInputStream(bufOut.getData(), 0, bufOut.getLength());
         cf = ColumnFamily.serializer().deserialize(new DataInputStream(bufIn));
         assert cf != null;
-        assert cf.name().equals("Standard1");
+assert cf.metadata().cfName.equals("Standard1");
         assert cf.getSortedColumns().size() == 1;
     }
 
---------------
-------------
@@ -40,7 +40,7 @@
 /**
  * Just like {@link Lucene40PostingsFormat} but with additional asserts.
  */
-public class AssertingPostingsFormat extends PostingsFormat {
+public final class AssertingPostingsFormat extends PostingsFormat {
   private final PostingsFormat in = new Lucene40PostingsFormat();
   
   public AssertingPostingsFormat() {
---------------
-------------
@@ -88,7 +88,7 @@
         table = null;
     }
     
-    public Message getMessage(int version)
+public Message getMessage(Integer version)
     {
         ByteArrayOutputStream bos = new ByteArrayOutputStream();
         DataOutputStream dos = new DataOutputStream(bos);
---------------
-------------
@@ -237,7 +237,7 @@
                     if (url == null) {
                         LOGGER.warn("No URL is defined for schema " + ns + ". This schema will not be validated");
                     } else {
-                        schemaSources.add(new StreamSource(url.openStream()));
+schemaSources.add(new StreamSource(url.openStream(), url.toExternalForm()));
                     }
                 }
                 schema = getSchemaFactory().newSchema(schemaSources.toArray(new Source[schemaSources.size()]));
---------------
-------------
@@ -125,7 +125,7 @@
           final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(
               "OutOfMemoryError likely caused by the Sun VM Bug described in "
               + "https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize "
-              + "with a a value smaller than the current chunks size (" + chunkSize + ")");
++ "with a value smaller than the current chunks size (" + chunkSize + ")");
           outOfMemoryError.initCause(e);
           throw outOfMemoryError;
         }
---------------
-------------
@@ -118,7 +118,7 @@
 		@param in			optional data
 
 		@exception IOException Can be thrown by any of the methods of InputStream.
-		@exception StandardException Standard Cloudscape policy.
+@exception StandardException Standard Derby policy.
 
 	 */
 	public final void doMe(Transaction xact, LogInstant instant, LimitObjectInput in) 
---------------
-------------
@@ -563,7 +563,7 @@
           start = dp.startOffset();
           end = dp.endOffset();
         }
-        if (start >= current.endOffset) {
+if (start >= current.endOffset || end > contentLength) {
           pq.offer(off);
           break;
         }
---------------
-------------
@@ -194,7 +194,7 @@
                 if (bundle != null) {
                     BundleContext ctx = bundle.getBundleContext();
                     if (ctx != null) {
-                        ctx.ungetService(reference);
+ctx.ungetService(ref);
                     }
                 }
             }
---------------
-------------
@@ -195,7 +195,7 @@
         // If the bundle being stopped is the system bundle,
         // do an orderly shutdown of all blueprint contexts now
         // so that service usage can actually be useful
-        if (bundle.getBundleId() == 0 && bundle.getState() == Bundle.STOPPING) {
+if (context.getBundle(0).equals(bundle) && bundle.getState() == Bundle.STOPPING) {
             String val = context.getProperty("org.apache.aries.blueprint.preemptiveShutdown");
             if (val == null || Boolean.parseBoolean(val)) {
                 stop(context);
---------------
-------------
@@ -80,7 +80,7 @@
       writer.addDocument(doc);
     }
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     searcher = newSearcher(reader);
   }
 
---------------
-------------
@@ -263,7 +263,7 @@
         int minCheckMs = Integer.MAX_VALUE;
         for (ColumnFamilyStore cfs : columnFamilyStores.values())
         {
-            minCheckMs = Math.min(minCheckMs, cfs.metadata.memtableFlushAfterMins * 60 * 1000);
+minCheckMs = Math.min(minCheckMs, cfs.getMemtableFlushAfterMins() * 60 * 1000);
         }
 
         Runnable runnable = new Runnable()
---------------
-------------
@@ -604,7 +604,7 @@
     field.setTokenStream(ts);
     writer.addDocument(doc);
     DirectoryReader reader = writer.getReader();
-    AtomicReader sr = reader.getSequentialSubReaders().get(0);
+AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);
     DocsAndPositionsEnum de = sr.termPositionsEnum(null, "field", new BytesRef("withPayload"));
     de.nextDoc();
     de.nextPosition();
---------------
-------------
@@ -96,7 +96,7 @@
           this.size = size;
           // Verify the file is long enough to hold all of our
           // docs
-          assert numTotalDocs >= size + docStoreOffset;
+assert numTotalDocs >= size + docStoreOffset: "numTotalDocs=" + numTotalDocs + " size=" + size + " docStoreOffset=" + docStoreOffset;
         }
       } else
         format = 0;
---------------
-------------
@@ -179,7 +179,7 @@
 				break;
 			case java.sql.Types.TIMESTAMP:
 				drdaType = DRDAConstants.DRDA_TYPE_NTIMESTAMP;
-				outlen[0] = 26;
+outlen[0] = appRequester.getTimestampLength();
 				break;
 			case java.sql.Types.CHAR:
 //				drdaType = DRDAConstants.DRDA_TYPE_NCHAR;
---------------
-------------
@@ -548,7 +548,7 @@
 		 */
 		if (! resultColumnList.columnTypesAndLengthsMatch())
  		{
-			resultSet = resultSet.genNormalizeResultSetNode(resultSet, true);
+resultSet = resultSet.genNormalizeResultSetNode(true);
 			resultColumnList.copyTypesAndLengthsToSource(resultSet.getResultColumns());
 								
  			if (hasCheckConstraints(dataDictionary, targetTableDescriptor))
---------------
-------------
@@ -1006,7 +1006,7 @@
      */
     private static class FreqQ extends PriorityQueue<Object[]> {
         FreqQ (int s) {
-            initialize(s);
+super(s);
         }
 
         @Override
---------------
-------------
@@ -358,7 +358,7 @@
       // catch that case here and put it back into our queue
       for (FastIDSet cluster : clusters) {
         double similarity = clusterSimilarity.getSimilarity(merged, cluster);
-        if (similarity > queue.get(queue.size() - 1).getSimilarity()) {
+if (queue.size() > 0 && similarity > queue.get(queue.size() - 1).getSimilarity()) {
           ListIterator<ClusterClusterPair> queueIterator = queue.listIterator();
           while (queueIterator.hasNext()) {
             if (similarity > queueIterator.next().getSimilarity()) {
---------------
-------------
@@ -43,7 +43,7 @@
     ir = iw.getReader();
     verifyCount(ir);
     ir.close();
-    iw.close();
+iw.shutdown();
     dir.close();
   }
   
---------------
-------------
@@ -83,7 +83,7 @@
     hits = is.search(new MatchAllDocsQuery(), null, 1000).scoreDocs;
     assertEquals(2, hits.length);
 
-    iw.close();
+iw.shutdown();
     ir.close();
     dir.close();
   }
---------------
-------------
@@ -122,7 +122,7 @@
     doc.add(new TextField("name", "name2", Field.Store.YES));
     iw.addDocument(doc);
     iw.commit();
-    iw.close();
+iw.shutdown();
 
     //commit will cause searcher to open with the new index dir
     assertU(commit());
---------------
-------------
@@ -17,7 +17,7 @@
 package org.apache.mahout.clustering.minhash;
 
 import org.apache.mahout.common.RandomUtils;
-import org.apache.mahout.vectorizer.encoders.MurmurHash;
+import org.apache.mahout.math.MurmurHash;
 
 import java.util.Random;
 
---------------
-------------
@@ -43,7 +43,7 @@
     private static void printUsage()
     {
         System.err.println("");
-        System.err.println("Usage: cascli --host hostname [--port <portname>]");
+System.err.println("Usage: cassandra-cli --host hostname [--port <portname>]");
         System.err.println("");
     }
 
---------------
-------------
@@ -25,7 +25,7 @@
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.NRTManager; // javadocs
-import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.IndexSearcher; // javadocs
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.Directory;
 
---------------
-------------
@@ -58,7 +58,7 @@
     private final Comparator<BytesRef> comp;
 
     public Writer(Directory dir, String id, Comparator<BytesRef> comp,
-        Counter bytesUsed, IOContext context, float acceptableOverheadRatio) throws IOException {
+Counter bytesUsed, IOContext context, float acceptableOverheadRatio) {
       super(dir, id, CODEC_NAME_IDX, CODEC_NAME_DAT, VERSION_CURRENT, bytesUsed, context, acceptableOverheadRatio, Type.BYTES_FIXED_SORTED);
       this.comp = comp;
     }
---------------
-------------
@@ -21,7 +21,7 @@
 
 import java.util.Iterator;
 
-abstract class AbstractTestVector extends MahoutTestCase {
+public abstract class AbstractTestVector extends MahoutTestCase {
 
   private static final double[] values = {1.1, 2.2, 3.3};
   private static final double[] gold = {0.0, 1.1, 0.0, 2.2, 0.0, 3.3, 0.0};
---------------
-------------
@@ -87,7 +87,7 @@
       consumers.finish(state.numDocs);
     };
     // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS
-    IOUtils.closeSafely(true, perDocConsumers.values());
+IOUtils.closeSafely(false, perDocConsumers.values());
   }
 
   @Override
---------------
-------------
@@ -80,7 +80,7 @@
     IndexSearcher s = new IndexSearcher(dir, true);
     Query q = new FieldScoreQuery(field,tp);
     log("test: "+q);
-    QueryUtils.check(q,s);
+QueryUtils.check(random, q,s);
     ScoreDoc[] h = s.search(q, null, 1000).scoreDocs;
     assertEquals("All docs should be matched!",N_DOCS,h.length);
     String prevID = "ID"+(N_DOCS+1); // greater than all ids of docs in this test
---------------
-------------
@@ -213,7 +213,7 @@
      * @param test Test to decorate
      * @return Decorated test
      */
-    private static Test attributesDatabase(
+public static Test attributesDatabase(
             final Properties attributes, Test test)
     {
         test = new ChangeConfigurationSetup(test) {
---------------
-------------
@@ -116,7 +116,7 @@
             ExecutorService executor = agentContext.getRegistrationExecutor();
             executor.submit(new Runnable() {
                 public void run() {
-                    agentContext.unregisterMBean(getName());
+agentContext.unregisterMBean(AbstractCompendiumHandler.this);
                 }
             });
             trackedId = null;
---------------
-------------
@@ -117,7 +117,7 @@
         Collection<IColumn> migrations = Migration.getLocalMigrations(from, to);
         for (IColumn col : migrations)
         {
-            final Migration migration = Migration.deserialize(new ByteArrayInputStream(col.value()));
+final Migration migration = Migration.deserialize(col.value());
             Future update = StageManager.getStage(StageManager.MIGRATION_STAGE).submit(new Runnable() 
             {
                 public void run()
---------------
-------------
@@ -186,7 +186,7 @@
 		{
 			return deflt;
 		}
-		return Boolean.getBoolean(result);
+return Boolean.valueOf(result).booleanValue();
 	}	
 
 	/* Returns text of node and all child nodes - without markup */
---------------
-------------
@@ -45,7 +45,7 @@
       Element e = elements.next();
       result += e.get() * getScoreForLabelFeature(label, e.index());
     }
-    return result / model.thetaNormalizer(label);
+return -result;
   }
   
   @Override
---------------
-------------
@@ -79,7 +79,7 @@
         if (!field.isTokenized()) {		  // un-tokenized field
           String stringValue = field.stringValue();
           final int valueLength = stringValue.length();
-          Token token = perThread.localToken.reinit(stringValue, fieldState.offset, fieldState.offset + valueLength);
+Token token = perThread.localToken.reinit(stringValue, 0, valueLength);
           boolean success = false;
           try {
             consumer.add(token);
---------------
-------------
@@ -656,7 +656,7 @@
         try {
           url = new URL(masterUrl);
           stream = url.openStream();
-          response = IOUtils.toString(stream);
+response = IOUtils.toString(stream, "UTF-8");
           if(response.contains("<str name=\"status\">success</str>")) {
             success = true;
           }
---------------
-------------
@@ -101,7 +101,7 @@
     assertTrue("Invalid version: "+version,
                version.equals(Constants.LUCENE_MAIN_VERSION+"-SNAPSHOT") ||
                version.equals(Constants.LUCENE_MAIN_VERSION));
-    assertTrue(version + " should start with: "+Constants.LUCENE_VERSION,
+assertTrue(Constants.LUCENE_VERSION + " should start with: "+version,
                Constants.LUCENE_VERSION.startsWith(version));
   }
 }
---------------
-------------
@@ -86,7 +86,7 @@
         this.max_keys = max_keys;
     }
 
-    public Message getMessage(int version) throws IOException
+public Message getMessage(Integer version) throws IOException
     {
         DataOutputBuffer dob = new DataOutputBuffer();
         serializer.serialize(this, dob, version);
---------------
-------------
@@ -164,7 +164,7 @@
         //remember in setup a locator is already created
         //hence expected value is 2
         assertEquals("The locator values returned by " +
-            "SYSIBM.BLOBCREATELOCATOR() are incorrect", 4, locator);
+"SYSIBM.BLOBCREATELOCATOR() are incorrect", 2, locator);
         cs.close();
     }
 
---------------
-------------
@@ -3222,7 +3222,7 @@
         message("merge segmentCodecs=" + merger.getSegmentCodecs());
         message("merge store matchedCount=" + merger.getMatchedSubReaderCount() + " vs " + merge.readers.size());
       }
-      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();
+anyNonBulkMerges |= merger.getAnyNonBulkMerges();
       
       assert mergedDocCount == totDocCount: "mergedDocCount=" + mergedDocCount + " vs " + totDocCount;
 
---------------
-------------
@@ -83,7 +83,7 @@
     }
 
     @Override
-    public int size() throws IOException {
+public int size() {
       return in.size();
     }
 
---------------
-------------
@@ -61,7 +61,7 @@
   InterfaceCombiningClassAdapter(String className,
       ClassLoader loader, Class<?> superclass, Collection<Class<?>> interfaces) {
     super(Opcodes.ASM4);
-    writer = new OSGiFriendlyClassWriter(ClassWriter.COMPUTE_FRAMES, loader, className, (superclass!=null)? superclass.getName(): null);
+writer = new OSGiFriendlyClassWriter(ClassWriter.COMPUTE_FRAMES, loader);
     ClassVisitor cv = new OSGiFriendlyClassVisitor(writer, ClassWriter.COMPUTE_FRAMES);
     adapter = new InterfaceUsingWovenProxyAdapter(cv, className, loader);
 
---------------
-------------
@@ -50,5 +50,5 @@
                     Iterable<Cooccurrence> cooccurrences,
                     double weightOfVectorA,
                     double weightOfVectorB,
-                    int numberOfColumns);
+long numberOfColumns);
 }
---------------
-------------
@@ -168,7 +168,7 @@
         // n-case
         else {
             // NOTE: All divisions must take place in BigDecimals, and all modulo operators must take place in BigIntegers.
-            final BigInteger ri = MAXIMUM(127);                             //  (used for addition later)
+final BigInteger ri = MAXIMUM;                                                  //  (used for addition later)
             final BigDecimal r  = new BigDecimal(ri);                                       // The entire range, 2**127
             Token start = (Token)i.next(); BigInteger ti = ((BigIntegerToken)start).token;  // The first token and its value
             Token t; BigInteger tim1 = ti;                                                  // The last token and its value (after loop)
---------------
-------------
@@ -46,7 +46,7 @@
      * the first time.<br>
      * @return length of the current payload in number of bytes
      */
-    int getPayloadLength();
+int getPayloadLength() throws IOException;
     
     /** 
      * Returns the payload data at the current term position.
---------------
-------------
@@ -226,7 +226,7 @@
         Clause clause = clauses.get(i);
         String s = clause.raw;
         // and and or won't be operators at the start or end
-        if (i>0 && i+1<clauses.size()) {
+if (lowercaseOperators && i>0 && i+1<clauses.size()) {
           if ("AND".equalsIgnoreCase(s)) {
             s="AND";
           } else if ("OR".equalsIgnoreCase(s)) {
---------------
-------------
@@ -75,7 +75,7 @@
 The implementation relies on the existance of page "auxiliary" pointers 
 which keep Object versions of the control row.
 <P>
-@see ControlRow#Get
+@see ControlRow#get
 @see ControlRow#release
 
 **/
---------------
-------------
@@ -45,7 +45,7 @@
     Shape point = new PointImpl(-118.243680, 34.052230);
 
     Document losAngeles = new Document();
-    losAngeles.add(new Field("name", "Los Angeles", StringField.TYPE_STORED));
+losAngeles.add(new StringField("name", "Los Angeles", Field.Store.YES));
     losAngeles.add(prefixGridStrategy.createField(fieldInfo, point, true, true));
 
     addDocumentsAndCommit(Arrays.asList(losAngeles));
---------------
-------------
@@ -197,7 +197,7 @@
       }
 
       // ... then all shards:
-      final Weight w = query.weight(searcher);
+final Weight w = searcher.createNormalizedWeight(query);
 
       final TopDocs[] shardHits = new TopDocs[subSearchers.length];
       for(int shardIDX=0;shardIDX<subSearchers.length;shardIDX++) {
---------------
-------------
@@ -93,7 +93,7 @@
     } else {
       String s = "gen=" + gen;
       if (numTermDeletes.get() != 0) {
-        s += " " + numTermDeletes.get() + " deleted terms (unique count=" + terms.size() + ")";
+s += " " + numTermDeletes.get() + " deleted terms (unique count=" + terms.size() + ") terms=" + terms.keySet();
       }
       if (queries.size() != 0) {
         s += " " + queries.size() + " deleted queries";
---------------
-------------
@@ -70,7 +70,7 @@
     iw.forceMerge(1);
     
     reader = iw.getReader();
-    iw.close();
+iw.shutdown();
   }
   
   @Override
---------------
-------------
@@ -21,7 +21,7 @@
 
 package org.apache.derby.impl.io.vfmem;
 
-import org.apache.derby.shared.common.sanity.SanityManager;
+import org.apache.derby.iapi.services.sanity.SanityManager;
 
 /**
  * Stores data in blocks, and supports reading/writing data from/into these
---------------
-------------
@@ -208,7 +208,7 @@
     }
 
     public List<ColumnOrSuperColumn> get_slice(String keyspace, String key, ColumnParent column_parent, SlicePredicate predicate, int consistency_level)
-    throws InvalidRequestException, NotFoundException, UnavailableException
+throws InvalidRequestException, UnavailableException
     {
         if (logger.isDebugEnabled())
             logger.debug("get_slice");
---------------
-------------
@@ -379,7 +379,7 @@
      * Once this happens the data associated with the individual column families
      * is also written to the column family store's memtable.
     */
-    public void apply(RowMutation mutation, Object serializedMutation, boolean writeCommitLog) throws IOException
+public void apply(RowMutation mutation, byte[] serializedMutation, boolean writeCommitLog) throws IOException
     {
         List<Memtable> memtablesToFlush = Collections.emptyList();
         if (logger.isDebugEnabled())
---------------
-------------
@@ -37,7 +37,7 @@
 
     public void doVerb(Message message)
     {
-    	byte[] body = (byte[])message.getMessageBody()[0];
+byte[] body = message.getMessageBody();
         Token token = StorageService.getPartitioner().getTokenFactory().fromByteArray(body);
         try
         {
---------------
-------------
@@ -37,7 +37,7 @@
     return new GermanNormalizationFilter(input);
   }
   
-  @Override
+//@Override
   public Object getMultiTermComponent() {
     return this;
   }
---------------
-------------
@@ -860,7 +860,7 @@
       //if (DEBUG) System.out.println("BTTW.finishTerm term=" + fieldInfo.name + ":" + toString(text) + " seg=" + segment + " df=" + stats.docFreq);
 
       blockBuilder.add(text, noOutputs.getNoOutput());
-      pending.add(new PendingTerm(new BytesRef(text), stats));
+pending.add(new PendingTerm(BytesRef.deepCopyOf(text), stats));
       postingsWriter.finishTerm(stats);
       numTerms++;
     }
---------------
-------------
@@ -19,7 +19,7 @@
 import java.io.IOException;
 import java.util.Comparator;
 
-import org.apache.lucene.index.values.codec.DocValuesConsumer;
+import org.apache.lucene.index.codecs.docvalues.DocValuesConsumer;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
---------------
-------------
@@ -295,7 +295,7 @@
     int hi = numSubReaders - 1;                  // for first element less
 
     while (hi >= lo) {
-      int mid = (lo + hi) >> 1;
+int mid = (lo + hi) >>> 1;
       int midValue = starts[mid];
       if (n < midValue)
         hi = mid - 1;
---------------
-------------
@@ -1586,7 +1586,7 @@
 		** This ProjectRestrictNode is not a No-Op if it does any
 		** restriction.
 		*/
-		if ( (restriction != null) ||
+if ( (restriction != null) || (constantRestriction != null) ||
 			 (restrictionList != null && restrictionList.size() > 0) )
 		{
 			return false;
---------------
-------------
@@ -96,7 +96,7 @@
                 }
                 else
                 {
-                    int id = cfm.cfId;
+Integer id = cfm.cfId;
                     if (!header.isDirty(id))
                     {
                         header.turnOn(id, logWriter.getFilePointer());
---------------
-------------
@@ -212,7 +212,7 @@
 
     protected Pattern getPattern() {
         if (pattern == null) {
-            pattern = Pattern.compile("\\Q" + placeholderPrefix + "\\E(.+)\\Q" + placeholderSuffix + "\\E");
+pattern = Pattern.compile("\\Q" + placeholderPrefix + "\\E(.+?)\\Q" + placeholderSuffix + "\\E");
         }
         return pattern;
     }
---------------
-------------
@@ -206,7 +206,7 @@
   }
   
   protected long sizeBytes(SegmentInfo info) throws IOException {
-    long byteSize = info.sizeInBytes();
+long byteSize = info.sizeInBytes(true);
     if (calibrateSizeByDeletes) {
       int delCount = writer.get().numDeletedDocs(info);
       double delRatio = (info.docCount <= 0 ? 0.0f : ((float)delCount / (float)info.docCount));
---------------
-------------
@@ -95,7 +95,7 @@
   }
   
   private String getSourceUrl() {
-    return "http" + (isSSLMode() ? "s" : "") +"://127.0.0.1:" + jetty.getLocalPort() + "/solr";
+return buildUrl(jetty.getLocalPort(), "/solr");
   }
   
   //TODO: fix this test to close its directories
---------------
-------------
@@ -279,7 +279,7 @@
     public static Migration deserialize(byte[] bytes) throws IOException
     {
         // deserialize
-        org.apache.cassandra.db.migration.avro.Migration mi = SerDeUtils.deserializeWithSchema(bytes);
+org.apache.cassandra.db.migration.avro.Migration mi = SerDeUtils.deserializeWithSchema(bytes, new org.apache.cassandra.db.migration.avro.Migration());
 
         // create an instance of the migration subclass
         Migration migration;
---------------
-------------
@@ -29,7 +29,7 @@
 
 public class InputDriver {
 
-  public static void main(String[] args) throws Exception {
+public static void main(String[] args) throws IOException {
     runJob(args[0], args[1]);
   }
 
---------------
-------------
@@ -871,7 +871,7 @@
    */
   public static XmlDoc doc(String... fieldsAndValues) {
     XmlDoc d = new XmlDoc();
-    d.xml = TestHarness.makeSimpleDoc(fieldsAndValues).toString();
+d.xml = TestHarness.makeSimpleDoc(fieldsAndValues);
     return d;
   }
 
---------------
-------------
@@ -558,7 +558,7 @@
                     modifiedRowCount > 1 ? (order >= lastOrder) :
                         (order > lastOrder);
                 assertTrue("matching triggers need to be fired in order creation:"
-                        +info, orderOk);
++info+". Triggers got fired in this order:"+TRIGGER_INFO.get().toString(), orderOk);
                 lastOrder = order;
                 continue;
             }
---------------
-------------
@@ -32,7 +32,7 @@
   
   static {
     Bundle b = FrameworkUtil.getBundle(AriesFrameworkUtil.class);
-    String bundleClassName = b.getClass().getName();
+String bundleClassName = b == null ? "": b.getClass().getName();
     if (isEquinox(bundleClassName)) {
       worker = new EquinoxWorker();
     } else if (bundleClassName.startsWith("org.apache.felix")) {
---------------
-------------
@@ -165,7 +165,7 @@
     assertTrue(r1.isCurrent());
 
     writer.commit();
-    assertFalse(r1.isCurrent());
+assertTrue(r1.isCurrent());
 
     assertEquals(200, r1.maxDoc());
 
---------------
-------------
@@ -36,7 +36,7 @@
  * superclass.
  */
 
-public final class BufferedRandomAccessFile extends RandomAccessFile implements FileDataInput
+public class BufferedRandomAccessFile extends RandomAccessFile implements FileDataInput
 {
     static final int LogBuffSz_ = 16; // 64K buffer
     public static final int BuffSz_ = (1 << LogBuffSz_);
---------------
-------------
@@ -58,7 +58,7 @@
             for (int i = 0; i < session.getSuperColumns(); i++)
             {
                 String superColumnName = "S" + Integer.toString(i);
-                superColumns.add(new SuperColumn(ByteBuffer.wrap(superColumnName.getBytes()), columns));
+superColumns.add(new SuperColumn(ByteBufferUtil.bytes(superColumnName), columns));
             }
         }
 
---------------
-------------
@@ -552,7 +552,7 @@
         latch.countDown();
         documentLatch.await(5000,TimeUnit.MILLISECONDS);
         // wait active for the commit
-        while(this.indexer.writer != null){}
+while(this.indexer.getWriter() != null){}
         
         IndexSearcher s = new IndexSearcher(this.dir);
         Hits h = s.search(new TermQuery(delTerm));
---------------
-------------
@@ -153,7 +153,7 @@
     private void validateRemoveWithNewData() throws IOException
     {
         ColumnFamilyStore store = Table.open("Keyspace1").getColumnFamilyStore("Super2");
-        ColumnFamily resolved = store.getColumnFamily(new NamesQueryFilter("key1", new QueryPath("Super2", "SC1".getBytes()), getBytes(2)));
+ColumnFamily resolved = store.getColumnFamily(new NamesQueryFilter("key1", new QueryPath("Super2", "SC1".getBytes()), getBytes(2)), Integer.MAX_VALUE);
         Collection<IColumn> subColumns = resolved.getSortedColumns().iterator().next().getSubColumns();
         assert subColumns.size() == 1;
         assert subColumns.iterator().next().timestamp() == 2;
---------------
-------------
@@ -19,7 +19,7 @@
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
-import com.spatial4j.core.util.GeohashUtils;
+import com.spatial4j.core.io.GeohashUtils;
 
 import java.util.Map;
 import java.io.IOException;
---------------
-------------
@@ -1127,7 +1127,7 @@
 
     public void onAlive(InetAddress endpoint, EndpointState state)
     {
-        if (!isClientMode && state.hasToken())
+if (!isClientMode && StorageService.instance.getTokenMetadata().isMember(endpoint))
             deliverHints(endpoint);
     }
 
---------------
-------------
@@ -55,7 +55,7 @@
         sb.append(random.nextBoolean() ? "aaa" : "bbb");
         sb.append(' ');
       }
-      body.setValue(sb.toString());
+body.setStringValue(sb.toString());
       w.addDocument(doc);
       sb.delete(0, sb.length());
     }
---------------
-------------
@@ -543,7 +543,7 @@
         for (String id : elevations.ids) {
           term.copyChars(id);
           if (seen.contains(id) == false  && termsEnum.seekExact(term, false)) {
-            docsEnum = termsEnum.docs(liveDocs, docsEnum, 0);
+docsEnum = termsEnum.docs(liveDocs, docsEnum, false);
             if (docsEnum != null) {
               int docId = docsEnum.nextDoc();
               if (docId == DocIdSetIterator.NO_MORE_DOCS ) continue;  // must have been deleted
---------------
-------------
@@ -127,6 +127,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new KeywordAnalyzer(), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new KeywordAnalyzer(), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -160,7 +160,7 @@
                   this, old, itemsArr[i].getKey(), itemsArr[i].getValue());
           if (!continueRegen) break;
         }
-        catch (Throwable e) {
+catch (Exception e) {
           SolrException.log(log, "Error during auto-warming of key:" + itemsArr[i].getKey(), e);
         }
       }
---------------
-------------
@@ -29,7 +29,7 @@
     private Term currentTerm = null;
     private TermEnum actualEnum = null;
     
-    public FilteredTermEnum(IndexReader reader, Term term) throws IOException {}
+public FilteredTermEnum() throws IOException {}
 
     /** Equality compare on the term */
     protected abstract boolean termCompare(Term term);
---------------
-------------
@@ -69,7 +69,7 @@
           SimpleOrderedMap<Object> groupResult = new SimpleOrderedMap<Object>();
           if (group.groupValue != null) {
             groupResult.add(
-                "groupValue", groupFieldType.toObject(groupField.createField(group.groupValue.utf8ToString(), 0.0f))
+"groupValue", groupFieldType.toObject(groupField.createField(group.groupValue.utf8ToString(), 1.0f))
             );
           } else {
             groupResult.add("groupValue", null);
---------------
-------------
@@ -106,7 +106,7 @@
   }
   
   @Override
-  public void build(SolrCore core, SolrIndexSearcher searcher) {
+public void build(SolrCore core, SolrIndexSearcher searcher) throws IOException {
     for (SolrSpellChecker c : checkers) {
       c.build(core, searcher);
     }
---------------
-------------
@@ -191,7 +191,7 @@
 
   public void testConcurrency() throws Exception {
     // tests that addTaxonomy and addCategory work in parallel
-    final int numCategories = atLeast(5000);
+final int numCategories = atLeast(10000);
     
     // build an input taxonomy index
     Directory src = newDirectory();
---------------
-------------
@@ -73,7 +73,7 @@
 
       doc.add(newField(r, "content7", "aaa bbb ccc ddd", Field.Store.NO, Field.Index.NOT_ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
 
-      final Field idField = newField("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
+final Field idField = newField(r, "id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
       doc.add(idField);
 
       final long stopTime = System.currentTimeMillis() + 500;
---------------
-------------
@@ -44,7 +44,7 @@
  * Customized version of {@link Lucene40Codec} that uses
  * {@link FixedGapTermsIndexWriter}.
  */
-public class Lucene40WithOrds extends PostingsFormat {
+public final class Lucene40WithOrds extends PostingsFormat {
     
   public Lucene40WithOrds() {
     super("Lucene40WithOrds");
---------------
-------------
@@ -32,7 +32,7 @@
    *         set will not include those provided by value within the application.
    * @throws ResolverException if the application cannot be resolved.  
    */
-  Set<BundleInfo> resolve (AriesApplication app) throws ResolverException ;
+Set<BundleInfo> resolve (AriesApplication app, ResolveConstraint... constraints) throws ResolverException ;
 
   /** 
    * Return the info for the requested bundle. If no matching bundle exists in the
---------------
-------------
@@ -136,7 +136,7 @@
 
 
 	/**
-		@exception StandardException Standard Cloudscape error policy
+@exception StandardException Standard Derby error policy
 	*/
 	public boolean needsRedo(Transaction xact)
 		 throws StandardException
---------------
-------------
@@ -113,7 +113,7 @@
 
   // inherit javadocs
   public boolean validateData(QualityQuery[] qq, PrintWriter logger) {
-    HashMap<String,QRelJudgement> missingQueries = (HashMap<String, QRelJudgement>) judgements.clone();
+HashMap<String,QRelJudgement> missingQueries = new HashMap<String, QRelJudgement>(judgements);
     ArrayList<String> missingJudgements = new ArrayList<String>();
     for (int i=0; i<qq.length; i++) {
       String id = qq[i].getQueryID();
---------------
-------------
@@ -133,7 +133,7 @@
    * Wraps the Reader with {@link PersianCharFilter}
    */
   @Override
-  protected Reader initReader(Reader reader) {
+protected Reader initReader(String fieldName, Reader reader) {
     return new PersianCharFilter(CharReader.get(reader)); 
   }
 }
---------------
-------------
@@ -36,7 +36,7 @@
   private int docBase;
   private int matches;
 
-  public FilterCollector(DocSet filter, Collector delegate) throws IOException {
+public FilterCollector(DocSet filter, Collector delegate) {
     this.filter = filter;
     this.delegate = delegate;
   }
---------------
-------------
@@ -144,7 +144,7 @@
       if (RUN_LENGTH != -1) {
         runLength = RUN_LENGTH;
       } else {
-        int[] runTimes = new int[] {5000,6000,10000,15000,15000,30000,30000,45000,90000,120000};
+int[] runTimes = new int[] {5000,6000,10000,15000,25000,30000,30000,45000,90000,120000};
         runLength = runTimes[random().nextInt(runTimes.length - 1)];
       }
       
---------------
-------------
@@ -273,7 +273,7 @@
      * @param onDiskType The object read that represents the type.
      * @return A type descriptor.
      */
-    private static TypeDescriptor getStoredType(Object onDiskType)
+public static TypeDescriptor getStoredType(Object onDiskType)
     {
         if (onDiskType instanceof OldRoutineType)
             return ((OldRoutineType) onDiskType).getCatalogType();
---------------
-------------
@@ -25,7 +25,7 @@
 
 public abstract class CustomBufferedIndexInput extends IndexInput {
   
-  public static final int BUFFER_SIZE = 1024;
+public static final int BUFFER_SIZE = 32768;
   
   private int bufferSize = BUFFER_SIZE;
   
---------------
-------------
@@ -39,7 +39,7 @@
   public final static String DOC_VALUES_SEGMENT_SUFFIX = "dv";
 
 
-  public Lucene40DocValuesConsumer(PerDocWriteState state, String segmentSuffix) throws IOException {
+public Lucene40DocValuesConsumer(PerDocWriteState state, String segmentSuffix) {
     super(state);
     this.segmentSuffix = segmentSuffix;
     mainDirectory = state.directory;
---------------
-------------
@@ -88,7 +88,7 @@
   private MultiReader(IndexReader[] subReaders, boolean[] decrefOnClose,
                       Collection<ReaderFinishedListener> readerFinishedListeners)
                       throws IOException {
-    this.subReaders =  subReaders.clone();
+this.subReaders =  subReaders;
     this.decrefOnClose = decrefOnClose;
     this.readerFinishedListeners = readerFinishedListeners;
     starts = new int[subReaders.length + 1];    // build starts array
---------------
-------------
@@ -217,6 +217,6 @@
     }
     private static CFMetaData jdbcCFMD(String ksName, String cfName, AbstractType comp)
     {
-        return new CFMetaData(ksName, cfName, ColumnFamilyType.Standard, comp, comp);
+return new CFMetaData(ksName, cfName, ColumnFamilyType.Standard, comp, null).defaultValidator(comp);
     }
 }
---------------
-------------
@@ -2193,7 +2193,7 @@
                 "CAST(NULL AS VARCHAR(128)) AS TYPE_SCHEM," +
                 "VARCHAR('', 128) AS TYPE_NAME," +
                 "VARCHAR('',128) AS ATTR_NAME," +
-                "SMALLINT(0) AS DATA_TYPE," +
+"0 AS DATA_TYPE," +
                 "VARCHAR('',129) AS ATTR_TYPE_NAME," +
                 "0 AS ATTR_SIZE," +
                 "0 AS DECIMAL_DIGITS," +
---------------
-------------
@@ -156,7 +156,7 @@
         {
             throw new IOError(e);
         }
-        logger.info("Deleted " + desc);
+logger.debug("Deleted {}", desc);
         return true;
     }
 
---------------
-------------
@@ -949,7 +949,7 @@
 
   static String getTimeElapsedSince(long l) {
     l = System.currentTimeMillis() - l;
-    return (l / (60000 * 60)) % 60 + ":" + (l / 60000) % 60 + ":" + (l / 1000)
+return (l / (60000 * 60)) + ":" + (l / 60000) % 60 + ":" + (l / 1000)
             % 60 + "." + l % 1000;
   }
 
---------------
-------------
@@ -245,7 +245,7 @@
         MessagingService.instance.registerVerbHandlers(Verb.SCHEMA_CHECK, new SchemaCheckVerbHandler());
 
         replicationStrategies = new HashMap<String, AbstractReplicationStrategy>();
-        for (String table : DatabaseDescriptor.getTables())
+for (String table : DatabaseDescriptor.getNonSystemTables())
             initReplicationStrategy(table);
 
         // spin up the streaming serivice so it is available for jmx tools.
---------------
-------------
@@ -55,7 +55,7 @@
         System.err.println("obtain implementations via META-INF/services. The byte code in the bundles is");
         System.err.println("modified so that the ThreadContextClassLoader is set appropriately for the ");
         System.err.println("duration of the java.util.ServiceLoader.load() call.");
-        System.err.println("To opt-in to this process, bundles need to have the following Manifest.MF");
+System.err.println("To opt-in to this process, bundles need to have the following MANIFEST.MF");
         System.err.println("header set:");
         System.err.println("    " + SpiFlyConstants.SPI_CONSUMER_HEADER + ": *");
         System.err.println("Modified bundles are written out under the following name:");
---------------
-------------
@@ -57,7 +57,7 @@
     String xml = 
       "<random>" +
       " <document>" +
-      "  <node name=\"id\" enhance=\"2.2\" value=\"12345\"/>" +
+"  <node name=\"id\" value=\"12345\"/>" +
       "  <node name=\"name\" value=\"kitten\"/>" +
       "  <node name=\"text\" enhance=\"3\" value=\"some other day\"/>" +
       "  <node name=\"title\" enhance=\"4\" value=\"A story\"/>" +
---------------
-------------
@@ -3249,7 +3249,7 @@
             if (numBytes >= 0) {
                 byte[] readBytes = new byte[numBytes];
                 System.arraycopy(value, 0, readBytes, 0, numBytes);
-                valueString = new String(readBytes);
+valueString = new String(readBytes, "US-ASCII");
                 assertEquals("FAIL - wrong substring value",
                         valueString, subStr);
             } else {
---------------
-------------
@@ -71,7 +71,7 @@
   public BlockTermsWriter(TermsIndexWriterBase termsIndexWriter,
       SegmentWriteState state, PostingsWriterBase postingsWriter)
       throws IOException {
-    final String termsFileName = IndexFileNames.segmentFileName(state.segmentName, state.formatId, TERMS_EXTENSION);
+final String termsFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_EXTENSION);
     this.termsIndexWriter = termsIndexWriter;
     out = state.directory.createOutput(termsFileName, state.context);
     boolean success = false;
---------------
-------------
@@ -254,7 +254,7 @@
 </p>
 <p>
 Additional user-supplied statistics can be added to the document as DocValues fields and
-accessed via {@link org.apache.lucene.index.AtomicReader#docValues}.
+accessed via {@link org.apache.lucene.index.AtomicReader#getNumericDocValues}.
 </p>
 <p>
 </body>
---------------
-------------
@@ -862,7 +862,7 @@
       }
       w.commit();
       IndexReader reader = w.getReader();
-      SortedDocValues docValues = MultiSimpleDocValues.simpleSortedValues(reader, "field");
+SortedDocValues docValues = MultiDocValues.getSortedValues(reader, "field");
       int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());
       BytesRef expected = new BytesRef();
       BytesRef actual = new BytesRef();
---------------
-------------
@@ -54,7 +54,7 @@
    */
   public static final String LOCK_DIR =
     System.getProperty("org.apache.lucene.lockdir",
-      System.getProperty("java.io.tmpdir"));
+System.getProperty("java.io.tmpdir", "."));
 
   private static MessageDigest DIGESTER;
 
---------------
-------------
@@ -106,7 +106,7 @@
     modelPaths.put("thetaNormalizer", new Path(modelBasePath + "/trainer-thetaNormalizer/part-*"));
     modelPaths.put("weight", new Path(modelBasePath + "/trainer-tfIdf/trainer-tfIdf/part-*"));
 
-    FileSystem fs = FileSystem.get(conf);
+FileSystem fs = FileSystem.get((new Path(modelBasePath)).toUri(), conf);
 
     log.info("Loading model from: {}", modelPaths);
 
---------------
-------------
@@ -600,7 +600,7 @@
 				resultSet.enhanceRCLForInsert(
 						numTableColumns, colMap, dataDictionary,
 						targetTableDescriptor, targetVTI);
-			resultColumnList.checkAutoincrement(resultSet.getResultColumns());
+resultColumnList.forbidOverrides(resultSet.getResultColumns());
 		}
 	}
 
---------------
-------------
@@ -1700,7 +1700,7 @@
 			ResultColumn resultColumn = (ResultColumn) elementAt(index);
 
 			/* Skip over generated columns */
-			if (resultColumn.isGenerated() || resultColumn.isGeneratedForUnmatchedColumnInInsert())
+if (resultColumn.isGenerated())
 			{
 				continue;
 			}
---------------
-------------
@@ -66,7 +66,7 @@
   private Initializer init2;
   
   @BeforeClass
-  public static void beforeClass() throws Exception {
+public static void beforeClass() {
     System.setProperty("solrcloud.skip.autorecovery", "true");
   }
   
---------------
-------------
@@ -1654,7 +1654,7 @@
             IColumn column = data.getColumn(expression.column_name);
             if (column == null)
                 return false;
-            int v = data.getComparator().compare(column.value(), expression.value);
+int v = data.metadata().getValueValidator(expression.column_name).compare(column.value(), expression.value);
             if (!satisfies(v, expression.op))
                 return false;
         }
---------------
-------------
@@ -1231,7 +1231,7 @@
         if ((database.securityMechanism == CodePoint.SECMEC_USRSSBPWD) &&
             (database.dbName.indexOf(Attribute.PASSWORD_ATTR) == -1))
         {
-            p.put(Attribute.CLIENT_SECURITY_MECHANISM,
+p.put(Attribute.DRDA_SECMEC,
                   String.valueOf(database.securityMechanism));
             p.put(Attribute.DRDA_SECTKN_IN,
                   DecryptionManager.toHexString(database.secTokenIn, 0,
---------------
-------------
@@ -89,7 +89,7 @@
              "org.apache.hadoop.io.serializer.JavaSerialization,org.apache.hadoop.io.serializer.WritableSerialization");
     // Dont ever forget this. People should keep track of how hadoop conf parameters and make or break a piece of code
     
-    FileSystem dfs = FileSystem.get(conf);
+FileSystem dfs = FileSystem.get(outPath.toUri(), conf);
     if (dfs.exists(outPath))
       dfs.delete(outPath, true);
 
---------------
-------------
@@ -2606,7 +2606,7 @@
         }
       }
     } finally {
-      IOUtils.closeSafely(true, cfsdir);
+IOUtils.closeSafely(false, cfsdir);
     }
     
     info.dir = directory;
---------------
-------------
@@ -57,7 +57,7 @@
     int numFullGroups = aLen / 3;
     int numBytesInPartialGroup = aLen - 3 * numFullGroups;
     int resultLen = 4 * ((aLen + 2) / 3);
-    StringBuffer result = new StringBuffer(resultLen);
+StringBuilder result = new StringBuilder(resultLen);
     char[] intToAlpha = intToBase64;
 
     // Translate all full groups from byte array elements to Base64
---------------
-------------
@@ -613,7 +613,7 @@
     
     // check dictionary and posting lists
     FieldsEnum fenum1 = MultiFields.getFields(index1).iterator();
-    FieldsEnum fenum2 = MultiFields.getFields(index1).iterator();
+FieldsEnum fenum2 = MultiFields.getFields(index2).iterator();
     String field1 = null;
     Bits liveDocs = MultiFields.getLiveDocs(index1);
     while((field1=fenum1.next()) != null) {
---------------
-------------
@@ -438,7 +438,7 @@
     long beforeCount = results.getResults().getNumFound();
     int cnt = TEST_NIGHTLY ? 2933 : 313;
     try {
-      suss.setConnectionTimeout(30000);
+suss.setConnectionTimeout(120000);
       for (int i = 0; i < cnt; i++) {
         index_specific(suss, id, docId++, "text_t", "some text so that it not's negligent work to parse this doc, even though it's still a pretty short doc");
       }
---------------
-------------
@@ -1949,7 +1949,7 @@
         for (KeySlice ks : slices)
         {
             css_.out.printf("-------------------\n");
-            css_.out.printf("RowKey: %s\n", new String(ks.key.array(),ks.key.position(),ks.key.remaining(), Charsets.UTF_8));
+css_.out.printf("RowKey: %s\n", ByteBufferUtil.string(ks.key, Charsets.UTF_8));
 
             Iterator<ColumnOrSuperColumn> iterator = ks.getColumnsIterator();
 
---------------
-------------
@@ -367,7 +367,7 @@
       /** Input must not be null */
       public static LastModFrom parse(final String s) {
         try {
-          return valueOf(s.toUpperCase());
+return valueOf(s.toUpperCase(Locale.ENGLISH));
         } catch (Exception e) {
           log.warn( "Unrecognized value for lastModFrom: " + s, e);
           return BOGUS;
---------------
-------------
@@ -32,7 +32,7 @@
       this.context = (Query)context.clone();        // clone before boost
       this.boost = boost;
 
-      context.setBoost(0.0f);                      // ignore context-only matches
+this.context.setBoost(0.0f);                      // ignore context-only matches
     }
 
     public Query rewrite(IndexReader reader) throws IOException {
---------------
-------------
@@ -26,7 +26,7 @@
 
   @Override
   protected double doComputeResult(int rowA, int rowB, Iterable<Cooccurrence> cooccurrences, double weightOfVectorA,
-      double weightOfVectorB, int numberOfColumns) {
+double weightOfVectorB, long numberOfColumns) {
 
     double n = 0.0;
     double sumXYdiff2 = 0.0;
---------------
-------------
@@ -80,7 +80,7 @@
         for (CFMetaData cfm : ksm.cfMetaData().values())
         {
             CFMetaData.purge(cfm);
-            table.dropCf(cfm.cfName);
+table.dropCf(cfm.cfId);
             SystemTable.markForRemoval(cfm);
         }
                         
---------------
-------------
@@ -144,7 +144,7 @@
   }
 
   public void testBoostsSimple() throws Exception {
-    Map<CharSequence,Float> boosts = new HashMap<CharSequence,Float>();
+Map<String,Float> boosts = new HashMap<String,Float>();
     boosts.put("b", Float.valueOf(5));
     boosts.put("t", Float.valueOf(10));
     String[] fields = { "b", "t" };
---------------
-------------
@@ -267,7 +267,7 @@
     /*
      * Verify the index
      */         
-    Searcher searcher = new IndexSearcher(dir);
+Searcher searcher = new IndexSearcher(dir, true);
     searcher.setSimilarity(new SimpleSimilarity());
         
     Term a = new Term("noTf", term);
---------------
-------------
@@ -785,7 +785,7 @@
     }
     // final check
     IndexReader r2 = IndexReader.openIfChanged(r);
-    if (r2 != r) {
+if (r2 != null) {
       r.close();
       r = r2;
     }
---------------
-------------
@@ -133,7 +133,7 @@
 			if (rsmd.isAutoIncrement(2))
 				throw new SQLException("column 2 is NOT ai!");
 			if (rsmd.isAutoIncrement(3))
-				throw new SQLException("column 2 is NOT ai!");
+throw new SQLException("column 3 is NOT ai!");
 			rs.close();
             s.close();
 		}
---------------
-------------
@@ -101,7 +101,7 @@
 				mavenBundle("org.apache.aries.subsystem", "org.apache.aries.subsystem.executor"),
 //				org.ops4j.pax.exam.container.def.PaxRunnerOptions.vmOption("-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005"),
 				PaxRunnerOptions.rawPaxRunnerOption("config", "classpath:ss-runner.properties"),
-				equinox().version("3.8.0.v20110621"));
+equinox().version("3.8.0-SNAPSHOT"));
 		options = updateOptions(options);
 		return options;
 	}
---------------
-------------
@@ -233,7 +233,7 @@
     for (DistribFieldFacet dff : fi.topFacets.values()) {
       ShardFacetCount[] counts = dff.getSorted();
       int ntop = Math.min(counts.length, dff.offset + dff.limit);
-      long smallestCount = counts[ntop-1].count;
+long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;
 
       for (int i=0; i<counts.length; i++) {
         ShardFacetCount sfc = counts[i];
---------------
-------------
@@ -253,7 +253,7 @@
    * @deprecated should be removed once all the deprecated setters are removed
    *             from IndexWriter.
    */
-  @Test
+@Test @Deprecated
   public void testIndexWriterSetters() throws Exception {
     // This test intentionally tests deprecated methods. The purpose is to pass
     // whatever the user set on IW to IWC, so that if the user calls
---------------
-------------
@@ -571,7 +571,7 @@
    * Constructs a function that returns <tt>Math.IEEEremainder(a,b)</tt>. <tt>a</tt> is a variable, <tt>b</tt> is
    * fixed.
    */
-  public static DoubleFunction IEEEremainder(final double b) {
+public static DoubleFunction mathIEEEremainder(final double b) {
     return new DoubleFunction() {
 
       public double apply(double a) {
---------------
-------------
@@ -471,7 +471,7 @@
       // Now search backwards for the rightmost segment that
       // falls into this level:
       float levelBottom;
-      if (maxLevel < levelFloor)
+if (maxLevel <= levelFloor)
         // All remaining segments fall into the min level
         levelBottom = -1.0F;
       else {
---------------
-------------
@@ -484,7 +484,7 @@
     try {
       final String path = URLDecoder.decode(nodeName.substring(1+_offset),
                                             "UTF-8");
-      return "http://" + hostAndPort + "/" + path;
+return "http://" + hostAndPort + (path.isEmpty() ? "" : ("/" + path));
     } catch (UnsupportedEncodingException e) {
       throw new IllegalStateException("JVM Does not seem to support UTF-8", e);
     }
---------------
-------------
@@ -386,7 +386,7 @@
     }
 
     @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) {
+public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) {
       return new RAMDocsEnum(ramField.termToDocs.get(current), liveDocs);
     }
 
---------------
-------------
@@ -134,7 +134,7 @@
     } catch (InterruptedException e) {
       // ignore ... shouldn't happen
     } catch (ExecutionException e) {
-      throw new IllegalStateException(e);
+throw new IllegalStateException(e.getCause());
     }
     buffer.clear();
 
---------------
-------------
@@ -221,7 +221,7 @@
 		}
 		finally
 		{
-            ntt.commitNoSync(Transaction.RELEASE_LOCKS);
+ntt.commit();
 
 			ntt.close();
 		}
---------------
-------------
@@ -441,7 +441,7 @@
             String rowWarning = xmlUtils.getNodeValue("/Storage/RowWarningThresholdInMB");
             if (rowWarning != null)
             {
-                rowWarningThreshold = Integer.parseInt(rowWarning) * 1024 * 1024;
+rowWarningThreshold = Long.parseLong(rowWarning) * 1024 * 1024;
                 if (rowWarningThreshold <= 0)
                     throw new ConfigurationException("Row warning threshold must be a positive integer");
             }
---------------
-------------
@@ -94,7 +94,7 @@
   // LUCENE-3849: make sure after .end() we see the "ending" posInc
   public void testEndStopword() throws Exception {
     CharArraySet stopSet = StopFilter.makeStopSet(TEST_VERSION_CURRENT, "of");
-    StopFilter stpf = new StopFilter(Version.LUCENE_40, new MockTokenizer(new StringReader("test of"), MockTokenizer.WHITESPACE, false), stopSet);
+StopFilter stpf = new StopFilter(TEST_VERSION_CURRENT, new MockTokenizer(new StringReader("test of"), MockTokenizer.WHITESPACE, false), stopSet);
     assertTokenStreamContents(stpf, new String[] { "test" },
                               new int[] {0},
                               new int[] {4},
---------------
-------------
@@ -428,7 +428,7 @@
      * Once this happens the data associated with the individual column families
      * is also written to the column family store's memtable.
     */
-    void apply(RowMutation mutation, DataOutputBuffer serializedMutation, boolean writeCommitLog) throws IOException
+void apply(RowMutation mutation, Object serializedMutation, boolean writeCommitLog) throws IOException
     {
         HashMap<ColumnFamilyStore,Memtable> memtablesToFlush = new HashMap<ColumnFamilyStore, Memtable>(2);
 
---------------
-------------
@@ -211,7 +211,7 @@
 				// Does the class exist?
 				realClass = cf.loadApplicationClass(checkClassName);
 			}
-			catch (Throwable t)
+catch (ClassNotFoundException t)
 			{
 				throw StandardException.newException(SQLState.LANG_TYPE_DOESNT_EXIST2, t, checkClassName);
 			}
---------------
-------------
@@ -140,7 +140,7 @@
           lookup.store(storeDir);
         }
       } catch (Exception e) {
-        throw new IOException(e);
+throw new IOException(e.toString());
       }
     }
   }
---------------
-------------
@@ -157,7 +157,7 @@
 
 	/**
 		Determine if this request can be granted.
-        <p)
+<p>
         Implements the grant/wait lock logic for row locks.  See the
         table in RowLock for more information.
 
---------------
-------------
@@ -4,5 +4,5 @@
 
 public interface MessageProducer
 {
-    public Message getMessage(int version) throws IOException;
+public Message getMessage(Integer version) throws IOException;
 }
---------------
-------------
@@ -52,7 +52,7 @@
     Field foo = newField("foo", "", TextField.TYPE_UNSTORED);
     doc.add(foo);
     for (int i = 0; i < 100; i++) {
-      foo.setValue(addValue());
+foo.setStringValue(addValue());
       writer.addDocument(doc);
     }
     reader = writer.getReader();
---------------
-------------
@@ -57,7 +57,7 @@
       if (currMap.submap==null) {
         // for now hardcode at 4.0, as its what the old code did.
         // would be nice to fix, but shouldn't store a version in each submap!!!
-        currMap.submap = new CharArrayMap<SlowSynonymMap>(Version.LUCENE_40, 1, ignoreCase());
+currMap.submap = new CharArrayMap<SlowSynonymMap>(Version.LUCENE_CURRENT, 1, ignoreCase());
       }
 
       SlowSynonymMap map = currMap.submap.get(str);
---------------
-------------
@@ -281,7 +281,7 @@
                                 }
                             }
                         };
-                        timeoutFuture = executors.schedule(r, 10, TimeUnit.SECONDS);
+timeoutFuture = executors.schedule(r, timeout, TimeUnit.MILLISECONDS);
                         state = State.WaitForInitialReferences;
                         break;
                     case WaitForInitialReferences:
---------------
-------------
@@ -1613,7 +1613,7 @@
     public void listenToUnitOfWork() {
         if (!listenToUnitOfWork_) {
             listenToUnitOfWork_ = true;
-            connection_.CommitAndRollbackListeners_.put(this,null);
+connection_.CommitAndRollbackListeners_.add(this);
         }
     }
 
---------------
-------------
@@ -43,7 +43,7 @@
   {
     try {
       // setup the server...
-      String url = "http://127.0.0.1/?core=xxx";
+String url = "http" + (isSSLMode() ? "s" : "") +  "://127.0.0.1/?core=xxx";
       HttpSolrServer s = new HttpSolrServer( url );
       Assert.fail( "CommonsHttpSolrServer should not allow a path with a parameter: "+s.getBaseURL() );
     }
---------------
-------------
@@ -331,7 +331,7 @@
   }
 
   @Override
-  public void end() throws IOException {
+public void end() {
     // set final offset
     final int finalOffset = correctOffset(scanner.yychar() + scanner.yylength());
     this.offsetAtt.setOffset(finalOffset, finalOffset);
---------------
-------------
@@ -57,7 +57,7 @@
     addDocument(writer, "C", "It shouldn't.");
     addDocument(writer, "D", "Should we, should we, should we.");
     reader2 = writer.getReader();
-    writer.close();
+writer.shutdown();
     
     // re-open the searcher since we added more docs
     searcher2 = newSearcher(reader2);
---------------
-------------
@@ -31,7 +31,7 @@
                                    Iterable<Cooccurrence> cooccurrences,
                                    double weightOfVectorA,
                                    double weightOfVectorB,
-                                   int numberOfColumns) {
+long numberOfColumns) {
     int cooccurrenceCount = countElements(cooccurrences);
     if (cooccurrenceCount == 0) {
       return Double.NaN;
---------------
-------------
@@ -50,7 +50,7 @@
     // but for preflex codec, the test can be very slow, so use less iterations.
     int num = atLeast(10);
     for (int i = 0; i < num; i++) {
-      field.setValue(_TestUtil.randomUnicodeString(random, 10));
+field.setStringValue(_TestUtil.randomUnicodeString(random, 10));
       writer.addDocument(doc);
     }
     reader = writer.getReader();
---------------
-------------
@@ -469,7 +469,7 @@
         if (len == 0) {
             return 0;
         }
-        if (len + offset > bytes.length) {
+if (len > bytes.length - offset) {
             throw new SqlException(agent_.logWriter_,
                     new ClientMessageId(SQLState.BLOB_LENGTH_TOO_LONG),
                     new Integer(len));
---------------
-------------
@@ -97,7 +97,7 @@
         try
         {
             String key = (String)(rowKey_.get());
-            ReadCommand readCommand = new SliceFromReadCommand(cfMetaData_.tableName, key, columnFamily_column, true, limit_);
+ReadCommand readCommand = new SliceFromReadCommand(cfMetaData_.tableName, key, columnFamily_column, true, offset_, limit_);
             row = StorageProxy.readProtocol(readCommand, StorageService.ConsistencyLevel.WEAK);
         }
         catch (Exception e)
---------------
-------------
@@ -149,7 +149,7 @@
     for(int i=0;i<numThread;i++)
       threads[i].join();
 
-    modifier.close();
+modifier.shutdown();
 
     for(int i=0;i<numThread;i++)
       assertTrue(! threads[i].failed);
---------------
-------------
@@ -32,7 +32,7 @@
 import org.apache.cassandra.CleanupHelper;
 import org.apache.cassandra.db.filter.IdentityQueryFilter;
 import org.apache.cassandra.db.filter.QueryPath;
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.utils.FBUtilities;
 
 import static junit.framework.Assert.assertEquals;
---------------
-------------
@@ -25,7 +25,7 @@
  * 
  * @lucene.internal
  */
-public final class BytesRefUtils {
+final class BytesRefUtils {
 
   private BytesRefUtils() {
   }
---------------
-------------
@@ -648,7 +648,7 @@
 
   }
 
-  public static void main(String[] args) throws Exception {
+public static void main(String[] args) {
     TernaryTree tt = new TernaryTree();
     tt.insert("Carlos", 'C');
     tt.insert("Car", 'r');
---------------
-------------
@@ -52,7 +52,7 @@
   private List<ServiceTracker> srs = new ArrayList<ServiceTracker>();
   public static final long DEFAULT_TIMEOUT = 60000; 
   public static final String ERROR_LEVEL = "ERROR";
-  public static final String DEFAULT_REPO_NAME="reporsitory.xml";
+public static final String DEFAULT_REPO_NAME="repository.xml";
 
   /**
    * Start OSGi framework and install the necessary bundles
---------------
-------------
@@ -231,7 +231,7 @@
     final int numDocumentsToIndex = 50 + random().nextInt(50);
     for (int i = 0; i < numThreads.length; i++) {
       AtomicInteger numDocs = new AtomicInteger(numDocumentsToIndex);
-      MockDirectoryWrapper dir = newDirectory();
+MockDirectoryWrapper dir = newMockDirectory();
       // mock a very slow harddisk sometimes here so that flushing is very slow
       dir.setThrottling(MockDirectoryWrapper.Throttling.SOMETIMES);
       IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT,
---------------
-------------
@@ -100,7 +100,7 @@
 	}
 	
 	protected void removeConstituent() {
-		removeConstituent(provisionTo, resource);
+removeConstituent(subsystem, resource);
 	}
 	
 	protected void removeReference() {
---------------
-------------
@@ -73,7 +73,7 @@
   public void close() {
     try {
       ExecutorUtil.shutdownAndAwaitTermination(updateExecutor);
-    } catch (Throwable e) {
+} catch (Exception e) {
       SolrException.log(log, e);
     } finally {
       clientConnectionManager.shutdown();
---------------
-------------
@@ -110,7 +110,7 @@
       return aborted;
     }
 
-    synchronized void checkAborted(Directory dir) throws MergeAbortedException {
+public synchronized void checkAborted(Directory dir) throws MergeAbortedException {
       if (aborted) {
         throw new MergeAbortedException("merge is aborted: " + segString(dir));
       }
---------------
-------------
@@ -189,7 +189,7 @@
         
         int newDocBase = 0;
         for (int i = 0; i < sequentialSubReaders.length; i++) {
-          build(newParent, sequentialSubReaders[i], i, newDocBase);
+children[i] = build(newParent, sequentialSubReaders[i], i, newDocBase);
           newDocBase += sequentialSubReaders[i].maxDoc();
         }
         return newParent;
---------------
-------------
@@ -51,7 +51,7 @@
 
   /** Must fully consume state, since after this call that
    *  TermState may be reused. */
-  public abstract DocsEnum docs(FieldInfo fieldInfo, BlockTermState state, Bits skipDocs, DocsEnum reuse, boolean needsFreqs) throws IOException;
+public abstract DocsEnum docs(FieldInfo fieldInfo, BlockTermState state, Bits skipDocs, DocsEnum reuse, int flags) throws IOException;
 
   /** Must fully consume state, since after this call that
    *  TermState may be reused. */
---------------
-------------
@@ -30,7 +30,7 @@
 public class StandardIndexReaderFactory extends IndexReaderFactory {
   
   @Override
-  public DirectoryReader newReader(Directory indexDir) throws IOException {
+public DirectoryReader newReader(Directory indexDir, SolrCore core) throws IOException {
     return DirectoryReader.open(indexDir, termInfosIndexDivisor);
   }
 }
---------------
-------------
@@ -84,7 +84,7 @@
 
     String sum = summary.toString().trim();
     String tit = getTitle();
-    if (sum.startsWith(tit) || sum.equals(""))
+if (sum.equals(""))
       return tit;
     else
       return sum;
---------------
-------------
@@ -65,7 +65,7 @@
     }
     
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     searcher = newSearcher(reader);
   }
   
---------------
-------------
@@ -79,7 +79,7 @@
         if (expectedRows == null)
             JDBC.assertEmpty(rs);
         else
-            JDBC.assertFullResultSet(rs, expectedRows);
+JDBC.assertUnorderedResultSet(rs, expectedRows);
         rs.close();
     }
     
---------------
-------------
@@ -59,7 +59,7 @@
     // Try to make an index that requires merging:
     w.getConfig().setMaxBufferedDocs(_TestUtil.nextInt(random, 2, 11));
     final int numStartDocs = atLeast(20);
-    final LineFileDocs docs = new LineFileDocs(random);
+final LineFileDocs docs = new LineFileDocs(random, defaultCodecSupportsDocValues());
     for(int docIDX=0;docIDX<numStartDocs;docIDX++) {
       w.addDocument(docs.nextDoc());
     }
---------------
-------------
@@ -60,7 +60,7 @@
     assertFalse(query.getTermsEnum(terms) instanceof PrefixTermsEnum);
     hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals("everything", 3, hits.length);
-    writer.close();
+writer.shutdown();
     reader.close();
     directory.close();
   }
---------------
-------------
@@ -184,7 +184,7 @@
       // Finally creating a suitable field with stream and adding it to a
       // master field-list, used during the build process (see
       // super.build())
-      FieldType ft = new FieldType(TextField.TYPE_UNSTORED);
+FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);
       ft.setOmitNorms(true);
       fieldList.add(new Field(e.getKey(), stream, ft));
     }
---------------
-------------
@@ -48,7 +48,7 @@
     addDoc("three four", iw, 300f);
     iw.close();
 
-    IndexReader ir = IndexReader.open(dir);
+IndexReader ir = IndexReader.open(dir, true);
     IndexSearcher is = new IndexSearcher(ir);
     ScoreDoc[] hits;
 
---------------
-------------
@@ -190,7 +190,7 @@
             if (isXml) {
               cfg = new ConfigSolrXmlBackCompat(loader, null, is, null, false);
             } else {
-              cfg = new SolrProperties(null, is, null);
+cfg = new SolrProperties(null, loader, is, null);
             }
           } finally {
             IOUtils.closeQuietly(is);
---------------
-------------
@@ -179,7 +179,7 @@
         sp.setBoost(query.getBoost());
         extractWeightedSpanTerms(terms, sp);
       }
-    } else if (query instanceof ConstantScoreRangeQuery) {
+} else if (highlightCnstScrRngQuery && query instanceof ConstantScoreRangeQuery) {
       ConstantScoreRangeQuery q = (ConstantScoreRangeQuery) query;
       Term lower = new Term(fieldName, q.getLowerVal());
       Term upper = new Term(fieldName, q.getUpperVal());
---------------
-------------
@@ -47,6 +47,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new HindiAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new HindiAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -51,7 +51,7 @@
 public class TestTermsEnum extends LuceneTestCase {
 
   public void test() throws Exception {
-    final LineFileDocs docs = new LineFileDocs(random);
+final LineFileDocs docs = new LineFileDocs(random, defaultCodecSupportsDocValues());
     final Directory d = newDirectory();
     final RandomIndexWriter w = new RandomIndexWriter(random, d);
     final int numDocs = atLeast(10);
---------------
-------------
@@ -413,7 +413,7 @@
         } finally {
             conn_.pendingEndXACallinfoOffset_ = -1; // indicate no pending callinfo
         }
-        if (rc != XAResource.XA_OK) {
+if ((rc != XAResource.XA_OK ) && (rc != XAResource.XA_RDONLY)) {
             throwXAException(rc, false);
         }
         if (conn_.agent_.loggingEnabled()) {
---------------
-------------
@@ -131,7 +131,7 @@
     Assert.assertEquals("Original should not change after copy is updated", auc1, w.getLearner().auc(), 1e-5);
 
     // this improvement is really quite lenient
-    Assert.assertTrue("AUC should improve substantially on copy", auc1 < w2.getLearner().auc() - 0.1);
+Assert.assertTrue("AUC should improve significantly on copy", auc1 < w2.getLearner().auc() - 0.05);
 
     // make sure that the copy didn't lose anything
     Assert.assertEquals(auc1, w.getLearner().auc(), 0);
---------------
-------------
@@ -30,7 +30,7 @@
       "852781,M,18.61,20.25,122.1,1094,0.0944,0.1066,0.149,0.07731,0.1697,0.05699,0.8529,1.849,5.632,93.54,0.01075,0.02722,0.05081,0.01911,0.02293,0.004217,21.31,27.26,139.9,1403,0.1338,0.2117,0.3446,0.149,0.2341,0.07421" };
 
   public void testSet() throws Exception {
-    Path inpath = new Path("target/test-classes/wdbc");
+Path inpath = new Path(this.getClass().getResource("/wdbc/").getPath());
     FileSystem fs = FileSystem.get(inpath.toUri(), new Configuration());
     DataSet dataset = FileInfoParser.parseFile(fs, inpath);
     DataSet.initialize(dataset);
---------------
-------------
@@ -31,7 +31,7 @@
 public class DistanceFunctionTest extends SolrTestCaseJ4 {
   @BeforeClass
   public static void beforeClass() throws Exception {
-    initCore("solrConfig-functionquery.xml", "schema11.xml");
+initCore("solrconfig-functionquery.xml", "schema11.xml");
   }
 
   @Test
---------------
-------------
@@ -45,7 +45,7 @@
     writer.close();
 
     PrefixQuery query = new PrefixQuery(new Term("category", "/Computers"));
-    IndexSearcher searcher = new IndexSearcher(directory);
+IndexSearcher searcher = new IndexSearcher(directory, true);
     ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals("All documents in /Computers category and below", 3, hits.length);
 
---------------
-------------
@@ -126,7 +126,7 @@
                 rm.add(cf);
         }
         Message message = rm.makeRowMutationMessage();
-        WriteResponseHandler responseHandler = new WriteResponseHandler(1, tableName);
+WriteResponseHandler responseHandler = new WriteResponseHandler(endpoint);
         MessagingService.instance.sendRR(message, new InetAddress[] { endpoint }, responseHandler);
 
         try
---------------
-------------
@@ -78,7 +78,7 @@
             {
                 if ( !done_.get() )
                 {
-                    long overall_timeout = System.currentTimeMillis() - startTime_ + timeout;
+long overall_timeout = timeout - (System.currentTimeMillis() - startTime_);
                     if(overall_timeout > 0)
                         bVal = condition_.await(overall_timeout, TimeUnit.MILLISECONDS);
                     else
---------------
-------------
@@ -58,7 +58,7 @@
     
     DirectUpdateHandler2 updater = (DirectUpdateHandler2)SolrCore.getSolrCore().getUpdateHandler();
     DirectUpdateHandler2.CommitTracker tracker = updater.tracker;
-    tracker.timeUpperBound = -1;
+tracker.timeUpperBound = 100000;
     tracker.docsUpperBound = 14;
     
     XmlUpdateRequestHandler handler = new XmlUpdateRequestHandler();
---------------
-------------
@@ -32,7 +32,7 @@
 
   public static HashFunction[] createHashFunctions(HashType type, int numFunctions) {
     HashFunction[] hashFunction = new HashFunction[numFunctions];
-    Random seed = new Random(11);
+Random seed = RandomUtils.getRandom(11);
     switch (type) {
       case LINEAR:
         for (int i = 0; i < numFunctions; i++) {
---------------
-------------
@@ -1648,7 +1648,7 @@
             throw new AssertionError(e);
         }
         long truncatedAt = System.currentTimeMillis();
-        snapshot(Table.getTimestampedSnapshotName("before-truncate"));
+snapshot(Table.getTimestampedSnapshotName(columnFamily));
 
         return CompactionManager.instance.submitTruncate(this, truncatedAt);
     }
---------------
-------------
@@ -224,7 +224,7 @@
   private Map<SegmentInfoPerCommit,Boolean> segmentsToMerge = new HashMap<SegmentInfoPerCommit,Boolean>();
   private int mergeMaxNumSegments;
 
-  protected Lock writeLock;
+private Lock writeLock;
 
   private volatile boolean closed;
   private volatile boolean closing;
---------------
-------------
@@ -60,7 +60,7 @@
    * @param filter DFA describing how terms should be filtered (set of stopwords, etc)
    */
   public MockAnalyzer(Random random, CharacterRunAutomaton runAutomaton, boolean lowerCase, CharacterRunAutomaton filter) {
-    super(new PerFieldReuseStrategy());
+super(PER_FIELD_REUSE_STRATEGY);
     // TODO: this should be solved in a different way; Random should not be shared (!).
     this.random = new Random(random.nextLong());
     this.runAutomaton = runAutomaton;
---------------
-------------
@@ -113,7 +113,7 @@
       this.field = field;
       this.doPackFST = doPackFST;
       this.acceptableOverheadRatio = acceptableOverheadRatio;
-      builder = new Builder<BytesRef>(FST.INPUT_TYPE.BYTE1, 0, 0, true, true, Integer.MAX_VALUE, outputs, null, doPackFST, acceptableOverheadRatio);
+builder = new Builder<BytesRef>(FST.INPUT_TYPE.BYTE1, 0, 0, true, true, Integer.MAX_VALUE, outputs, null, doPackFST, acceptableOverheadRatio, true);
     }
 
     private class PostingsWriter extends PostingsConsumer {
---------------
-------------
@@ -61,7 +61,7 @@
       booleanQuery.add(new TermQuery(new Term(FIELD, "36")), BooleanClause.Occur.SHOULD);
      
      
-      IndexSearcher indexSearcher = new IndexSearcher(directory);
+IndexSearcher indexSearcher = new IndexSearcher(directory, true);
       ScoreDoc[] hits = indexSearcher.search(booleanQuery, filter, 1000).scoreDocs;
       assertEquals("Number of matched documents", 1, hits.length);
 
---------------
-------------
@@ -56,7 +56,7 @@
 		This attributes properties set has the second default properties set as
 		its default. This set (which could be null) contains the properties
 		that the user set on their DriverManager.getConnection() call, and are thus
-		not owned by cloudscape code, and thus must not be modified by cloudscape
+not owned by Derby code, and thus must not be modified by Derby
 		code.
 		<P>
 		When create is false the properties object contains all the properties
---------------
-------------
@@ -1094,7 +1094,7 @@
     doc.add(newTextField("field", "the wizard of ozzy", Field.Store.NO));
     w.addDocument(doc);
     IndexReader r = DirectoryReader.open(w, true);
-    w.close();
+w.shutdown();
     IndexSearcher s = newSearcher(r);
     
     Query q = getQuery("\"wizard of ozzy\"",a);
---------------
-------------
@@ -649,7 +649,7 @@
           
           if (gen == -1) {
             // Neither approach found a generation
-            throw new IndexNotFoundException("no segments* file found in " + directory + ": files: " + Arrays.toString(files));
+throw new FileNotFoundException("no segments* file found in " + directory + ": files: " + Arrays.toString(files));
           }
         }
 
---------------
-------------
@@ -414,7 +414,7 @@
                         // both tokens are on the same side of the wrap point
                         return o1.compareTo(o2);
                     }
-                    return -o1.compareTo(o2);
+return o2.compareTo(o1);
                 }
             };
             Collections.sort(keys, comparator);
---------------
-------------
@@ -4486,7 +4486,7 @@
 				if (SanityManager.DEBUG)
 					trace("short parameter value is: "+paramVal);
  				// DB2 does not have a BOOLEAN java.sql.bit type, it's sent as small
-				if (pmeta.getParameterType(i+1) == JDBC30Translation.BOOLEAN)
+if (pmeta.getParameterType(i+1) == Types.BOOLEAN)
 					ps.setBoolean(i+1, (paramVal == 1));
 				else
 					ps.setShort(i+1, paramVal);
---------------
-------------
@@ -105,7 +105,7 @@
 
     public String explainPlan()
     {
-        StringBuffer sb = new StringBuffer();
+StringBuilder sb = new StringBuilder();
         
         String prefix =
             String.format("%s Column Family: Batch SET a set of columns: \n" +
---------------
-------------
@@ -26,7 +26,7 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.*;
+import static org.apache.lucene.analysis.VocabularyAssert.*;
 
 /**
  * Simple tests for {@link FrenchMinimalStemFilter}
---------------
-------------
@@ -409,7 +409,7 @@
 
 		try {
 			xaResource.end(xid,xaflags);
-            xid = null;
+this.xid = null;
 			if (SanityManager.DEBUG)
 			{
 				connThread.trace("ended XA transaction. xid =  " + xid +
---------------
-------------
@@ -208,7 +208,7 @@
       
       boolean success = false;
       try {
-        success = syncStrategy.sync(zkController, core, leaderProps);
+success = syncStrategy.sync(zkController, core, leaderProps, weAreReplacement);
       } catch (Exception e) {
         SolrException.log(log, "Exception while trying to sync", e);
         success = false;
---------------
-------------
@@ -66,7 +66,7 @@
 
     reader = iw.getReader();
     searcher = newSearcher(reader);
-    iw.close();
+iw.shutdown();
   }
 
   @AfterClass
---------------
-------------
@@ -132,7 +132,7 @@
      * @param newToken token to move this node to.
      * This node will unload its data onto its neighbors, and bootstrap to the new token.
      */
-    public void move(String newToken) throws InterruptedException;
+public void move(String newToken) throws IOException, InterruptedException;
 
     /**
      * This node will unload its data onto its neighbors, and bootstrap to share the range
---------------
-------------
@@ -5141,7 +5141,7 @@
 					// arguments are variable part of a message
 					Object[] args = ce.getArguments();
 					for (int i = 0; args != null &&  i < args.length; i++)
-						sqlerrmc += args[i].toString() + separator;
+sqlerrmc += args[i] + separator;
 					
 					// Severe exceptions need to be logged in the error log
 					// also log location and non-localized message will be
---------------
-------------
@@ -907,7 +907,7 @@
 			/* If joinClause is a parameter, (where ?), then we assume
 			 * it will be a nullable boolean.
 			 */
-			if (joinClause.isParameterNode())
+if (joinClause.requiresTypeFromContext())
 			{
 				joinClause.setType(new DataTypeDescriptor(TypeId.BOOLEAN_ID, true));
 			}
---------------
-------------
@@ -233,7 +233,7 @@
 
     for (int i = offset; i < offset + num; i++) {
       Document doc = new Document();
-      doc.add(new Field("id", i + "", TextField.TYPE_STORED));
+doc.add(new TextField("id", i + "", Field.Store.YES));
       sourceTypes[i] = valueType;
       switch (valueType) {
       case VAR_INTS:
---------------
-------------
@@ -221,7 +221,7 @@
 
      loadPluginInfo(DirectoryFactory.class,"directoryFactory",false, true);
      loadPluginInfo(IndexDeletionPolicy.class,indexConfigPrefix+"/deletionPolicy",false, true);
-     loadPluginInfo(CodecFactory.class,"mainIndex/codecFactory",false, false);
+loadPluginInfo(CodecFactory.class,"codecFactory",false, false);
      loadPluginInfo(IndexReaderFactory.class,"indexReaderFactory",false, true);
      loadPluginInfo(UpdateRequestProcessorChain.class,"updateRequestProcessorChain",false, false);
      loadPluginInfo(UpdateLog.class,"updateHandler/updateLog",false, false);
---------------
-------------
@@ -117,7 +117,7 @@
       double clVal = calculatePrior(next) * calculateLikelihood(tokenizedDoc, next);
       if (clVal > max) {
         max = clVal;
-        foundClass = next.clone();
+foundClass = BytesRef.deepCopyOf(next);
       }
     }
     return new ClassificationResult<BytesRef>(foundClass, max);
---------------
-------------
@@ -68,7 +68,7 @@
 			throw new SubsystemException("Cannot stop from state " + state);
 		// The following states must wait.
 		if (EnumSet.of(State.INSTALLING, State.RESOLVING, State.STARTING, State.STOPPING).contains(state)) {
-			waitForStateChange();
+waitForStateChange(state);
 			return new StartAction(instigator, requestor, target).run();
 		}
 		// The following states mean the requested state has already been attained.
---------------
-------------
@@ -88,7 +88,7 @@
 
     clone.input = (InputStream)input.clone();
     clone.termInfo = new TermInfo(termInfo);
-    clone.growBuffer(term.text.length());
+if (term != null) clone.growBuffer(term.text.length());
 
     return clone;
   }
---------------
-------------
@@ -617,7 +617,7 @@
          "'NOSUCHCOL' is not a column in table or VTI 'S1.T1'."},
 
         {"grant select on nosuch.t1 to " + users[0].name, "42Y07", "Schema 'NOSUCH' does not exist"},
-        {"grant select on s1.nosuch to " + users[0].name, "42X05", "Table 'S1.NOSUCH' does not exist."},
+{"grant select on s1.nosuch to " + users[0].name, "42X05", "Table/View 'S1.NOSUCH' does not exist."},
         {"grant execute on function nosuch.f0 to " + users[0].name, "42Y07", "Schema 'NOSUCH' does not exist"},
         {"grant execute on function s1.nosuch to " + users[0].name, "42Y03",
          "'S1.NOSUCH' is not recognized as a function or procedure."},
---------------
-------------
@@ -151,7 +151,7 @@
     // lob length
     static final short LOBLENGTH = 4;
 
-    static final String UTF8ENCODING = "UTF8";
+public static final String UTF8ENCODING = "UTF8";
 
     private static final int OVERRIDE_TABLE_SIZE = 0xff;
 
---------------
-------------
@@ -287,7 +287,7 @@
    * Concatenates content from multiple fields
    */
   protected String concatFields(SolrInputDocument doc, String[] fields) {
-    StringBuffer sb = new StringBuffer();
+StringBuilder sb = new StringBuilder();
     for (String fieldName : inputFields) {
       log.debug("Appending field "+fieldName);
       if (doc.containsKey(fieldName)) {
---------------
-------------
@@ -28,5 +28,5 @@
 public abstract class DocValuesFormat {
   public abstract PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException;
   public abstract PerDocValues docsProducer(SegmentReadState state) throws IOException;
-  public abstract void files(Directory dir, SegmentInfo info, int formatId, Set<String> files) throws IOException;
+public abstract void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException;
 }
---------------
-------------
@@ -155,7 +155,7 @@
       }
       line = br.readLine();
     }
-    writer.close();
+writer.shutdown();
 
     //open searcher
     // this example never closes it reader!
---------------
-------------
@@ -123,7 +123,7 @@
   }
 
   @Override
-  public void end() throws IOException {
+public void end() {
     // set final offset
     final int finalOffset = correctOffset(tokenEnd);
     offsetAtt.setOffset(finalOffset, finalOffset);
---------------
-------------
@@ -389,7 +389,7 @@
         };
     }
 
-    public void clearUnsafe()
+void clearUnsafe()
     {
         columnFamilies_.clear();
     }
---------------
-------------
@@ -59,7 +59,7 @@
 
     public Writer(Directory dir, String id, Comparator<BytesRef> comp,
         Counter bytesUsed, IOContext context, boolean fasterButMoreRam) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context, fasterButMoreRam);
+super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context, fasterButMoreRam, Type.BYTES_VAR_SORTED);
       this.comp = comp;
       size = 0;
     }
---------------
-------------
@@ -195,7 +195,7 @@
     int hi = indexTerms.length - 1;
 
     while (hi >= lo) {
-      int mid = (lo + hi) >> 1;
+int mid = (lo + hi) >>> 1;
       int delta = term.compareTo(indexTerms[mid]);
       if (delta < 0)
 	hi = mid - 1;
---------------
-------------
@@ -141,7 +141,7 @@
         this.resultCode + "\t" +
         this.mimeType + "\t" +
         this.size + "\t" +
-        "\"" + this.title.replace('\"', (char)0xff ).replace('\n',' ').replace('\r',' ') + "\"";
+"\"" + this.title.replace('\t',' ').replace('\"', (char)0xff ).replace('\n',' ').replace('\r',' ') + "\"";
     }
 
 
---------------
-------------
@@ -46,7 +46,7 @@
 
   A b-tree scan controller corresponds to an instance of an open b-tree scan.
   <P>
-  <B>Concurrency Notes<\B>
+<B>Concurrency Notes</B>
   <P>
   The concurrency rules are derived from OpenBTree.
   <P>
---------------
-------------
@@ -155,7 +155,7 @@
 
 
   // like getId, but also accepts dashes for legacy fields
-  String getFieldName(QueryParsing.StrParser sp) throws ParseException {
+String getFieldName(QueryParsing.StrParser sp) {
     sp.eatws();
     int id_start = sp.pos;
     char ch;
---------------
-------------
@@ -74,7 +74,7 @@
         // generate random token
         String chars = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789";
         Random r = new Random();
-        StringBuffer buffer = new StringBuffer();
+StringBuilder buffer = new StringBuilder();
         for (int j = 0; j < 16; j++) {
             buffer.append(chars.charAt(r.nextInt(chars.length())));
         }
---------------
-------------
@@ -40,7 +40,7 @@
 
     public final DecoratedKey key;
     public final QueryPath path;
-    private final IFilter filter;
+public final IFilter filter;
     private final IFilter superFilter;
 
     public QueryFilter(DecoratedKey key, QueryPath path, IFilter filter)
---------------
-------------
@@ -1382,7 +1382,7 @@
 	String LANG_INVALID_CALL_TO_EXECUTE_UPDATE		                   = "X0Y79.S";
 	String LANG_NULL_DATA_IN_NON_NULL_COLUMN               	   	   	   = "X0Y80.S";
     String LANG_IGNORE_MISSING_INDEX_ROW_DURING_DELETE                 = "X0Y83.S";
-    String LANG_TOO_MUCH_CONTENTION_ON_SEQUENCE                 = "X0Y84.S";
+String LANG_TOO_MUCH_CONTENTION_ON_SEQUENCE                 = "X0Y84.T";
 	String LANG_UNKNOWN_SEQUENCE_PREALLOCATOR                                = "X0Y85.S";
 	String LANG_CANT_FLUSH_PREALLOCATOR                                = "X0Y86.S";
 
---------------
-------------
@@ -37,7 +37,7 @@
 	global transactions, i.e., it does not know of in-doubt global transactions
 	re-created by recovery.
 
-	<P>	The following is an overall design of the JTA implementation in cloudscape,
+<P>	The following is an overall design of the JTA implementation in Derby,
 	most of it has little to do with the ResourceAdapter interface itself.
 	<P><B>Design Overview </B>
 
---------------
-------------
@@ -47,7 +47,7 @@
  * keys at an Endpoint. Monitor load information for a 5 minute
  * interval and then do load balancing operations if necessary.
  */
-public final class StorageLoadBalancer implements IEndPointStateChangeSubscriber
+public class StorageLoadBalancer implements IEndPointStateChangeSubscriber
 {
     class LoadBalancer implements Runnable
     {
---------------
-------------
@@ -189,7 +189,7 @@
             protected void afterExecute(Runnable r, Throwable t)
             {
                 super.afterExecute(r, t);
-                cassandraServer.clientState.logout();
+cassandraServer.logout();
             }
         };
         serverEngine = new CustomTThreadPoolServer(new TProcessorFactory(processor),
---------------
-------------
@@ -248,7 +248,7 @@
   }
 
   @Override
-  public NumericDocValues simpleNormValues(String field) throws IOException {
+public NumericDocValues getNormValues(String field) throws IOException {
     ensureOpen();
     return core.getSimpleNormValues(field);
   }
---------------
-------------
@@ -165,7 +165,7 @@
       }
     }
     reader = iw.getReader();
-    iw.close();
+iw.shutdown();
     searcher = newSearcher(reader);
     if (VERBOSE) {
       System.out.println("  searcher=" + searcher);
---------------
-------------
@@ -86,7 +86,7 @@
     }
 
     /*
-     param@ key -- value whose hash is used to fill
+@param key -- value whose hash is used to fill
      the filter_.
      This is a general purpose API.
      */
---------------
-------------
@@ -278,7 +278,7 @@
           continue;
         }
 
-        docsEnum = termsEnum.docs(null, docsEnum, 0);
+docsEnum = termsEnum.docs(null, docsEnum, false);
         int doc;
         while ((doc = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
           vals[doc] = fval;
---------------
-------------
@@ -205,7 +205,7 @@
   /** Whether the relevance score is needed to sort documents. */
   boolean needsScores() {
     for (SortField sortField : fields) {
-      if (sortField.getType() == SortField.Type.SCORE) {
+if (sortField.needsScores()) {
         return true;
       }
     }
---------------
-------------
@@ -52,7 +52,7 @@
 
     System.out.println("Full merge...");
     writer.forceMerge(1);
-    writer.close();
+writer.shutdown();
     System.out.println("Done.");
   }
 }
---------------
-------------
@@ -151,7 +151,7 @@
 
               @Override
               public boolean isIndexTerm(BytesRef term, TermStats stats) {
-                return random.nextInt(gap) == 17;
+return rand.nextInt(gap) == 17;
               }
 
               @Override
---------------
-------------
@@ -79,7 +79,7 @@
     Option centroidJSonOpt = obuilder.withLongName("json").withRequired(false).withDescription(
             "Output the centroid as JSON.  Otherwise it substitutes in the terms for vector cell entries")
             .withShortName("j").create();
-    Option sizeOpt = obuilder.withLongName("sizeOnly").withRequired(true).
+Option sizeOpt = obuilder.withLongName("sizeOnly").withRequired(false).
             withDescription("Dump only the size of the vector").withShortName("sz").create();
     Option helpOpt = obuilder.withLongName("help").withDescription("Print out help").withShortName("h")
             .create();
---------------
-------------
@@ -41,7 +41,7 @@
  *
  * @see org.apache.lucene.index.IndexDeletionPolicy
  */
-public class SolrDeletionPolicy implements IndexDeletionPolicy, NamedListInitializedPlugin {
+public class SolrDeletionPolicy extends IndexDeletionPolicy implements NamedListInitializedPlugin {
   public static Logger log = LoggerFactory.getLogger(SolrCore.class);
 
   private String maxCommitAge = null;
---------------
-------------
@@ -81,7 +81,7 @@
     assertQ("Basic summarization",
             sumLRF.makeRequest("tv_text:vector"),
             "//lst[@name='highlighting']/lst[@name='1']",
-            "//lst[@name='1']/arr[@name='tv_text']/str[.=' fast <em>vector</em> highlighter test']"
+"//lst[@name='1']/arr[@name='tv_text']/str[.='basic fast <em>vector</em> highlighter test']"
             );
   }
 }
---------------
-------------
@@ -221,7 +221,7 @@
   protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {
     //System.out.println("DTO uninvert field=" + field + " prefix=" + termPrefix);
     final long startTime = System.currentTimeMillis();
-    prefix = termPrefix == null ? null : new BytesRef(termPrefix);
+prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);
 
     final int maxDoc = reader.maxDoc();
     final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number
---------------
-------------
@@ -253,7 +253,7 @@
         // do some reads.
         store = Table.open(oldCfm.tableName).getColumnFamilyStore(cfName);
         assert store != null;
-        ColumnFamily cfam = store.getColumnFamily(QueryFilter.getSliceFilter(dk, new QueryPath(cfName), "".getBytes(), "".getBytes(), null, false, 1000));
+ColumnFamily cfam = store.getColumnFamily(QueryFilter.getSliceFilter(dk, new QueryPath(cfName), "".getBytes(), "".getBytes(), false, 1000));
         assert cfam.getSortedColumns().size() == 100; // should be good enough?
         
         // do some writes
---------------
-------------
@@ -50,7 +50,7 @@
     IndexWriter writer = new IndexWriter(runData.getDirectory(),
                                          config.get("autocommit", DEFAULT_AUTO_COMMIT),
                                          runData.getAnalyzer(),
-                                         false, IndexWriter.MaxFieldLength.LIMITED);
+false);
     CreateIndexTask.setIndexWriterConfig(writer, config);
     runData.setIndexWriter(writer);
     return 1;
---------------
-------------
@@ -117,7 +117,7 @@
                     bufIn.reset(bufOut.getData(), bufOut.getLength());
 
                     /* The key is the table name */
-                    String key = bufIn.readUTF();
+bufIn.readUTF();
                     /* read the size of the data we ignore this value */
                     bufIn.readInt();
                     tableMetadata_ = Table.TableMetadata.serializer().deserialize(bufIn);
---------------
-------------
@@ -163,7 +163,7 @@
       IndexInput bloomIn = null;
       boolean success = false;
       try {
-        bloomIn = state.dir.openInput(bloomFileName, state.context);
+bloomIn = state.directory.openInput(bloomFileName, state.context);
         CodecUtil.checkHeader(bloomIn, BLOOM_CODEC_NAME, BLOOM_CODEC_VERSION,
             BLOOM_CODEC_VERSION);
         // // Load the hash function used in the BloomFilter
---------------
-------------
@@ -31,7 +31,7 @@
 
 public class OutputDriver {
 
-  public static void main(String[] args) throws Exception {
+public static void main(String[] args) throws IOException {
     runJob(args[0], args[1]);
   }
 
---------------
-------------
@@ -81,7 +81,7 @@
     // TODO: this could be more efficient, but the sortable types should be deprecated instead
     input.utf8ToChars(charsRef);
     final char[] indexedToReadable = indexedToReadable(charsRef.toString()).toCharArray();
-    charsRef.copy(indexedToReadable, 0, indexedToReadable.length);
+charsRef.copyChars(indexedToReadable, 0, indexedToReadable.length);
     return charsRef;
   }
 
---------------
-------------
@@ -56,7 +56,7 @@
   public DistributedQueue(SolrZkClient zookeeper, String dir, List<ACL> acl) {
     this.dir = dir;
     
-    ZkCmdExecutor cmdExecutor = new ZkCmdExecutor(30);
+ZkCmdExecutor cmdExecutor = new ZkCmdExecutor(zookeeper.getZkClientTimeout());
     try {
       cmdExecutor.ensureExists(dir, zookeeper);
     } catch (KeeperException e) {
---------------
-------------
@@ -71,7 +71,7 @@
 			xaDataSource = new EmbeddedXADataSource();
 		}
 
-		xaDataSource.setDatabaseName(shortDbName);
+xaDataSource.setDatabaseName(getShortDbName());
 		appendAttrString(p);
 		if (attrString != null)
 			xaDataSource.setConnectionAttributes(attrString);
---------------
-------------
@@ -136,7 +136,7 @@
     indexStream.seek(HEADER_LENGTH_IDX + docID * 8L);
   }
 
-  public final void visitDocument(int n, StoredFieldVisitor visitor) throws CorruptIndexException, IOException {
+public final void visitDocument(int n, StoredFieldVisitor visitor) throws IOException {
     seekIndex(n);
     fieldsStream.seek(indexStream.readLong());
 
---------------
-------------
@@ -679,7 +679,7 @@
         if (buildOnCommit)  {
           buildSpellIndex(newSearcher);
         } else if (buildOnOptimize) {
-          if (newSearcher.getIndexReader().getSequentialSubReaders().size() == 1)  {
+if (newSearcher.getIndexReader().leaves().size() == 1)  {
             buildSpellIndex(newSearcher);
           } else  {
             LOG.info("Index is not optimized therefore skipping building spell check index for: " + checker.getDictionaryName());
---------------
-------------
@@ -92,7 +92,7 @@
   @Override
   public void configure(JobConf job) {
     try {
-      Parameters params = Parameters.fromString(job.get("bayes.parameters", ""));
+Parameters params = new Parameters(job.get("bayes.parameters", ""));
       if (params.get("dataSource").equals("hbase")) {
         useHbase = true;
       } else {
---------------
-------------
@@ -221,7 +221,7 @@
  
     for (Node node : nodes) {
       List<AddRequest> alist = adds.get(node);
-      if (alist == null || alist.size() < limit) return false;
+if (alist == null || alist.size() < limit) continue;
   
       UpdateRequestExt ureq = new UpdateRequestExt();
       
---------------
-------------
@@ -218,7 +218,7 @@
     */
     public void deliverHints(final InetAddress to)
     {
-        if (queuedDeliveries.contains(to))
+if (!queuedDeliveries.add(to))
             return;
 
         Runnable r = new WrappedRunnable()
---------------
-------------
@@ -109,7 +109,7 @@
                                        new BytesRef("field"),
                                        MultiFields.getLiveDocs(mergedReader),
                                        null,
-                                       false);
+0);
     assertTrue(termDocs != null);
     assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
 
---------------
-------------
@@ -288,7 +288,7 @@
                         columnFamily = cFamily.cloneMeShallow();
                         SuperColumn container = new SuperColumn(superColumn.name());
                         container.markForDeleteAt(superColumn.getLocalDeletionTime(), superColumn.getMarkedForDeleteAt());
-                        container.addColumn(subColumn.name(), subColumn);
+container.addColumn(subColumn);
                         columnFamily.addColumn(container);
                     }
                 }
---------------
-------------
@@ -52,7 +52,7 @@
             {
                 if (logger_.isDebugEnabled())
                     logger_.debug(srm.toString());
-                StreamOut.transferRanges(srm.target_, srm.ranges_, null);
+StreamOut.transferRanges(srm.target_, srm.table_, srm.ranges_, null);
             }
         }
         catch (IOException ex)
---------------
-------------
@@ -76,7 +76,7 @@
    * Get the list of DeployedService-Import
    * @return DeployedService-Import
    */
-  public Collection<Filter> getDeployedServiceImport() throws InvalidAttributeException;
+public Collection<Filter> getDeployedServiceImport();
   
   /**
    * get the contents of deployment manifest in a map
---------------
-------------
@@ -35,7 +35,7 @@
         {
             byte[] key = String.valueOf(j).getBytes();
             RowMutation rm = new RowMutation("Keyspace1", key);
-            rm.add(new QueryPath("Standard1", null, "0".getBytes()), new byte[0], j);
+rm.add(new QueryPath("Standard1", null, "0".getBytes()), new byte[0], new TimestampClock(j));
             rm.apply();
         }
         store.forceBlockingFlush();
---------------
-------------
@@ -50,7 +50,7 @@
   public static final BytesRefIterator EMPTY = new BytesRefIterator() {
 
     @Override
-    public BytesRef next() throws IOException {
+public BytesRef next() {
       return null;
     }
     
---------------
-------------
@@ -110,7 +110,7 @@
     int numHashFunctions = Integer.valueOf(getOption(MinhashOptionCreator.NUM_HASH_FUNCTIONS));
     int keyGroups = Integer.valueOf(getOption(MinhashOptionCreator.KEY_GROUPS));
     int numReduceTasks = Integer.parseInt(getOption(MinhashOptionCreator.NUM_REDUCERS));
-    boolean debugOutput = Boolean.parseBoolean(getOption(MinhashOptionCreator.DEBUG_OUTPUT));
+boolean debugOutput = hasOption(MinhashOptionCreator.DEBUG_OUTPUT);
 
     runJob(input,
            output,
---------------
-------------
@@ -185,7 +185,7 @@
         assertEquals(2, config.get("A2"));
         
         //delete
-        mbean.delete("org.apache.aries.jmx.test.ServiceA", a.getLocation());
+mbean.deleteForLocation("org.apache.aries.jmx.test.ServiceA", a.getLocation());
         
         Thread.sleep(1000);
         assertNull(managedServiceA.getConfig());
---------------
-------------
@@ -61,7 +61,7 @@
     
     // Do sanity checks - we don't want to find old style config
     failIfFound("solr/@coreLoadThreads");
-    failIfFound("solr/@persist");
+failIfFound("solr/@persistent");
     failIfFound("solr/@sharedLib");
     failIfFound("solr/@zkHost");
     
---------------
-------------
@@ -872,7 +872,7 @@
       }
 
       @Override
-      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) {
+public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) {
         if (reuse == null || !(reuse instanceof MemoryDocsEnum)) {
           reuse = new MemoryDocsEnum();
         }
---------------
-------------
@@ -102,7 +102,7 @@
     boolean success = false;
     final IndexWriter w = new IndexWriter(target, config);
     try {
-      final List<AtomicReaderContext> leaves = reader.getTopReaderContext().leaves();
+final List<AtomicReaderContext> leaves = reader.leaves();
       final IndexReader[] subReaders = new IndexReader[leaves.size()];
       int i = 0;
       for (final AtomicReaderContext ctx : leaves) {
---------------
-------------
@@ -26,7 +26,7 @@
  * To change this template use File | Settings | File Templates.
  */
 
-public final class HashingSchemes
+public class HashingSchemes
 {
     public static final String SHA_1 = "SHA-1";
     public static final String SHA1 = "SHA1";
---------------
-------------
@@ -475,7 +475,7 @@
           payloadLength = postings.readVInt();
           //System.out.println("PR     new payload len=" + payloadLength);
         }
-        position += code >> 1;
+position += code >>> 1;
         payloadRetrieved = false;
       } else {
         position += postings.readVInt();
---------------
-------------
@@ -87,7 +87,7 @@
     public java.io.InputStream getAsciiStream(String columnName) throws SQLException { throw notImplemented( "io.InputStream getAsciiStream" ); }
     public java.io.InputStream getUnicodeStream(String columnName) throws SQLException { throw notImplemented( "io.InputStream getUnicodeStream" ); }
     public java.io.InputStream getBinaryStream(String columnName) throws SQLException { throw notImplemented( "io.InputStream getBinaryStream" ); }
-    public SQLWarning getWarnings() throws SQLException { throw notImplemented( "getWarnings" ); }
+public SQLWarning getWarnings() throws SQLException { return null; }
     public void clearWarnings() throws SQLException { throw notImplemented( "clearWarnings" ); }
     public String getCursorName() throws SQLException { throw notImplemented( "getCursorName" ); }
     public Object getObject(int columnIndex) throws SQLException { throw notImplemented( "getObject" ); }
---------------
-------------
@@ -87,7 +87,7 @@
     reader = writer.getReader();
     searcher1 = newSearcher(reader);
     searcher2 = newSearcher(reader);
-    writer.close();
+writer.shutdown();
   }
 
   @Override
---------------
-------------
@@ -94,7 +94,7 @@
     writer.close(true);
 
     DirectoryReader reader = DirectoryReader.open(dir, 1);
-    assertEquals(1, reader.getSequentialSubReaders().size());
+assertEquals(1, reader.leaves().size());
 
     IndexSearcher searcher = new IndexSearcher(reader);
 
---------------
-------------
@@ -755,7 +755,7 @@
     Directory dir = newDirectory();
     IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
     cfg.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
+RandomIndexWriter iw = new RandomIndexWriter(random(), dir, cfg);
     Document doc = new Document();
     IntField field = new IntField("f", 0, Store.YES);
     doc.add(field);
---------------
-------------
@@ -35,7 +35,7 @@
     public void testSnitch() throws InterruptedException, IOException, ConfigurationException
     {
         // do this because SS needs to be initialized before DES can work properly.
-        StorageService.instance.initClient();
+StorageService.instance.initClient(0);
         int sleeptime = 150;
         DynamicEndpointSnitch dsnitch = new DynamicEndpointSnitch(new SimpleSnitch());
         InetAddress self = FBUtilities.getBroadcastAddress();
---------------
-------------
@@ -99,7 +99,7 @@
   public void map(IntWritable docId, VectorWritable document, Context context)
       throws IOException, InterruptedException{
     /* where to get docTopics? */
-    Vector topicVector = new DenseVector(new double[numTopics]).assign(1/numTopics);
+Vector topicVector = new DenseVector(new double[numTopics]).assign(1.0/numTopics);
     modelTrainer.train(document.get(), topicVector, true, maxIters);
   }
 
---------------
-------------
@@ -56,7 +56,7 @@
   
   @Override
   protected long size(SegmentInfo info) throws IOException {
-    long byteSize = info.sizeInBytes();
+long byteSize = info.sizeInBytes(true);
     float delRatio = (info.docCount <= 0 ? 0.0f : ((float)info.getDelCount() / (float)info.docCount));
     return (info.docCount <= 0 ?  byteSize : (long)((1.0f - delRatio) * byteSize));
   }
---------------
-------------
@@ -27,7 +27,7 @@
 
 public class Job {
 
-  public static void main(String[] args) throws Exception {
+public static void main(String[] args) throws IOException {
     if (args.length == 5) {
       String input = args[0];
       String output = args[1];
---------------
-------------
@@ -56,7 +56,7 @@
 
   @Override
   public void add(long v) throws IOException {
-    assert v >= 0 && v <= PackedInts.maxValue(bitsPerValue);
+assert bitsPerValue == 64 || (v >= 0 && v <= PackedInts.maxValue(bitsPerValue)) : bitsPerValue;
     assert !finished;
     if (valueCount != -1 && written >= valueCount) {
       throw new EOFException("Writing past end of stream");
---------------
-------------
@@ -73,7 +73,7 @@
                         
                         // Set time out: Stops DDMReader.fill() from
                         // waiting indefinitely when timeSlice is set.
-                        if (timeSlice != 0)
+if (timeSlice > 0)
                             clientSocket.setSoTimeout(timeSlice);
                         
                         //create a new Session for this socket
---------------
-------------
@@ -76,7 +76,7 @@
     public final static String mbrshipCleanerVerbHandler_ = "MBRSHIP-CLEANER-VERB-HANDLER";
     public final static String bsMetadataVerbHandler_ = "BS-METADATA-VERB-HANDLER";
     public final static String calloutDeployVerbHandler_ = "CALLOUT-DEPLOY-VERB-HANDLER";
-    public static String rangeVerbHandler_ = "RANGE-VERB-HANDLER";
+public final static String rangeVerbHandler_ = "RANGE-VERB-HANDLER";
 
     public static enum ConsistencyLevel
     {
---------------
-------------
@@ -504,7 +504,7 @@
             systemMeta.cfMetaData.put(SystemTable.STATUS_CF, new CFMetaData(Table.SYSTEM_TABLE,
                                                                             SystemTable.STATUS_CF,
                                                                             "Standard",
-                                                                            new UTF8Type(),
+new BytesType(),
                                                                             null,
                                                                             "persistent metadata for the local node",
                                                                             0.0,
---------------
-------------
@@ -107,7 +107,7 @@
   };
 
   public Query makeQuery(String queryText) throws ParseException {
-    Query q = (new QueryParser(Version.LUCENE_CURRENT, field, new WhitespaceAnalyzer())).parse(queryText);
+Query q = (new QueryParser(TEST_VERSION_CURRENT, field, new WhitespaceAnalyzer())).parse(queryText);
     return q;
   }
 
---------------
-------------
@@ -804,7 +804,7 @@
     public TopFieldDocs call() throws IOException {
       assert slice.leaves.length == 1;
       final TopFieldDocs docs = searcher.search(Arrays.asList(slice.leaves),
-          weight, after, nDocs, sort, true, doDocScores, doMaxScore);
+weight, after, nDocs, sort, true, doDocScores || sort.needsScores(), doMaxScore);
       lock.lock();
       try {
         final AtomicReaderContext ctx = slice.leaves[0];
---------------
-------------
@@ -76,7 +76,7 @@
 
         this.context = context;
         handlers = new NamespaceHandlerRegistryImpl(context);
-        executors = Executors.newScheduledThreadPool(3);
+executors = Executors.newScheduledThreadPool(3, new BlueprintThreadFactory("Blueprint Extender"));
         eventDispatcher = new BlueprintEventDispatcher(context, executors);
         containers = new HashMap<Bundle, BlueprintContainerImpl>();
 
---------------
-------------
@@ -405,7 +405,7 @@
   // input can be null, host, or url_prefix://host
   private String getHostAddress(String host) throws IOException {
 
-    if (host == null) {
+if (host == null || host.length() == 0) {
       String hostaddress;
       try {
         hostaddress = InetAddress.getLocalHost().getHostAddress();
---------------
-------------
@@ -310,7 +310,7 @@
         {
             for(byte[] c : del.predicate.column_names)
             {
-                if (del.super_column == null && DatabaseDescriptor.getColumnFamilyType(rm.table_, cfName).equals("Super"))
+if (del.super_column == null && DatabaseDescriptor.getColumnFamilyType(rm.table_, cfName) == ColumnFamilyType.Super)
                     rm.delete(new QueryPath(cfName, c), del.timestamp);
                 else
                     rm.delete(new QueryPath(cfName, del.super_column, c), del.timestamp);
---------------
-------------
@@ -34,7 +34,7 @@
   /** we will manually instantiate preflex-rw here */
   @BeforeClass
   public static void beforeClass() {
-    LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE = true;
+LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true;
   }
 
   private static String makeDifficultRandomUnicodeString(Random r) {
---------------
-------------
@@ -240,7 +240,7 @@
     }
 
     public void testSetBufferSize() throws IOException {
-      File indexDir = new File(TEMP_DIR, "testSetBufferSize");
+File indexDir = _TestUtil.getTempDir("testSetBufferSize");
       MockFSDirectory dir = new MockFSDirectory(indexDir, random);
       try {
         IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(
---------------
-------------
@@ -182,7 +182,7 @@
    */
   ParallelTaxonomyArrays add(int ordinal, int parentOrdinal) {
     if (ordinal >= parents.length) {
-      int[] newarray = ArrayUtil.grow(parents);
+int[] newarray = ArrayUtil.grow(parents, ordinal + 1);
       newarray[ordinal] = parentOrdinal;
       return new ParallelTaxonomyArrays(newarray);
     }
---------------
-------------
@@ -171,7 +171,7 @@
   {
     Object result = null;
     
-    result = ServiceHelper.getService(parentName.getInterface(), parentName.getFilter(), parentName.getServiceName(), name, false, env);
+result = ServiceHelper.getService(parentName, name, false, env);
     
     if (result == null) {
       throw new NameNotFoundException(name.toString());
---------------
-------------
@@ -110,7 +110,7 @@
     public static String string(ByteBuffer buffer, int offset, int length, Charset charset)
     {
         if (buffer.hasArray())
-            return new String(buffer.array(), buffer.arrayOffset() + offset, length + buffer.arrayOffset(), charset);
+return new String(buffer.array(), buffer.arrayOffset() + offset, length, charset);
 
         byte[] buff = getArray(buffer, offset, length);
         return new String(buff, charset);
---------------
-------------
@@ -36,7 +36,7 @@
         key[j] = (char)r.nextInt(127);
       }
       String keyStr = new String(key);
-      String hmapKey = ignoreCase ? keyStr.toLowerCase() : keyStr; 
+String hmapKey = ignoreCase ? keyStr.toLowerCase(Locale.ENGLISH) : keyStr;
 
       int val = r.nextInt();
 
---------------
-------------
@@ -2724,7 +2724,7 @@
     // or, at most the write.lock file
     final int extraFileCount;
     if (files.length == 1) {
-      assertEquals("write.lock", files[0]);
+assertTrue(files[0].endsWith("write.lock"));
       extraFileCount = 1;
     } else {
       assertEquals(0, files.length);
---------------
-------------
@@ -92,7 +92,7 @@
         		for(IColumn subColumn : subColumns)
         		{
         			if (offset_ <=0 ){
-        				filteredSuperColumn.addColumn(subColumn.name(), subColumn);
+filteredSuperColumn.addColumn(subColumn);
         				countLimit_--;
         			} else
         				offset_--;
---------------
-------------
@@ -1012,7 +1012,7 @@
                     SQLState.BLOB_NONPOSITIVE_LENGTH,
                     new Long(length));
         }
-        if (length > (this.length() - pos)) {
+if (length > (this.length() - (pos -1))) {
             throw Util.generateCsSQLException(
                     SQLState.POS_AND_LENGTH_GREATER_THAN_LOB,
                     new Long(pos), new Long(length));
---------------
-------------
@@ -318,7 +318,7 @@
       writer.addDocument(doc);
     }
     IndexReader reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     
     int numAsserts = atLeast(100);
     for (int i = 0; i < numAsserts; i++) {
---------------
-------------
@@ -35,7 +35,7 @@
 
   private boolean indexed;
   private boolean stored;
-  private boolean tokenized;
+private boolean tokenized = true;
   private boolean storeTermVectors;
   private boolean storeTermVectorOffsets;
   private boolean storeTermVectorPositions;
---------------
-------------
@@ -59,7 +59,7 @@
       eigenVectors.assignRow(i, v);
       i++;
     }
-    assertEquals("number of eigenvectors", 9, i);
+assertEquals("number of eigenvectors", 10, i);
   }
 
   @Test
---------------
-------------
@@ -357,7 +357,7 @@
         if (is == null)
             inFile = new BufferedReader(new FileReader(srcFile));
         else
-            inFile = new BufferedReader(new InputStreamReader(is));
+inFile = new BufferedReader(new InputStreamReader(is, "UTF-8"));
         outFile = new PrintWriter
         ( new BufferedWriter(new FileWriter(dstFile), 10000), true );
 
---------------
-------------
@@ -38,7 +38,7 @@
 
 		Returns the value returned by the method.
 
-		@exception 	StandardException	Standard Cloudscape error policy
+@exception 	StandardException	Standard Derby error policy
 	*/
 
 	public Object invoke(Object ref)
---------------
-------------
@@ -151,7 +151,7 @@
 	{
         for (SubqueryNode sqn : this)
 		{
-            if (sqn.getResultSet().referencesSessionSchema())
+if (sqn.referencesSessionSchema())
 			{
 				return true;
 			}
---------------
-------------
@@ -36,7 +36,7 @@
         key[j] = (char)r.nextInt(127);
       }
       String keyStr = new String(key);
-      String hmapKey = ignoreCase ? keyStr.toLowerCase() : keyStr; 
+String hmapKey = ignoreCase ? keyStr.toLowerCase(Locale.ENGLISH) : keyStr;
 
       int val = r.nextInt();
 
---------------
-------------
@@ -17,7 +17,7 @@
 package org.apache.solr.search;
 
 import org.apache.lucene.search.*;
-import org.apache.lucene.search.function.*;
+import org.apache.solr.search.function.*;
 import org.apache.lucene.queryParser.ParseException;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.Term;
---------------
-------------
@@ -312,7 +312,7 @@
         rm.apply();
 
         ColumnFamilyStore cfs = table.getColumnFamilyStore("Indexed2");
-        ColumnDefinition old = cfs.metadata.column_metadata.get(ByteBufferUtil.bytes("birthdate"));
+ColumnDefinition old = cfs.metadata.getColumn_metadata().get(ByteBufferUtil.bytes("birthdate"));
         ColumnDefinition cd = new ColumnDefinition(old.name, old.validator.getClass().getName(), IndexType.KEYS, "birthdate_index");
         cfs.addIndex(cd);
         while (!SystemTable.isIndexBuilt("Keyspace1", cfs.getIndexedColumnFamilyStore(ByteBufferUtil.bytes("birthdate")).columnFamily))
---------------
-------------
@@ -27,7 +27,7 @@
 
 
 /**
-  Class to hold a cloudscape Product version.
+Class to hold a Derby Product version.
 
   This class includes the following product version features.
 
---------------
-------------
@@ -54,7 +54,7 @@
     writer.close();
 
     // Delete one doc so we get a .del file:
-    IndexReader reader = IndexReader.open(dir);
+IndexReader reader = IndexReader.open(dir, false);
     Term searchTerm = new Term("id", "7");
     int delCount = reader.deleteDocuments(searchTerm);
     assertEquals("didn't delete the right number of documents", 1, delCount);
---------------
-------------
@@ -289,7 +289,7 @@
    * @see #classRules
    */
   private static final String [] IGNORED_INVARIANT_PROPERTIES = {
-    "user.timezone"
+"user.timezone", "java.rmi.server.randomIDs"
   };
 
   /** Filesystem-based {@link Directory} implementations. */
---------------
-------------
@@ -119,7 +119,7 @@
           if (term == null) {
             break;
           }
-          final Short termval = parser.parseShort(term);
+final short termval = parser.parseShort(term);
           docs = termsEnum.docs(null, docs);
           while (true) {
             final int docID = docs.nextDoc();
---------------
-------------
@@ -102,7 +102,7 @@
     }
     SolrSpellChecker spellChecker = getSpellChecker(params);
     if (params.getBool(SPELLCHECK_BUILD, false)) {
-      spellChecker.build(rb.req.getCore());
+spellChecker.build(rb.req.getCore(), rb.req.getSearcher());
       rb.rsp.add("command", "build");
     } else if (params.getBool(SPELLCHECK_RELOAD, false)) {
       spellChecker.reload();
---------------
-------------
@@ -84,7 +84,7 @@
     if (gen.nextDouble() < p) {
       target = 1;
     }
-    return new AdaptiveLogisticRegression.TrainingExample(i, target, data);
+return new AdaptiveLogisticRegression.TrainingExample(i, null, target, data);
   }
 
   @Test
---------------
-------------
@@ -540,7 +540,7 @@
         Thread.sleep(100);
         removed = !zkStateReader.getClusterState().hasCollection(message.getStr(collection));
         if (removed) {
-          Thread.sleep(100); // just a bit of time so it's more likely other
+Thread.sleep(300); // just a bit of time so it's more likely other
                              // readers see on return
           break;
         }
---------------
-------------
@@ -197,7 +197,7 @@
             /* seek to the lowest position where any CF has non-flushed data */
             int lowPos = CommitLogHeader.getLowestPosition(clHeader);
             if (lowPos == 0)
-                break;
+continue;
 
             reader.seek(lowPos);
             if (logger.isDebugEnabled())
---------------
-------------
@@ -1158,7 +1158,7 @@
 
       setRollbackSegmentInfos(segmentInfos);
 
-      docWriter = new DocumentsWriter(directory, this, conf.getIndexingChain());
+docWriter = new DocumentsWriter(directory, this, conf.getIndexingChain(), conf.getMaxThreadStates());
       docWriter.setInfoStream(infoStream);
       docWriter.setMaxFieldLength(maxFieldLength);
 
---------------
-------------
@@ -561,7 +561,7 @@
     if (updateHandler == null) {
       initDirectoryFactory();
     } else {
-      directoryFactory = updateHandler.getIndexWriterProvider().getDirectoryFactory();
+directoryFactory = updateHandler.getSolrCoreState().getDirectoryFactory();
     }
     
     initIndex();
---------------
-------------
@@ -69,7 +69,7 @@
     double longUpperRight = upperRight.getLng();
     double longLowerLeft = lowerLeft.getLng();
 
-    CartesianTierPlotter ctp = new CartesianTierPlotter( miles, projector, tierPrefix, minTier, maxTier );
+CartesianTierPlotter ctp = new CartesianTierPlotter( CartesianTierPlotter.bestFit(miles, minTier, maxTier), projector, tierPrefix);
     Shape shape = new Shape(ctp.getTierLevelId());
 
     if (longUpperRight < longLowerLeft) { // Box cross the 180 meridian
---------------
-------------
@@ -658,7 +658,7 @@
 		// 1)start a read-only global transaction 
 		// 2)finish that read-only transaction
 		// 3)start another global transaction 
-		System.out.println("TESTING READ_ONLY TRANSACTION FOLLOWED BY WRTIABLE TRANSACTION");
+System.out.println("TESTING READ_ONLY TRANSACTION FOLLOWED BY WRITABLE TRANSACTION");
 		XAConnection xac5 = dsx.getXAConnection();
 		Xid xid5a = new cdsXid(5, (byte) 119, (byte) 129);
 		Connection conn5 = xac5.getConnection();
---------------
-------------
@@ -272,7 +272,7 @@
         case 0: queryShape = randomPoint(); break;
         case 1:case 2:case 3:
           if (!indexedAtLeastOneShapePair) { // avoids ShapePair.relate(ShapePair), which isn't reliable
-            queryShape = randomShapePairRect(biasContains);
+queryShape = randomShapePairRect(!biasContains);//invert biasContains for query side
             break;
           }
         default: queryShape = randomRectangle();
---------------
-------------
@@ -276,7 +276,7 @@
     writer.addDocument(doc);
     writer.commit();
     SegmentCommitInfo info = writer.newestSegment();
-    writer.close();
+writer.shutdown();
     return info;
   }
 
---------------
-------------
@@ -360,7 +360,7 @@
      * Set that the database is encrypted. Read-only database can not 
      * be reencrypted, nothing to do in this case. 
      */
-    public void setDatabaseEncrypted()
+public void setDatabaseEncrypted(boolean flushLog)
     {
         // nothing to do for a read-only database.
     }
---------------
-------------
@@ -220,7 +220,7 @@
         }
 
         File file = new File(dataFile);
-        assert file.exists();
+assert file.exists() : "attempted to delete non-existing file " + dataFile;
         /* delete the data file */
         if (!file.delete())
         {
---------------
-------------
@@ -166,7 +166,7 @@
       Automaton expected = BasicOperations.intersection(termsAutomaton, automaton);
       TreeSet<BytesRef> found = new TreeSet<BytesRef>();
       while (te.next() != null) {
-        found.add(new BytesRef(te.term()));
+found.add(BytesRef.deepCopyOf(te.term()));
       }
       
       Automaton actual = DaciukMihovAutomatonBuilder.build(found);     
---------------
-------------
@@ -282,7 +282,7 @@
   /**
    * Class environment setup rule.
    */
-  public static final TestRuleSetupAndRestoreClassEnv classEnvRule;
+static final TestRuleSetupAndRestoreClassEnv classEnvRule;
 
   /**
    * Suite failure marker (any error in the test or suite scope).
---------------
-------------
@@ -52,7 +52,7 @@
   /* CHECKME: These should be the same as for the tokenizer. How? */
   final char truncator = '*';
   final char anyChar = '?';
-  final char quote = '\u005c"';
+final char quote = '"';
   final char fieldOperator = ':';
   final char comma = ','; /* prefix list separator */
   final char carat = '^'; /* weight operator */
---------------
-------------
@@ -74,7 +74,7 @@
         writer.optimize();
         writer.close();
 
-        r = IndexReader.open(index);
+r = IndexReader.open(index, true);
         s = new IndexSearcher(r);
 
 //System.out.println("Set up " + getName());
---------------
-------------
@@ -35,6 +35,6 @@
     */
 
     public InterruptDetectedException() {
-        super("nospc.U");
+super("intrp.U");
     }
 }
---------------
-------------
@@ -175,7 +175,7 @@
                 writer.append(key, bytes);
             }
         }
-        cfStore.storeLocation(writer.closeAndOpenReader());
+cfStore.addSSTable(writer.closeAndOpenReader());
         columnFamilies_.clear();       
     }
 }
---------------
-------------
@@ -26,7 +26,7 @@
 import org.apache.lucene.index.codecs.standard.*;
 import org.apache.lucene.index.codecs.pulsing.*;
 import org.apache.lucene.index.values.DocValues;
-import org.apache.lucene.index.values.codec.DocValuesConsumer;
+import org.apache.lucene.index.codecs.docvalues.DocValuesConsumer;
 import org.apache.lucene.store.*;
 import java.util.*;
 import java.io.*;
---------------
-------------
@@ -110,7 +110,7 @@
     final int index;
     String current;
 
-    public FieldsEnumWithSlice(FieldsEnum fields, ReaderSlice slice, int index) throws IOException {
+public FieldsEnumWithSlice(FieldsEnum fields, ReaderSlice slice, int index) {
       this.slice = slice;
       this.index = index;
       assert slice.length >= 0: "length=" + slice.length;
---------------
-------------
@@ -18,7 +18,7 @@
 import java.util.ArrayList;
 import java.util.Collection;
 
-import org.apache.aries.subsystem.core.internal.ResourceHelper;
+import org.apache.aries.subsystem.core.ResourceHelper;
 import org.osgi.framework.wiring.Capability;
 import org.osgi.framework.wiring.Requirement;
 import org.osgi.framework.wiring.Resource;
---------------
-------------
@@ -50,7 +50,7 @@
     public final static double DEFAULT_ROW_CACHE_SIZE = 0.0;
     public final static double DEFAULT_KEY_CACHE_SIZE = 200000;
     public final static double DEFAULT_READ_REPAIR_CHANCE = 1.0;
-    public final static boolean DEFAULT_REPLICATE_ON_WRITE = true;
+public final static boolean DEFAULT_REPLICATE_ON_WRITE = false;
     public final static int DEFAULT_ROW_CACHE_SAVE_PERIOD_IN_SECONDS = 0;
     public final static int DEFAULT_KEY_CACHE_SAVE_PERIOD_IN_SECONDS = 3600;
     public final static int DEFAULT_GC_GRACE_SECONDS = 864000;
---------------
-------------
@@ -445,7 +445,7 @@
     final String s = a.toDot();
     Writer w = new OutputStreamWriter(new FileOutputStream("/x/tmp/out.dot"));
     w.write(s);
-    w.close();
+w.shutdown();
     System.out.println("TEST: saved to /x/tmp/out.dot");
   }
   */
---------------
-------------
@@ -134,7 +134,7 @@
     Collections.sort(dirs);
     int i=1;
     for(OldBackupDirectory dir : dirs) {
-      if( i > numberToKeep-1 ) {
+if( i++ > numberToKeep-1 ) {
         SnapPuller.delTree(dir.dir);
       }
     }   
---------------
-------------
@@ -180,7 +180,7 @@
   private static final class HyperbolicSweetSpotSimilarity 
     extends SweetSpotSimilarity {
     @Override
-    public float tf(int freq) {
+public float tf(float freq) {
       return hyperbolicTf(freq);
     }
   };
---------------
-------------
@@ -93,7 +93,7 @@
     } else {
       String s = "gen=" + gen;
       if (numTermDeletes.get() != 0) {
-        s += " " + numTermDeletes.get() + " deleted terms (unique count=" + terms.size() + ") terms=" + terms.keySet();
+s += " " + numTermDeletes.get() + " deleted terms (unique count=" + terms.size() + ")";
       }
       if (queries.size() != 0) {
         s += " " + queries.size() + " deleted queries";
---------------
-------------
@@ -163,7 +163,7 @@
         }
       };
 
-      checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 200);
     }
   }
   
---------------
-------------
@@ -231,7 +231,7 @@
     }
     
     // read the patch string
-    StringBuffer result = new StringBuffer();
+StringBuilder result = new StringBuilder();
     final char base = 'a' - 1;
     char deletes = base;
     char equals = base;
---------------
-------------
@@ -56,7 +56,7 @@
   }
 
   @Override
-  public long toLongID(String stringID) throws TasteException {
+public long toLongID(String stringID) {
     return hash(stringID);
   }
 
---------------
-------------
@@ -72,7 +72,7 @@
     int num = atLeast(200);
     for (int i = 0; i < num; i++) {
       String s = _TestUtil.randomUnicodeString(random);
-      field.setValue(s);
+field.setStringValue(s);
       terms.add(s);
       writer.addDocument(doc);
     }
---------------
-------------
@@ -61,7 +61,7 @@
 import org.apache.lucene.index.IndexReader;
 
 /** A Query that matches documents within an exclusive range. */
-public final class RangeQuery extends Query
+public class RangeQuery extends Query
 {
     private Term lowerTerm;
     private Term upperTerm;
---------------
-------------
@@ -134,7 +134,7 @@
     if (!termsEnum.seekExact(termBytes, false)) {
       return -1;
     }
-    DocsEnum docs = termsEnum.docs(MultiFields.getLiveDocs(r), null, 0);
+DocsEnum docs = termsEnum.docs(MultiFields.getLiveDocs(r), null, false);
     int id = docs.nextDoc();
     if (id != DocIdSetIterator.NO_MORE_DOCS) {
       int next = docs.nextDoc();
---------------
-------------
@@ -113,7 +113,7 @@
       for (ShardRequest sreq : rb.finished) {
         if ((sreq.purpose & ShardRequest.PURPOSE_GET_DEBUG) == 0) continue;
         for (ShardResponse srsp : sreq.responses) {
-          NamedList sdebug = (NamedList)srsp.rsp.getResponse().get("debug");
+NamedList sdebug = (NamedList)srsp.getSolrResponse().getResponse().get("debug");
           info = (NamedList)merge(sdebug, info, excludeSet);
 
           NamedList sexplain = (NamedList)sdebug.get("explain");
---------------
-------------
@@ -95,7 +95,7 @@
     public RequestSchedulerId request_scheduler_id;
     public RequestSchedulerOptions request_scheduler_options;
 
-    public RawKeyspace[] keyspaces;
+public List<RawKeyspace> keyspaces;
     
     public static enum CommitLogSync {
         periodic,
---------------
-------------
@@ -166,7 +166,7 @@
 			return;
 		}
 		if ( locale == null || locale.toString().equals("none") ){
-			res = ResourceBundle.getBundle(MESSAGE_FILE);
+res = ResourceBundle.getBundle(messageFileName);
 		}
 		else
 		try {
---------------
-------------
@@ -80,7 +80,7 @@
     boolean success = false;
     try {
       version = CodecUtil.checkHeader(in, metaCodec, 
-                                      Lucene45DocValuesFormat.VERSION_CURRENT,
+Lucene45DocValuesFormat.VERSION_START,
                                       Lucene45DocValuesFormat.VERSION_CURRENT);
       numerics = new HashMap<Integer,NumericEntry>();
       ords = new HashMap<Integer,NumericEntry>();
---------------
-------------
@@ -34,7 +34,7 @@
 /**
  * CQL query compression
  */
-public enum Compression implements TEnum {
+public enum Compression implements org.apache.thrift.TEnum {
   GZIP(1);
 
   private final int value;
---------------
-------------
@@ -148,7 +148,7 @@
           //4.1 Query a small box getting nothing
           checkHits(q(queryCenter, radiusDeg - smallRadius/2), 0, null);
           //4.2 Query a large box enclosing the cluster, getting everything
-          checkHits(q(queryCenter, radiusDeg*3*1.01), points.size(), null);
+checkHits(q(queryCenter, radiusDeg*3 + smallRadius/2), points.size(), null);
           //4.3 Query a medium box getting some (calculate the correct solution and verify)
           double queryDist = radiusDeg * 2;
 
---------------
-------------
@@ -100,7 +100,7 @@
     assertEquals(2, hits.length);
     
     // test parsable toString()
-    QueryParser qp = new QueryParser("key", analyzer);
+QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "key", analyzer);
     hits = is.search(qp.parse(new MatchAllDocsQuery().toString()), null, 1000).scoreDocs;
     assertEquals(2, hits.length);
 
---------------
-------------
@@ -143,7 +143,7 @@
   }
 
   public void testBoostsSimple() throws Exception {
-    Map<CharSequence,Float> boosts = new HashMap<CharSequence,Float>();
+Map<String,Float> boosts = new HashMap<String,Float>();
     boosts.put("b", Float.valueOf(5));
     boosts.put("t", Float.valueOf(10));
     String[] fields = { "b", "t" };
---------------
-------------
@@ -67,7 +67,7 @@
 
     public BoostedWeight(Searcher searcher) throws IOException {
       this.searcher = searcher;
-      this.qWeight = q.weight(searcher);
+this.qWeight = q.createWeight(searcher);
       this.context = boostVal.newContext();
       boostVal.createWeight(context,searcher);
     }
---------------
-------------
@@ -157,7 +157,7 @@
 
   @Override
   public String toString() {
-    return super.toString() + " lockFactory=" + getLockFactory();
+return getClass().getSimpleName() + '@' + Integer.toHexString(hashCode()) + " lockFactory=" + getLockFactory();
   }
 
   /**
---------------
-------------
@@ -28,5 +28,5 @@
      * This callback if registered with the StreamContextManager is 
      * called when the stream from a host is completely handled. 
     */
-    public void onStreamCompletion(InetAddress from, PendingFile pendingFile, CompletedFileStatus streamStatus) throws IOException;
+public void onStreamCompletion(InetAddress from, PendingFile pendingFile, FileStatus streamStatus) throws IOException;
 }
---------------
-------------
@@ -371,7 +371,7 @@
                         // if we're not writing to the commit log, we are replaying the log, so marking
                         // the log header with "you can discard anything written before the context" is not valid
                         final int cfId = DatabaseDescriptor.getTableMetaData(table_).get(columnFamily_).cfId;
-                        logger_.info("Discarding " + cfId);
+logger_.debug("Discarding {}", cfId);
                         CommitLog.instance().discardCompletedSegments(cfId, ctx);
                     }
                 }
---------------
-------------
@@ -533,7 +533,7 @@
   private final static class FSTTermsEnum extends TermsEnum {
     private final FieldInfo field;
     private final BytesRefFSTEnum<BytesRef> fstEnum;
-    private final ByteArrayDataInput buffer = new ByteArrayDataInput(null);
+private final ByteArrayDataInput buffer = new ByteArrayDataInput();
     private boolean didDecode;
 
     private int docFreq;
---------------
-------------
@@ -704,7 +704,7 @@
                     SQLState.BLOB_NONPOSITIVE_LENGTH,
                     new Long(length));
         }
-        if (length > (this.length() - pos)) {
+if (length > (this.length() - (pos -1))) {
             throw Util.generateCsSQLException(
                     SQLState.POS_AND_LENGTH_GREATER_THAN_LOB,
                     new Long(pos), new Long(length));
---------------
-------------
@@ -254,7 +254,7 @@
       fieldExpl.setDescription("fieldWeight("+field+":"+query+" in "+doc+
                                "), product of:");
 
-      Scorer scorer = (Scorer) scorer(reader, true, false);
+Scorer scorer = scorer(reader, true, false);
       if (scorer == null) {
         return new Explanation(0.0f, "no matching docs");
       }
---------------
-------------
@@ -56,7 +56,7 @@
       }
 
       @Override
-      protected Reader initReader(Reader reader) {
+protected Reader initReader(String fieldName, Reader reader) {
         reader = new MockCharFilter(reader, 0);
         reader = new MappingCharFilter(map, reader);
         return reader;
---------------
-------------
@@ -347,7 +347,7 @@
       throw new IllegalArgumentException("cannot change value type from " + fieldsData.getClass().getSimpleName() + " to BytesRef");
     }
     if (type.indexed()) {
-      throw new IllegalArgumentException("cannot set a Reader value on an indexed field");
+throw new IllegalArgumentException("cannot set a BytesRef value on an indexed field");
     }
     fieldsData = value;
   }
---------------
-------------
@@ -1598,7 +1598,7 @@
             String apppropsjvmflags = ap.getProperty("jvmflags");
             if (apppropsjvmflags != null)
             {
-                if (jvmflags != null)
+if (jvmflags != null && jvmflags.length() > 0)
                     jvmflags = apppropsjvmflags + "^" + jvmflags;
                 else
                     jvmflags = apppropsjvmflags;
---------------
-------------
@@ -109,7 +109,7 @@
   }
   
   @Override
-  public void end() throws IOException {
+public void end() {
     final int finalOffset = (length < 0) ? offset : offset + length;
     offsetAtt.setOffset(correctOffset(finalOffset), correctOffset(finalOffset));
   }  
---------------
-------------
@@ -42,6 +42,6 @@
 
 public class Constants {
 
-  public static final String VERSION = "11.1.0";
+public static final String VERSION = "12.0.0";
 
 }
---------------
-------------
@@ -386,7 +386,7 @@
     }
 
     @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) {
+public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) {
       return new RAMDocsEnum(ramField.termToDocs.get(current), liveDocs);
     }
 
---------------
-------------
@@ -576,7 +576,7 @@
         return columnFamiliesMap;
     }
 
-    public List<String> get_key_range(String tablename, String columnFamily, String startWith, String stopAt, int maxResults) throws InvalidRequestException, TException
+public List<String> get_key_range(String tablename, String columnFamily, String startWith, String stopAt, int maxResults, int consistency_level) throws InvalidRequestException, TException
     {
         if (logger.isDebugEnabled())
             logger.debug("get_key_range");
---------------
-------------
@@ -62,7 +62,7 @@
     this.pre = (pre >=0) ? pre : 0;
     this.post = (post >= 0) ? post : 0;
 
-    if (!include.getField().equals(exclude.getField()))
+if (include.getField() != null && exclude.getField() != null && !include.getField().equals(exclude.getField()))
       throw new IllegalArgumentException("Clauses must have same field.");
   }
 
---------------
-------------
@@ -89,7 +89,7 @@
             throw new RuntimeException("Cannot read system table! Are you upgrading a pre-release version?");
 
         ByteBuffer value = avroschema.value();
-        Schema schema = Schema.parse(ByteBufferUtil.string(value, value.position(), value.remaining()));
+Schema schema = Schema.parse(ByteBufferUtil.string(value));
 
         // deserialize keyspaces using schema
         Collection<KSMetaData> keyspaces = new ArrayList<KSMetaData>();
---------------
-------------
@@ -606,7 +606,7 @@
 						+ " SQLSTATE: " + m);
 			}
 		}
-		if (e.getMessage().equals(null)) {
+if (e.getMessage() == null) {
 			System.out.println("NULL error message detected");
 			System.out.println("Here is the NULL exection - " + e.toString());
 			System.out.println("Stack trace of the NULL exception - ");
---------------
-------------
@@ -170,7 +170,7 @@
     List<VectorWritable> points = getPointsWritable(REFERENCE);
     List<VectorWritable> seeds = getPointsWritable(SEEDS);
 
-    Configuration conf = new Configuration();
+Configuration conf = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points, true, new Path(input, "file1"), fs, conf);
     ClusteringTestUtils.writePointsToFile(seeds, true, new Path(seedsPath, "part-seeds"), fs, conf);
 
---------------
-------------
@@ -49,7 +49,7 @@
     try {
       Map<String,Object> context = new HashMap<String,Object>();
       assert scorer != null;
-      context.put("scorer", new ScoreFunctionValues(scorer));
+context.put("scorer", scorer);
       scores = source.getValues(context, readerContext);
     } catch (IOException e) {
       throw new RuntimeException(e);
---------------
-------------
@@ -61,7 +61,7 @@
         assert splitPoint != -1;
 
         // and decode the token and key
-        String token = ByteBufferUtil.string(fromdisk, fromdisk.position(), splitPoint, UTF_8);
+String token = ByteBufferUtil.string(fromdisk, fromdisk.position(), splitPoint - fromdisk.position(), UTF_8);
         ByteBuffer key = fromdisk.duplicate();
         key.position(splitPoint + 1);
         return new DecoratedKey<BigIntegerToken>(new BigIntegerToken(token), key);
---------------
-------------
@@ -42,7 +42,7 @@
     Set articles = new HashSet();
     articles.add("l");
     articles.add("M");
-    TokenFilter filter = new ElisionFilter(tokenizer, articles);
+TokenFilter filter = new ElisionFilter(Version.LUCENE_CURRENT, tokenizer, articles);
     List tas = filtre(filter);
     assertEquals("embrouille", tas.get(4));
     assertEquals("O'brian", tas.get(6));
---------------
-------------
@@ -107,7 +107,7 @@
     modelPaths.put("thetaNormalizer", new Path(modelBasePath + "/trainer-thetaNormalizer/part-*"));
     modelPaths.put("weight", new Path(modelBasePath + "/trainer-tfIdf/trainer-tfIdf/part-*"));
 
-    FileSystem fs = FileSystem.get(conf);
+FileSystem fs = FileSystem.get((new Path(modelBasePath)).toUri(), conf);
 
     log.info("Loading model from: {}", modelPaths);
 
---------------
-------------
@@ -21,7 +21,7 @@
  */
 
 
-import org.apache.cassandra.config.DatabaseDescriptor.ConfigurationException;
+import org.apache.cassandra.config.ConfigurationException;
 import org.apache.cassandra.thrift.AccessLevel;
 import org.apache.cassandra.thrift.AuthenticationException;
 import org.apache.cassandra.thrift.AuthenticationRequest;
---------------
-------------
@@ -124,7 +124,7 @@
     final Random random = new Random(random().nextLong());
     for (int i=0; i<iter; i++) {
       tenum.seekCeil(new BytesRef("val"));
-      tdocs = _TestUtil.docs(random, tenum, MultiFields.getLiveDocs(reader), tdocs, false);
+tdocs = _TestUtil.docs(random, tenum, MultiFields.getLiveDocs(reader), tdocs, 0);
       while (tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
         ret += tdocs.docID();
       }
---------------
-------------
@@ -37,7 +37,7 @@
   /** Returns a collection of all terms matched by this query.*/
   public abstract Collection getTerms();
 
-  protected Weight createWeight(Searcher searcher) {
+protected Weight createWeight(Searcher searcher) throws IOException {
     return new SpanWeight(this, searcher);
   }
 
---------------
-------------
@@ -142,7 +142,7 @@
     // dodge jre bug http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7104012
     final UncaughtExceptionHandler savedHandler = Thread.getDefaultUncaughtExceptionHandler();
     Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
-      @Override
+// Not in Java 5: @Override
       public void uncaughtException(Thread thread, Throwable throwable) {
         assumeTrue("not failing due to jre bug ", !isJREBug7104012(throwable));
         // otherwise its some other bug, pass to default handler
---------------
-------------
@@ -36,7 +36,7 @@
     return new HindiNormalizationFilter(input);
   }
   
-  @Override
+//@Override
   public Object getMultiTermComponent() {
     return this;
   }
---------------
-------------
@@ -93,7 +93,7 @@
 				case Types.LONGVARCHAR:
 				case Types.CLOB:
 					break;
-				case org.apache.derby.iapi.reference.JDBC20Translation.SQL_TYPES_JAVA_OBJECT:
+case Types.JAVA_OBJECT:
 				case Types.OTHER:	
 				{
 					throw StandardException.newException(SQLState.LANG_UNARY_FUNCTION_BAD_TYPE, 
---------------
-------------
@@ -164,7 +164,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new FrenchAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new FrenchAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
   
   /** test accent-insensitive */
---------------
-------------
@@ -1420,7 +1420,7 @@
   }
 
 
-  private Fields fields;
+private volatile Fields fields;
 
   /** @lucene.internal */
   void storeFields(Fields fields) {
---------------
-------------
@@ -3892,7 +3892,7 @@
           mergeInit(merge);
 
           if (infoStream != null)
-            message("now merge\n  merge=" + merge.segString(directory) + "\n  merge=" + merge + "\n  index=" + segString());
+message("now merge\n  merge=" + merge.segString(directory) + "\n  index=" + segString());
 
           mergeMiddle(merge);
           mergeSuccess(merge);
---------------
-------------
@@ -786,7 +786,7 @@
         }
       }
 
-      if (writer != null) writer.close();
+if (writer != null) writer.shutdown();
 
     } finally {
       solrCoreState.getCommitLock().unlock();
---------------
-------------
@@ -134,7 +134,7 @@
 	 */
 	public float	getFloat() throws StandardException
 	{
-		if (Math.abs(value) > Float.MAX_VALUE)
+if (Float.isInfinite((float)value))
 			throw StandardException.newException(SQLState.LANG_OUTSIDE_RANGE_FOR_DATATYPE, TypeId.REAL_NAME);
 		return (float) value;
 	}
---------------
-------------
@@ -195,7 +195,7 @@
       if (c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')'
           || c == ':' || c == '^' || c == '[' || c == ']' || c == '\"'
           || c == '{' || c == '}' || c == '~' || c == '*' || c == '?'
-          || c == '|' || c == '&') {
+|| c == '|' || c == '&' || c == '/') {
         sb.append('\\');
       }
       sb.append(c);
---------------
-------------
@@ -44,7 +44,7 @@
    * passed to createWeight()
    */
   public DocValues getValues(Map context, IndexReader reader) throws IOException {
-    return null;
+return getValues(reader);
   }
 
   public abstract boolean equals(Object o);
---------------
-------------
@@ -62,7 +62,7 @@
         }
         if (value == null) {
           value = new CreationPlaceholder();
-          innerCache.put(reader, value);
+innerCache.put(key, value);
         }
       }
       if (value instanceof CreationPlaceholder) {
---------------
-------------
@@ -1613,7 +1613,7 @@
     public void listenToUnitOfWork() {
         if (!listenToUnitOfWork_) {
             listenToUnitOfWork_ = true;
-            connection_.CommitAndRollbackListeners_.add(this);
+connection_.CommitAndRollbackListeners_.put(this,null);
         }
     }
 
---------------
-------------
@@ -60,7 +60,7 @@
     // delete the output directory
     JobConf conf = new JobConf(DirichletJob.class);
     Path outPath = new Path(output);
-    FileSystem fs = FileSystem.get(conf);
+FileSystem fs = FileSystem.get(outPath.toUri(), conf);
     if (fs.exists(outPath)) {
       fs.delete(outPath, true);
     }
---------------
-------------
@@ -95,7 +95,7 @@
         return udpAddr_;
     }
 
-    public static IPartitioner getPartitioner() {
+public static IPartitioner<?> getPartitioner() {
         return partitioner_;
     }
 
---------------
-------------
@@ -179,7 +179,7 @@
      * @param conChild connection object used to obtain synchronization object
      */
     TemporaryClob (String data, ConnectionChild conChild)
-                          throws IOException, SQLException, StandardException {
+throws IOException, StandardException {
         if (conChild == null) {
             throw new NullPointerException("conChild cannot be <null>");
         }
---------------
-------------
@@ -245,7 +245,7 @@
         MessagingService.instance.registerVerbHandlers(Verb.SCHEMA_CHECK, new SchemaCheckVerbHandler());
 
         replicationStrategies = new HashMap<String, AbstractReplicationStrategy>();
-        for (String table : DatabaseDescriptor.getNonSystemTables())
+for (String table : DatabaseDescriptor.getTables())
             initReplicationStrategy(table);
 
         // spin up the streaming serivice so it is available for jmx tools.
---------------
-------------
@@ -63,7 +63,7 @@
       ParameteredGeneralizations.configureParameters(this, jobConf);
     }
     try {
-      FileSystem fs = FileSystem.get(jobConf);
+FileSystem fs = FileSystem.get(weightsFile.get().toUri(), jobConf);
       if (weightsFile.get() != null) {
         Vector weights = (Vector) vectorClass.get().newInstance();
         if (!fs.exists(weightsFile.get())) {
---------------
-------------
@@ -132,7 +132,7 @@
         return timestamp;
     }
 
-    public long mostRecentChangeAt()
+public long mostRecentLiveChangeAt()
     {
         return timestamp;
     }
---------------
-------------
@@ -3085,7 +3085,7 @@
     checkpoint();
 
     if (infoStream.isEnabled("IW")) {
-      infoStream.message("IW", "after commit: " + segString());
+infoStream.message("IW", "after commitMerge: " + segString());
     }
 
     if (merge.maxNumSegments != -1 && !dropSegment) {
---------------
-------------
@@ -153,7 +153,7 @@
 					associationState = TRO_FAIL;
 					if (SQLState.DEADLOCK.equals(se.getMessageId()))
 						rollbackOnlyCode = XAException.XA_RBDEADLOCK;
-					else if (SQLState.LOCK_TIMEOUT.equals(se.getMessageId()))
+else if (se.isLockTimeout())
 						rollbackOnlyCode = XAException.XA_RBTIMEOUT;					
 					else
 						rollbackOnlyCode = XAException.XA_RBOTHER;
---------------
-------------
@@ -36,7 +36,7 @@
         this.map = map;
     }
 
-    public Multiple(Properties props) {
+public Multiple(Properties props, String disambiguator) {
         this.properties = props;
     }
 
---------------
-------------
@@ -49,7 +49,7 @@
  * to this class.
  */
 // nocommit don't suppress any:
-@SuppressCodecs({"Direct", "Memory", "Lucene41", "MockRandom"})
+@SuppressCodecs({"Direct", "Memory", "Lucene41", "MockRandom", "Lucene40", "Compressing"})
 public class TestDemoDocValue extends LuceneTestCase {
 
   public void testDemoNumber() throws IOException {
---------------
-------------
@@ -79,7 +79,7 @@
         cf = ColumnFamily.serializer().deserialize(new DataInputStream(bufIn));
         for (String cName : map.navigableKeySet())
         {
-            ByteBuffer val = cf.getColumn(ByteBuffer.wrap(cName.getBytes())).value();
+ByteBuffer val = cf.getColumn(ByteBufferUtil.bytes(cName)).value();
             assert new String(val.array(),val.position(),val.remaining()).equals(map.get(cName));
         }
         assert cf.getColumnNames().size() == map.size();
---------------
-------------
@@ -43,7 +43,7 @@
 
 /**
 
-	EmbeddedXADataSource is Derby's XADataSource implementation.
+EmbeddedXADataSource is Derby's XADataSource implementation for JDBC3.0 and JDBC2.0.
 	
 
 	<P>An XADataSource is a factory for XAConnection objects.  It represents a
---------------
-------------
@@ -247,7 +247,7 @@
     return doReopen(false);
   }
     
-  protected IndexReader doReopen(boolean doClone) throws CorruptIndexException, IOException {
+private IndexReader doReopen(boolean doClone) throws CorruptIndexException, IOException {
     ensureOpen();
     
     boolean reopened = false;
---------------
-------------
@@ -247,7 +247,7 @@
       if (staticStats.contains(attribute) && attribute != null
               && attribute.length() > 0) {
         try {
-          String getter = "get" + attribute.substring(0, 1).toUpperCase()
+String getter = "get" + attribute.substring(0, 1).toUpperCase(Locale.ENGLISH)
                   + attribute.substring(1);
           Method meth = infoBean.getClass().getMethod(getter);
           val = meth.invoke(infoBean);
---------------
-------------
@@ -229,7 +229,7 @@
                                                          "",
                                                          gen);
         long modTime = dir.fileModified(fileName);
-        assertTrue("commit point was older than " + SECONDS + " seconds but did not get deleted", lastDeleteTime - modTime < (SECONDS*1000));
+assertTrue("commit point was older than " + SECONDS + " seconds (" + (lastDeleteTime - modTime) + " msec) but did not get deleted", lastDeleteTime - modTime <= (SECONDS*1000));
       } catch (IOException e) {
         // OK
         break;
---------------
-------------
@@ -78,7 +78,7 @@
 	<LI>TABLETYPE varchar(9) - not nullable.  'T' for user table, 'S' for system table </LI>
 	<LI>LOCKCOUNT varchar(5) - not nullable.  Internal lock count.</LI>
 	<LI>INDEXNAME varchar(128) - normally null.  If non-null, a lock is held on 
-	the index, this can only happen if this is not a user transaction.</LI>
+the index.</LI>
 	</UL>
 
  */
---------------
-------------
@@ -168,7 +168,7 @@
       Document doc = FileDocument.Document(file);
       writer.addDocument(doc);
       writer.flush();
-      return writer.segmentInfos.info(writer.segmentInfos.size()-1);
+return writer.newestSegment();
    }
 
 
---------------
-------------
@@ -328,7 +328,7 @@
 			)
 			{
 				if (server != null)
-					server.consoleExceptionPrintTrace(e);
+server.consoleExceptionPrint(e);
 				else
 					e.printStackTrace();  // default output stream is System.out
 			}
---------------
-------------
@@ -1756,7 +1756,7 @@
         // Passing in null to prevent uninteresting failures.
         assertStatsOK(st, 
             indexOrConstraint, "FOO", indexName, 
-            "{0}", "1", "0", "1", "0", "0", "btree",
+"{0}", "1", "0", null, "0", "0", "btree",
             ">= on first 1 column(s).","> on first 1 column(s).","None", null);
 
         st.execute("drop table foo");
---------------
-------------
@@ -33,7 +33,7 @@
 	is minimally covered.
 */
 
-class executeUpdate
+public class executeUpdate
 {
 
 	public static void main (String args[])
---------------
-------------
@@ -898,7 +898,7 @@
         if (len == 0) {
             return 0;
         }
-        if (len + offset > bytes.length) {
+if (len > bytes.length - offset) {
             throw Util.generateCsSQLException(SQLState.BLOB_LENGTH_TOO_LONG,
                     new Long(len));
         }
---------------
-------------
@@ -76,7 +76,7 @@
     }
     finally {
       writer.commit();
-      writer.close();
+writer.shutdown();
       ramDir.close();
     }
   }
---------------
-------------
@@ -220,7 +220,7 @@
       } else if (t <= 1) {
         chars[i++] = (char) random.nextInt(0x80);
       } else if (2 == t) {
-        chars[i++] = (char) nextInt(random, 0x80, 0x800);
+chars[i++] = (char) nextInt(random, 0x80, 0x7ff);
       } else if (3 == t) {
         chars[i++] = (char) nextInt(random, 0x800, 0xd7ff);
       } else if (4 == t) {
---------------
-------------
@@ -48,7 +48,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -554,7 +554,7 @@
 
     assertNotNull("null DocsEnum", docsEnum);
     int initialDocID = docsEnum.docID();
-    assertTrue("inital docID should be -1 or NO_MORE_DOCS", initialDocID == -1 || initialDocID == DocsEnum.NO_MORE_DOCS);
+assertTrue("inital docID should be -1 or NO_MORE_DOCS: " + docsEnum, initialDocID == -1 || initialDocID == DocsEnum.NO_MORE_DOCS);
 
     if (VERBOSE) {
       if (prevDocsEnum == null) {
---------------
-------------
@@ -55,6 +55,6 @@
         ColumnFamily retrieved = store.getColumnFamily(QueryFilter.getIdentityFilter(dk, new QueryPath("Standard1")));
         assert retrieved.isMarkedForDelete();
         assertNull(retrieved.getColumn("Column1".getBytes()));
-        assertNull(ColumnFamilyStore.removeDeleted(retrieved, Integer.MAX_VALUE));
+assertNull(Util.cloneAndRemoveDeleted(retrieved, Integer.MAX_VALUE));
     }
 }
---------------
-------------
@@ -75,7 +75,7 @@
    * @param reader
    */
   final void add(IndexReader reader) {
-    for (final AtomicReaderContext ctx : reader.getTopReaderContext().leaves()) {
+for (final AtomicReaderContext ctx : reader.leaves()) {
       final AtomicReader r = ctx.reader();
       mergeState.readers.add(r);
     }
---------------
-------------
@@ -64,7 +64,7 @@
     Vector value = values.next().get();
     Iterator<Element> it = value.iterateNonZero();
     Vector vector = new RandomAccessSparseVector(key
-        .toString(), Integer.MAX_VALUE, value.getNumNondefaultElements());
+.toString(), (int)featureCount, value.getNumNondefaultElements());
     while (it.hasNext()) {
       Element e = it.next();
       if (!dictionary.containsKey(e.index())) continue;
---------------
-------------
@@ -104,7 +104,7 @@
     MultiTermQuery wq = new WildcardQuery(new Term("field", ""));
     wq.setRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
     assertMatches(searcher, wq, 0);
-    BooleanQuery expected = new BooleanQuery();
+BooleanQuery expected = new BooleanQuery(true);
     assertEquals(searcher.rewrite(expected), searcher.rewrite(wq));
   }
   
---------------
-------------
@@ -1105,7 +1105,7 @@
 	 Shutdown the server directly (If you have the original object)
 	 No Network communication needed.
 	*/
-	private void directShutdownInternal() {
+void directShutdownInternal() {
 		// DERBY-2109: the direct, unchecked shutdown is made private
 		shutdown = true;
 		synchronized(shutdownSync) {						
---------------
-------------
@@ -48,7 +48,7 @@
   @Override
   public void tearDown() throws Exception {
     super.tearDown();
-    writer.close();
+writer.shutdown();
     dir.close();
   }
 
---------------
-------------
@@ -107,7 +107,7 @@
         for (byte[] key : keys)
         {
             // confirm that the bloom filter does not reject any keys
-            file.seek(reader.getPosition(reader.partitioner.decorateKey(key)).position);
+file.seek(reader.getPosition(reader.partitioner.decorateKey(key)));
             assert Arrays.equals(key, FBUtilities.readShortByteArray(file));
         }
     }
---------------
-------------
@@ -55,7 +55,7 @@
       + " */" + NL + NL;
     
   
-  public static void main(String args[]) throws Exception {
+public static void main(String args[]) {
     outputHeader();
     outputMacro("ALetterSupp",         "[:WordBreak=ALetter:]");
     outputMacro("FormatSupp",          "[:WordBreak=Format:]");
---------------
-------------
@@ -60,7 +60,7 @@
   public void setUp() throws Exception {
     super.setUp();
     PayloadHelper helper = new PayloadHelper();
-    searcher = helper.setUp(similarity, 1000);
+searcher = helper.setUp(random, similarity, 1000);
     indexReader = searcher.getIndexReader();
   }
 
---------------
-------------
@@ -103,7 +103,7 @@
     private Collection<HoldingDataBeanImpl> holdings;
     
     @OneToOne(fetch=FetchType.LAZY)
-    @JoinColumn(name="PROFILE_USERID", columnDefinition="VARCHAR(250)")
+@JoinColumn(name="PROFILE_USERID", columnDefinition="VARCHAR(255)")
     private AccountProfileDataBeanImpl profile;
 
     /* Accessor methods for relationship fields are only included for the AccountProfile profileID */
---------------
-------------
@@ -55,7 +55,7 @@
       w.addDocument(doc);
     }
     final IndexReader r = w.getReader();
-    w.close();
+w.shutdown();
 
     final int cloneCount = dir.getInputCloneCount();
     //System.out.println("merge clone count=" + cloneCount);
---------------
-------------
@@ -318,7 +318,7 @@
     // and any fields needed for merging.
     sreq.params.set(ResponseBuilder.FIELD_SORT_VALUES,"true");
 
-    if (rb.getSortSpec().includesScore()) {
+if ( (rb.getFieldFlags() & SolrIndexSearcher.GET_SCORES)!=0 || rb.getSortSpec().includesScore()) {
       sreq.params.set(CommonParams.FL, rb.req.getSchema().getUniqueKeyField().getName() + ",score");
     } else {
       sreq.params.set(CommonParams.FL, rb.req.getSchema().getUniqueKeyField().getName());      
---------------
-------------
@@ -51,6 +51,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new HungarianAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new HungarianAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -53,7 +53,7 @@
    * <p><em>This method is expensive and should only be called for discovery
    * of new service providers on the given classpath/classloader!</em>
    */
-  public void reload(ClassLoader classloader) {
+public synchronized void reload(ClassLoader classloader) {
     final LinkedHashMap<String,S> services = new LinkedHashMap<String,S>(this.services);
     final SPIClassIterator<S> loader = SPIClassIterator.get(clazz, classloader);
     while (loader.hasNext()) {
---------------
-------------
@@ -970,7 +970,7 @@
       try {
         upgrader = IndexUpgrader.parseArgs(args.toArray(new String[0]));
       } catch (Exception e) {
-        throw new AssertionError("unable to parse args: " + args, e);
+throw new RuntimeException("unable to parse args: " + args, e);
       }
       upgrader.upgrade();
       
---------------
-------------
@@ -48,7 +48,7 @@
     // PrefixFilter combined with ConstantScoreQuery
     PrefixFilter filter = new PrefixFilter(new Term("category", "/Computers"));
     Query query = new ConstantScoreQuery(filter);
-    IndexSearcher searcher = new IndexSearcher(directory);
+IndexSearcher searcher = new IndexSearcher(directory, true);
     ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals(4, hits.length);
 
---------------
-------------
@@ -1,6 +1,6 @@
 /*
  
-Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun
+Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun_Distributed
  
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -27,7 +27,7 @@
   public static final boolean VERBOSE = Boolean.getBoolean("tests.verbose");
 
   /** The Lucene {@link Version} used by the example code. */
-  public static final Version EXAMPLE_VER = Version.LUCENE_31;
+public static final Version EXAMPLE_VER = Version.LUCENE_40;
   
   public static void log(Object msg) {
     if (VERBOSE) {
---------------
-------------
@@ -41,7 +41,7 @@
   static final class JustCompileSearcher extends Searcher {
 
     @Override
-    protected Weight createWeight(Query query) throws IOException {
+public Weight createNormalizedWeight(Query query) throws IOException {
       throw new UnsupportedOperationException(UNSUPPORTED_MSG);
     }
     
---------------
-------------
@@ -82,7 +82,7 @@
         br.copyBytes(term.bytes());
         assert termsEnum != null;
         if (termsEnum.seekCeil(br) == TermsEnum.SeekStatus.FOUND) {
-          docs = termsEnum.docs(acceptDocs, docs, false);
+docs = termsEnum.docs(acceptDocs, docs, 0);
           while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
             result.set(docs.docID());
           }
---------------
-------------
@@ -52,7 +52,7 @@
       doc.add(fld);
       writer.addDocument(doc);
     }
-    writer.close();
+writer.shutdown();
     
   }
   
---------------
-------------
@@ -193,7 +193,7 @@
     /**
      * Apply without touching the commitlog. For testing.
      */
-    public void 2applyUnsafe() throws IOException
+public void applyUnsafe() throws IOException
     {
         Table.open(table_).apply(this, getSerializedBuffer(), false);
     }
---------------
-------------
@@ -70,7 +70,7 @@
     writer.addDocument(doc);
     writer.close();
     
-    IndexReader reader = IndexReader.open(dir);
+IndexReader reader = IndexReader.open(dir, true);
     TermPositions termPositions = reader.termPositions(new Term("preanalyzed", "term1"));
     assertTrue(termPositions.next());
     assertEquals(1, termPositions.freq());
---------------
-------------
@@ -333,7 +333,7 @@
   }
   
   @Override
-  protected void visitSubScorers(Query parent, Occur relationship, ScorerVisitor<Query, Query, Scorer> visitor) {
+public void visitSubScorers(Query parent, Occur relationship, ScorerVisitor<Query, Query, Scorer> visitor) {
     super.visitSubScorers(parent, relationship, visitor);
     final Query q = weight.getQuery();
     SubScorer sub = scorers;
---------------
-------------
@@ -178,7 +178,7 @@
       private int base = 0;
       private Scorer scorer;
       @Override
-      public void setScorer(Scorer scorer) throws IOException {
+public void setScorer(Scorer scorer) {
         this.scorer = scorer;
       }
       @Override
---------------
-------------
@@ -971,7 +971,7 @@
     final java.math.BigDecimal getBigDecimal(int column) throws SqlException {
         switch (jdbcTypes_[column - 1]) {
         case java.sql.Types.BOOLEAN:
-            return new java.math.BigDecimal( getInt( column ) );
+return java.math.BigDecimal.valueOf(getLong(column));
         case java.sql.Types.DECIMAL:
             return get_DECIMAL(column);
         case java.sql.Types.REAL:
---------------
-------------
@@ -1153,7 +1153,7 @@
 
       setRollbackSegmentInfos(segmentInfos);
 
-      docWriter = new DocumentsWriter(directory, this, conf.getIndexingChain());
+docWriter = new DocumentsWriter(directory, this, conf.getIndexingChain(), conf.getMaxThreadStates());
       docWriter.setInfoStream(infoStream);
       docWriter.setMaxFieldLength(maxFieldLength);
 
---------------
-------------
@@ -132,7 +132,7 @@
   private void openExampleIndex() throws IOException {
     //Create a RAM-based index from our test data file
     RAMDirectory rd = new RAMDirectory();
-    IndexWriterConfig iwConfig = new IndexWriterConfig(Version.LUCENE_40, analyzer);
+IndexWriterConfig iwConfig = new IndexWriterConfig(Version.LUCENE_CURRENT, analyzer);
     IndexWriter writer = new IndexWriter(rd, iwConfig);
     InputStream dataIn = getServletContext().getResourceAsStream("/WEB-INF/data.tsv");
     BufferedReader br = new BufferedReader(new InputStreamReader(dataIn, IOUtils.CHARSET_UTF_8));
---------------
-------------
@@ -190,7 +190,7 @@
       boolean success = false;
       try {
         for (FieldInfo field : mergeState.fieldInfos) {
-          if (field.isIndexed() && !field.omitsNorms()) {
+if (field.isIndexed() && !field.omitsNorms() && field.getNormType() != null) {
             List<NumericDocValues> toMerge = new ArrayList<NumericDocValues>();
             for (AtomicReader reader : mergeState.readers) {
               NumericDocValues norms = reader.simpleNormValues(field.name);
---------------
-------------
@@ -165,7 +165,7 @@
     assertTrue(r1.isCurrent());
 
     writer.commit();
-    assertTrue(r1.isCurrent());
+assertFalse(r1.isCurrent());
 
     assertEquals(200, r1.maxDoc());
 
---------------
-------------
@@ -85,7 +85,7 @@
 		Set the locale for the service at boot time. The passed-in
 		properties must be the one passed to the boot method.
 
-		@exception StandardException	Cloudscape error.
+@exception StandardException	Derby error.
 	 */
 	public Locale setLocale(Properties serviceProperties,
 							String userDefinedLocale)
---------------
-------------
@@ -230,7 +230,7 @@
 
           if (postingsEnum == null) {
             // term does exist, but has no positions
-            assert termsEnum.docs(liveDocs, null, false) != null: "termstate found but no term exists in reader";
+assert termsEnum.docs(liveDocs, null, 0) != null: "termstate found but no term exists in reader";
             throw new IllegalStateException("field \"" + term.field() + "\" was indexed without position data; cannot run PhraseQuery (term=" + term.text() + ")");
           }
 
---------------
-------------
@@ -293,7 +293,7 @@
 
   private void logConfig() {
     if (!LOG.isInfoEnabled()) return;
-    StringBuffer config = new StringBuffer();
+StringBuilder config = new StringBuilder();
     config.append("user : ").append(user).append(System.getProperty("line.separator"));
     config.append("pwd : ").append(password).append(System.getProperty("line.separator"));
     config.append("protocol : ").append(protocol).append(System.getProperty("line.separator"));
---------------
-------------
@@ -2227,7 +2227,7 @@
             else
             {
                 sb.append(stub ? 'D' : 'C');
-                sb.append(containerId.getContainerId());
+sb.append(Long.toHexString(containerId.getContainerId()));
                 sb.append(".DAT");
             }
             return storageFactory.newStorageFile( sb.toString());
---------------
-------------
@@ -147,7 +147,7 @@
     // A positioned update section must come from the same package as its query section.
     Section getPositionedUpdateSection(Section querySection) throws SqlException {
         Connection connection = agent_.connection_;
-        return getDynamicSection(connection.resultSetHoldability_);
+return getDynamicSection(connection.holdability());
     }
 
     // Get a section for a jdbc 1 positioned update/delete for the corresponding query.
---------------
-------------
@@ -61,7 +61,7 @@
         addDocument(writer, "3", "I think it should work.");
         addDocument(writer, "4", "I think it should work.");
         writer.close();
-        searcher = new IndexSearcher(mDirectory);
+searcher = new IndexSearcher(mDirectory, true);
     }
 
     protected void tearDown() throws Exception {
---------------
-------------
@@ -107,7 +107,7 @@
 
     private final Deque<MailProcessor> processors = new ArrayDeque<MailProcessor>();
     private final ChunkedWriter writer;
-    private Deque<Long> messageCounts = new ArrayDeque<Long>();
+private final Deque<Long> messageCounts = new ArrayDeque<Long>();
 
     public PrefixAdditionDirectoryWalker(MailProcessor processor, ChunkedWriter writer) {
       processors.addFirst(processor);
---------------
-------------
@@ -82,7 +82,7 @@
 
 			if (groupByList.findGroupingColumn(cr) == null)
 			{
-				throw StandardException.newException(SQLState.LANG_INVALID_GROUPED_SELECT_LIST);
+throw StandardException.newException(SQLState.LANG_INVALID_COL_REF_GROUPED_SELECT_LIST, cr.getSQLColumnName());
 			}
 		} 
 		
---------------
-------------
@@ -26,7 +26,7 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.*;
+import static org.apache.lucene.analysis.VocabularyAssert.*;
 
 /**
  * Simple tests for {@link HungarianLightStemFilter}
---------------
-------------
@@ -287,7 +287,7 @@
       TermsFilter right = termsFilter(singleField ? random().nextBoolean() : false, terms);
       assertEquals(right, left);
       assertEquals(right.hashCode(), left.hashCode());
-      if (i > 0) {
+if (uniqueTerms.size() > 1) {
         List<Term> asList = new ArrayList<Term>(uniqueTerms);
         asList.remove(0);
         TermsFilter notEqual = termsFilter(singleField ? random().nextBoolean() : false, asList);
---------------
-------------
@@ -45,7 +45,7 @@
 {
     private static boolean debug_system_procedures_created = false;
 
-    abstract void testList(Connection conn) throws SQLException;
+abstract public void testList(Connection conn) throws SQLException;
 
     void runTests(String[] argv)
         throws Throwable
---------------
-------------
@@ -240,7 +240,7 @@
       this.bottomTerm = bottomTerm;
       // clone the term before potentially doing something with it
       // this is a rare but wonderful occurrence anyway
-      queuedBottom = new BytesRef(term);
+queuedBottom = BytesRef.deepCopyOf(term);
     }
     
     return term;
---------------
-------------
@@ -100,7 +100,7 @@
     {
         List<Pair<SSTableReader, Long>> tableLengthPairs = new ArrayList<Pair<SSTableReader, Long>>();
         for(SSTableReader table: collection)
-            tableLengthPairs.add(new Pair<SSTableReader, Long>(table, table.length()));
+tableLengthPairs.add(new Pair<SSTableReader, Long>(table, table.onDiskLength()));
         return tableLengthPairs;
     }
 
---------------
-------------
@@ -156,7 +156,7 @@
             {
                 if (range.contains(myRange))
                 {
-                    List<InetAddress> preferred = DatabaseDescriptor.getEndPointSnitch().getSortedListByProximity(address, rangeAddresses.get(range));
+List<InetAddress> preferred = DatabaseDescriptor.getEndpointSnitch().getSortedListByProximity(address, rangeAddresses.get(range));
                     myRangeAddresses.putAll(myRange, preferred);
                     break;
                 }
---------------
-------------
@@ -50,7 +50,7 @@
 
     private class SeekCountingDirectory extends MockDirectoryWrapper {
       public SeekCountingDirectory(Directory delegate) {
-        super(delegate);
+super(random, delegate);
       }
 
       @Override
---------------
-------------
@@ -90,7 +90,7 @@
       }
       
     }};
-    BooleanScorer bs = new BooleanScorer(sim, 1, Arrays.asList(scorers), null);
+BooleanScorer bs = new BooleanScorer(sim, 1, Arrays.asList(scorers), null, scorers.length);
     
     assertEquals("should have received 3000", 3000, bs.nextDoc());
     assertEquals("should have received NO_MORE_DOCS", DocIdSetIterator.NO_MORE_DOCS, bs.nextDoc());
---------------
-------------
@@ -93,7 +93,7 @@
     } else {
       String s = "gen=" + gen;
       if (numTermDeletes.get() != 0) {
-        s += " " + numTermDeletes.get() + " deleted terms (unique count=" + terms.size() + ") terms=" + terms.keySet();
+s += " " + numTermDeletes.get() + " deleted terms (unique count=" + terms.size() + ")";
       }
       if (queries.size() != 0) {
         s += " " + queries.size() + " deleted queries";
---------------
-------------
@@ -378,7 +378,7 @@
       Collection<String> files = new HashSet<String>(commit.getFileNames());
       for (String fileName : files) {
         if(fileName.endsWith(".lock")) continue;
-        File file = new File(core.getIndexDir(), fileName);
+File file = new File(core.getNewIndexDir(), fileName);
         Map<String, Object> fileMeta = getFileInfo(file);
         result.add(fileMeta);
       }
---------------
-------------
@@ -146,7 +146,7 @@
         syncToMe(zkController, collection, shardId, leaderProps, core.getCoreDescriptor());
         
       } else {
-        log.info("Leader's attempt to sync with shard failed, moving to the next canidate");
+log.info("Leader's attempt to sync with shard failed, moving to the next candidate");
         // lets see who seems ahead...
       }
       
---------------
-------------
@@ -179,7 +179,7 @@
             }
             sis = null;
           } catch (IOException e) {
-            if (SegmentInfos.generationFromSegmentsFileName(fileName) <= currentGen) {
+if (SegmentInfos.generationFromSegmentsFileName(fileName) <= currentGen && directory.fileLength(fileName) > 0) {
               throw e;
             } else {
               // Most likely we are opening an index that
---------------
-------------
@@ -285,7 +285,7 @@
       assertTrue(((SortingDocsAndPositionsEnum) sortedPositions).reused(reuse)); // make sure reuse worked
     }
     doc = 0;
-    while ((doc = sortedPositions.advance(doc)) != DocIdSetIterator.NO_MORE_DOCS) {
+while ((doc = sortedPositions.advance(doc + _TestUtil.nextInt(random(), 1, 5))) != DocIdSetIterator.NO_MORE_DOCS) {
       int freq = sortedPositions.freq();
       assertEquals("incorrect freq for doc=" + doc, sortedValues[doc].intValue() / 10 + 1, freq);
       for (int i = 0; i < freq; i++) {
---------------
-------------
@@ -98,7 +98,7 @@
 	public GenericResultDescription(ResultColumnDescriptor[] columns, 
 					String statementType) 
 	{
-		this.columns = columns;
+this.columns = (ResultColumnDescriptor[]) ArrayUtil.copy( columns );
 		this.statementType = statementType;
 	}
 
---------------
-------------
@@ -824,7 +824,7 @@
     try {
       core = coreContainer.getCore(cname);
       if (core != null) {
-        syncStrategy = new SyncStrategy(core.getCoreDescriptor().getCoreContainer().getUpdateShardHandler());
+syncStrategy = new SyncStrategy(core.getCoreDescriptor().getCoreContainer());
         
         Map<String,Object> props = new HashMap<String,Object>();
         props.put(ZkStateReader.BASE_URL_PROP, zkController.getBaseUrl());
---------------
-------------
@@ -134,7 +134,7 @@
   }
   
  /** Returns the id for the nth document in this set. */ 
-  public final float id(int n) throws IOException {
+public final int id(int n) throws IOException {
     return hitDoc(n).id;
   }
 
---------------
-------------
@@ -3257,7 +3257,7 @@
                 "where c.referenceid = t.tableid and t.tablename='D3175'");
         JDBC.assertUnorderedResultSet(rs, new String[][]{
                     {"X", "1", "VARCHAR(12)", null, null, null, null, "D3175", "T", "R"},
-                    {"ID", "2", "INTEGER NOT NULL", "GENERATED_BY_DEFAULT", "22", "1", "1", "D3175", "T", "R"}
+{"ID", "2", "INTEGER NOT NULL", "GENERATED_BY_DEFAULT", "3", "1", "1", "D3175", "T", "R"}
                 });
     }
 
---------------
-------------
@@ -66,7 +66,7 @@
      */
     public void printRing(PrintStream outs)
     {
-        Map<Range, List<String>> rangeMap = probe.getRangeToEndPointMap();
+Map<Range, List<String>> rangeMap = probe.getRangeToEndPointMap(null);
         List<Range> ranges = new ArrayList<Range>(rangeMap.keySet());
         Collections.sort(ranges);
         Set<String> liveNodes = probe.getLiveNodes();
---------------
-------------
@@ -212,7 +212,7 @@
         for (Integer cfId : cfLastWrite.keySet())
         {
             CFMetaData m = DatabaseDescriptor.getCFMetaData(cfId);
-            sb.append(m == null ? m.cfName : "<deleted>").append(" (").append(cfId).append("), ");
+sb.append(m == null ? "<deleted>" : m.cfName).append(" (").append(cfId).append("), ");
         }
         return sb.toString();
     }
---------------
-------------
@@ -79,7 +79,7 @@
       }
 
       if (terms != null) { // TODO this check doesn't make sense, decide which variable its supposed to be for
-        br.copy(term.bytes());
+br.copyBytes(term.bytes());
         if (termsEnum.seekCeil(br) == TermsEnum.SeekStatus.FOUND) {
           docs = termsEnum.docs(acceptDocs, docs);
           while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {
---------------
-------------
@@ -188,7 +188,7 @@
 
     @Override
     public int numDocs() {
-      return (int) liveDocs.cardinality();
+return liveDocs.cardinality();
     }
 
     /**
---------------
-------------
@@ -448,7 +448,7 @@
 
     @Override
     public void setNextReader(AtomicReaderContext context) throws IOException {
-      this.docBase = context.docBase;
+docBase = context.docBase;
       for (int i = 0; i < comparators.length; i++) {
         queue.setComparator(i, comparators[i].setNextReader(context));
       }
---------------
-------------
@@ -81,7 +81,7 @@
             Assert.assertEquals("2.0", expectedMF.getMainAttributes().getValue("Bundle-ManifestVersion"));
             Assert.assertEquals("testbundle", expectedMF.getMainAttributes().getValue("Bundle-SymbolicName"));
             Assert.assertEquals("Bar Bar", expectedMF.getMainAttributes().getValue("Foo"));
-            Assert.assertEquals("osgi.serviceloader; filter:=\"(osgi.serviceloader=org.apache.aries.spifly.mysvc.SPIProvider)\";cardinality:=multiple,",
+Assert.assertEquals("osgi.serviceloader; filter:=\"(osgi.serviceloader=org.apache.aries.spifly.mysvc.SPIProvider)\";cardinality:=multiple",
                     expectedMF.getMainAttributes().getValue(SpiFlyConstants.REQUIRE_CAPABILITY));
             String importPackage = expectedMF.getMainAttributes().getValue("Import-Package");
             Assert.assertTrue(
---------------
-------------
@@ -495,7 +495,7 @@
     }
   }
 
-  private final static class TermsEnumWithSlice {
+final static class TermsEnumWithSlice {
     private final ReaderSlice subSlice;
     private TermsEnum terms;
     public BytesRef current;
---------------
-------------
@@ -54,7 +54,7 @@
  *
  */
 
-public class TernaryOperatorNode extends ValueNode
+public class TernaryOperatorNode extends OperatorNode
 {
 	String		operator;
 	String		methodName;
---------------
-------------
@@ -36,7 +36,7 @@
   }
 
   public NamedVector(Vector delegate, String name) {
-    if (delegate == null) {
+if (delegate == null || name == null) {
       throw new IllegalArgumentException();
     }
     this.delegate = delegate;
---------------
-------------
@@ -125,7 +125,7 @@
           }
         break;
       case (AND):
-        bits.and(((DocIdBitSet)chain[i].getDocIdSet(reader)).getBitSet());
+bits.and(chain[i].bits(reader));
         break;
       case (OR):
         bits.or(((DocIdBitSet)chain[i].getDocIdSet(reader)).getBitSet());
---------------
-------------
@@ -365,7 +365,7 @@
     synchronized String getTempSSTablePath()
     {
         String fname = getTempSSTableFileName();
-        return new File(DatabaseDescriptor.getDataFileLocationForTable(table_), fname).getAbsolutePath();
+return new File(DatabaseDescriptor.getNextAvailableDataLocation() + File.separator + table_, fname).getAbsolutePath();
     }
 
     public String getTempSSTableFileName()
---------------
-------------
@@ -91,7 +91,7 @@
         Row row;
         try
         {
-            row = StorageProxy.readProtocol(command, ConsistencyLevel.ONE);
+row = StorageProxy.readProtocol(command, consistency_level);
         }
         catch (IOException e)
         {
---------------
-------------
@@ -209,7 +209,7 @@
       super(orig.categoryPath, num);
       this.orig = orig;
       setDepth(orig.getDepth());
-      setNumLabel(orig.getNumLabel());
+setNumLabel(0); // don't label anything as we're over-sampling
       setResultMode(orig.getResultMode());
       setSortOrder(orig.getSortOrder());
     }
---------------
-------------
@@ -711,7 +711,7 @@
         // The following precondition matches CLI semantics, see SQLDisconnect()
         if (!autoCommit_ && inUnitOfWork_ && !allowCloseInUOW_()) {
             throw new SqlException(agent_.logWriter_,
-                    new MessageId (SQLState.CANNOT_CLOSE_ACTIVE_XA_CONNECTION));                   
+new MessageId (SQLState.CANNOT_CLOSE_ACTIVE_CONNECTION));
         }
     }
 
---------------
-------------
@@ -36,7 +36,7 @@
  * <br>Filter query example: <code>fq={!frange l=0 u=2.2}sum(user_ranking,editor_ranking)</code> 
  */
 public class FunctionRangeQParserPlugin extends QParserPlugin {
-  public static String NAME = "frange";
+public static final String NAME = "frange";
 
   @Override
   public void init(NamedList args) {
---------------
-------------
@@ -132,7 +132,7 @@
     for (int i=0; i<n; ++i)
       fields[i] = comparators[i].sortValue(doc);
     doc.fields = fields;
-    if (maxscore > 1.0f) doc.score /= maxscore;   // normalize scores
+//if (maxscore > 1.0f) doc.score /= maxscore;   // normalize scores
     return doc;
   }
 
---------------
-------------
@@ -290,7 +290,7 @@
     Document doc = new Document();
     doc.add(newTextField("body", "blah the footest blah", Field.Store.NO));
     iw.addDocument(doc);
-    iw.close();
+iw.shutdown();
     
     MultiFieldQueryParser mfqp = 
       new MultiFieldQueryParser(TEST_VERSION_CURRENT, new String[] {"body"}, analyzer);
---------------
-------------
@@ -26,7 +26,7 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.*;
+import static org.apache.lucene.analysis.VocabularyAssert.*;
 
 /**
  * Simple tests for {@link FinnishLightStemFilter}
---------------
-------------
@@ -50,7 +50,7 @@
   public SimpleBundleInfo(BundleManifest bm, String location) { 
     _contentName = new ContentImpl(
         bm.getSymbolicName(), 
-        ManifestHeaderProcessor.parseBundleSymbolicName(bm.getSymbolicName()).getValue());
+ManifestHeaderProcessor.parseBundleSymbolicName(bm.getSymbolicName()).getAttributes());
     _version = bm.getVersion();
     _attributes = bm.getRawAttributes();
     _location = location;
---------------
-------------
@@ -172,7 +172,7 @@
         final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(
               "OutOfMemoryError likely caused by the Sun VM Bug described in "
               + "https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize "
-              + "with a a value smaller than the current chunk size (" + chunkSize + ")");
++ "with a value smaller than the current chunk size (" + chunkSize + ")");
         outOfMemoryError.initCause(e);
         throw outOfMemoryError;
       }
---------------
-------------
@@ -183,7 +183,7 @@
       @Override
       public Object getData(String query) {
         log(DIHLogLevels.ENTITY_META, "query", query);
-        long start = System.currentTimeMillis();
+long start = System.nanoTime();
         try {
           return ds.getData(query);
         } catch (DataImportHandlerException de) {
---------------
-------------
@@ -158,7 +158,7 @@
   }
 
   @Override
-  void startDocument() throws IOException {
+void startDocument() {
     assert clearLastVectorFieldName();
     reset();
   }
---------------
-------------
@@ -107,7 +107,7 @@
         hasProx |= isIndexed && indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
         hasFreq |= isIndexed && indexOptions != IndexOptions.DOCS_ONLY;
         infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
-          omitNorms, storePayloads, indexOptions, null, isIndexed && !omitNorms? Type.BYTES_VAR_STRAIGHT : null);
+omitNorms, storePayloads, indexOptions, null, isIndexed && !omitNorms? Type.FIXED_INTS_8 : null);
       }
 
       if (input.getFilePointer() != input.length()) {
---------------
-------------
@@ -1094,7 +1094,7 @@
       Codec.setDefault(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));
     }
 
-    final LineFileDocs docs = new LineFileDocs(random);
+final LineFileDocs docs = new LineFileDocs(random, defaultCodecSupportsDocValues());
     final int RUN_TIME_MSEC = atLeast(500);
     final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(-1).setRAMBufferSizeMB(64);
     final File tempDir = _TestUtil.getTempDir("fstlines");
---------------
-------------
@@ -182,7 +182,7 @@
       // These characters are part of the query syntax and must be escaped
       if (c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':'
         || c == '^' || c == '[' || c == ']' || c == '\"' || c == '{' || c == '}' || c == '~'
-        || c == '*' || c == '?' || c == '|' || c == '&'
+|| c == '*' || c == '?' || c == '|' || c == '&'  || c == ';'
         || Character.isWhitespace(c)) {
         sb.append('\\');
       }
---------------
-------------
@@ -130,7 +130,7 @@
           OpenMode.APPEND).setMaxBufferedDocs(2));
       
       DirectoryReader reader = DirectoryReader.open(directory);
-      assertEquals("reader=" + reader, 1, reader.getSequentialSubReaders().size());
+assertEquals("reader=" + reader, 1, reader.leaves().size());
       assertEquals(expectedDocCount, reader.numDocs());
       reader.close();
     }
---------------
-------------
@@ -215,7 +215,7 @@
       }
     }
 
-    assertQ(req("fl", "*,score", "q", qry.toString()),
+assertQ(req("fl", "*,score", "indent", "true", "q", qry.toString()),
             "//*[@numFound='1']");
     clearIndex();
   }
---------------
-------------
@@ -680,7 +680,7 @@
                 completed.await();
                 if (exception == null)
                 {
-                    logger.info(String.format("Repair session %s (on cfs %s, range %s) completed successfully", getName()), cfnames, range);
+logger.info(String.format("Repair session %s (on cfs %s, range %s) completed successfully", getName(), cfnames, range));
                 }
                 else
                 {
---------------
-------------
@@ -79,7 +79,7 @@
     private static final Logger logger_ = Logger.getLogger(CommitLog.class);
     private static final Map<String, CommitLogHeader> clHeaders_ = new HashMap<String, CommitLogHeader>();
 
-    public static final class CommitLogContext
+public static class CommitLogContext
     {
         /* Commit Log associated with this operation */
         public final String file;
---------------
-------------
@@ -26,7 +26,7 @@
  * <br>Example: <code>q=foo {!maxscore v=$myq}&myq=A OR B OR C</code>
  */
 public class MaxScoreQParserPlugin extends LuceneQParserPlugin {
-  public static String NAME = "maxscore";
+public static final String NAME = "maxscore";
 
   @Override
   public QParser createParser(String qstr, SolrParams localParams, SolrParams params, SolrQueryRequest req) {
---------------
-------------
@@ -107,7 +107,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    conf = new Configuration();
+conf = getConfiguration();
     fs = FileSystem.get(conf);
   }
   
---------------
-------------
@@ -45,6 +45,6 @@
   @Override
   protected SolrServer createNewSolrServer()
   {
-    return new EmbeddedSolrServer( h.getCore() );
+return new EmbeddedSolrServer( h.getCoreContainer(), "" );
   }
 }
---------------
-------------
@@ -140,7 +140,7 @@
     assertTrue(auc2 > auc1);
   }
 
-  @Test
+//  @Test
   public void adaptiveLogisticRegressionRoundTrip() {
     AdaptiveLogisticRegression learner = new AdaptiveLogisticRegression(2, 5, new L1());
     learner.setInterval(200);
---------------
-------------
@@ -265,7 +265,7 @@
 	*/
 	String DEADLOCK = "40001";
 	String LOCK_TIMEOUT = "40XL1";
-    String LOCK_TIMEOUT_LOG = "40XL2";
+String LOCK_TIMEOUT_LOG = "40XL1.T.1";
 
 	/*
 	** Store - access.protocol.Interface statement exceptions
---------------
-------------
@@ -105,7 +105,7 @@
   synchronized final void doDelete(int n) throws IOException {
     numDocs = -1;				  // invalidate cache
     int i = readerIndex(n);			  // find segment num
-    readers[i].delete(n - starts[i]);		  // dispatch to segment reader
+readers[i].doDelete(n - starts[i]);		  // dispatch to segment reader
   }
 
   private final int readerIndex(int n) {	  // find reader for doc n:
---------------
-------------
@@ -425,7 +425,7 @@
         schemaFile = new File(solrLoader.getInstanceDir() + "conf" + File.separator + dcore.getSchemaName());
       }
       if(schemaFile. exists()){
-        String key = schemaFile.getAbsolutePath()+":"+new SimpleDateFormat("yyyyMMddhhmmss").format(new Date(schemaFile.lastModified()));
+String key = schemaFile.getAbsolutePath()+":"+new SimpleDateFormat("yyyyMMddhhmmss", Locale.US).format(new Date(schemaFile.lastModified()));
         schema = indexSchemaCache.get(key);
         if(schema == null){
           log.info("creating new schema object for core: " + dcore.name);
---------------
-------------
@@ -79,7 +79,7 @@
                 endpoints.add(message.getFrom());
 
                 // compute maxLiveColumns to prevent short reads -- see https://issues.apache.org/jira/browse/CASSANDRA-2643
-                int liveColumns = cf.getLiveColumnCount();
+int liveColumns = cf == null ? 0 : cf.getLiveColumnCount();
                 if (liveColumns > maxLiveColumns)
                     maxLiveColumns = liveColumns;
             }
---------------
-------------
@@ -55,7 +55,7 @@
     System.out.println("done");
   }
   
-  public static void main(String[] args) throws IOException, ClassNotFoundException {
+public static void main(String[] args) throws IOException {
     DictionaryFormat format;
     if (args[0].equalsIgnoreCase("ipadic")) {
       format = DictionaryFormat.IPADIC;
---------------
-------------
@@ -123,7 +123,7 @@
         
         provZip.deleteOnExit();
         
-        mbean.addInformation(provZip.toURL().toExternalForm());
+mbean.addInformationFromZip(provZip.toURL().toExternalForm());
         
         //check the info has been added
         
---------------
-------------
@@ -218,7 +218,7 @@
     public void testDecode() throws IOException
     {
         ByteBuffer bytes = ByteBuffer.wrap(new byte[]{(byte)0xff, (byte)0xfe});
-        ByteBufferUtil.string(bytes, Charsets.UTF_8);
+ByteBufferUtil.string(bytes);
     }
 
     @Test
---------------
-------------
@@ -53,7 +53,7 @@
       writer.addDocument(doc);
     }
     writer.close();
-    searcher = new IndexSearcher(directory);
+searcher = new IndexSearcher(directory, true);
   }
 
   protected String[] docFields = {
---------------
-------------
@@ -120,7 +120,7 @@
 
   protected final void assertFileNotInZooKeeper(String fileName) throws Exception {
     // Stolen from AbstractBadConfigTestBase
-    String errString = "returned non ok status:404, message:Not Found";
+String errString = "Not Found";
     ignoreException(Pattern.quote(errString));
     String rawContent = null;
     try {
---------------
-------------
@@ -3262,7 +3262,7 @@
 	*/
 	public int getSQLStateType()
 	{
-		return JDBC30Translation.SQL_STATE_SQL99;
+return sqlStateSQL99;
 	}
 
 	/**
---------------
-------------
@@ -92,7 +92,7 @@
       if (data != null) {
         props.load(new StringReader(new String(data, "UTF-8")));
       }
-    } catch (Throwable e) {
+} catch (Exception e) {
       log.warn(
           "Could not read DIH properties from " + path + " :" + e.getClass(), e);
     }
---------------
-------------
@@ -76,7 +76,7 @@
 
 			if (firstTime) {
 				Logs.reportString("----------------------------------------------");
-				Logs.reportMessage("CSLOOK_StoredProcHeader");
+Logs.reportMessage("DBLOOK_StoredProcHeader");
 				Logs.reportString("----------------------------------------------\n");
 			}
 
---------------
-------------
@@ -29,7 +29,7 @@
 
   @Override
   protected double doComputeResult(int rowA, int rowB, Iterable<Cooccurrence> cooccurrences, double weightOfVectorA,
-      double weightOfVectorB, int numberOfColumns) {
+double weightOfVectorB, long numberOfColumns) {
 
     double sumXY = 0.0;
     for (Cooccurrence cooccurrence : cooccurrences) {
---------------
-------------
@@ -97,7 +97,7 @@
     }
     catch( Throwable t ) {
       // catch this so our filter still works
-      log.error( "Could not start Solr. Check solr/home property", t);
+log.error( "Could not start Solr. Check solr/home property and the logs", t);
       SolrConfig.severeErrors.add( t );
       SolrCore.log( t );
     }
---------------
-------------
@@ -47,7 +47,7 @@
 
         try
         {
-            Truncation t = Truncation.serializer().deserialize(new DataInputStream(buffer));
+Truncation t = Truncation.serializer().deserialize(new DataInputStream(buffer), message.getVersion());
             logger.debug("Applying {}", t);
 
             try
---------------
-------------
@@ -549,7 +549,7 @@
         ft.setStoreTermVectors(true);
         ft.setStoreTermVectorOffsets(random.nextBoolean());
         ft.setStoreTermVectorPositions(random.nextBoolean());
-        if (ft.storeTermVectorPositions() && !PREFLEX_IMPERSONATION_IS_ACTIVE) {
+if (ft.storeTermVectorPositions() && !OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
           ft.setStoreTermVectorPayloads(random.nextBoolean());
         }
       }
---------------
-------------
@@ -307,7 +307,7 @@
         if (cf == null)
             return false;
         IColumn c = cf.getColumn(BOOTSTRAP);
-        return c.value().get(0) == 1;
+return c.value().get(c.value().position()) == 1;
     }
 
     public static void setBootstrapped(boolean isBootstrapped)
---------------
-------------
@@ -248,7 +248,7 @@
     }
     
     @Override
-    public int getUniqueFieldCount() {
+public int size() {
       return fields.size();
     }
 
---------------
-------------
@@ -729,7 +729,7 @@
 
           if (sortByCount) {
             if (c>min) {
-              BytesRef termCopy = new BytesRef(term);
+BytesRef termCopy = BytesRef.deepCopyOf(term);
               queue.add(new CountPair<BytesRef,Integer>(termCopy, c));
               if (queue.size()>=maxsize) min=queue.last().val;
             }
---------------
-------------
@@ -60,7 +60,7 @@
    *         configuration or <code>null</code>, if the implemented
    *         {@link QueryConfigHandler} has no configuration for that field
    */
-  public FieldConfig getFieldConfig(CharSequence fieldName) {
+public FieldConfig getFieldConfig(String fieldName) {
     FieldConfig fieldConfig = new FieldConfig(fieldName);
 
     for (FieldConfigListener listener : this.listeners) {
---------------
-------------
@@ -47,7 +47,7 @@
    * @return true if input docid should be in the result set, false otherwise.
    * @see #FilteredDocIdSetIterator(DocIdSetIterator)
    */
-  abstract protected boolean match(int doc) throws IOException;
+protected abstract boolean match(int doc);
 	
   @Override
   public int docID() {
---------------
-------------
@@ -77,7 +77,7 @@
     private static String d_columnMap_        = "COLUMN_MAP";
     private static String d_columnKey_        = "COLUMN_KEY";
     private static String d_columnValue_      = "COLUMN_VALUE";
-    private static String d_columnTimestamp_  = "ColumnIMESTAMP";
+private static String d_columnTimestamp_  = "COLUMN_TIMESTAMP";
 
     private static Map<String, Double> tableKeysCachedFractions_;
     /*
---------------
-------------
@@ -756,7 +756,7 @@
             // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?
             // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)
             // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?
-            docsEnum = termsEnum.docs(null, docsEnum, false);
+docsEnum = termsEnum.docs(null, docsEnum, 0);
             c=0;
 
             if (docsEnum instanceof MultiDocsEnum) {
---------------
-------------
@@ -116,7 +116,7 @@
     {
         try
         {
-            configFileName_ = System.getProperty("storage-config") + System.getProperty("file.separator") + "storage-conf.xml";
+configFileName_ = System.getProperty("storage-config") + File.separator + "storage-conf.xml";
             if (logger_.isDebugEnabled())
               logger_.debug("Loading settings from " + configFileName_);
             XMLUtils xmlUtils = new XMLUtils(configFileName_);
---------------
-------------
@@ -60,7 +60,7 @@
 
   A b-tree scan controller corresponds to an instance of an open b-tree scan.
   <P>
-  <B>Concurrency Notes<\B>
+<B>Concurrency Notes</B>
   <P>
   The concurrency rules are derived from OpenBTree.
   <P>
---------------
-------------
@@ -259,7 +259,7 @@
         this(sqlca, 0, true);
         // only set the error code for the first exception in the chain (we
         // don't know the error code for the rest)
-        errorcode_ = sqlca.getSqlCode();
+errorcode_ = sqlca.getErrorCode();
         if ( logWriter != null )
         {
             logWriter.traceDiagnosable(this);
---------------
-------------
@@ -188,7 +188,7 @@
     this.infoStream = infoStream;
     final Iterator<ThreadState> it = perThreadPool.getAllPerThreadsIterator();
     while (it.hasNext()) {
-      it.next().perThread.docState.infoStream = infoStream;
+it.next().perThread.setInfoStream(infoStream);
     }
   }
 
---------------
-------------
@@ -191,7 +191,7 @@
       if (docValue < value) {
         return -1;
       } else if (docValue > value) {
-        return -1;
+return 1;
       } else {
         return 0;
       }
---------------
-------------
@@ -30,7 +30,7 @@
 
   @BeforeClass
   public static void beforeClass() throws Exception {
-    initCore("solrConfig.xml", "schema.xml");
+initCore("solrconfig.xml", "schema.xml");
     numberOfDocs = 0;
     for (String[] doc : DOCUMENTS) {
       assertNull(h.validateUpdate(adoc("id", Integer.toString(numberOfDocs), "url", doc[0], "title", doc[1], "snippet", doc[2])));
---------------
-------------
@@ -83,7 +83,7 @@
         suite.addTest(ScrollCursors2Test.suite());
         suite.addTest(NullIfTest.suite());
         suite.addTest(InListMultiProbeTest.suite());
-        //suite.addTest(SecurityPolicyReloadingTest.suite());
+suite.addTest(SecurityPolicyReloadingTest.suite());
         suite.addTest(CurrentOfTest.suite());
         suite.addTest(UnaryArithmeticParameterTest.suite());
         suite.addTest(HoldCursorTest.suite());
---------------
-------------
@@ -109,7 +109,7 @@
         }
     }
 
-    public List<InetAddress> getEndPoint(String key)
+public List<InetAddress> getEndPoint(byte[] key)
     {
         if (tokenMetadata == null)
             throw new RuntimeException("Must refresh endpoints before looking up a key.");
---------------
-------------
@@ -105,7 +105,7 @@
         if (fractOrAbs < 0)
             throw new UnsupportedOperationException("unexpected negative value " + fractOrAbs);
 
-        if (0 < fractOrAbs && fractOrAbs < 1)
+if (0 < fractOrAbs && fractOrAbs <= 1)
         {
             // fraction
             return Math.max(1, (long)(fractOrAbs * total));
---------------
-------------
@@ -121,7 +121,7 @@
             zkServer.runFromConfig(sc);
           }
           log.info("ZooKeeper Server exited.");
-        } catch (Throwable e) {
+} catch (Exception e) {
           log.error("ZooKeeper Server ERROR", e);
           throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);
         }
---------------
-------------
@@ -192,7 +192,7 @@
       List<String> unsatisfiedRequirements = new LinkedList<String>();
 
       for (Map.Entry<String, Set<String>> filterEntry : refinedReqs.entrySet()) {
-        log.debug("unable to satisfied the filter , filter = " + filterEntry.getKey() + "required by "+filterEntry.getValue());
+log.debug("unable to satisfy the filter , filter = " + filterEntry.getKey() + "required by "+filterEntry.getValue());
        
         String reason = extractConsumableMessageInfo(filterEntry.getKey(),filterEntry.getValue());
 
---------------
-------------
@@ -66,7 +66,7 @@
     private static final String READ_CONSISTENCY_LEVEL = "cassandra.consistencylevel.read";
     private static final String WRITE_CONSISTENCY_LEVEL = "cassandra.consistencylevel.write";
     
-    private static final Logger logger = LoggerFactory.getLogger(ColumnFamilyInputFormat.class);
+private static final Logger logger = LoggerFactory.getLogger(ConfigHelper.class);
 
 
     /**
---------------
-------------
@@ -157,7 +157,7 @@
 
     SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, path, Text.class, TopKStringPatterns.class);
 
-    if ("true".equals(params.get("useFPG2"))) {
+if ("true".equals(params.get(PFPGrowth.USE_FPG2))) {
       org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj<String> fp 
         = new org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj<String>();
       Collection<String> features = new HashSet<String>();
---------------
-------------
@@ -231,7 +231,7 @@
    * Use this constant when creating Analyzers and any other version-dependent stuff.
    * <p><b>NOTE:</b> Change this when development starts for new Lucene version:
    */
-  public static final Version TEST_VERSION_CURRENT = Version.LUCENE_43;
+public static final Version TEST_VERSION_CURRENT = Version.LUCENE_44;
 
   /**
    * True if and only if tests are run in verbose mode. If this flag is false
---------------
-------------
@@ -323,7 +323,7 @@
         float norm = fieldBoosts[n] * similarity.lengthNorm(fi.name, fieldLengths[n]);
         OutputStream norms = directory.createFile(segment + ".f" + n);
         try {
-          norms.writeByte(similarity.encodeNorm(norm));
+norms.writeByte(Similarity.encodeNorm(norm));
         } finally {
           norms.close();
         }
---------------
-------------
@@ -60,7 +60,7 @@
   final static BytesRef PAYLOAD = SimpleTextFieldsWriter.PAYLOAD;
 
   public SimpleTextFieldsReader(SegmentReadState state) throws IOException {
-    in = state.dir.openInput(SimpleTextPostingsFormat.getPostingsFileName(state.segmentInfo.name, state.formatId), state.context);
+in = state.dir.openInput(SimpleTextPostingsFormat.getPostingsFileName(state.segmentInfo.name, state.segmentSuffix), state.context);
    
     fieldInfos = state.fieldInfos;
   }
---------------
-------------
@@ -139,7 +139,7 @@
           optKey(DefaultOptionCreator.METHOD_OPTION),
           DefaultOptionCreator.SEQUENTIAL_METHOD
       };
-      new FuzzyKMeansDriver().run(args);
+FuzzyKMeansDriver.main(args);
       long count = HadoopUtil.countRecords(new Path(output, "clusteredPoints/part-m-0"), conf);
       assertTrue(count > 0);
     }
---------------
-------------
@@ -59,7 +59,7 @@
    * @return The probability
    * @see Model#featureWeight (String, String)
    */
-  public double documentProbability(Model model, String label, String[] document);
+public double documentWeight(Model model, String label, String[] document);
 
   
 }
---------------
-------------
@@ -21,6 +21,6 @@
 public class TestSimpleLRUCache extends BaseTestLRU {
   public void testLRUCache() throws Exception {
     final int n = 100;
-    testCache(new SimpleLRUCache(n), n);
+testCache(new SimpleLRUCache<Integer,Object>(n), n);
   }
 }
---------------
-------------
@@ -250,7 +250,7 @@
   @Override
   public NumericDocValues getNormValues(String field) throws IOException {
     ensureOpen();
-    return core.getSimpleNormValues(field);
+return core.getNormValues(field);
   }
 
   /**
---------------
-------------
@@ -72,7 +72,7 @@
 	}
 
 	/**
-		@exception StandardException Standard Cloudscape error policy
+@exception StandardException Standard Derby error policy
 	 */
 	protected void doIt(BaseContainerHandle handle)
 		throws StandardException {
---------------
-------------
@@ -378,7 +378,7 @@
         throw new IllegalArgumentException("field '" + field + "' was indexed without offsets, cannot highlight");
       }
       int end = dp.endOffset();
-      if (start > current.endOffset) {
+if (start >= current.endOffset) {
         if (current.startOffset >= 0) {
           // finalize current
           current.score *= scorer.norm(current.startOffset);
---------------
-------------
@@ -810,7 +810,7 @@
     throws IOException, UnavailableException, TimeoutException
     {
         if (logger.isDebugEnabled())
-            logger.debug(command.toString());
+logger.debug("Command/ConsistencyLevel is {}/{}", command.toString(), consistency_level);
         long startTime = System.nanoTime();
         List<Row> rows;
         // now scan until we have enough results
---------------
-------------
@@ -195,7 +195,7 @@
    * RAMOutputStream.BUFFER_SIZE (now 1024) bytes.
    */
 
-  final long getRecomputedActualSizeInBytes() {
+final synchronized long getRecomputedActualSizeInBytes() {
     long size = 0;
     Iterator it = fileMap.values().iterator();
     while (it.hasNext())
---------------
-------------
@@ -26,7 +26,7 @@
 
   @Override
   protected double doComputeResult(int rowA, int rowB, Iterable<Cooccurrence> cooccurrences, double weightOfVectorA,
-      double weightOfVectorB, int numberOfColumns) {
+double weightOfVectorB, long numberOfColumns) {
 
     int n = 0;
     double sumXY = 0.0;
---------------
-------------
@@ -43,6 +43,6 @@
 
 public class Constants {
 
-  public static final String VERSION = "19.1.0";
+public static final String VERSION = "19.2.0";
 
 }
---------------
-------------
@@ -45,7 +45,7 @@
   }
   
   static int idToIndex(long itemID) {
-    return (int) itemID ^ (int) (itemID >>> 32);
+return 0x7FFFFFFF & (int) itemID ^ (int) (itemID >>> 32);
   }
   
 }
---------------
-------------
@@ -9,5 +9,5 @@
     public double rows_cached = CFMetaData.DEFAULT_ROW_CACHE_SIZE; 
     public double keys_cached = CFMetaData.DEFAULT_KEY_CACHE_SIZE; 
     public double read_repair_chance = CFMetaData.DEFAULT_READ_REPAIR_CHANCE;
-    public boolean preloadRowCache = CFMetaData.DEFAULT_PRELOAD_ROW_CACHE;
+public boolean preload_row_cache = CFMetaData.DEFAULT_PRELOAD_ROW_CACHE;
 }
---------------
-------------
@@ -218,7 +218,7 @@
       throw new RuntimeException("Unable to end & close TokenStream after analyzing range part: " + part, e);
     }
       
-    return new BytesRef(bytes);
+return BytesRef.deepCopyOf(bytes);
   }
   
   @Override
---------------
-------------
@@ -451,7 +451,7 @@
   /**
    * @exception ParseException throw in overridden method to disallow
    */
-  protected Query getFieldQuery(String field, String queryText)
+protected Query getFieldQuery(String field, String queryText, boolean quoted)
       throws ParseException {
     throw new UnsupportedOperationException();
   }
---------------
-------------
@@ -93,7 +93,7 @@
   */
   public static void main(String args[]) {
         // adjust the application in accordance with derby.ui.locale and derby.ui.codeset
-        LocalizedResource.getInstance();
+LocalizedResource.getInstance().init();
 
 		LocalizedOutput out;
 
---------------
-------------
@@ -76,6 +76,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new BulgarianAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new BulgarianAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -39,7 +39,7 @@
    * {@link IndexDocValues} instances for this segment and codec.
    */
   public SepDocValuesProducer(SegmentReadState state) throws IOException {
-    docValues = load(state.fieldInfos, state.segmentInfo.name, state.segmentInfo.docCount, state.dir, state.formatId, state.context);
+docValues = load(state.fieldInfos, state.segmentInfo.name, state.segmentInfo.docCount, state.dir, state.context);
   }
   
   @Override
---------------
-------------
@@ -26,7 +26,7 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.*;
+import static org.apache.lucene.analysis.VocabularyAssert.*;
 
 /**
  * Simple tests for {@link GermanLightStemFilter}
---------------
-------------
@@ -117,7 +117,7 @@
                                          field("last",   "jones")     }));
     
     writer.close();
-    searcher = new IndexSearcher(directory);
+searcher = new IndexSearcher(directory, true);
   }
 
   public void tearDown() throws Exception {
---------------
-------------
@@ -112,6 +112,6 @@
     start = newText.getBeginIndex();
     end = newText.getEndIndex();
     text = newText;
-    current = newText.getIndex();
+current = start;
   }
 }
---------------
-------------
@@ -82,7 +82,7 @@
 
     // for Driver.getCompatibileJREVersions()
     public final static String[] dncCompatibleJREVersions =
-            {"1.4", "1.5", "1.6"};
+{"1.5", "1.6", "1.7"};
 
     //---------------------- database URL protocols ------------------------------
 
---------------
-------------
@@ -62,7 +62,7 @@
     if (directory!=null)
       synchronized (directory) {             // Ensure addition of buffer and adjustment to directory size are atomic wrt directory
         buffers.add(buffer);
-        directory.sizeInBytes += size;
+directory.sizeInBytes.getAndAdd(size);
         sizeInBytes += size;
       }
     else
---------------
-------------
@@ -195,7 +195,7 @@
       if (c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')'
           || c == ':' || c == '^' || c == '[' || c == ']' || c == '\"'
           || c == '{' || c == '}' || c == '~' || c == '*' || c == '?'
-          || c == '|' || c == '&') {
+|| c == '|' || c == '&' || c == '/') {
         sb.append('\\');
       }
       sb.append(c);
---------------
-------------
@@ -41,7 +41,7 @@
 
   @Override
   protected int hashForProbe(String originalForm, Vector data, String name, int i) {
-    return hash(name, i, data.size());
+return hash(name, originalForm, WORD_LIKE_VALUE_HASH_SEED + i, data.size());
   }
 
    /**
---------------
-------------
@@ -58,7 +58,7 @@
     writer.addDocument(doc4);
     writer.addDocument(doc5);
     IndexReader reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     
     IndexSearcher searcher = newSearcher(reader);
     
---------------
-------------
@@ -295,7 +295,7 @@
 
     @Override
     public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      bytesRef.copy(missingValue);
+bytesRef.copyBytes(missingValue);
       return bytesRef;
     }
 
---------------
-------------
@@ -42,7 +42,7 @@
 			QueryResult result = twitter.search(query);
 		    List<Tweet> tweets = result.getTweets(); 
 		    System.out.println("hits:" + tweets.size());
-		    for (Tweet tweet : result.getTweets()) {
+for (Tweet tweet : tweets) {
 		        System.out.println(tweet.getFromUser() + ":" + StringEscapeUtils.unescapeXml(tweet.getText()));
 		    }
 		} catch (Exception e) {
---------------
-------------
@@ -69,7 +69,7 @@
   }
   
   public void test() throws Exception {
-    NumericDocValues fooNorms = MultiSimpleDocValues.simpleNormValues(reader, "foo");
+NumericDocValues fooNorms = MultiDocValues.getNormValues(reader, "foo");
     assertNotNull(fooNorms);
     for (int i = 0; i < reader.maxDoc(); i++) {
       assertEquals(expected.get(i).longValue(), fooNorms.get(i));
---------------
-------------
@@ -387,7 +387,7 @@
          resumeStreaming();        
     }
     
-    void doPendingWrites()
+public void doPendingWrites()
     {
         synchronized(this)
         {
---------------
-------------
@@ -156,7 +156,7 @@
   {
     float boost = getBoost();
     return (boost!=1.0?"(":"") + func.toString()
-            + (getBoost()==0 ? "" : ")^"+getBoost());
++ (boost==1.0 ? "" : ")^"+boost);
   }
 
 
---------------
-------------
@@ -265,7 +265,7 @@
                 int position = 0;
                 for(int j=0;j<termDocFreq;j++) {
                   final int code = prox.readVInt();
-                  position += code >> 1;
+position += code >>> 1;
                 
                 final int payloadLength;
                 if ((code & 1) != 0) {
---------------
-------------
@@ -123,7 +123,7 @@
   private void addGroupField(Document doc, String groupField, String value, boolean canUseIDV) {
     doc.add(new Field(groupField, value, TextField.TYPE_STORED));
     if (canUseIDV) {
-      doc.add(new DocValuesField(groupField, new BytesRef(value), Type.BYTES_VAR_SORTED));
+doc.add(new SortedBytesDocValuesField(groupField, new BytesRef(value)));
     }
   }
 
---------------
-------------
@@ -130,7 +130,7 @@
         indexerNamespace.put(LAST_INDEX_TIME, EPOCH);
       }
       indexerNamespace.put(INDEX_START_TIME, dataImporter.getIndexStartTime());
-      indexerNamespace.put("request", reqParams.getRawParams());
+indexerNamespace.put("request", new HashMap<String,Object>(reqParams.getRawParams()));
       for (Entity entity : dataImporter.getConfig().getEntities()) {
         String key = entity.getName() + "." + SolrWriter.LAST_INDEX_KEY;
         Object lastIndex = persistedProperties.get(key);
---------------
-------------
@@ -32,7 +32,7 @@
   private final KeywordAttribute keywordAtt = addAttribute(KeywordAttribute.class);
   private final HindiStemmer stemmer = new HindiStemmer();
   
-  protected HindiStemFilter(TokenStream input) {
+public HindiStemFilter(TokenStream input) {
     super(input);
   }
   
---------------
-------------
@@ -1604,7 +1604,7 @@
         Gossiper.instance.addLocalApplicationState(ApplicationState.STATUS, valueFactory.left(getLocalToken()));
         try
         {
-            Thread.sleep(2 * Gossiper.intervalInMillis_);
+Thread.sleep(2 * Gossiper.intervalInMillis);
         }
         catch (InterruptedException e)
         {
---------------
-------------
@@ -698,7 +698,7 @@
         int sqlcode = readFdocaInt();
         byte[] sqlstate = readFdocaBytes(5);
         byte[] sqlerrproc = readFdocaBytes(8);
-        NetSqlca netSqlca = new NetSqlca(netAgent_.netConnection_, sqlcode, sqlstate, sqlerrproc, typdef.getCcsidSbc());
+NetSqlca netSqlca = new NetSqlca(netAgent_.netConnection_, sqlcode, sqlstate, sqlerrproc);
 
         parseSQLCAXGRP(typdef, netSqlca);
 
---------------
-------------
@@ -134,7 +134,7 @@
      */
     private Cassandra.Client getClient() throws TTransportException
     {
-        TTransport tr = new TSocket("localhost", 9170);
+TTransport tr = new TSocket("localhost", DatabaseDescriptor.getRpcPort());
         TProtocol proto = new TBinaryProtocol(tr);
         Cassandra.Client client = new Cassandra.Client(proto);
         tr.open();
---------------
-------------
@@ -59,5 +59,5 @@
     return errCode;
   }
 
-  public static final String MSG = " Processing Documemt # ";
+public static final String MSG = " Processing Document # ";
 }
---------------
-------------
@@ -147,7 +147,7 @@
       int idx = 0;
       for (Weight w : weights) {
         Scorer subScorer = w.scorer(reader, true, false);
-        if (subScorer != null && subScorer.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
+if (subScorer != null) {
           scorers[idx++] = subScorer;
         }
       }
---------------
-------------
@@ -749,7 +749,7 @@
             {"C"}
         };
         
-        JDBC.assertFullResultSet(rs, expRS, true);
+JDBC.assertUnorderedResultSet(rs, expRS, true);
         
         assertStatementError("42Y55", st,
             " drop table t2");
---------------
-------------
@@ -1202,7 +1202,7 @@
 					// closed sometime later by the connection
 					// outside of finalization.
 					if (a.isSingleExecution())
-						lresults.finalizeActivation = a;
+lresults.singleUseActivation = a;
 
 					updateCount = -1;
 					retval = true;
---------------
-------------
@@ -166,7 +166,7 @@
 		final Bundle bundle;
 		InputStream is = ((RepositoryContent)resource).getContent();
 		try {
-			bundle = provisionTo.getRegion().installBundle(getLocation(), is);
+bundle = provisionTo.getRegion().installBundleAtLocation(getLocation(), is);
 		}
 		catch (BundleException e) {
 			throw new SubsystemException(e);
---------------
-------------
@@ -92,7 +92,7 @@
     } else {
       if (reader.termDocsEnum(reader.getLiveDocs(), term.field(), term.bytes()) != null) {
         // term does exist, but has no positions
-        throw new IllegalStateException("field \"" + term.field() + "\" was indexed with Field.omitTermFreqAndPositions=true; cannot run SpanTermQuery (term=" + term.text() + ")");
+throw new IllegalStateException("field \"" + term.field() + "\" was indexed without position data; cannot run SpanTermQuery (term=" + term.text() + ")");
       } else {
         // term does not exist
         return TermSpans.EMPTY_TERM_SPANS;
---------------
-------------
@@ -360,7 +360,7 @@
    *  are in fact {@link BooleanPreference}s
    */
   protected User buildUser(String id, List<Preference> prefs) {
-    if (!prefs.isEmpty() || prefs.get(0) instanceof BooleanPreference) {
+if (!prefs.isEmpty() && prefs.get(0) instanceof BooleanPreference) {
       // If first is a BooleanPreference, assuming all are, so, want to use BooleanPrefUser
       FastSet<Object> itemIDs = new FastSet<Object>(prefs.size());
       for (Preference pref : prefs) {
---------------
-------------
@@ -107,7 +107,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random(), directory);
     writer.commit();
     IndexReader ir = writer.getReader();
-    writer.close();
+writer.shutdown();
     IndexSearcher searcher = newSearcher(ir);
     Weight fake = new TermQuery(new Term("fake", "weight")).createWeight(searcher);
     Scorer s = new SimpleScorer(fake);
---------------
-------------
@@ -82,7 +82,7 @@
 	}
 	
 	/**
-		@exception StandardException Standard Cloudscape error policy
+@exception StandardException Standard Derby error policy
 	*/
 	public void open() throws StandardException
 	{
---------------
-------------
@@ -293,7 +293,7 @@
 
   private void logConfig() {
     if (!LOG.isInfoEnabled()) return;
-    StringBuffer config = new StringBuffer();
+StringBuilder config = new StringBuilder();
     config.append("user : ").append(user).append(System.getProperty("line.separator"));
     config.append("pwd : ").append(password).append(System.getProperty("line.separator"));
     config.append("protocol : ").append(protocol).append(System.getProperty("line.separator"));
---------------
-------------
@@ -53,7 +53,7 @@
         Iterator<Token> iter = TokenMetadata.ringIterator(tokens, token);
         while (endpoints.size() < replicas && iter.hasNext())
         {
-            endpoints.add(metadata.getEndPoint(iter.next()));
+endpoints.add(metadata.getEndpoint(iter.next()));
         }
 
         return endpoints;
---------------
-------------
@@ -223,7 +223,7 @@
               final int pos = termPositions.nextPosition();
               if (pos < 0)
                 throw new RuntimeException("term " + term + ": doc " + doc + ": pos " + pos + " is out of bounds");
-              if (pos <= lastPos)
+if (pos < lastPos)
                 throw new RuntimeException("term " + term + ": doc " + doc + ": pos " + pos + " < lastPos " + lastPos);
             }
           }
---------------
-------------
@@ -73,7 +73,7 @@
     private MemtableThreadPoolExecutor executor_;
 
     private int threshold_ = DatabaseDescriptor.getMemtableSize()*1024*1024;
-    private int thresholdCount_ = DatabaseDescriptor.getMemtableObjectCount()*1024*1024;
+private int thresholdCount_ = (int)(DatabaseDescriptor.getMemtableObjectCount()*1024*1024);
     private AtomicInteger currentSize_ = new AtomicInteger(0);
     private AtomicInteger currentObjectCount_ = new AtomicInteger(0);
 
---------------
-------------
@@ -134,7 +134,7 @@
         if (!exceptions[i].endsWith(suffix))
           throw new RuntimeException("useless exception '" + exceptions[i] + "' does not end with '" + suffix + "'");
       }
-      this.exceptions = new CharArraySet(Version.LUCENE_31,
+this.exceptions = new CharArraySet(Version.LUCENE_CURRENT,
            Arrays.asList(exceptions), false);
     }
 
---------------
-------------
@@ -22,5 +22,5 @@
  * A callback to indicate that a destroy operation has completed
  */
 public interface DestroyCallback {
-  public void callback(Object key);
+public void callback();
 }
---------------
-------------
@@ -42,7 +42,7 @@
  
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), a, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
   }
   
   /** 
---------------
-------------
@@ -41,7 +41,7 @@
  * stop words, excluding non-alpha-numeric tokens, and porter stemming.
  */
 public final class MailArchivesClusteringAnalyzer extends StopwordAnalyzerBase {
-  private static final Version LUCENE_VERSION = Version.LUCENE_41;
+private static final Version LUCENE_VERSION = Version.LUCENE_42;
   
   // extended set of stop words composed of common mail terms like "hi",
   // HTML tags, and Java keywords asmany of the messages in the archives
---------------
-------------
@@ -100,7 +100,7 @@
      */
     public static synchronized void updateToken(InetAddress ep, Token token)
     {
-        if (ep == FBUtilities.getBroadcastAddress())
+if (ep == FBUtilities.getLocalAddress())
             return;
         IPartitioner p = StorageService.getPartitioner();
         ColumnFamily cf = ColumnFamily.create(Table.SYSTEM_TABLE, STATUS_CF);
---------------
-------------
@@ -225,7 +225,7 @@
 
 			} else {
 
-				if (ldapServer.startsWith(dfltLDAPURL))
+if (ldapServer.startsWith(dfltLDAPURL) || ldapServer.startsWith("ldaps://") )
 					this.providerURL = ldapServer;
 				else if (ldapServer.startsWith("//"))
 					this.providerURL = "ldap:" + ldapServer;
---------------
-------------
@@ -91,7 +91,7 @@
     private class SimplePrefixTermsEnum extends FilteredTermsEnum {
       private final BytesRef prefix;
 
-      private SimplePrefixTermsEnum(TermsEnum tenum, BytesRef prefix) throws IOException {
+private SimplePrefixTermsEnum(TermsEnum tenum, BytesRef prefix) {
         super(tenum);
         this.prefix = prefix;
         setInitialSeekTerm(new BytesRef(""));
---------------
-------------
@@ -70,7 +70,7 @@
       new PhoneticFilterFactory(new HashMap<String,String>());
       fail();
     } catch (IllegalArgumentException expected) {
-      assertTrue(expected.getMessage().contains("Missing required parameter"));
+assertTrue(expected.getMessage().contains("Configuration Error: missing parameter 'encoder'"));
     }
   }
   
---------------
-------------
@@ -679,7 +679,7 @@
         if (buildOnCommit)  {
           buildSpellIndex(newSearcher);
         } else if (buildOnOptimize) {
-          if (newSearcher.getIndexReader().getSequentialSubReaders().size() == 1)  {
+if (newSearcher.getIndexReader().leaves().size() == 1)  {
             buildSpellIndex(newSearcher);
           } else  {
             LOG.info("Index is not optimized therefore skipping building spell check index for: " + checker.getDictionaryName());
---------------
-------------
@@ -283,7 +283,7 @@
       assertTrue(((SortingDocsAndPositionsEnum) sortedPositions).reused(reuse)); // make sure reuse worked
     }
     doc = 0;
-    while ((doc = sortedPositions.advance(doc)) != DocIdSetIterator.NO_MORE_DOCS) {
+while ((doc = sortedPositions.advance(doc + _TestUtil.nextInt(random(), 1, 5))) != DocIdSetIterator.NO_MORE_DOCS) {
       int freq = sortedPositions.freq();
       assertEquals("incorrect freq for doc=" + doc, sortedValues[doc].intValue() / 10 + 1, freq);
       for (int i = 0; i < freq; i++) {
---------------
-------------
@@ -1242,7 +1242,7 @@
 
 					updateCount = resultsToWrap.modifiedRowCount();
 
-					resultsToWrap.finish();	// Don't need the result set any more
+resultsToWrap.close();	// Don't need the result set any more
 					results = null; // note that we have none.
 
                     int dynamicResultCount = 0;
---------------
-------------
@@ -347,7 +347,7 @@
   /** use reuters and the exhaust mechanism, but to be faster, add 20 docs only... */
   public static class Reuters20DocMaker extends ReutersDocMaker {
     private int nDocs=0;
-    protected DocData getNextDocData() throws Exception {
+protected synchronized DocData getNextDocData() throws Exception {
       if (nDocs>=20 && !forever) {
         throw new NoMoreDataException();
       }
---------------
-------------
@@ -156,7 +156,7 @@
     SolrZkClient.numCloses.getAndSet(0);
 
     
-    if (endNumOpens-numOpens != endNumCloses-numCloses) {
+if (endNumOpens-zkClientNumOpens != endNumCloses-zkClientNumCloses) {
       String msg = "ERROR: SolrZkClient opens=" + (endNumOpens-zkClientNumOpens) + " closes=" + (endNumCloses-zkClientNumCloses);
       log.error(msg);
       testsFailed = true;
---------------
-------------
@@ -1110,7 +1110,7 @@
     }
 
     public void testOpenReaderAfterDelete() throws IOException {
-      File dirFile = new File(TEMP_DIR, "deletetest");
+File dirFile = _TestUtil.getTempDir("deletetest");
       Directory dir = newFSDirectory(dirFile);
       try {
         IndexReader.open(dir, false);
---------------
-------------
@@ -544,7 +544,7 @@
     indexInfo.add("maxDoc", reader.maxDoc());
 
     indexInfo.add("version", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?
-    indexInfo.add("segmentCount", reader.getTopReaderContext().leaves().size());
+indexInfo.add("segmentCount", reader.leaves().size());
     indexInfo.add("current", reader.isCurrent() );
     indexInfo.add("hasDeletions", reader.hasDeletions() );
     indexInfo.add("directory", dir );
---------------
-------------
@@ -189,7 +189,7 @@
 
     public CustomWeight(Searcher searcher) throws IOException {
       this.similarity = getSimilarity(searcher);
-      this.subQueryWeight = subQuery.weight(searcher);
+this.subQueryWeight = subQuery.createWeight(searcher);
       this.valSrcWeights = new Weight[valSrcQueries.length];
       for(int i = 0; i < valSrcQueries.length; i++) {
         this.valSrcWeights[i] = valSrcQueries[i].createWeight(searcher);
---------------
-------------
@@ -307,7 +307,7 @@
         String encStr = params.getFieldParam(fname,ENCAPSULATOR);
         char fenc = encStr==null || encStr.length()==0 ? (char)-2 : encStr.charAt(0);
         String escStr = params.getFieldParam(fname,ESCAPE);
-        char fesc = escStr==null || encStr.length()==0 ? CSVStrategy.ESCAPE_DISABLED : escStr.charAt(0);
+char fesc = escStr==null || escStr.length()==0 ? CSVStrategy.ESCAPE_DISABLED : escStr.charAt(0);
 
         CSVStrategy fstrat = new CSVStrategy(fsep,fenc,CSVStrategy.COMMENTS_DISABLED,fesc, false, false, false, false);
         adders[i] = new CSVLoader.FieldSplitter(fstrat, adders[i]);
---------------
-------------
@@ -326,7 +326,7 @@
       }
 
       @Override
-      protected Reader initReader(Reader reader) {
+protected Reader initReader(String fieldName, Reader reader) {
         return new MappingCharFilter(normMap, CharReader.get(reader));
       }
     };
---------------
-------------
@@ -304,7 +304,7 @@
    *  writer. */
   public Status checkIndex(List<String> onlySegments, CodecProvider codecs) throws IOException {
     NumberFormat nf = NumberFormat.getInstance();
-    SegmentInfos sis = new SegmentInfos();
+SegmentInfos sis = new SegmentInfos(codecs);
     Status result = new Status();
     result.dir = dir;
     try {
---------------
-------------
@@ -458,7 +458,7 @@
 
         public View switchMemtable(Memtable newMemtable)
         {
-            Set<Memtable> newPending = ImmutableSet.<Memtable>builder().addAll(memtablesPendingFlush).add(newMemtable).build();
+Set<Memtable> newPending = ImmutableSet.<Memtable>builder().addAll(memtablesPendingFlush).add(memtable).build();
             return new View(newMemtable, newPending, sstables, compacting);
         }
 
---------------
-------------
@@ -77,6 +77,6 @@
       }
     };
     
-    checkRandomData(random(), a, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -117,7 +117,7 @@
 
   public SolrIndexSearcher(SolrCore core, String path, IndexSchema schema, SolrIndexConfig config, String name, boolean enableCache, DirectoryFactory directoryFactory) throws IOException {
     // we don't need to reserve the directory because we get it from the factory
-    this(core, schema,name, core.getIndexReaderFactory().newReader(directoryFactory.get(path, config.lockType)), true, enableCache, false, directoryFactory);
+this(core, schema,name, core.getIndexReaderFactory().newReader(directoryFactory.get(path, config.lockType), core), true, enableCache, false, directoryFactory);
   }
 
   public SolrIndexSearcher(SolrCore core, IndexSchema schema, String name, DirectoryReader r, boolean closeReader, boolean enableCache, boolean reserveDirectory, DirectoryFactory directoryFactory) throws IOException {
---------------
-------------
@@ -148,7 +148,7 @@
       provider = c.newInstance();
       provider.init(args);
     } catch (Exception e) {
-      throw new SolrException(ErrorCode.BAD_REQUEST, "Error instantiating exhange rate provider "+exchangeRateProviderClass+": " + e.getMessage(), e);
+throw new SolrException(ErrorCode.BAD_REQUEST, "Error instantiating exchange rate provider "+exchangeRateProviderClass+": " + e.getMessage(), e);
     }
   }
 
---------------
-------------
@@ -135,7 +135,7 @@
         writer.addDocument(doc4);
 	writer.optimize();
 	IndexSearcher searcher = new IndexSearcher(indexStore);
-	Query query1 = new TermQuery(new Term("body", "m?tal"));       // 1
+Query query1 = new WildcardQuery(new Term("body", "m?tal"));       // 1
         Query query2 = new WildcardQuery(new Term("body", "metal?"));  // 2
         Query query3 = new WildcardQuery(new Term("body", "metals?")); // 1
         Query query4 = new WildcardQuery(new Term("body", "m?t?ls"));  // 3
---------------
-------------
@@ -44,7 +44,7 @@
 
   @Override
   public DocIdSet getDocIdSet(final IndexReader reader) throws IOException {
-    final Weight weight = query.weight(new IndexSearcher(reader));
+final Weight weight = new IndexSearcher(reader).createNormalizedWeight(query);
     return new DocIdSet() {
       @Override
       public DocIdSetIterator iterator() throws IOException {
---------------
-------------
@@ -607,7 +607,7 @@
    * is active and {@link #RANDOM_MULTIPLIER}, but also with some random fudge.
    */
   public static int atLeast(Random random, int i) {
-    int min = (TEST_NIGHTLY ? 5*i : i) * RANDOM_MULTIPLIER;
+int min = (TEST_NIGHTLY ? 3*i : i) * RANDOM_MULTIPLIER;
     int max = min+(min/2);
     return _TestUtil.nextInt(random, min, max);
   }
---------------
-------------
@@ -826,7 +826,7 @@
 		try {
 			return fr.getAsStream(externalName, generationId);
 		} catch (java.io.IOException ioe) {
-			throw StandardException.newException(SQLState.LANG_FILE_ERROR, ioe.toString(),ioe);	
+throw StandardException.newException(SQLState.LANG_FILE_ERROR, ioe, ioe.toString());
 		}
 	}
 
---------------
-------------
@@ -568,7 +568,7 @@
 
 			try	{
 				try	{
-					theResults.finish(); // release the result set, don't just close it
+theResults.close();
 				    
 				    if (this.singleUseActivation != null)
 				    {
---------------
-------------
@@ -47,7 +47,7 @@
 
     if (filters != null) {
       for (Query filt : filters)
-        h += filters.hashCode();
+h += filt.hashCode();
     }
 
     sfields = (this.sort !=null) ? this.sort.getSort() : defaultSort;
---------------
-------------
@@ -145,7 +145,7 @@
     int num = 100 * RANDOM_MULTIPLIER;
     for (int i = 0; i < num; i++) {
       ir1 = IndexReader.open(dir1, false);
-      doTest(10,100);
+doTest(10,10);
       ir1.close();
     }
     dir1.close();
---------------
-------------
@@ -613,7 +613,7 @@
     
     // check dictionary and posting lists
     FieldsEnum fenum1 = MultiFields.getFields(index1).iterator();
-    FieldsEnum fenum2 = MultiFields.getFields(index1).iterator();
+FieldsEnum fenum2 = MultiFields.getFields(index2).iterator();
     String field1 = null;
     Bits liveDocs = MultiFields.getLiveDocs(index1);
     while((field1=fenum1.next()) != null) {
---------------
-------------
@@ -711,7 +711,7 @@
                 }
                 // make sure there is actually non-tombstone content associated w/ this key
                 // TODO record the key source(s) somehow and only check that source (e.g., memtable or sstable)
-                if (ColumnFamilyStore.removeDeleted(cfs.getColumnFamily(new SliceQueryFilter(current, new QueryPath(cfName), "", "", true, 1)), Integer.MAX_VALUE) != null)
+if (cfs.getColumnFamily(new SliceQueryFilter(current, new QueryPath(cfName), "", "", true, 1), Integer.MAX_VALUE) != null)
                 {
                     keys.add(current);
                 }
---------------
-------------
@@ -92,7 +92,7 @@
     writer.close();
     
 
-    IndexSearcher searcher = new IndexSearcher(store);
+IndexSearcher searcher = new IndexSearcher(store, true);
     
     TermPositions pos = searcher.getIndexReader().termPositions(new Term("field", "1"));
     pos.next();
---------------
-------------
@@ -315,7 +315,7 @@
     }
     
     TermAllGroupsCollector collector = new TermAllGroupsCollector(groupField);
-    Filter mainQueryFilter = docsOrig.getTopFilter(); // This returns a filter that only matches documents matching with q param and fq params
+Filter mainQueryFilter = docs.getTopFilter(); // This returns a filter that only matches documents matching with q param and fq params
     searcher.search(facetQuery, mainQueryFilter, collector);
     return collector.getGroupCount();
   }
---------------
-------------
@@ -1,6 +1,6 @@
 /*
 
-   Derby - Class com.ihost.cs.Matchable
+Derby - Class org.apache.derby.iapi.util.Matchable
 
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -163,7 +163,7 @@
         return null;
       }
 
-      final DocIdSet parents = parentsFilter.getDocIdSet(readerContext);
+final DocIdSet parents = parentsFilter.getDocIdSet(readerContext, readerContext.reader.getLiveDocs());
       // TODO: once we do random-access filters we can
       // generalize this:
       if (parents == null) {
---------------
-------------
@@ -330,7 +330,7 @@
 
         assertEquals( myName + ": serverCameUp = " + serverCameUp, _outcome.serverShouldComeUp(), serverCameUp );
 
-        if (!runsWithEmma()) {
+if (!(runsWithEmma() || runsWithJaCoCo())) {
             // With Emma we run without the security manager, so we can't
             // assert on seeing it.
             assertTrue( myName + "\nExpected: " +
---------------
-------------
@@ -63,7 +63,7 @@
 
 			if (firstTime) {
 				Logs.reportString("----------------------------------------------");
-				Logs.reportMessage("CSLOOK_ChecksHeader");
+Logs.reportMessage("DBLOOK_ChecksHeader");
 				Logs.reportString("----------------------------------------------\n");
 			}
 
---------------
-------------
@@ -40,7 +40,7 @@
 import org.apache.cassandra.db.filter.QueryPath;
 import org.apache.cassandra.db.filter.SliceQueryFilter;
 import org.apache.cassandra.db.filter.NamesQueryFilter;
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableReader;
 
 public class ColumnFamilyStoreTest extends CleanupHelper
 {
---------------
-------------
@@ -79,7 +79,7 @@
 
     public ColumnFamily getColumnFamily()
     {
-        return reader.getEmptyColumnFamily();
+return reader == null ? null : reader.getEmptyColumnFamily();
     }
 
     protected IColumn computeNext()
---------------
-------------
@@ -423,7 +423,7 @@
 
     public void forceFlushBinary()
     {
-        if (memtable_.isClean())
+if (binaryMemtable_.get().isClean())
             return;
 
         submitFlush(binaryMemtable_.get());
---------------
-------------
@@ -92,7 +92,7 @@
      */
     public void printRing(PrintStream outs)
     {
-        Map<Range, List<String>> rangeMap = probe.getRangeToEndPointMap(null);
+Map<Range, List<String>> rangeMap = probe.getRangeToEndpointMap(null);
         List<Range> ranges = new ArrayList<Range>(rangeMap.keySet());
         Collections.sort(ranges);
         Set<String> liveNodes = probe.getLiveNodes();
---------------
-------------
@@ -340,7 +340,7 @@
         cause = stacktraces.next();
       // RuntimeException instead of IOException because
       // super() does not throw IOException currently:
-      throw new RuntimeException("MockRAMDirectory: cannot close: there are still open files: " + openFiles, cause);
+throw new RuntimeException("MockDirectoryWrapper: cannot close: there are still open files: " + openFiles, cause);
     }
     open = false;
     delegate.close();
---------------
-------------
@@ -961,7 +961,7 @@
 
   @Nightly
   public void testBigSet() throws IOException {
-    testRandomWords(atLeast(50000), atLeast(1));
+testRandomWords(_TestUtil.nextInt(random, 50000, 60000), atLeast(1));
   }
 
   private static String inputToString(int inputMode, IntsRef term) {
---------------
-------------
@@ -18,7 +18,7 @@
 
 package org.apache.cassandra.utils;
 
-public class Pair<T1, T2>
+public final class Pair<T1, T2>
 {
     public final T1 left;
     public final T2 right;
---------------
-------------
@@ -161,7 +161,7 @@
     }
     
     @Override
-    public void release() throws IOException {
+public void close() throws IOException {
       FileSystem fs = FileSystem.newInstance(lockPath.toUri(), conf);
       try {
         if (fs.exists(new Path(lockPath, lockName))
---------------
-------------
@@ -260,7 +260,7 @@
     } else {
       res = entries.keySet().toArray(new String[entries.size()]);
       // Add the segment name
-      String seg = fileName.substring(0, fileName.indexOf('.'));
+String seg = IndexFileNames.parseSegmentName(fileName);
       for (int i = 0; i < res.length; i++) {
         res[i] = seg + res[i];
       }
---------------
-------------
@@ -96,7 +96,7 @@
     }
     if (ids.size() == 0) return;
 
-    StringBuffer sb = new StringBuffer("id:(");
+StringBuilder sb = new StringBuilder("id:(");
     for (String id : ids) {
       sb.append(id).append(' ');
       model.remove(id);
---------------
-------------
@@ -27,7 +27,7 @@
 import java.util.Map;
 
 import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.io.sstable.SSTable;
 
 import org.junit.Test;
 
---------------
-------------
@@ -69,7 +69,7 @@
     }
     //System.out.println("addCount:"+addCount);
     //System.out.println("delCount:"+delCount);
-    writer.close();
+writer.shutdown();
     mainDir.close();
   }
 
---------------
-------------
@@ -61,7 +61,7 @@
     {
         try
         {
-            long timeout = System.currentTimeMillis() - startTime + DatabaseDescriptor.getRpcTimeout();
+long timeout = DatabaseDescriptor.getRpcTimeout() - (System.currentTimeMillis() - startTime);
             boolean success;
             try
             {
---------------
-------------
@@ -131,7 +131,7 @@
       // normal version
       constantVersion = Constants.LUCENE_MAIN_VERSION;
     }
-    assertTrue("Invalid version: "+version,
+assertTrue("Invalid version: "+version + " vs " + constantVersion,
                version.equals(constantVersion));
     assertTrue(Constants.LUCENE_VERSION + " should start with: "+version,
                Constants.LUCENE_VERSION.startsWith(version));
---------------
-------------
@@ -131,6 +131,6 @@
 
   /** blast some random strings through the analyzer */
   public void testRandom() throws Exception {
-    checkRandomData(random(), getTestAnalyzer(), 10000 * RANDOM_MULTIPLIER); 
+checkRandomData(random(), getTestAnalyzer(), 1000 * RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -1731,7 +1731,7 @@
                 sessionState.out.printf("      Compaction Strategy: %s%n", cf_def.compaction_strategy);
                 if (!cf_def.compaction_strategy_options.isEmpty())
                 {
-                    sessionState.out.printf("      Compaction Strategy Options: %s%n", cf_def.compaction_strategy);
+sessionState.out.println("      Compaction Strategy Options:");
                     for (Map.Entry<String, String> e : cf_def.compaction_strategy_options.entrySet())
                         sessionState.out.printf("        %s: %s%n", e.getKey(), e.getValue());
                 }
---------------
-------------
@@ -65,7 +65,7 @@
   @Test
   public void baseUIMAAnalyzerIntegrationTest() throws Exception {
     Directory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_40, analyzer));
+IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_50, analyzer));
     // add the first doc
     Document doc = new Document();
     String dummyTitle = "this is a dummy title ";
---------------
-------------
@@ -51,7 +51,7 @@
     while (!shutdown) {
       try {
         // Wait a while for existing tasks to terminate
-        shutdown = pool.awaitTermination(30, TimeUnit.SECONDS);
+shutdown = pool.awaitTermination(60, TimeUnit.SECONDS);
       } catch (InterruptedException ie) {
         // Preserve interrupt status
         Thread.currentThread().interrupt();
---------------
-------------
@@ -105,7 +105,7 @@
 
     AtomicReaderContext[] leaves = ReaderUtil.leaves(r.getTopReaderContext());
     for (int i = 0; i < leaves.length; i++) {
-      f.getDocIdSet(leaves[i]);
+f.getDocIdSet(leaves[i], leaves[i].reader.getLiveDocs());
     }
     r.close();
   }
---------------
-------------
@@ -57,7 +57,7 @@
     }
 
     @Override
-    public int size() throws IOException {
+public int size() {
       return in.size();
     }
 
---------------
-------------
@@ -84,7 +84,7 @@
 	 * Simple text indicating any limits execeeded while generating
 	 * the class file.
 	 */
-	private String limitMsg;
+String limitMsg;
 	
 	//
 	// ClassBuilder interface
---------------
-------------
@@ -77,7 +77,7 @@
       abuilder.withName("dictionaryType").withMinimum(1).withMaximum(1).create()).withDescription(
       "The dictionary file type (text|sequencefile)").withShortName("dt").create();
     Option centroidJSonOpt = obuilder.withLongName("json").withRequired(false).withDescription(
-      "Output the centroid as JSON.  Otherwise it substitues in the terms for vector cell entries")
+"Output the centroid as JSON.  Otherwise it substitutes in the terms for vector cell entries")
         .withShortName("j").create();
     Option helpOpt = obuilder.withLongName("help").withDescription("Print out help").withShortName("h")
         .create();
---------------
-------------
@@ -393,7 +393,7 @@
 
                 String key = "Note for DERBY-" + issue.getKey();
                 //println("Release note: "+issue.getKey()+" - "+issue.getTitle());
-                Element paragraph = outputDoc.createElement(PARAGRAPH);
+Element paragraph = outputDoc.createElement(SPAN);
                 paragraph.appendChild(outputDoc.createTextNode(key + ": "));
                 cloneChildren(summaryText, paragraph);
                 insertLine(issuesSection);
---------------
-------------
@@ -500,7 +500,7 @@
       TermsEnum termsEnum = terms.iterator(null);
       DocsEnum docs = null;
       while(termsEnum.next() != null) {
-        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);
+docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, true);
         while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
           totalTokenCount2 += docs.freq();
         }
---------------
-------------
@@ -91,7 +91,7 @@
           endOffset = off;
           cp = readCodePoint();
         } while (cp >= 0 && isTokenChar(cp));
-        offsetAtt.setOffset(startOffset, endOffset);
+offsetAtt.setOffset(correctOffset(startOffset), correctOffset(endOffset));
         streamState = State.INCREMENT;
         return true;
       }
---------------
-------------
@@ -80,7 +80,7 @@
       line = d.readLine();
     }
     d.close();
-    writer.close();
+writer.shutdown();
     reader = DirectoryReader.open(dir);
     searcher = newSearcher(reader);
 
---------------
-------------
@@ -110,7 +110,7 @@
             configFileName = getStorageConfigPath();
             
             if (logger.isDebugEnabled())
-                logger.debug("Loading settings from " + configFileName);
+logger.info("Loading settings from " + configFileName);
             
             InputStream input = new FileInputStream(new File(configFileName));
             org.yaml.snakeyaml.constructor.Constructor constructor = new org.yaml.snakeyaml.constructor.Constructor(Config.class);
---------------
-------------
@@ -177,7 +177,7 @@
 
     while (tokenStream.incrementToken()) {
       termAttribute.fillBytesRef();
-      bytesRefs.add(new BytesRef(bytesRef));
+bytesRefs.add(BytesRef.deepCopyOf(bytesRef));
     }
 
     tokenStream.end();
---------------
-------------
@@ -134,7 +134,7 @@
         if (!exceptions[i].endsWith(suffix))
           throw new RuntimeException("useless exception '" + exceptions[i] + "' does not end with '" + suffix + "'");
       }
-      this.exceptions = new CharArraySet(Version.LUCENE_50,
+this.exceptions = new CharArraySet(Version.LUCENE_CURRENT,
            Arrays.asList(exceptions), false);
     }
 
---------------
-------------
@@ -306,7 +306,7 @@
                 q <= 0);
 
       // power iterations
-      for (int i = 0; i < q; q--) {
+for (int i = 0; i < q; i++) {
 
         qPath = new Path(outputPath, String.format("ABt-job-%d", i + 1));
         ABtJob.run(conf,
---------------
-------------
@@ -46,7 +46,7 @@
   }
   
   public void testRandomStrings() throws IOException {
-    checkRandomData(random(), analyzer, atLeast(10000));
+checkRandomData(random(), analyzer, atLeast(1000));
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -184,7 +184,7 @@
     w.addDocument(doc);
     w.forceMerge(1);
     DirectoryReader r = w.getReader();
-    BinaryDocValues s = FieldCache.DEFAULT.getTerms(getOnlySegmentReader(r), "field");
+BinaryDocValues s = FieldCache.DEFAULT.getTerms(getOnlySegmentReader(r), "field", false);
 
     BytesRef bytes1 = new BytesRef();
     s.get(0, bytes1);
---------------
-------------
@@ -32,7 +32,7 @@
      */
     public IdentityQueryFilter()
     {
-        super(ArrayUtils.EMPTY_BYTE_ARRAY, ArrayUtils.EMPTY_BYTE_ARRAY, null, false, Integer.MAX_VALUE);
+super(ArrayUtils.EMPTY_BYTE_ARRAY, ArrayUtils.EMPTY_BYTE_ARRAY, false, Integer.MAX_VALUE);
     }
 
     public SuperColumn filterSuperColumn(SuperColumn superColumn, int gcBefore)
---------------
-------------
@@ -146,6 +146,6 @@
         df++;
       }
     }
-    return new TermStats(df, totTF);
+return new TermStats(df, indexOptions == IndexOptions.DOCS_ONLY ? -1 : totTF);
   }
 }
---------------
-------------
@@ -93,7 +93,7 @@
         continue;
       }
       try {
-        Class clazz = DocBuilder.loadClass(trans);
+Class clazz = DocBuilder.loadClass(trans, context.getSolrCore());
         if (clazz.newInstance() instanceof Transformer) {
           transformers.add((Transformer) clazz.newInstance());
         } else {
---------------
-------------
@@ -86,7 +86,7 @@
   @After
   public void tearDown() throws Exception {
     originalIndex.close();
-    indexWriter.close();
+indexWriter.shutdown();
     dir.close();
     super.tearDown();
   }
---------------
-------------
@@ -149,7 +149,7 @@
       throw new IOException("tempDir undefined, cannot run test");
 
     String dirName = tempDir + "/luceneTestThreadedOptimize";
-    directory = FSDirectory.getDirectory(dirName, null, false);
+directory = FSDirectory.getDirectory(dirName);
     runTest(directory, false, null);
     runTest(directory, true, null);
     runTest(directory, false, new ConcurrentMergeScheduler());
---------------
-------------
@@ -41,7 +41,7 @@
   @Override
   public UpdateRequestProcessor getInstance(SolrQueryRequest req, SolrQueryResponse rsp,
           UpdateRequestProcessor next) {
-    return new UIMAUpdateRequestProcessor(next, req.getCore(),
+return new UIMAUpdateRequestProcessor(next, req.getCore().getName(),
             new SolrUIMAConfigurationReader(args).readSolrUIMAConfiguration());
   }
 
---------------
-------------
@@ -87,7 +87,7 @@
     // NOTE: this does only test the chunked reads and NOT if the Bug is triggered.
     //final int tmpFileSize = 1024 * 1024 * 5;
     final int inputBufferSize = 128;
-    File tmpInputFile = File.createTempFile("IndexInput", "tmpFile");
+File tmpInputFile = _TestUtil.createTempFile("IndexInput", "tmpFile", TEMP_DIR);
     tmpInputFile.deleteOnExit();
     writeBytes(tmpInputFile, TEST_FILE_LENGTH);
 
---------------
-------------
@@ -652,7 +652,7 @@
     GroupingSpecification groupSpec = rb.getGroupingSpec();
     if (rb.mergedTopGroups.isEmpty()) {
       for (String field : groupSpec.getFields()) {
-        rb.mergedTopGroups.put(field, new TopGroups(null, null, 0, 0, new GroupDocs[]{}));
+rb.mergedTopGroups.put(field, new TopGroups(null, null, 0, 0, new GroupDocs[]{}, Float.NaN));
       }
       rb.resultIds = new HashMap<Object, ShardDoc>();
     }
---------------
-------------
@@ -59,7 +59,7 @@
   public void testURLStream() throws IOException 
   {
     String content = null;
-    URL url = new URL( "http://svn.apache.org/repos/asf/lucene/solr/trunk/" );
+URL url = new URL( "http://svn.apache.org/repos/asf/lucene/dev/trunk/" );
     InputStream in = url.openStream();
     try {
       content = IOUtils.toString( in );
---------------
-------------
@@ -95,7 +95,7 @@
     public RequestSchedulerId request_scheduler_id;
     public RequestSchedulerOptions request_scheduler_options;
 
-    public List<RawKeyspace> keyspaces;
+public RawKeyspace[] keyspaces;
     
     public static enum CommitLogSync {
         periodic,
---------------
-------------
@@ -75,7 +75,7 @@
     List<SegToken> result = new ArrayList<SegToken>();
     int s = -1, count = 0, size = tokenListTable.size();
     List<SegToken> tokenList;
-    short index = 0;
+int index = 0;
     while (count < size) {
       if (isStartExist(s)) {
         tokenList = tokenListTable.get(s);
---------------
-------------
@@ -3412,7 +3412,7 @@
 	 *
 	 * @exception StandardException		Thrown on error
 	 */
-	public ResultColumnList getAllResultColumns(String allTableName)
+public ResultColumnList getAllResultColumns(TableName allTableName)
 			throws StandardException
 	{
 		return getResultColumnsForList(allTableName, resultColumns, tableName);
---------------
-------------
@@ -112,7 +112,7 @@
     public String toString() {
         if (cachedString == null) {
             cachedString =
-                ccsidManager.convertToUCS2(buffer);
+ccsidManager.convertToJavaString(buffer);
         }
         return cachedString;
     }
---------------
-------------
@@ -896,7 +896,7 @@
         //   w.close();
         // }
       } else {
-        assert sumTotalTermFreq == 0;
+assert sumTotalTermFreq == 0 || fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY && sumTotalTermFreq == -1;
         assert sumDocFreq == 0;
         assert docCount == 0;
       }
---------------
-------------
@@ -39,7 +39,7 @@
   Random rng;
 
   @Override
-  protected void setUp() throws Exception {
+protected void setUp() {
     rng = new MersenneTwisterRNG();
   }
 
---------------
-------------
@@ -82,7 +82,7 @@
         assert sstr != null;
         ColumnFamilyStore cfs = Table.open("Keyspace1").getColumnFamilyStore("Indexed1");
         cfs.addSSTable(sstr);
-        cfs.buildSecondaryIndexes(cfs.getSSTables(), cfs.getIndexedColumns());
+cfs.maybeBuildSecondaryIndexes(cfs.getSSTables(), cfs.getIndexedColumns());
         
         IndexExpression expr = new IndexExpression(ByteBufferUtil.bytes("birthdate"), IndexOperator.EQ, ByteBufferUtil.bytes(1L));
         IndexClause clause = new IndexClause(Arrays.asList(expr), ByteBufferUtil.EMPTY_BYTE_BUFFER, 100);
---------------
-------------
@@ -97,7 +97,7 @@
       vocabCountString = job.get("cnaivebayes.vocabCount", vocabCountString);
       vocabCount = stringifier.fromString(vocabCountString);
       
-      Parameters params = Parameters.fromString(job.get("bayes.parameters", ""));
+Parameters params = new Parameters(job.get("bayes.parameters", ""));
       alphaI = Double.valueOf(params.get("alpha_i", "1.0"));
       
     } catch (IOException ex) {
---------------
-------------
@@ -189,7 +189,7 @@
             protected void afterExecute(Runnable r, Throwable t)
             {
                 super.afterExecute(r, t);
-                cassandraServer.logout();
+cassandraServer.clientState.logout();
             }
         };
         serverEngine = new CustomTThreadPoolServer(new TProcessorFactory(processor),
---------------
-------------
@@ -556,7 +556,7 @@
      */
     public Method getDestroyMethod(Object instance) throws ComponentDefinitionException {
         Method method = null;        
-        if (destroyMethod != null && destroyMethod.length() > 0) {
+if (instance != null && destroyMethod != null && destroyMethod.length() > 0) {
             method = ReflectionUtils.getLifecycleMethod(instance.getClass(), destroyMethod);
             if (method == null) {
                 throw new ComponentDefinitionException("Component '" + getName() + "' does not have destroy-method: " + destroyMethod);
---------------
-------------
@@ -124,7 +124,7 @@
   }
 
   protected void check(SpanQuery q, int[] docs) throws Exception {
-    CheckHits.checkHitCollector(q, null, searcher, docs);
+CheckHits.checkHitCollector(random, q, null, searcher, docs);
   }
 
   public void testRewrite0() throws Exception {
---------------
-------------
@@ -100,7 +100,7 @@
     makeRequest(handler, req("action", "enable"));
 
     assertTrue(healthcheckFile.exists());
-    assertNotNull(FileUtils.readFileToString(healthcheckFile), "UTF-8");
+assertNotNull(FileUtils.readFileToString(healthcheckFile, "UTF-8"));
 
     // now verify that the handler response with success
 
---------------
-------------
@@ -392,7 +392,7 @@
         ssProxy.loadBalance();
     }
 
-    public void move(String newToken) throws InterruptedException
+public void move(String newToken) throws IOException, InterruptedException
     {
         ssProxy.move(newToken);
     }
---------------
-------------
@@ -314,7 +314,7 @@
                       public final void collect(int doc) throws IOException {
                         //System.out.println("Q1: Doc=" + doc + " score=" + score);
                         float score = scorer.score();
-                        assertTrue(score==1.0f);
+assertTrue("got score=" + score, score==1.0f);
                         super.collect(doc);
                       }
                     });
---------------
-------------
@@ -161,7 +161,7 @@
 	public BigDecimal	getBigDecimal()
 	{
 		if (isNull()) return null;
-		return new BigDecimal(value);
+return new BigDecimal(Float.toString(value));
 	}
 
     // for lack of a specification: 0 or null is false,
---------------
-------------
@@ -72,7 +72,7 @@
   private JobContext getJobContext(Configuration conf, JobID jobID) throws
       ClassNotFoundException, NoSuchMethodException, IllegalAccessException,
       InvocationTargetException, InstantiationException {
-    Class<? extends JobContext> clazz = null;
+Class<? extends JobContext> clazz;
     if (!JobContext.class.isInterface()) {
       clazz = JobContext.class;
     } else {
---------------
-------------
@@ -286,7 +286,7 @@
 
 
       for (String line; (line=r.readLine())!=null;) {
-        int delimIndex = line.indexOf(delimiter);
+int delimIndex = line.lastIndexOf(delimiter);
         if (delimIndex < 0) continue;
 
         int endIndex = line.length();
---------------
-------------
@@ -88,7 +88,7 @@
         output, measure, t1, t2, true, 0.0, false);
     // run ClusterDumper
     ClusterDumper clusterDumper = new ClusterDumper(new Path(output,
-        "clusters-0"), new Path(output, "clusteredPoints"));
+"clusters-0-final"), new Path(output, "clusteredPoints"));
     clusterDumper.printClusters(null);
   }
 
---------------
-------------
@@ -48,7 +48,7 @@
       if (doFail && Thread.currentThread().getName().equals("main")) {
         StackTraceElement[] trace = new Exception().getStackTrace();
         for (int i = 0; i < trace.length; i++) {
-          if ("doFlush".equals(trace[i].getMethodName())) {
+if ("flush".equals(trace[i].getMethodName())) {
             hitExc = true;
             throw new IOException("now failing during flush");
           }
---------------
-------------
@@ -77,7 +77,7 @@
 	{
 		TypeId	operandType;
 
-		super.bindExpression(fromList, subqueryList, 
+bindOperand(fromList, subqueryList,
 				aggregateVector);
 
 		/*
---------------
-------------
@@ -202,7 +202,7 @@
     }
     
     if (this.cores == null) {
-      ((HttpServletResponse)response).sendError( 503, "Server is shutting down" );
+((HttpServletResponse)response).sendError( 503, "Server is shutting down or failed to initialize" );
       return;
     }
     CoreContainer cores = this.cores;
---------------
-------------
@@ -43,7 +43,7 @@
   public static DefaultOptionBuilder inputOption() {
     return new DefaultOptionBuilder().withLongName("input").withRequired(false).withShortName("i").withArgument(
         new ArgumentBuilder().withName("input").withMinimum(1).withMaximum(1).create()).withDescription(
-        "Path to job input directory. Must be a SequenceFile of VectorWritable");
+"Path to job input directory.");
   }
 
   /**
---------------
-------------
@@ -42,6 +42,6 @@
 
 public class Constants {
 
-  public static final String VERSION = "2.1.0";
+public static final String VERSION = "2.2.0";
 
 }
---------------
-------------
@@ -228,7 +228,7 @@
   }
 
   @Override
-  public int getUniqueFieldCount() {
+public int size() {
     return -1;
   }
 
---------------
-------------
@@ -181,7 +181,7 @@
                 {"XJ004","Database '{0}' not found.","40000"},
                 {"XJ015","Derby system shutdown.","50000"},
                 {"XJ028","The URL '{0}' is not properly formed.","40000"},
-                {"XJ040","Failed to start database '{0}', see the next exception for details.","40000"},
+{"XJ040","Failed to start database '{0}' with class loader {1}, see the next exception for details.","40000"},
                 {"XJ041","Failed to create database '{0}', see the next exception for details.","40000"},
                 {"XJ049","Conflicting create attributes specified.","40000"},
                 {"XJ05B","JDBC attribute '{0}' has an invalid value '{1}', valid values are '{2}'.","40000"},
---------------
-------------
@@ -43,7 +43,7 @@
 
     public void doVerb(Message message)
     {
-        byte[] body = (byte[])message.getMessageBody()[0];
+byte[] body = message.getMessageBody();
         
         try
         {
---------------
-------------
@@ -43,7 +43,7 @@
     writer.commit();
   }
   
-  private static IndexWriterConfig newWriterConfig() throws IOException {
+private static IndexWriterConfig newWriterConfig() {
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
     conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);
     conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);
---------------
-------------
@@ -872,7 +872,7 @@
       }
 
       @Override
-      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) {
+public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) {
         if (reuse == null || !(reuse instanceof MemoryDocsEnum)) {
           reuse = new MemoryDocsEnum();
         }
---------------
-------------
@@ -59,7 +59,7 @@
     {
         try
         {
-            String dataCenter = endpointSnitch.getLocation(message.getFrom());
+String dataCenter = endpointSnitch.getDatacenter(message.getFrom());
             Object blockFor = responseCounts.get(dataCenter);
             // If this DC needs to be blocked then do the below.
             if (blockFor != null)
---------------
-------------
@@ -188,7 +188,7 @@
                 for (Range r : sortedRanges)
                 {
                     // Looping over every KS:CF:Range, get the splits size and add it to the count
-                    allTokens.put(r.right, allTokens.get(r.right) + StorageService.instance.getSplits(ks, cfmd.cfName, r, 1).size());
+allTokens.put(r.right, allTokens.get(r.right) + StorageService.instance.getSplits(ks, cfmd.cfName, r, DatabaseDescriptor.getIndexInterval()).size());
                 }
             }
         }
---------------
-------------
@@ -126,7 +126,7 @@
 
   public static final String COLL_PROP_PREFIX = "property.";
 
-  public static final Set<String> KNOWN_CLUSTER_PROPS = ImmutableSet.of("legacyCloud");
+public static final Set<String> KNOWN_CLUSTER_PROPS = ImmutableSet.of("legacyCloud","urlScheme");
 
   public static final Map<String,Object> COLL_PROPS = ZkNodeProps.makeMap(
       ROUTER, DocRouter.DEFAULT_NAME,
---------------
-------------
@@ -45,7 +45,7 @@
   public T get()
   {
     if (serviceObject == null && ref.getBundle() != null) {
-      serviceObject = (T) ctx.getService(ref);
+serviceObject = (T) Utils.getServicePrivileged(ctx, ref);
     }
     
     return serviceObject;
---------------
-------------
@@ -1,4 +1,4 @@
-package demo.HTMLParser;
+package org.apache.lucene.HTMLParser;
 
 /* ====================================================================
  * The Apache Software License, Version 1.1
---------------
-------------
@@ -32,7 +32,7 @@
  * to the Lucene query parser expression <code>myfield:foo*</code>
  */
 public class PrefixQParserPlugin extends QParserPlugin {
-  public static String NAME = "prefix";
+public static final String NAME = "prefix";
 
   @Override
   public void init(NamedList args) {
---------------
-------------
@@ -721,7 +721,7 @@
       return ((TermPositions)termDocs).nextPosition();
     }
 
-    public int getPayloadLength() {
+public int getPayloadLength() throws IOException {
       return ((TermPositions)termDocs).getPayloadLength();
     }
 
---------------
-------------
@@ -54,7 +54,7 @@
         ByteArrayOutputStream bos = new ByteArrayOutputStream();
         DataOutputStream dos = new DataOutputStream( bos );
         BootstrapInitiateMessage.serializer().serialize(biMessage, dos);
-        return new Message( StorageService.getLocalStorageEndPoint(), "", StorageService.bootStrapInitiateVerbHandler_, new Object[]{bos.toByteArray()} );
+return new Message( StorageService.getLocalStorageEndPoint(), "", StorageService.bootStrapInitiateVerbHandler_, bos.toByteArray() );
     }
     
     protected StreamContextManager.StreamContext[] streamContexts_ = new StreamContextManager.StreamContext[0];
---------------
-------------
@@ -1139,7 +1139,7 @@
     }
 
     public void testOpenReaderAfterDelete() throws IOException {
-      File dirFile = new File(TEMP_DIR, "deletetest");
+File dirFile = _TestUtil.getTempDir("deletetest");
       Directory dir = newFSDirectory(dirFile);
       try {
         IndexReader.open(dir, false);
---------------
-------------
@@ -43,7 +43,7 @@
       chunkSizeInMB = 1984;
     }
     maxChunkSizeInBytes = chunkSizeInMB * 1024 * 1024;
-    fs = FileSystem.get(conf);
+fs = FileSystem.get(output.toUri(), conf);
     currentChunkID = 0;
     writer = new SequenceFile.Writer(fs, conf, getPath(currentChunkID), Text.class, Text.class);
   }
---------------
-------------
@@ -50,7 +50,7 @@
   @Override
   public void tearDown() throws Exception {
     super.tearDown();
-    writer.close();
+writer.shutdown();
     directory.close();
   }
 
---------------
-------------
@@ -34,7 +34,7 @@
         buffer_ = ByteBuffer.allocate(4);
     }
 
-    public byte[] read() throws IOException, ReadNotCompleteException
+public byte[] read() throws IOException
     {        
         return doRead(buffer_);
     }
---------------
-------------
@@ -84,7 +84,7 @@
       return new FixedIntBlockIndexInput(dir.openInput(fileName, context)) {
 
         @Override
-        protected BlockReader getBlockReader(final IndexInput in, final int[] buffer) throws IOException {
+protected BlockReader getBlockReader(final IndexInput in, final int[] buffer) {
           return new BlockReader() {
             public void seek(long pos) {}
             public void readBlock() throws IOException {
---------------
-------------
@@ -746,7 +746,7 @@
 	 */
 	public static String parseRoleId(String roleName) throws StandardException
 	{
-		roleName.trim();
+roleName = roleName.trim();
 		// NONE is a special case and is not allowed with its special
 		// meaning in SET ROLE <value specification>. Even if there is
 		// a role with case normal form "NONE", we require it to be
---------------
-------------
@@ -23,7 +23,7 @@
 
 import org.apache.derby.shared.common.reference.JDBC40Translation;
 import org.apache.derby.shared.common.reference.SQLState;
-import org.apache.derby.iapi.services.sanity.SanityManager;
+import org.apache.derby.shared.common.sanity.SanityManager;
 
 import java.io.InputStream;
 import java.io.Reader;
---------------
-------------
@@ -87,7 +87,7 @@
 		System.out.println("creating");
 		s.executeUpdate("CREATE TABLE atable (a INT, b LONG VARCHAR FOR BIT DATA)");
 		conn.commit();
-		java.io.File file = new java.io.File("short.txt");
+java.io.File file = new java.io.File("short.utf");
 		int fileLength = (int) file.length();
 
 		// first, create an input stream
---------------
-------------
@@ -319,7 +319,7 @@
   @Override
   public final String toString(String f) {
     StringBuilder buffer = new StringBuilder();
-    if (!field.equals(f)) {
+if (field == null || !field.equals(f)) {
       buffer.append(field);
       buffer.append(":");
     }
---------------
-------------
@@ -274,7 +274,7 @@
       if (obj instanceof IndexReader) {
         IndexReader[] subs = ((IndexReader)obj).getSequentialSubReaders();
         for (int j = 0; (null != subs) && (j < subs.length); j++) {
-          all.add(subs[j].getFieldCacheKey());
+all.add(subs[j].getCoreCacheKey());
         }
       }
       
---------------
-------------
@@ -67,7 +67,7 @@
 		@param mode is either ENCRYPT or DECRYPT.  The CipherProvider can only
 				do encryption or decryption but not both.
 
-		@exception StandardException Standard Cloudscape Error Policy
+@exception StandardException Standard Derby Error Policy
 	 */
 	CipherProvider createNewCipher(int mode)
 		 throws StandardException;
---------------
-------------
@@ -44,7 +44,7 @@
      * @param agentContext JMXAgentContext instance.
      */
     public UserAdminMBeanHandler(JMXAgentContext agentContext) {
-        super(agentContext, org.osgi.service.useradmin.UserAdmin.class.getCanonicalName());
+super(agentContext, "org.osgi.service.useradmin.UserAdmin");
     }
 
     /**
---------------
-------------
@@ -61,7 +61,7 @@
     field2.setStringValue("jumps over lazy brown dog");
     iw.addDocument(doc);
     reader = iw.getReader();
-    iw.close();
+iw.shutdown();
     searcher = newSearcher(reader);
     searcher.setSimilarity(sim);
   }
---------------
-------------
@@ -2385,7 +2385,7 @@
      * @return true if the server supports this
      */
     final public boolean serverSupportsBooleanValues() {
-        return supportsUDTs_;
+return supportsBooleanValues_;
     }
 
     //------------helper methods for meta data info call methods------------------
---------------
-------------
@@ -595,7 +595,7 @@
    * </p>
    */
   public void setFollowRedirects(boolean followRedirects) {
-    this.followRedirects = true;
+this.followRedirects = followRedirects;
     HttpClientUtil.setFollowRedirects(httpClient,  followRedirects);
   }
   
---------------
-------------
@@ -122,7 +122,7 @@
     {
         logger.info("Writing " + this);
         String path = cfs.getFlushPath();
-        SSTableWriter writer = new SSTableWriter(path, sortedKeys.size(), StorageService.getPartitioner());
+SSTableWriter writer = new SSTableWriter(path, sortedKeys.size(), cfs.metadata, cfs.partitioner_);
 
         for (DecoratedKey key : sortedKeys)
         {
---------------
-------------
@@ -67,7 +67,7 @@
         // options without argument
         options.addOption("B",  BATCH_OPTION,   "enabled batch mode (suppress output; errors are fatal)");
         options.addOption(null, UNFRAME_OPTION, "use cassandra server's unframed transport");
-        options.addOption(null, DEBUG_OPTION,   "display stack traces");
+options.addOption(null, DEBUG_OPTION,   "display stack-traces (NOTE: We print strack-traces in the places where it makes sense even without --debug)");
         options.addOption("?",  HELP_OPTION,    "usage help");
         options.addOption("v",  VERBOSE_OPTION, "verbose output when using batch mode");
     }
---------------
-------------
@@ -72,7 +72,7 @@
         } else if (sqlState.startsWith(SQLState.INTEGRITY_VIOLATION_PREFIX)) {
             ex = new SQLIntegrityConstraintViolationException(message, sqlState,
                     errCode);
-        } else if (sqlState.startsWith(SQLState.AUTHORIZATION_PREFIX)) {
+} else if (sqlState.startsWith(SQLState.AUTHORIZATION_SPEC_PREFIX)) {
             ex = new SQLInvalidAuthorizationSpecException(message, sqlState,
                     errCode);
         } else if (sqlState.startsWith(SQLState.TRANSACTION_PREFIX) ||
---------------
-------------
@@ -65,7 +65,7 @@
     public static void prepareClass() throws Exception
     {
         LOCAL = FBUtilities.getLocalAddress();
-        tablename = "Keyspace4";
+tablename = "Keyspace5";
         StorageService.instance.initServer();
         // generate a fake endpoint for which we can spoof receiving/sending trees
         REMOTE = InetAddress.getByName("127.0.0.2");
---------------
-------------
@@ -96,7 +96,7 @@
     }
     if (ids.size() == 0) return;
 
-    StringBuffer sb = new StringBuffer("id:(");
+StringBuilder sb = new StringBuilder("id:(");
     for (String id : ids) {
       sb.append(id).append(' ');
       model.remove(id);
---------------
-------------
@@ -87,7 +87,7 @@
         }
         finally
         {
-            logger.debug("memtable memory usage is {} bytes with {} live", liveBytes + flushingBytes, liveBytes);
+logger.trace("memtable memory usage is {} bytes with {} live", liveBytes + flushingBytes, liveBytes);
         }
     }
 
---------------
-------------
@@ -55,7 +55,7 @@
     // this allows to easily identify a matching (exact) phrase 
     // when all PhrasePositions have exactly the same position.
     for (int i = 0; i < postings.length; i++) {
-      PhrasePositions pp = new PhrasePositions(postings[i].postings, postings[i].position);
+PhrasePositions pp = new PhrasePositions(postings[i].postings, postings[i].position, i);
       if (last != null) {			  // add next to end of list
         last.next = pp;
       } else {
---------------
-------------
@@ -419,7 +419,7 @@
    *@return The <code>String</code> that indexes the node argument.
    */
   protected String getKey(TSTNode node) {
-    StringBuffer getKeyBuffer = new StringBuffer();
+StringBuilder getKeyBuffer = new StringBuilder();
     getKeyBuffer.setLength(0);
     getKeyBuffer.append("" + node.splitchar);
     TSTNode currentNode;
---------------
-------------
@@ -140,7 +140,7 @@
     String directoryContainingConvertedInput = output + Constants.DIRECTORY_CONTAINING_CONVERTED_INPUT;
     InputDriver.runJob(input, directoryContainingConvertedInput);
     MeanShiftCanopyJob.runJob(directoryContainingConvertedInput, output + "/meanshift", measureClassName, t1,
-      t2, convergenceDelta, maxIterations);
+t2, convergenceDelta, maxIterations, true);
     FileStatus[] status = dfs.listStatus(new Path(output + "/meanshift"));
     OutputDriver.runJob(status[status.length - 1].getPath().toString(), output
                                                                         + CLUSTERED_POINTS_OUTPUT_DIRECTORY);
---------------
-------------
@@ -253,7 +253,7 @@
       for (int i = 0; i < len; ++i) {
         terms[i] = RandomPicks.randomFrom(random(), sampleTerms);
         if (weird) {
-          positionsIncrements[i] = random().nextInt(1 << 18);
+positionsIncrements[i] = _TestUtil.nextInt(random(), 1, 1 << 18);
           startOffsets[i] = random().nextInt();
           endOffsets[i] = random().nextInt();
         } else if (i == 0) {
---------------
-------------
@@ -53,7 +53,7 @@
     
     assertNull(MultiFields.getTermPositionsEnum(reader, null, "foo", new BytesRef("test")));
     
-    DocsEnum de = _TestUtil.docs(random(), reader, "foo", new BytesRef("test"), null, null, DocsEnum.FLAG_FREQS);
+DocsEnum de = _TestUtil.docs(random(), reader, "foo", new BytesRef("test"), null, null, true);
     while (de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
       assertEquals(2, de.freq());
     }
---------------
-------------
@@ -134,7 +134,7 @@
 		boolean encryptDB = Boolean.valueOf(finfo.getProperty(Attribute.DATA_ENCRYPTION)).booleanValue();		
 		String encryptpassword = finfo.getProperty(Attribute.BOOT_PASSWORD);
 
-		if (dbname.length() == 0 || (encryptDB = true && encryptpassword == null)) {
+if (dbname.length() == 0 || (encryptDB && encryptpassword == null)) {
 
 			// with no database name we can have shutdown or a database name
 
---------------
-------------
@@ -55,6 +55,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new EnglishAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new EnglishAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -66,7 +66,7 @@
 import org.apache.aries.util.filesystem.FileSystem;
 import org.apache.aries.util.filesystem.IDirectory;
 import org.apache.aries.util.filesystem.IFile;
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 import org.apache.aries.util.manifest.BundleManifest;
 import org.apache.aries.util.manifest.ManifestProcessor;
 import org.osgi.framework.BundleContext;
---------------
-------------
@@ -1,6 +1,6 @@
 /*
  
-Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun
+Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun_Local_StateTest_part1
  
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -108,7 +108,7 @@
       writer.deleteDocuments(new Term("id", "" + docID));
       docStates[docID] = 2;
     }
-    writer.close();
+writer.shutdown();
     return docStates;
   }
 
---------------
-------------
@@ -88,7 +88,7 @@
     // prepare a small index with just a few documents.  
     super.setUp();
     dir = new RAMDirectory();
-    anlzr = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+anlzr = new StandardAnalyzer(TEST_VERSION_CURRENT);
     IndexWriter iw = new IndexWriter(dir, anlzr,
                                      IndexWriter.MaxFieldLength.LIMITED);
     // add docs not exactly in natural ID order, to verify we do check the order of docs by scores
---------------
-------------
@@ -53,7 +53,7 @@
 
     public void doVerb(Message message)
     {
-        byte[] bytes = (byte[]) message.getMessageBody()[0];
+byte[] bytes = message.getMessageBody();
         /* Obtain a Row Mutation Context from TLS */
         RowMutationContext rowMutationCtx = tls_.get();
         if ( rowMutationCtx == null )
---------------
-------------
@@ -51,7 +51,7 @@
   }
   
   private void assertChunking(Random random, int chunkSize) throws Exception {
-    File path = File.createTempFile("mmap" + chunkSize, "tmp", workDir);
+File path = _TestUtil.createTempFile("mmap" + chunkSize, "tmp", workDir);
     path.delete();
     path.mkdirs();
     MMapDirectory dir = new MMapDirectory(path);
---------------
-------------
@@ -390,7 +390,7 @@
       thread.join();
     }
 
-    writer.close();
+writer.shutdown();
     if (VERBOSE) {
       System.out.println("TEST: close reader=" + reader);
     }
---------------
-------------
@@ -1399,7 +1399,7 @@
     @Override
     protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
         throws IOException {
-      return new DocTermOrds(reader, key.field);
+return new DocTermOrds(reader, null, key.field);
     }
   }
 
---------------
-------------
@@ -320,7 +320,7 @@
     }
 
     public void _testStressLocks(LockFactory lockFactory, String indexDirName) throws IOException {
-        FSDirectory fs1 = FSDirectory.getDirectory(indexDirName, lockFactory);
+FSDirectory fs1 = FSDirectory.getDirectory(indexDirName, lockFactory, false);
 
         // First create a 1 doc index:
         IndexWriter w = new IndexWriter(fs1, new WhitespaceAnalyzer(), true);
---------------
-------------
@@ -57,7 +57,7 @@
     private int lastDocId = -1;
 
     protected PackedIntsWriter(Directory dir, String id, Counter bytesUsed,
-        IOContext context) throws IOException {
+IOContext context) {
       super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context, Type.VAR_INTS);
       bytesRef = new BytesRef(8);
     }
---------------
-------------
@@ -30,7 +30,7 @@
 import org.apache.derby.iapi.sql.conn.LanguageConnectionContext;
 import org.apache.derby.iapi.sql.execute.RunTimeStatistics;
 import org.apache.derby.iapi.sql.execute.xplain.XPLAINVisitor;
-import org.apache.derby.impl.sql.execute.rts.ResultSetStatistics;
+import org.apache.derby.iapi.sql.execute.ResultSetStatistics;
 /**
  * This is the Default Visitor which produces explain information like the 
  * old getRuntimeStatistics() approach. <br/>
---------------
-------------
@@ -3018,7 +3018,7 @@
         this.cKey = other.cKey;
         this.stream = other.stream;
         this._clobValue = other._clobValue;
-        this.localeFinder = localeFinder;
+this.localeFinder = other.localeFinder;
     }
 
     /**
---------------
-------------
@@ -56,7 +56,7 @@
 
         try
         {
-            GossipDigestSynMessage gDigestMessage = GossipDigestSynMessage.serializer().deserialize(dis);
+GossipDigestSynMessage gDigestMessage = GossipDigestSynMessage.serializer().deserialize(dis, message.getVersion());
             /* If the message is from a different cluster throw it away. */
             if ( !gDigestMessage.clusterId_.equals(DatabaseDescriptor.getClusterName()) )
             {
---------------
-------------
@@ -70,7 +70,7 @@
   
   public final boolean isProxy(Object proxy)
   {
-    return (getInvocationHandler(proxy) instanceof ProxyHandler);
+return (proxy != null && getInvocationHandler(proxy) instanceof ProxyHandler);
   }
   
   protected abstract Object createNewProxy(Bundle clientBundle, Collection<Class<?>> classes,
---------------
-------------
@@ -129,7 +129,7 @@
         System.out.println("zkcli.sh -zkhost localhost:9983 -cmd " + UPCONFIG + " -" + CONFDIR + " /opt/solr/collection1/conf" + " -" + CONFNAME + " myconf");
         System.out.println("zkcli.sh -zkhost localhost:9983 -cmd " + DOWNCONFIG + " -" + CONFDIR + " /opt/solr/collection1/conf" + " -" + CONFNAME + " myconf");
         System.out.println("zkcli.sh -zkhost localhost:9983 -cmd " + LINKCONFIG + " -" + COLLECTION + " collection1" + " -" + CONFNAME + " myconf");
-        System.out.println("zkcli.sh -zkhost localhost:9983 -cmd " + MAKEPATH + " /apache/solr/data.txt 'config data'");
+System.out.println("zkcli.sh -zkhost localhost:9983 -cmd " + MAKEPATH + " /apache/solr");
         System.out.println("zkcli.sh -zkhost localhost:9983 -cmd " + PUT + " /solr.conf 'conf data'");
         System.out.println("zkcli.sh -zkhost localhost:9983 -cmd " + CLEAR + " /solr");
         System.out.println("zkcli.sh -zkhost localhost:9983 -cmd " + LIST);
---------------
-------------
@@ -280,7 +280,7 @@
                                 // only when timeSlice is set.
                             }
 							currentTimeSlice = getTimeSlice();
-						} while ((currentTimeSlice == 0)  || 
+} while ((currentTimeSlice <= 0)  ||
 							(System.currentTimeMillis() - timeStart < currentTimeSlice));
 
 						break;
---------------
-------------
@@ -125,7 +125,7 @@
           final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(
               "OutOfMemoryError likely caused by the Sun VM Bug described in "
               + "https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize "
-              + "with a value smaller than the current chunks size (" + chunkSize + ")");
++ "with a value smaller than the current chunk size (" + chunkSize + ")");
           outOfMemoryError.initCause(e);
           throw outOfMemoryError;
         }
---------------
-------------
@@ -68,7 +68,7 @@
       writer.addDocument(doc);
     }
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     query = new TermQuery(new Term("content", "test"));
   }
   
---------------
-------------
@@ -580,7 +580,7 @@
       Set<String> collections = clusterState.getCollections();
       for (String collection : collections) {
         slices = new ArrayList<Slice>();
-        slices.addAll(clusterState.getActiveSlices(collection));
+slices.addAll(clusterState.getSlices(collection));
       }
     }
     
---------------
-------------
@@ -207,7 +207,7 @@
       return false;
     }
     if (target <= currentDoc) {
-      target = currentDoc + 1;
+return true;
     }
     do {
       Scorer top = (Scorer) scorerQueue.top();
---------------
-------------
@@ -115,7 +115,7 @@
         // perform compatibility init
         cores = new CoreContainer(new SolrResourceLoader(instanceDir));
         SolrConfig cfg = solrConfigFilename == null ? new SolrConfig() : new SolrConfig(solrConfigFilename);
-        CoreDescriptor dcore = new CoreDescriptor(cores, "", cfg.getResourceLoader().getInstanceDir());
+CoreDescriptor dcore = new CoreDescriptor(cores, "", ".");
         SolrCore singlecore = new SolrCore(null, null, cfg, null, dcore);
         abortOnConfigurationError = cfg.getBool(
                 "abortOnConfigurationError", abortOnConfigurationError);
---------------
-------------
@@ -96,7 +96,7 @@
   public static class Lucene40NormsDocValuesConsumer extends Lucene40DocValuesConsumer {
 
     public Lucene40NormsDocValuesConsumer(PerDocWriteState state,
-        String segmentSuffix) throws IOException {
+String segmentSuffix) {
       super(state, segmentSuffix);
     }
 
---------------
-------------
@@ -682,7 +682,7 @@
         }
 
         for (RandomDoc otherSideDoc : otherMatchingDocs) {
-          DocsEnum docsEnum = MultiFields.getTermDocsEnum(topLevelReader, MultiFields.getLiveDocs(topLevelReader), "id", new BytesRef(otherSideDoc.id), 0);
+DocsEnum docsEnum = MultiFields.getTermDocsEnum(topLevelReader, MultiFields.getLiveDocs(topLevelReader), "id", new BytesRef(otherSideDoc.id), false);
           assert docsEnum != null;
           int doc = docsEnum.nextDoc();
           expectedResult.set(doc);
---------------
-------------
@@ -612,7 +612,7 @@
     // If running in an Oozie workflow as a Java action, need to add the
     // Configuration resource provided by Oozie to this job's config.
     String oozieActionConfXml = System.getProperty("oozie.action.conf.xml");
-    if (oozieActionConfXml != null) {
+if (oozieActionConfXml != null && conf != null) {
       conf.addResource(new Path("file:///", oozieActionConfXml));
       log.info("Added Oozie action Configuration resource {0} to the Hadoop Configuration", oozieActionConfXml);
     }      
---------------
-------------
@@ -996,7 +996,7 @@
 	*/
 	public int getTypeFormatId() 
     {
-		return StoredFormatIds.ACCESS_B2I_V3_ID;
+return StoredFormatIds.ACCESS_B2I_V4_ID;
 	}
 
 
---------------
-------------
@@ -29,7 +29,7 @@
 
   @BeforeClass
   public static void beforeClass() throws Exception {
-    initCore("solrconfig-basic.xml", "schema_codec.xml");
+initCore("solrconfig_codec.xml", "schema_codec.xml");
   }
 
   public void testPostingsFormats() {
---------------
-------------
@@ -52,7 +52,7 @@
                 final UUID version = UUIDGen.makeType1UUID(col.name());
                 if (version.timestamp() > DatabaseDescriptor.getDefsVersion().timestamp())
                 {
-                    final Migration m = Migration.deserialize(new ByteArrayInputStream(col.value()));
+final Migration m = Migration.deserialize(col.value());
                     assert m.getVersion().equals(version);
                     StageManager.getStage(StageManager.MIGRATION_STAGE).submit(new WrappedRunnable()
                     {
---------------
-------------
@@ -81,7 +81,7 @@
       iw.addDocument(document);
     }
     reader = iw.getReader();
-    iw.close();
+iw.shutdown();
     searcher = newSearcher(reader);
   }
 
---------------
-------------
@@ -435,7 +435,7 @@
 	    bra > ket ||
 	    ket > limit)
 	{
-	    System.err.println("faulty slice operation");
+throw new IllegalArgumentException("faulty slice operation: bra=" + bra + ",ket=" + ket + ",limit=" + limit);
 	// FIXME: report error somehow.
 	/*
 	    fprintf(stderr, "faulty slice operation:\n");
---------------
-------------
@@ -22,7 +22,7 @@
 import org.apache.solr.request.SolrQueryRequest;
 
 public class SpatialBoxQParserPlugin extends SpatialFilterQParserPlugin {
-  public static String NAME = "bbox";
+public static final String NAME = "bbox";
 
   @Override
   public QParser createParser(String qstr, SolrParams localParams,
---------------
-------------
@@ -33,7 +33,7 @@
   
   @Override
   public final float score(BasicStats stats, float tfn) {
-    float lambda = (float)stats.getTotalTermFreq() / stats.getNumberOfDocuments();
+float lambda = (float)(stats.getTotalTermFreq()+1) / (stats.getNumberOfDocuments()+1);
     return (float)(tfn * log2(tfn / lambda)
         + (lambda + 1 / (12 * tfn) - tfn) * LOG2_E
         + 0.5 * log2(2 * Math.PI * tfn));
---------------
-------------
@@ -68,7 +68,7 @@
         assertAllBetween(last2, j, bd2, ids);
         last2 = j + 1;
       }
-      assertEquals(uniqueValues.size(), queue.numGlobalTermDeletes());
+assertEquals(j+1, queue.numGlobalTermDeletes());
     }
     assertEquals(uniqueValues, bd1.terms.keySet());
     assertEquals(uniqueValues, bd2.terms.keySet());
---------------
-------------
@@ -161,7 +161,7 @@
     */
     private void setNextFileName()
     {
-        logFile_ = DatabaseDescriptor.getLogFileLocation() + System.getProperty("file.separator") +
+logFile_ = DatabaseDescriptor.getLogFileLocation() + File.separator +
                    "CommitLog-" + System.currentTimeMillis() + ".log";
     }
 
---------------
-------------
@@ -148,7 +148,7 @@
     if (isNumeric(data)) {
       return Double.parseDouble(data);
     }
-    return 0.0;
+return Double.NaN;
   }
 
   public static boolean isNumeric(String str) {
---------------
-------------
@@ -91,7 +91,7 @@
         iw.commit();
       }
     }
-    iw.close();
+iw.shutdown();
 
     DirectoryReader rd = DirectoryReader.open(d);
     for (AtomicReaderContext leave : rd.leaves()) {
---------------
-------------
@@ -96,7 +96,7 @@
   }
 
   /** @see #closeWhileHandlingException(Exception, Closeable...) */
-  public static <E extends Exception> void closeWhileHandlingException(E priorException, Iterable<Closeable> objects) throws E, IOException {
+public static <E extends Exception> void closeWhileHandlingException(E priorException, Iterable<? extends Closeable> objects) throws E, IOException {
     Throwable th = null;
 
     for (Closeable object : objects) {
---------------
-------------
@@ -189,7 +189,7 @@
 			// the location is different, try it
 			String userdir =  System.getProperty("user.dir");
 			String sep =  System.getProperty("file.separator");
-			fr = new BufferedReader (new InputStreamReader(new FileInputStream(userdir + sep + "extin" + sep + filename),"UTF-8"));
+fr = new BufferedReader (new InputStreamReader(new FileInputStream(userdir + sep + filename),"UTF-8"));
 		}
 		tkn = new StreamTokenizer(fr);
 		int val;
---------------
-------------
@@ -57,7 +57,7 @@
       } else {
         id++;
       }
-      ((Field) doc.getField("docid")).setValue(myID);
+((Field) doc.getField("docid")).setStringValue(myID);
       w.updateDocument(new Term("docid", myID), doc);
 
       if (docIter >= SIZE && random.nextInt(50) == 17) {
---------------
-------------
@@ -32,7 +32,7 @@
  *     created from the lucene syntax string that matches documents with inStock=true.
  */
 public class NestedQParserPlugin extends QParserPlugin {
-  public static String NAME = "query";
+public static final String NAME = "query";
 
   @Override
   public void init(NamedList args) {
---------------
-------------
@@ -106,7 +106,7 @@
 import org.apache.derby.impl.sql.execute.rts.RealUpdateResultSetStatistics;
 import org.apache.derby.impl.sql.execute.rts.RealVTIStatistics;
 import org.apache.derby.impl.sql.execute.rts.RealRowCountStatistics;
-import org.apache.derby.impl.sql.execute.rts.ResultSetStatistics;
+import org.apache.derby.iapi.sql.execute.ResultSetStatistics;
 import org.apache.derby.impl.sql.execute.rts.RunTimeStatisticsImpl;
 import org.apache.derby.impl.sql.execute.rts.RealWindowResultSetStatistics;
 
---------------
-------------
@@ -246,7 +246,7 @@
             for (String shard : sreq.actualShards) {
               ModifiableSolrParams params = new ModifiableSolrParams(sreq.params);
               params.remove(ShardParams.SHARDS);      // not a top-level request
-              params.remove("distrib");               // not a top-level request
+params.set("distrib", "false");               // not a top-level request
               params.remove("indent");
               params.remove(CommonParams.HEADER_ECHO_PARAMS);
               params.set(ShardParams.IS_SHARD, true);  // a sub (shard) request
---------------
-------------
@@ -541,7 +541,7 @@
 		if (is==null) throw ijException.resourceNotFound();
 		oldGrabbers.push(commandGrabber[currCE]);
 	    commandGrabber[currCE] = 
-                new StatementFinder(langUtil.getNewInput(new BufferedInputStream(is, BUFFEREDFILESIZE)));
+new StatementFinder(langUtil.getNewEncodedInput(new BufferedInputStream(is, BUFFEREDFILESIZE), "UTF8"));
 		fileInput = true;
 	}
 
---------------
-------------
@@ -38,7 +38,7 @@
 
   @Override
   protected void map(WritableComparable<?> key, VectorWritable point, Context context) throws IOException, InterruptedException {
-    MeanShiftCanopy canopy = new MeanShiftCanopy(point.get(), nextCanopyId++, measure);
+MeanShiftCanopy canopy = MeanShiftCanopy.initialCanopy(point.get(), nextCanopyId++, measure);
     context.write(new Text(key.toString()), canopy);
   }
 
---------------
-------------
@@ -66,7 +66,7 @@
     );
 
     assertJQ(req("qt",rh, SpellCheckComponent.COMPONENT_NAME, "true", "q","bluo", SpellCheckComponent.SPELLCHECK_COUNT,"3", SpellCheckComponent.SPELLCHECK_EXTENDED_RESULTS,"true")
-       ,"/spellcheck/suggestions/[1]/suggestion==[{'word':'blue','freq':1}, {'word':'blud','freq':1}, {'word':'boue','freq':1}]"
+,"/spellcheck/suggestions/[1]/suggestion==[{'word':'blud','freq':1}, {'word':'blue','freq':1}, {'word':'blee','freq':1}]"
     );
   }
 
---------------
-------------
@@ -89,7 +89,7 @@
     for (int i = 1; i < docText.length; i++) {
       qtxt += ' ' + docText[i]; // large query so that search will be longer
     }
-    QueryParser queryParser = new QueryParser(Version.LUCENE_CURRENT, FIELD_NAME, new WhitespaceAnalyzer());
+QueryParser queryParser = new QueryParser(TEST_VERSION_CURRENT, FIELD_NAME, new WhitespaceAnalyzer());
     query = queryParser.parse(qtxt);
     
     // warm the searcher
---------------
-------------
@@ -120,7 +120,7 @@
       Reader reader) {
     final Tokenizer source = new StandardTokenizer(matchVersion, reader);
     TokenStream result = new StandardFilter(matchVersion, source);
-    result = new LowerCaseFilter(matchVersion, source);
+result = new LowerCaseFilter(matchVersion, result);
     result = new StopFilter(matchVersion, result, stopwords);
     if (!stemExclusionSet.isEmpty()) {
       result = new KeywordMarkerFilter(result, stemExclusionSet);
---------------
-------------
@@ -51,7 +51,7 @@
 
     JobConf jobConf = new JobConf(SlopeOnePrefsToDiffsJob.class);
 
-    FileSystem fs = FileSystem.get(jobConf);
+FileSystem fs = FileSystem.get(outputPathPath.toUri(), jobConf);
     if (fs.exists(outputPathPath)) {
       fs.delete(outputPathPath, true);
     }
---------------
-------------
@@ -230,7 +230,7 @@
         base.close();
       }
       @Override
-      public IndexInput openFullSlice() throws IOException {
+public IndexInput openFullSlice() {
         return (IndexInput) base.clone();
       }
     };
---------------
-------------
@@ -97,7 +97,7 @@
     // omitNorms is true
     for (FieldInfo fi : reader.getFieldInfos()) {
       if (fi.isIndexed()) {
-        assertTrue(fi.omitsNorms() == (reader.simpleNormValues(fi.name) == null));
+assertTrue(fi.omitsNorms() == (reader.getNormValues(fi.name) == null));
       }
     }
     reader.close();
---------------
-------------
@@ -906,7 +906,7 @@
     public static AbstractType getSubComparator(String tableName, String cfName)
     {
         assert tableName != null;
-        return getCFMetaData(tableName, cfName).comparator;
+return getCFMetaData(tableName, cfName).subcolumnComparator;
     }
 
     public static Map<String, Map<String, CFMetaData>> getTableToColumnFamilyMap()
---------------
-------------
@@ -301,7 +301,7 @@
 
       f.add( "type", (ftype==null)?null:ftype.getTypeName() );
       f.add( "schema", getFieldFlags( sfield ) );
-      if (schema.getDynamicPattern(sfield.getName()) != null) {
+if (sfield != null && schema.getDynamicPattern(sfield.getName()) != null) {
     	  f.add("dynamicBase", schema.getDynamicPattern(sfield.getName()));
       }
 
---------------
-------------
@@ -30,7 +30,7 @@
 
 public class OutputDriver {
 
-  public static void main(String[] args) throws Exception {
+public static void main(String[] args) throws IOException {
     runJob(args[0], args[1]);
   }
 
---------------
-------------
@@ -58,7 +58,7 @@
         Token.serializer().serialize(token, dos);
 
         /* Construct the token update message to be sent */
-        Message tokenUpdateMessage = new Message( new EndPoint(FBUtilities.getHostAddress(), port_), "", StorageService.tokenVerbHandler_, new Object[]{bos.toByteArray()} );
+Message tokenUpdateMessage = new Message( new EndPoint(FBUtilities.getHostAddress(), port_), "", StorageService.tokenVerbHandler_, bos.toByteArray() );
         
         BufferedReader bufReader = new BufferedReader( new InputStreamReader( new FileInputStream(file) ) );
         String line = null;
---------------
-------------
@@ -47,7 +47,7 @@
   // like super is called, then our members are initialized:
   Map openFiles;
 
-  public MockRAMDirectory() throws IOException {
+public MockRAMDirectory() {
     super();
     if (openFiles == null) {
       openFiles = new HashMap();
---------------
-------------
@@ -170,7 +170,7 @@
 
     rt.close();
     mgr.close();
-    w.close();
+w.shutdown();
     dir.close();
   }
 }
---------------
-------------
@@ -228,7 +228,7 @@
    * This only supports bytes.length <= blockSize */
   public void copy(BytesRef bytes, BytesRef out) throws IOException {
     int left = blockSize - upto;
-    if (bytes.length > left) {
+if (bytes.length > left || currentBlock==null) {
       if (currentBlock != null) {
         blocks.add(currentBlock);
         blockEnd.add(upto);
---------------
-------------
@@ -435,7 +435,7 @@
 
       if (extendedResults) {
         Integer o = origVsFreq.get(original);
-        if (o != null) result.add(token, o);
+if (o != null) result.addFrequency(token, o);
         for (SuggestWord word : suggestions)
           result.add(token, word.string, word.freq);
       } else {
---------------
-------------
@@ -166,7 +166,7 @@
 			return;
 		}
 		if ( locale == null || locale.toString().equals("none") ){
-			res = ResourceBundle.getBundle(MESSAGE_FILE);
+res = ResourceBundle.getBundle(messageFileName);
 		}
 		else
 		try {
---------------
-------------
@@ -884,7 +884,7 @@
   public int docId(AtomicReader reader, Term term) throws IOException {
     int docFreq = reader.docFreq(term);
     assertEquals(1, docFreq);
-    DocsEnum termDocsEnum = reader.termDocsEnum(null, term.field, term.bytes, 0);
+DocsEnum termDocsEnum = reader.termDocsEnum(null, term.field, term.bytes, false);
     int nextDoc = termDocsEnum.nextDoc();
     assertEquals(DocIdSetIterator.NO_MORE_DOCS, termDocsEnum.nextDoc());
     return nextDoc;
---------------
-------------
@@ -47,7 +47,7 @@
       FieldBoostMapAttribute fieldBoostMapAttr = this.config.getAttribute(FieldBoostMapAttribute.class);
       BoostAttribute boostAttr = fieldConfig.addAttribute(BoostAttribute.class);
       
-      Float boost = fieldBoostMapAttr.getFieldBoostMap().get(fieldConfig.getFieldName());
+Float boost = fieldBoostMapAttr.getFieldBoostMap().get(fieldConfig.getField());
 
       if (boost != null) {
         boostAttr.setBoost(boost.floatValue());
---------------
-------------
@@ -69,7 +69,7 @@
 
     Query q = new ValueSourceQuery(vs);
     log("test: " + q);
-    QueryUtils.check(q, s);
+QueryUtils.check(random, q, s);
     ScoreDoc[] h = s.search(q, null, 1000).scoreDocs;
     assertEquals("All docs should be matched!", N_DOCS, h.length);
     String prevID = inOrder
---------------
-------------
@@ -54,7 +54,7 @@
  *
  */
 
-public class TernaryOperatorNode extends OperatorNode
+public class TernaryOperatorNode extends ValueNode
 {
 	String		operator;
 	String		methodName;
---------------
-------------
@@ -196,7 +196,7 @@
       indexReaderLock.readLock().lock();
       // TODO (Facet): avoid Multi*?
       Bits liveDocs = MultiFields.getLiveDocs(indexReader);
-      DocsEnum docs = MultiFields.getTermDocsEnum(indexReader, liveDocs, Consts.FULL, new BytesRef(path), 0);
+DocsEnum docs = MultiFields.getTermDocsEnum(indexReader, liveDocs, Consts.FULL, new BytesRef(path), false);
       if (docs != null && docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
         ret = docs.docID();
       }
---------------
-------------
@@ -99,7 +99,7 @@
     newLine();
     
     write(TYPE);
-    final NumericField.DataType numericType = field.numericDataType();
+final NumericField.DataType numericType = field.fieldType().numericType();
 
     if (numericType != null) {
       switch (numericType) {
---------------
-------------
@@ -612,7 +612,7 @@
             url = sb.toString();
         }
 
-        Connection conn =  findDriver().connect(url, info);
+Connection conn =  findDriver().connect( url, info, loginTimeout );
 
         // JDBC driver's getConnection method returns null if
         // the driver does not handle the request's URL.
---------------
-------------
@@ -333,7 +333,7 @@
 			)
 			{
 				if (server != null)
-					server.consoleExceptionPrint(e);
+server.consoleExceptionPrintTrace(e);
 				else
 					e.printStackTrace();  // default output stream is System.out
 			}
---------------
-------------
@@ -84,7 +84,7 @@
 
     public String explainPlan()
     {
-        StringBuffer sb = new StringBuffer();
+StringBuilder sb = new StringBuilder();
         
         String prefix =
             String.format("%s Column Family: Batch SET a set of Super Columns: \n" +
---------------
-------------
@@ -201,7 +201,7 @@
                                          meta.getEigenValue(),
                                          Math.abs(1 - meta.getCosAngle()),
                                          s.index());
-        log.info("appending {} to {}", ev, path);
+//log.info("appending {} to {}", ev, path);
         Writable vw = new VectorWritable(ev);
         iw.set(s.index());
         seqWriter.append(iw, vw);
---------------
-------------
@@ -309,7 +309,7 @@
     buffer.append('(');
     final String segmentsFile = segmentInfos.getCurrentSegmentFileName();
     if (segmentsFile != null) {
-      buffer.append(segmentsFile);
+buffer.append(segmentsFile).append(":").append(segmentInfos.getVersion());
     }
     if (writer != null) {
       buffer.append(":nrt");
---------------
-------------
@@ -68,7 +68,7 @@
         nextRow call.  In this case the client should severe all references to 
         the row after returning it from getNextRowFromRowSource().
 
-		@exception StandardException Cloudscape Standard Error Policy
+@exception StandardException Standard Derby Error Policy
 	 */
 	public DataValueDescriptor[] getNextRowFromRowSource() 
         throws StandardException;
---------------
-------------
@@ -197,7 +197,7 @@
             try
             {
                 thriftClient_.insert(tableName, key, new ColumnPath(columnFamily, null, columnName.getBytes("UTF-8")),
-                                     value.getBytes(), System.currentTimeMillis(), 1);
+value.getBytes(), System.currentTimeMillis(), ConsistencyLevel.ONE);
             }
             catch (UnsupportedEncodingException e)
             {
---------------
-------------
@@ -39,7 +39,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    conf = new Configuration();
+conf = getConfiguration();
   }
 
   @Test
---------------
-------------
@@ -66,7 +66,7 @@
     doc.add(newTextField("field", "first auto update", Field.Store.NO));
     writer.addDocument(doc);
     writer.forceMerge(1);
-    writer.close();
+writer.shutdown();
 
     IndexReader reader = DirectoryReader.open(directory);
     IndexSearcher searcher = newSearcher(reader);
---------------
-------------
@@ -215,7 +215,7 @@
       }
 
       @Override
-      protected Reader initReader(Reader reader) {
+protected Reader initReader(String fieldName, Reader reader) {
         return new MappingCharFilter(norm, CharReader.get(reader));
       }
     };
---------------
-------------
@@ -195,7 +195,7 @@
    *  startDocID.  Returns the IndexInput (the fieldStream),
    *  already seeked to the starting point for startDocID.*/
   final IndexInput rawDocs(int[] lengths, int startDocID, int numDocs) throws IOException {
-    indexStream.seek(startDocID * 8L);
+indexStream.seek((docStoreOffset+startDocID) * 8L);
     long startOffset = indexStream.readLong();
     long lastOffset = startOffset;
     int count = 0;
---------------
-------------
@@ -155,7 +155,7 @@
 
  	Document doc = new Document();
  	// add time that is in the future
- 	doc.add(Field.Keyword("datefield", DateField.timeToString(now - 888888)));
+doc.add(Field.Keyword("datefield", DateField.timeToString(now + 888888)));
  	doc.add(Field.Text("body", "Today is a very sunny day in New York City"));
   	writer.addDocument(doc);
  	writer.optimize();
---------------
-------------
@@ -231,7 +231,7 @@
         numSegments++;
       } else {
         if (upperBound * mergeFactor <= maxMergeDocs) {
-          assertTrue("maxMergeDocs=" + maxMergeDocs + "; numSegments=" + numSegments + "; upperBound=" + upperBound + "; mergeFactor=" + mergeFactor, numSegments < mergeFactor);
+assertTrue("maxMergeDocs=" + maxMergeDocs + "; numSegments=" + numSegments + "; upperBound=" + upperBound + "; mergeFactor=" + mergeFactor + "; segs=" + writer.segString(), numSegments < mergeFactor);
         }
 
         do {
---------------
-------------
@@ -330,7 +330,7 @@
           } else if (SKIP.equals(onError)) {
             LOG.warn(msg, e);
             Map<String, Object> map = new HashMap<String, Object>();
-            map.put(SKIP_DOC, Boolean.TRUE);
+map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);
             rows.add(map);
           } else if (CONTINUE.equals(onError)) {
             LOG.warn(msg, e);
---------------
-------------
@@ -516,7 +516,7 @@
 
     TopDocs docs2 = full.search(queryE, filt, nDocs, sort);
     
-    // assertEquals(docs1.scoreDocs[0].score, docs2.scoreDocs[0].score, 1e-6);
+assertEquals(docs1.scoreDocs[0].score, docs2.scoreDocs[0].score, 1e-6);
   }
 
 
---------------
-------------
@@ -159,7 +159,7 @@
      */
     public static List<DecoratedKey> getIndexedDecoratedKeys()
     {
-        final Range range = StorageService.instance().getLocalPrimaryRange();
+final Range range = StorageService.instance.getLocalPrimaryRange();
 
         Predicate<SSTable> cfpred = Predicates.alwaysTrue();
         return getIndexedDecoratedKeysFor(cfpred, new Predicate<DecoratedKey>()
---------------
-------------
@@ -3053,7 +3053,7 @@
 	public void updateObject(int columnIndex, Object x) throws SQLException {
 		checksBeforeUpdateXXX("updateObject", columnIndex);
 		int colType = getColumnType(columnIndex);
-		if (colType == org.apache.derby.iapi.reference.JDBC20Translation.SQL_TYPES_JAVA_OBJECT) {
+if (colType == Types.JAVA_OBJECT) {
 			try {
 				((UserDataValue) getDVDforColumnToBeUpdated(columnIndex, "updateObject")).setValue(x);
 				return;
---------------
-------------
@@ -571,7 +571,7 @@
       if (indexWriter != null) {
         indexWriter.close();
         indexWriter = null;
-      } else {
+} else if (indexReader != null) {
         indexReader.close();
         indexReader = null;
       }
---------------
-------------
@@ -84,7 +84,7 @@
     while (fieldIterator.hasNext()) {
       Fieldable field = (Fieldable) fieldIterator.next();
       add(field.name(), field.isIndexed(), field.isTermVectorStored(), field.isStorePositionWithTermVector(),
-              field.isStoreOffsetWithTermVector(), field.getOmitNorms());
+field.isStoreOffsetWithTermVector(), field.getOmitNorms(), false, field.getOmitTf());
     }
   }
 
---------------
-------------
@@ -133,7 +133,7 @@
       String id = s.getIndexReader().document(sd[i].doc).get(ID_FIELD);
       log("-------- " + i + ". Explain doc " + id);
       log(s.explain(q, sd[i].doc));
-      float expectedScore = N_DOCS - i;
+float expectedScore = N_DOCS - i - 1;
       assertEquals("score of result " + i + " shuould be " + expectedScore + " != " + score, expectedScore, score, TEST_SCORE_TOLERANCE_DELTA);
       String expectedId = inOrder
               ? id2String(N_DOCS - i) // in-order ==> larger  values first
---------------
-------------
@@ -116,7 +116,7 @@
   /** print some useful debugging information about the environment */
   static void printDebuggingInformation() {
     if (classEnvRule != null) {
-      System.err.println("NOTE: test params are: codec=" + Codec.getDefault() +
+System.err.println("NOTE: test params are: codec=" + classEnvRule.codec +
           ", sim=" + classEnvRule.similarity +
           ", locale=" + classEnvRule.locale +
           ", timezone=" + (classEnvRule.timeZone == null ? "(null)" : classEnvRule.timeZone.getID()));
---------------
-------------
@@ -150,7 +150,7 @@
 
       while (tokenStream.incrementToken()) {
         bytesAtt.fillBytesRef();
-        tokens.add(new BytesRef(bytes));
+tokens.add(BytesRef.deepCopyOf(bytes));
       }
 
       tokenStream.end();
---------------
-------------
@@ -47,7 +47,7 @@
 
         public String toString()
         {
-            StringBuffer sb = new StringBuffer("<");
+StringBuilder sb = new StringBuilder("<");
             for (int idx = types_.size(); idx > 0; idx--)
             {
                 sb.append(types_.toString());
---------------
-------------
@@ -939,7 +939,7 @@
               int docId = 12;
               for(int i=0;i<13;i++) {
                 reader.deleteDocument(docId);
-                reader.setNorm(docId, "contents", (float) 2.0);
+reader.setNorm(docId, "content", (float) 2.0);
                 docId += 12;
               }
             }
---------------
-------------
@@ -94,7 +94,7 @@
    * @param format format of the segments info file
    * @param input input handle to read segment info from
    */
-  public SegmentInfo(Directory dir, int format, IndexInput input) throws IOException {
+SegmentInfo(Directory dir, int format, IndexInput input) throws IOException {
     this.dir = dir;
     name = input.readString();
     docCount = input.readInt();
---------------
-------------
@@ -112,7 +112,7 @@
         //service unget
         verify(bundleContext).ungetService(reference);
         //unregister is invoked on context
-        verify(agentContext).unregisterMBean(name);
+verify(agentContext).unregisterMBean(target);
 
     }
 
---------------
-------------
@@ -271,7 +271,7 @@
         hits = searcher.search(new TermQuery(aaa), null, 1000).scoreDocs;
         dir.tweakBufferSizes();
         assertEquals(35, hits.length);
-        writer.close();
+writer.shutdown();
         reader.close();
       } finally {
         TestUtil.rm(indexDir);
---------------
-------------
@@ -194,7 +194,7 @@
    *  searches still in process in other threads won't be
    *  affected, and they should still call {@link #release}
    *  after they are done. */
-  @Override
+// Not in Java 5: @Override
   public void close() throws IOException {
     swapSearcher(null);
   }
---------------
-------------
@@ -102,7 +102,7 @@
           BasicAutomata.makeString("brown"),
           BasicAutomata.makeString("bob")));
       
-      public Automaton getAutomaton(String name) throws IOException {
+public Automaton getAutomaton(String name) {
         if (name.equals("quickBrown")) return quickBrownAutomaton;
         else return null;
       }
---------------
-------------
@@ -78,7 +78,7 @@
         continue;
       }
       long df = dictionary.get(e.index());
-      if (maxDf > -1 && df > maxDf) {
+if (maxDf > -1 && (100.0 * df) / vectorCount > maxDf) {
         continue;
       }
       if (df < minDf) {
---------------
-------------
@@ -74,7 +74,7 @@
       }
     };
     
-    checkRandomData(random(), a, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -69,7 +69,7 @@
       MILLISECOND_FORMAT.setTimeZone(GMT);
     }
     
-    final Calendar calInstance = Calendar.getInstance(GMT);
+final Calendar calInstance = Calendar.getInstance(GMT, Locale.US);
   }
   
   private static final ThreadLocal<DateFormats> FORMATS = new ThreadLocal<DateFormats>() {
---------------
-------------
@@ -93,7 +93,7 @@
       } else {
         localeStr = localeObj.toString();
       }
-      availableLocales.get(localeStr);
+locale = availableLocales.get(localeStr);
       if(locale==null) {
         throw new DataImportHandlerException(SEVERE, "Unsupported locale: " + localeStr);
       }
---------------
-------------
@@ -41,7 +41,7 @@
 	The driver automatically supports the correct JDBC specification version
 	for the Java Virtual Machine's environment.
 	<UL>
-	<LI> JDBC 3.0 - Java 2 - JDK 1.4
+<LI> JDBC 3.0 - Java 2 - JDK 1.4, J2SE 5.0
 	<LI> JDBC 2.0 - Java 2 - JDK 1.2,1.3
 	</UL>
 
---------------
-------------
@@ -495,7 +495,7 @@
     }
   }
 
-  private final static class TermsEnumWithSlice {
+final static class TermsEnumWithSlice {
     private final ReaderSlice subSlice;
     private TermsEnum terms;
     public BytesRef current;
---------------
-------------
@@ -152,7 +152,7 @@
       String[] operations = opParam.split(",");
 
       for (String operation : operations) {
-        Integer enabledOp = OPERATORS.get(operation.trim().toUpperCase(Locale.getDefault()));
+Integer enabledOp = OPERATORS.get(operation.trim().toUpperCase(Locale.ROOT));
 
         if (enabledOp != null) {
           enabledOps |= enabledOp;
---------------
-------------
@@ -84,7 +84,7 @@
 	 * generated, if there are no constructors then
 	 * the default no-arg constructor will be defined.
 	 */
-	ByteArray getClassBytecode();
+ByteArray getClassBytecode() throws StandardException;
 
 	/**
 	 * the class's unqualified name
---------------
-------------
@@ -100,7 +100,7 @@
 
         // our last action in the above loop was to switch d and p, so p now
         // actually has the most recent cost counts
-        return 1.0f - ((float) p[n] / Math.min(other.length(), sa.length));
+return 1.0f - ((float) p[n] / Math.max(other.length(), sa.length));
     }
 
 }
---------------
-------------
@@ -95,7 +95,7 @@
         // now sync and set the tasks' values (which allows thread calling get() to proceed)
         try
         {
-            CommitLog.instance().sync();
+CommitLog.instance.sync();
         }
         catch (IOException e)
         {
---------------
-------------
@@ -226,7 +226,7 @@
     handleResponse(action.toString().toLowerCase(Locale.ROOT), m, rsp);
   }
 
-  public static long DEFAULT_ZK_TIMEOUT = 60*1000;
+public static long DEFAULT_ZK_TIMEOUT = 180*1000;
 
   private void handleResponse(String operation, ZkNodeProps m,
                               SolrQueryResponse rsp) throws KeeperException, InterruptedException {
---------------
-------------
@@ -410,7 +410,7 @@
       }
       Thread.sleep(50);
     }
-    throw new RuntimeException("No registered leader was found, collection:" + collection + " slice:" + shard);
+throw new SolrException(ErrorCode.SERVICE_UNAVAILABLE, "No registered leader was found, collection:" + collection + " slice:" + shard);
   }
 
   /**
---------------
-------------
@@ -2171,7 +2171,7 @@
                   return se.getMessage();
               }
   
-              if ( cs == null )
+if ( cs == null || cs.getLocation() == null )
                   return null;        
       
               URL result = cs.getLocation ();
---------------
-------------
@@ -47,7 +47,7 @@
  */
 public class UpdateRequest extends AbstractUpdateRequest {
   
-  private static final String VER = "ver";
+public static final String VER = "ver";
   public static final String OVERWRITE = "ow";
   public static final String COMMIT_WITHIN = "cw";
   private Map<SolrInputDocument,Map<String,Object>> documents = null;
---------------
-------------
@@ -111,7 +111,7 @@
 
       doc.add(newField(r, "content7", "aaa bbb ccc ddd", Field.Store.NO, Field.Index.NOT_ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
 
-      final Field idField = newField("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
+final Field idField = newField(r, "id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
       doc.add(idField);
 
       final long stopTime = System.currentTimeMillis() + 500;
---------------
-------------
@@ -795,7 +795,7 @@
       if (!core.isReloaded() && ulog != null) {
         // disable recovery in case shard is in construction state (for shard splits)
         Slice slice = getClusterState().getSlice(collection, shardId);
-        if (!Slice.CONSTRUCTION.equals(slice.getState()) && !isLeader) {
+if (!Slice.CONSTRUCTION.equals(slice.getState()) || !isLeader) {
           Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()
               .getUpdateLog().recoverFromLog();
           if (recoveryFuture != null) {
---------------
-------------
@@ -91,7 +91,7 @@
     conf.set("io.serializations",
              "org.apache.hadoop.io.serializer.JavaSerialization,org.apache.hadoop.io.serializer.WritableSerialization");
     // Dont ever forget this. People should keep track of how hadoop conf parameters and make or break a piece of code
-    FileSystem dfs = FileSystem.get(conf);
+FileSystem dfs = FileSystem.get(outPath.toUri(), conf);
     if (dfs.exists(outPath))
       dfs.delete(outPath, true);
 
---------------
-------------
@@ -290,7 +290,7 @@
 
     // constructor
     private CustomScorer(CustomScoreProvider provider, CustomWeight w, float qWeight,
-        Scorer subQueryScorer, Scorer[] valSrcScorers) throws IOException {
+Scorer subQueryScorer, Scorer[] valSrcScorers) {
       super(w);
       this.qWeight = qWeight;
       this.subQueryScorer = subQueryScorer;
---------------
-------------
@@ -75,7 +75,7 @@
    * <p>
    * @param compiled CompiledAutomaton
    */
-  public AutomatonTermsEnum(TermsEnum tenum, CompiledAutomaton compiled) throws IOException {
+public AutomatonTermsEnum(TermsEnum tenum, CompiledAutomaton compiled) {
     super(tenum);
     this.finite = compiled.finite;
     this.runAutomaton = compiled.runAutomaton;
---------------
-------------
@@ -76,7 +76,7 @@
 import org.apache.aries.util.filesystem.FileSystem;
 import org.apache.aries.util.filesystem.IDirectory;
 import org.apache.aries.util.filesystem.IFile;
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 import org.apache.aries.util.manifest.BundleManifest;
 import org.junit.Before;
 import org.junit.BeforeClass;
---------------
-------------
@@ -243,7 +243,7 @@
 		utilInstance = new utilMain(1, out, ignoreErrors);
 		utilInstance.initFromEnvironment();
 		utilInstance.setMtUse(true);
-		utilInstance.go(in, out, (java.util.Properties) null);
+utilInstance.go(in, out);
 		log.flush();
 		out.flush();
 	}
---------------
-------------
@@ -125,7 +125,7 @@
    * Set a sampling-threshold
    * @see #getSamplingThreshold()
    */
-  public void setSampingThreshold(int samplingThreshold) {
+public void setSamplingThreshold(int samplingThreshold) {
     this.samplingThreshold = samplingThreshold;
   }
 
---------------
-------------
@@ -56,7 +56,7 @@
   public DistributedQueue(SolrZkClient zookeeper, String dir, List<ACL> acl) {
     this.dir = dir;
     
-    ZkCmdExecutor cmdExecutor = new ZkCmdExecutor(30);
+ZkCmdExecutor cmdExecutor = new ZkCmdExecutor(zookeeper.getZkClientTimeout());
     try {
       cmdExecutor.ensureExists(dir, zookeeper);
     } catch (KeeperException e) {
---------------
-------------
@@ -48,7 +48,7 @@
       writer.addDocument(doc);
     }
     writer.close();
-    searcher = new IndexSearcher(directory);
+searcher = new IndexSearcher(directory, true);
   }
 
   private String[] docFields = {
---------------
-------------
@@ -48,7 +48,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -18,7 +18,7 @@
 
         long L1 = ByteBuffer.wrap(o1).order(ByteOrder.LITTLE_ENDIAN).getLong();
         long L2 = ByteBuffer.wrap(o2).order(ByteOrder.LITTLE_ENDIAN).getLong();
-        return new Long(L1).compareTo(L2);
+return Long.valueOf(L1).compareTo(Long.valueOf(L2));
     }
 
     public String getString(byte[] bytes)
---------------
-------------
@@ -28,7 +28,7 @@
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.codecs.preflex.TermInfo;
-import org.apache.lucene.index.values.codec.DocValuesConsumer;
+import org.apache.lucene.index.codecs.docvalues.DocValuesConsumer;
 import org.apache.lucene.store.IndexOutput;
 
 import java.io.IOException;
---------------
-------------
@@ -185,7 +185,7 @@
         int iter = 0;
         while (iter < 50) {
             
-            traceit("-- " + iter++);
+println("-- " + iter++);
             
             // remove database directory so we can start fresh each time;
             // the memory leak also manifests when a different directory is
---------------
-------------
@@ -41,7 +41,7 @@
     {
         if ( args.length != 3 )
         {
-            System.out.println("Usage : java com.facebook.infrastructure.tools.TokenUpdater <ip:port> <token> <file containing node token info>");
+System.out.println("Usage : java org.apache.cassandra.tools.TokenUpdater <ip:port> <token> <file containing node token info>");
             System.exit(1);
         }
         
---------------
-------------
@@ -44,7 +44,7 @@
     return doGetCandidateItems(itemIDs, dataModel);
   }
 
-  abstract FastIDSet doGetCandidateItems(long[] preferredItemIDs, DataModel dataModel) throws TasteException;
+protected abstract FastIDSet doGetCandidateItems(long[] preferredItemIDs, DataModel dataModel) throws TasteException;
 
   @Override
   public void refresh(Collection<Refreshable> alreadyRefreshed) {
---------------
-------------
@@ -119,7 +119,7 @@
   }
 
   @Override
-  protected boolean requiresDocScores() {
+public boolean requiresDocScores() {
     return false;
   }
 }
---------------
-------------
@@ -46,7 +46,7 @@
 import org.apache.lucene.index.codecs.CodecProvider;
 import org.apache.lucene.index.values.DocValues.SortedSource;
 import org.apache.lucene.index.values.DocValues.Source;
-import org.apache.lucene.index.values.codec.DocValuesCodec;
+import org.apache.lucene.index.codecs.docvalues.DocValuesCodec;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.util.BytesRef;
---------------
-------------
@@ -207,7 +207,7 @@
       final String address = "localhost:" + (8963 + i) + "_solr";
       liveNodes.add(address);
       
-      solrZkClientMock.getBaseUrlForNodeName(address);
+zkStateReaderMock.getBaseUrlForNodeName(address);
       expectLastCall().andAnswer(new IAnswer<Object>() {
         @Override
         public Object answer() throws Throwable {
---------------
-------------
@@ -1128,7 +1128,7 @@
     }
 
     public void testOpenReaderAfterDelete() throws IOException {
-      File dirFile = new File(TEMP_DIR, "deletetest");
+File dirFile = _TestUtil.getTempDir("deletetest");
       Directory dir = newFSDirectory(dirFile);
       try {
         IndexReader.open(dir, false);
---------------
-------------
@@ -20,7 +20,7 @@
 import java.io.IOException;
 import java.util.Set;
 
-import org.apache.lucene.codecs.DocValuesWriterBase;
+import org.apache.lucene.codecs.lucene40.values.DocValuesWriterBase;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
---------------
-------------
@@ -68,7 +68,7 @@
             && method.getDeclaringClass() == Object.class) {
         Object targetObject = args[0];
         if (proxyManager.isProxy(targetObject)) {
-          args[0] = proxyManager.unwrap(proxy).call();
+args[0] = proxyManager.unwrap(targetObject).call();
         }
     } else if (method.getName().equals("finalize") && method.getParameterTypes().length == 0) {
         // special case finalize, don't route through to delegate because that will get its own call
---------------
-------------
@@ -396,7 +396,7 @@
 
 		// Get the cost estimate for the child
 		// RESOLVE - we will eventually include the cost of the sort
-		CostEstimate costEstimate = child.getCostEstimate(); 
+CostEstimate costEstimate = child.getFinalCostEstimate();
 
 		mb.push(costEstimate.rowCount());
 		mb.push(costEstimate.getEstimatedCost());
---------------
-------------
@@ -249,7 +249,7 @@
         String              classpath = BaseTestCase.getSystemProperty( "java.class.path" );
         boolean         skipHostName = false;
 
-        al.add( "java" );
+al.add( BaseTestCase.getJavaExecutableName() );
         al.add( "-classpath" );
         al.add( classpath );
         
---------------
-------------
@@ -90,7 +90,7 @@
             String row = rowPrefix + nRows;
             ColumnPath col = new ColumnPath("Standard1").setSuper_column(null).setColumn("col1".getBytes());
 
-            List<InetAddress> endPoints = tester.ringCache.getEndPoint(row);
+List<InetAddress> endPoints = tester.ringCache.getEndPoint(row.getBytes());
             String hosts="";
             for (int i = 0; i < endPoints.size(); i++)
                 hosts = hosts + ((i > 0) ? "," : "") + endPoints.get(i);
---------------
-------------
@@ -297,7 +297,7 @@
           .setMergeScheduler(new SerialMergeScheduler()).setMergePolicy(
               new LogDocMergePolicy()));
 
-      Directory[] indexDirs = {new MockDirectoryWrapper(new RAMDirectory(dir))};
+Directory[] indexDirs = {new MockDirectoryWrapper(random, new RAMDirectory(dir))};
       writer.addIndexes(indexDirs);
       writer.optimize();
       writer.close();
---------------
-------------
@@ -111,7 +111,7 @@
       //DateFilter filter = DateFilter.Before("modified", Time(1997,00,01));
       //System.out.println(filter);
 
-	hits = searcher.search(query, null);
+hits = searcher.search(query);
 
 	System.out.println(hits.length() + " total results");
 	for (int i = 0 ; i < hits.length() && i < 10; i++) {
---------------
-------------
@@ -320,7 +320,7 @@
         try {
           list.add(new Attribute(attribute, getAttribute(attribute)));
         } catch (Exception e) {
-          LOG.warn("Could not get attibute " + attribute);
+LOG.warn("Could not get attribute " + attribute);
         }
       }
 
---------------
-------------
@@ -2985,7 +2985,7 @@
                     }
 						
 					if (checkpointInstant == LogCounter.INVALID_LOG_INSTANT &&
-										getMirrorControlFileName().exists())
+privExists(getMirrorControlFileName()))
                     {
 						checkpointInstant =
                             readControlFile(
---------------
-------------
@@ -28,7 +28,7 @@
     public void addColumn(IColumn column);
 
     public boolean isMarkedForDelete();
-    public long getMarkedForDeleteAt();
+public IClock getMarkedForDeleteAt();
 
     public AbstractType getComparator();
 }
---------------
-------------
@@ -52,7 +52,7 @@
   public AbstractVisitingPrefixTreeFilter(Shape queryShape, String fieldName, SpatialPrefixTree grid,
                                           int detailLevel, int prefixGridScanLevel) {
     super(queryShape, fieldName, grid, detailLevel);
-    this.prefixGridScanLevel = Math.max(1, Math.min(prefixGridScanLevel, grid.getMaxLevels() - 1));
+this.prefixGridScanLevel = Math.max(0, Math.min(prefixGridScanLevel, grid.getMaxLevels() - 1));
     assert detailLevel <= grid.getMaxLevels();
   }
 
---------------
-------------
@@ -105,7 +105,7 @@
     for(Thread t : indexThreads) {
       t.join();
     }
-    w.close();
+w.shutdown();
     d.close();
 
     TestUtil.rm(tmpDir);
---------------
-------------
@@ -85,7 +85,7 @@
     }
 
     // Close should force cache to clear since all files are sync'd
-    w.close();
+w.shutdown();
 
     final String[] cachedFiles = cachedDir.listCachedFiles();
     for(String file : cachedFiles) {
---------------
-------------
@@ -402,7 +402,7 @@
 				break;
 			off += len;
 
-			int available = in.available();
+int available = Math.max(1, in.available());
 			int extraSpace = available - (tmpData.length - off);
 			if (extraSpace > 0)
 			{
---------------
-------------
@@ -130,7 +130,7 @@
             migration.apply();
             
             // note that we're storing this in the system table, which is not replicated
-            logger.debug("Applying migration " + newVersion.toString());
+logger.info("Applying migration {} {}", newVersion.toString(), toString());
             migration = new RowMutation(Table.SYSTEM_TABLE, LAST_MIGRATION_KEY);
             migration.add(new QueryPath(SCHEMA_CF, null, LAST_MIGRATION_KEY), ByteBuffer.wrap(UUIDGen.decompose(newVersion)), now);
             migration.apply();
---------------
-------------
@@ -410,7 +410,7 @@
         // do it up front & cache
         long sum = 0;
         int upto = i;
-        while(upto < orderedTerms.length && orderedTerms[i].equals(field)) {
+while(upto < orderedTerms.length && orderedTerms[i].field().equals(field)) {
           sum += orderedTerms[i].getTotalTermFreq();
           upto++;
         }
---------------
-------------
@@ -35,7 +35,7 @@
   @Test
   public void testRollingUpdates() throws Exception {
     Random random = new Random(random().nextLong());
-    final MockDirectoryWrapper dir = newDirectory();
+final BaseDirectoryWrapper dir = newDirectory();
     dir.setCheckIndexOnClose(false); // we use a custom codec provider
     final LineFileDocs docs = new LineFileDocs(random, defaultCodecSupportsDocValues());
 
---------------
-------------
@@ -193,7 +193,7 @@
             }
             sis = null;
           } catch (IOException e) {
-            if (SegmentInfos.generationFromSegmentsFileName(fileName) <= currentGen) {
+if (SegmentInfos.generationFromSegmentsFileName(fileName) <= currentGen && directory.fileLength(fileName) > 0) {
               throw e;
             } else {
               // Most likely we are opening an index that
---------------
-------------
@@ -2273,7 +2273,7 @@
         else if (testType.equals("java"))
         {
             sysProp.put("user.dir", outDir.getCanonicalPath());
-            javaPath = "org.apache.derbyTesting." + testDirName;
+javaPath = "org.apache.derbyTesting.functionTests.tests." + testDirName;
             String[] args = new String[2];
             args[0] = "-p";
             args[1] = propString;
---------------
-------------
@@ -165,7 +165,7 @@
     public long totalBytesSize() throws IOException {
       long total = 0;
       for (SegmentInfo info : segments) {
-        total += info.sizeInBytes();
+total += info.sizeInBytes(true);
       }
       return total;
     }
---------------
-------------
@@ -130,7 +130,7 @@
             File file = new File(files.get(0).getFilename());
             if (logger.isDebugEnabled())
               logger.debug("Streaming " + file.length() + " length file " + file + " ...");
-            MessagingService.instance.stream(file.getAbsolutePath(), 0L, file.length(), FBUtilities.getLocalAddress(), to);
+MessagingService.instance.stream(file.getAbsolutePath(), to);
         }
     }
 
---------------
-------------
@@ -215,7 +215,7 @@
             return;
         }
         indexCfs.unregisterMBean();
-        SystemTable.setIndexRemoved(metadata.tableName, indexCfs.columnFamily);
+SystemTable.setIndexRemoved(metadata.ksName, indexCfs.columnFamily);
         indexCfs.removeAllSSTables();
     }
 
---------------
-------------
@@ -58,6 +58,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new CatalanAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new CatalanAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -78,7 +78,7 @@
 
     @Override
     public void setNextReader(AtomicReaderContext context) throws IOException {
-      this.docBase = docBase;
+docBase = context.docBase;
     }
     
     @Override
---------------
-------------
@@ -47,7 +47,7 @@
         return serializer;
     }
 
-    public Message getMessage(int version) throws IOException
+public Message getMessage(Integer version) throws IOException
     {
         ByteArrayOutputStream bos = new ByteArrayOutputStream();
         DataOutputStream dos = new DataOutputStream(bos);
---------------
-------------
@@ -110,7 +110,7 @@
     tf.addTerm(new Term(fieldName, "content1"));
     
     MultiReader multi = new MultiReader(reader1, reader2);
-    for (AtomicReaderContext context : multi.getTopReaderContext().leaves()) {
+for (AtomicReaderContext context : multi.leaves()) {
       FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());
       assertTrue("Must be >= 0", bits.cardinality() >= 0);      
     }
---------------
-------------
@@ -168,7 +168,7 @@
     File dir = new File(testDirPath);
     File[] subdirs = dir.listFiles();
 
-    ResultAnalyzer resultAnalyzer = new ResultAnalyzer(model.getLabels());
+ResultAnalyzer resultAnalyzer = new ResultAnalyzer(model.getLabels(), defaultCat);
 
     if (subdirs != null) {
       for (int loop = 0; loop < subdirs.length; loop++) {
---------------
-------------
@@ -130,7 +130,7 @@
         StreamOutManager.get(target).addFilesToStream(pendingFiles);
         StreamInitiateMessage biMessage = new StreamInitiateMessage(pendingFiles);
         Message message = StreamInitiateMessage.makeStreamInitiateMessage(biMessage);
-        message.addHeader(StreamOut.TABLE_NAME, table.getBytes());
+message.setHeader(StreamOut.TABLE_NAME, table.getBytes());
         if (logger.isDebugEnabled())
           logger.debug("Sending a stream initiate message to " + target + " ...");
         MessagingService.instance.sendOneWay(message, target);
---------------
-------------
@@ -116,7 +116,7 @@
     }
     Map<String, Map<String, String>> parsedFragHost = ManifestHeaderProcessor.parseImportString(fragmentHostHeader);
     if(parsedFragHost.size() != 1)
-      throw new InvalidAttributeException(MessageUtil.getMessage("APPUTILS0001W",
+throw new InvalidAttributeException(MessageUtil.getMessage("MORE_THAN_ONE_FRAG_HOST",
           new Object[] {fragmentHostHeader}, 
           "An internal error occurred. A bundle fragment manifest must define exactly one Fragment-Host entry. The following entry was found" + fragmentHostHeader + "."));
     
---------------
-------------
@@ -59,7 +59,7 @@
   public final class PayloadAnalyzer extends Analyzer {
 
     public PayloadAnalyzer() {
-      super(new PerFieldReuseStrategy());
+super(PER_FIELD_REUSE_STRATEGY);
     }
 
     @Override
---------------
-------------
@@ -102,7 +102,7 @@
   // all the similarities that we rotate through
   /** The DFR basic models to test. */
   static BasicModel[] BASIC_MODELS = {
-    new BasicModelBE(), /* TODO: enable new BasicModelD(), */ new BasicModelG(),
+/* TODO: enable new BasicModelBE(), */ /* TODO: enable new BasicModelD(), */ new BasicModelG(),
     new BasicModelIF(), new BasicModelIn(), new BasicModelIne(),
     /* TODO: enable new BasicModelP() */
   };
---------------
-------------
@@ -91,7 +91,7 @@
     Field idField = new Field("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
     doc.add(idField);
     
-    IndexWriter writer = new IndexWriter(dir, true, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
     MyMergeScheduler ms = new MyMergeScheduler();
     writer.setMergeScheduler(ms);
     writer.setMaxBufferedDocs(2);
---------------
-------------
@@ -94,7 +94,7 @@
 	 */
 	public QueryTreeNode bind() throws StandardException
 	{
-		privileges = (PrivilegeNode) privileges.bind( new HashMap(), grantees);
+privileges = (PrivilegeNode) privileges.bind( new HashMap(), grantees, true);
 		return this;
 	} // end of bind
 
---------------
-------------
@@ -1027,7 +1027,7 @@
 
         try
         {
-            state().hasColumnFamilyAccess((String)columnFamily, Permission.WRITE);
+state().hasColumnFamilyAccess(columnFamily.toString(), Permission.WRITE);
             schedule();
             StorageProxy.truncateBlocking(state().getKeyspace(), columnFamily.toString());
         }
---------------
-------------
@@ -435,7 +435,7 @@
 	    bra > ket ||
 	    ket > limit)
 	{
-	    System.err.println("faulty slice operation");
+throw new IllegalArgumentException("faulty slice operation: bra=" + bra + ",ket=" + ket + ",limit=" + limit);
 	// FIXME: report error somehow.
 	/*
 	    fprintf(stderr, "faulty slice operation:\n");
---------------
-------------
@@ -427,7 +427,7 @@
             String msg = ec + " " + ss + " " + se.getMessage();
             util.DEBUG("3. startSlave No connection as expected: " + msg);
             assertTrue("3. Unexpected SQLException: " + msg, 
-                    SQLState.CANNOT_START_SLAVE_ALREADY_BOOTED.equals(ss));
+SQLState.CANNOT_CONNECT_TO_DB_IN_SLAVE_MODE.startsWith(ss));
         }
     }
     
---------------
-------------
@@ -467,7 +467,7 @@
         StringBuffer    buffer = new StringBuffer();
         String          classpath = getSystemProperty( "java.class.path" );
 
-        buffer.append( "java -classpath " );
+buffer.append( getJavaExecutableName() + " -classpath " );
         buffer.append( classpath );
         buffer.append( " -Demma.verbosity.level=silent");
         buffer.append( " org.apache.derby.drda.NetworkServerControl -p " + portNumber + " " + commandSpecifics );
---------------
-------------
@@ -297,7 +297,7 @@
         len += n;
       }
 
-      return new String(output, 0, output.length);
+return new String(output, 0, len);
     } finally {
       if (input != null) input.close();
     }
---------------
-------------
@@ -45,7 +45,7 @@
    * <code>WILDCARD_CHAR</code> will cause an exception to be thrown.
    */
   public WildcardTermEnum(IndexReader reader, Term term) throws IOException {
-    super(reader, term);
+super();
     searchTerm = term;
     field = searchTerm.field();
     text = searchTerm.text();
---------------
-------------
@@ -13,7 +13,7 @@
 
 import org.apache.lucene.facet.index.params.CategoryListParams;
 import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter.OrdinalMap;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.encoding.IntDecoder;
 import org.apache.lucene.util.encoding.IntEncoder;
---------------
-------------
@@ -126,7 +126,7 @@
     /** called by failure detection code to notify that housekeeping should be performed on downed sockets. */
     public void convict(InetAddress ep)
     {
-        logger_.debug("Resetting pool for " + ep);
+logger_.trace("Resetting pool for " + ep);
         getConnectionPool(ep).reset();
     }
 
---------------
-------------
@@ -468,7 +468,7 @@
 	public Process execJavaCmd(String[] cmd) throws IOException {
 	    int totalSize = 3 + cmd.length;
 	    String[] tcmd = new String[totalSize];
-	    tcmd[0] = "java";
+tcmd[0] = getJavaExecutableName();
 	    tcmd[1] = "-classpath";
 	    tcmd[2] = BaseTestCase.getSystemProperty("java.class.path");
 
---------------
-------------
@@ -89,7 +89,7 @@
 
             BundleContext compositeBundleContext = cb.getCompositeFramework().getBundleContext();
             // install the blueprint sample onto the framework associated with the composite bundle
-            MavenArtifactProvisionOption mapo = CoreOptions.mavenBundle().groupId("org.apache.aries.blueprint").artifactId("org.apache.aries.blueprint.sample").version( "0.1-incubating-SNAPSHOT");
+MavenArtifactProvisionOption mapo = mavenBundleInTest("org.apache.aries.blueprint", "org.apache.aries.blueprint.sample");
             // let's use input stream to avoid invoking mvn url handler which isn't avail in the child framework.
             InputStream is = new URL(mapo.getURL()).openStream();
             Bundle bundle = compositeBundleContext.installBundle(mapo.getURL(), is);
---------------
-------------
@@ -1,6 +1,6 @@
 /*
 
-   Derby - Class org.apache.derby.impl.services.bytecode.CodeChunk
+Derby - Class org.apache.derbyTesting.junit.XATestUtil
 
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -67,7 +67,7 @@
         Table.open(keyspace).getColumnFamilyStore(columnFamily).truncate();
     }
 
-    public Message getMessage(int version) throws IOException
+public Message getMessage(Integer version) throws IOException
     {
         ByteArrayOutputStream bos = new ByteArrayOutputStream();
         DataOutputStream dos = new DataOutputStream(bos);
---------------
-------------
@@ -457,7 +457,7 @@
             else
             {
                 /* Gossip with the seed with some probability. */
-                double probability = seeds_.size() / ( liveEndpoints_.size() + unreachableEndpoints_.size() );
+double probability = seeds_.size() / (double)( liveEndpoints_.size() + unreachableEndpoints_.size() );
                 double randDbl = random_.nextDouble();
                 if ( randDbl <= probability )
                     sendGossip(message, seeds_);
---------------
-------------
@@ -301,7 +301,7 @@
 
     Directory dir = FSDirectory.open(tempIndexPath);
 
-    IndexWriterConfig iwc = new IndexWriterConfig(Version.LUCENE_46, indexAnalyzer);
+IndexWriterConfig iwc = new IndexWriterConfig(Version.LUCENE_CURRENT, indexAnalyzer);
     iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE);
     iwc.setRAMBufferSizeMB(ramBufferSizeMB);
     IndexWriter writer = new IndexWriter(dir, iwc);
---------------
-------------
@@ -53,7 +53,7 @@
 	EmbeddedXADataSource automatically supports the correct JDBC specification version
 	for the Java Virtual Machine's environment.
 	<UL>
-	<LI> JDBC 3.0 - Java 2 - JDK 1.4
+<LI> JDBC 3.0 - Java 2 - JDK 1.4, J2SE 5.0
 	<LI> JDBC 2.0 - Java 2 - JDK 1.2,1.3
 	</UL>
 
---------------
-------------
@@ -59,7 +59,7 @@
   /** Default set of articles for ElisionFilter */
   public static final CharArraySet DEFAULT_ARTICLES = CharArraySet.unmodifiableSet(
       new CharArraySet(Version.LUCENE_CURRENT, Arrays.asList(
-          "l", "m", "t", "qu", "n", "s", "j"), true));
+"l", "m", "t", "qu", "n", "s", "j", "d", "c", "jusqu", "quoiqu", "lorsqu", "puisqu"), true));
 
   /**
    * Contains words that should be indexed but not stemmed.
---------------
-------------
@@ -152,7 +152,7 @@
     createAlias("testalias", "collection2,collection1");
     
     // search with new cloud client
-    CloudSolrServer cloudSolrServer = new CloudSolrServer(zkServer.getZkAddress());
+CloudSolrServer cloudSolrServer = new CloudSolrServer(zkServer.getZkAddress(), random().nextBoolean());
     query = new SolrQuery("*:*");
     query.set("collection", "testalias");
     res = cloudSolrServer.query(query);
---------------
-------------
@@ -746,7 +746,7 @@
 	 */
 	public static String parseRoleId(String roleName) throws StandardException
 	{
-		roleName.trim();
+roleName = roleName.trim();
 		// NONE is a special case and is not allowed with its special
 		// meaning in SET ROLE <value specification>. Even if there is
 		// a role with case normal form "NONE", we require it to be
---------------
-------------
@@ -383,7 +383,7 @@
           .append("=*))");
       else
           filter.append("(").append(PersistenceContextProvider.PROXY_FACTORY_EMF_ATTRIBUTE)
-                  .append("=*)");
+.append("=true)");
 
       // Add the empty name filter if necessary
       if (!"".equals(unitName))
---------------
-------------
@@ -1327,7 +1327,7 @@
 	String LANG_CURSOR_ALREADY_EXISTS                                  = "X0X60.S";
 	String LANG_INDEX_COLUMN_NOT_EQUAL                                 = "X0X61.S";
 	String LANG_INCONSISTENT_ROW_LOCATION                              = "X0X62.S";
-	String LANG_FILE_ERROR                                             = "X0X63.S";
+String LANG_IO_EXCEPTION                                           = "X0X63.S";
 	String LANG_COLUMN_NOT_ORDERABLE_DURING_EXECUTION                  = "X0X67.S";
 	String LANG_OBJECT_NOT_FOUND_DURING_EXECUTION                      = "X0X81.S";
 	String LANG_NON_KEYED_INDEX                                        = "X0X85.S";
---------------
-------------
@@ -109,7 +109,7 @@
 
     // Open near-real-time searcher
     searcher = new IndexSearcher(DirectoryReader.open(writer, true));
-    writer.close();
+writer.shutdown();
   }
 
   private ValueSource getDistanceValueSource() {
---------------
-------------
@@ -3704,7 +3704,7 @@
           // This merge (and, generally, any change to the
           // segments) may now enable new merges, so we call
           // merge policy & update pending merges.
-          if (success && !merge.isAborted() && (merge.optimize || (!closed && !closing))) {
+if (success && !merge.isAborted() && !closed && !closing) {
             updatePendingMerges(merge.maxNumSegmentsOptimize, merge.optimize);
           }
         }
---------------
-------------
@@ -106,7 +106,7 @@
         {
             if (Arrays.equals(column.name(), DEFINITION_SCHEMA_COLUMN_NAME))
                 continue;
-            org.apache.cassandra.config.avro.KsDef ks = SerDeUtils.<org.apache.cassandra.config.avro.KsDef>deserialize(schema, column.value());
+org.apache.cassandra.config.avro.KsDef ks = SerDeUtils.deserialize(schema, column.value(), new org.apache.cassandra.config.avro.KsDef());
             keyspaces.add(KSMetaData.inflate(ks));
         }
         return keyspaces;
---------------
-------------
@@ -109,7 +109,7 @@
       }
       double rescoredSimilarity = rescorer == null ? similarity : rescorer.rescore(userID, similarity);
       if (!Double.isNaN(rescoredSimilarity) && (!full || rescoredSimilarity > lowestTopValue)) {
-        topUsers.add(new SimilarUser(userID, similarity));
+topUsers.add(new SimilarUser(userID, rescoredSimilarity));
         if (full) {
           topUsers.poll();
         } else if (topUsers.size() > howMany) {
---------------
-------------
@@ -634,7 +634,7 @@
       try {
         do {
           Term term = termEnum.term();
-          if (term==null || term.field() != field) break;
+if (term==null || term.field() != field || t >= mterms.length) break;
 
           // store term text
           mterms[t] = term.text();
---------------
-------------
@@ -78,7 +78,7 @@
 
   @Override
   public void testSize() {
-    assertEquals("size", 7, getTestVector().getNumNondefaultElements());
+assertEquals("size", 3, getTestVector().getNumNondefaultElements());
   }
 
   @Override
---------------
-------------
@@ -59,6 +59,6 @@
 
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    return new Lucene3xFields(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.termsIndexDivisor);
+return new Lucene3xFields(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.termsIndexDivisor);
   }
 }
---------------
-------------
@@ -998,7 +998,7 @@
     assertQ(req("id:42 AND subword:IBM's")
             ,"*[count(//doc)=1]"
             );
-    assertQ(req("id:42 AND subword:IBM'sx")
+assertQ(req("id:42 AND subword:\"IBM'sx\"")
             ,"*[count(//doc)=0]"
             );
 
---------------
-------------
@@ -42,7 +42,7 @@
 
   @Test
   public void testTokenizeDocuments() throws Exception {
-    Configuration configuration = new Configuration();
+Configuration configuration = getConfiguration();
     Path input = new Path(getTestTempDirPath(), "inputDir");
     Path output = new Path(getTestTempDirPath(), "outputDir");
     FileSystem fs = FileSystem.get(input.toUri(), configuration);
---------------
-------------
@@ -188,7 +188,7 @@
 
             // do the insert
             thriftClient_.insert(tableName, key, columnFamily + ":" + columnName,
-                                 value.getBytes(), System.currentTimeMillis(), true);
+value.getBytes(), System.currentTimeMillis(), 1);
 
             css_.out.println("Value inserted.");
         }
---------------
-------------
@@ -323,7 +323,7 @@
 		int svrcod, String rdbnam, String srvdgn)
 	{
         if ( SanityManager.DEBUG )
-            System.out.println("agentError in " + agent);
+agent.trace("agentError in " + agent);
 		Object[] oa = {new Integer(svrcod), rdbnam, srvdgn};
 		return new DRDAProtocolException(DRDA_AgentError,
 										agent,
---------------
-------------
@@ -194,7 +194,7 @@
 
     public String getTableName()
     {
-        return cfs.getTable().name;
+return cfs.table.name;
     }
 
     /**
---------------
-------------
@@ -438,7 +438,7 @@
 			/* Now that we've finally goobered stuff up, bind and validate
 			 * the check constraints and generation clauses.
 			 */
-			if  (numGenerationClauses > 0) { tableElementList.bindAndValidateGenerationClauses( sd, fromList, generatedColumns ); }
+if  (numGenerationClauses > 0) { tableElementList.bindAndValidateGenerationClauses( sd, fromList, generatedColumns, null ); }
 			if  (numCheckConstraints > 0) { tableElementList.bindAndValidateCheckConstraints(fromList); }
             if ( numReferenceConstraints > 0) { tableElementList.validateForeignKeysOnGenerationClauses( fromList, generatedColumns ); }
 		}
---------------
-------------
@@ -529,7 +529,7 @@
      */
     public String getFlushPath()
     {
-        long guessedSize = 2 * DatabaseDescriptor.getMemtableThroughput() * 1024*1024; // 2* adds room for keys, column indexes
+long guessedSize = 2 * metadata.memtableThroughputInMb * 1024*1024; // 2* adds room for keys, column indexes
         String location = DatabaseDescriptor.getDataFileLocationForTable(table.name, guessedSize);
         if (location == null)
             throw new RuntimeException("Insufficient disk space to flush");
---------------
-------------
@@ -130,7 +130,7 @@
     }
 
     @Override
-    public int getUniqueFieldCount() throws IOException {
+public int size() throws IOException {
       // TODO: add faster implementation!
       int c = 0;
       final FieldsEnum it = iterator();
---------------
-------------
@@ -298,7 +298,7 @@
         + ">\n"
         + "    <core name=\""
         + CoreContainer.DEFAULT_DEFAULT_CORE_NAME
-        + "\" shard=\"${shard:}\" collection=\"${collection:}\" instanceDir=\"collection1\" />\n"
++ "\" shard=\"${shard:}\" collection=\"${collection:collection1}\" instanceDir=\"collection1\" />\n"
         + "  </cores>\n" + "</solr>";
 
   @Override
---------------
-------------
@@ -62,7 +62,7 @@
 
   @Override
   public void testSize() {
-    assertEquals("size", 3, getTestVector().getNumNondefaultElements());
+assertEquals("size", 3, getTestVector().getNumNonZeroElements());
   }
 
   @Override
---------------
-------------
@@ -59,7 +59,7 @@
   }
 
   public static String docValuesId(String segmentsName, int fieldId) {
-    return segmentsName + "-" + fieldId;
+return segmentsName + "_" + fieldId;
   }
   
   
---------------
-------------
@@ -367,7 +367,7 @@
         {
             // We want to pass this down to RunTest so it will
             // run an individual test with jvmflags like -nojit
-            jvmProps.addElement("jvmflags=" + '"' + jvmflags + '"');
+jvmProps.addElement("jvmflags=" + jvmflags);
         }
 
         if ( (timeout != null) && (timeout.length()>0) )
---------------
-------------
@@ -28,7 +28,7 @@
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.*;
+import static org.apache.lucene.analysis.VocabularyAssert.*;
 
 /**
  * Simple tests for {@link PortugueseLightStemFilter}
---------------
-------------
@@ -280,7 +280,7 @@
     }
 
     final IndexReader r = w.getReader();
-    w.close();
+w.shutdown();
 
     final IndexSearcher s = newSearcher(r);
     int counter = 0;
---------------
-------------
@@ -80,7 +80,7 @@
    * @param b ScoreDoc
    * @return <code>true</code> if document <code>a</code> should be sorted after document <code>b</code>.
    */
-  protected final boolean lessThan (final Object a, final Object b) {
+protected boolean lessThan (final Object a, final Object b) {
     final ScoreDoc docA = (ScoreDoc) a;
     final ScoreDoc docB = (ScoreDoc) b;
 
---------------
-------------
@@ -26,7 +26,7 @@
   private final SegmentInfoWriter writer = new PreFlexRWSegmentInfoWriter();
 
   @Override
-  public SegmentInfoWriter getSegmentInfosWriter() {
+public SegmentInfoWriter getSegmentInfoWriter() {
     return writer;
   }
 }
---------------
-------------
@@ -942,7 +942,7 @@
         stream.write(out);
       }
 
-      public void write(Writer writer, SolrQueryRequest request, SolrQueryResponse response) throws IOException {
+public void write(Writer writer, SolrQueryRequest request, SolrQueryResponse response) {
         throw new RuntimeException("This is a binary writer , Cannot write to a characterstream");
       }
 
---------------
-------------
@@ -115,7 +115,7 @@
 	{
 		if (node instanceof ValueNode)
 		{
-			if (ignoreParameters && ((ValueNode)node).isParameterNode())
+if (ignoreParameters && ((ValueNode)node).requiresTypeFromContext())
 				return node;
 				
 			if (((ValueNode)node).getOrderableVariantType() <= variantType)
---------------
-------------
@@ -121,7 +121,7 @@
         		{"08004","Database connection refused.","40000"},
         		{"08004","User '{0}' cannot shut down database '{1}'. Only the database owner can perform this operation.","40000"},
         		{"08004","User '{0}' cannot (re)encrypt database '{1}'. Only the database owner can perform this operation.","40000"},
-        		{"08004","User '{0}' cannot hard upgrade database '{1}'. Only the database owner can perform this operation.","40000"},
+{"08004","User '{0}' cannot upgrade database '{1}'. Only the database owner can perform this operation.","40000"},
         		{"08004","Connection refused to database '{0}' because it is in replication slave mode.","40000"},
         		{"08004","User '{0}' cannot issue a replication operation on database '{1}'. Only the database owner can perform this operation.","40000"},
         		{"08004","Missing permission for user '{0}' to shutdown system [{1}].","40000"},
---------------
-------------
@@ -244,7 +244,7 @@
     fis = new FileInputStream(new File(tempDir, SOLR_PERSIST_XML));
     try {
       Document document = builder.parse(fis);
-      assertTrue(exists("/solr/cores/core[@name='collection1' and (@instanceDir='./' or @instanceDir='.\\')]", document));
+assertTrue(exists("/solr/cores/core[@name='collection1' and @instanceDir='.']", document));
     } finally {
       fis.close();
     }
---------------
-------------
@@ -51,6 +51,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new LatvianAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new LatvianAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -228,7 +228,7 @@
         		{"XSLA1","Log Record has been sent to the stream, but it cannot be applied to the store (Object {0}).  This may cause recovery problems also.","45000"},
         		{"XSLA2","System will shutdown, got I/O Exception while accessing log file.","45000"},
         		{"XSLA3","Log Corrupted, has invalid data in the log stream.","45000"},
-        		{"XSLA4","Cannot write to the log, most likely the log is full.  Please delete unnecessary files.  It is also possible that the file system is read only, or the disk has failed, or some other problems with the media.  ","45000"},
+{"XSLA4","Error encountered when attempting to write the transaction recovery log. Most likely the disk holding the recovery log is full. If the disk is full, the only way to proceed is to free up space on the disk by either expanding it or deleting files not related to Derby. It is also possible that the file system and/or disk where the Derby transaction log resides is read-only. The error can also be encountered if the disk or file system has failed.","45000"},
         		{"XSLA5","Cannot read log stream for some reason to rollback transaction {0}.","45000"},
         		{"XSLA6","Cannot recover the database.","45000"},
         		{"XSLA7","Cannot redo operation {0} in the log.","45000"},
---------------
-------------
@@ -198,7 +198,7 @@
    * 
    * @lucene.experimental
    */
-  protected void visitSubScorers(Query parent, Occur relationship,
+public void visitSubScorers(Query parent, Occur relationship,
       ScorerVisitor<Query, Query, Scorer> visitor) {
     if (weight == null)
       throw new UnsupportedOperationException();
---------------
-------------
@@ -137,7 +137,7 @@
   final synchronized void doClose() throws IOException {
     if (deletedDocsDirty) {
       synchronized (directory) {		  // in- & inter-process sync
-        new Lock.With(directory.makeLock("IndexWriter.COMMIT_LOCK_NAME"),
+new Lock.With(directory.makeLock(IndexWriter.COMMIT_LOCK_NAME),
           IndexWriter.COMMIT_LOCK_TIMEOUT) {
           public Object doBody() throws IOException {
             deletedDocs.write(directory, segment + ".tmp");
---------------
-------------
@@ -41,7 +41,7 @@
   protected void tearDown() {
   }
 
-  public void test() {
+public void test() throws IOException {
     //Positive test of FieldInfos
     assertTrue(testDoc != null);
     FieldInfos fieldInfos = new FieldInfos();
---------------
-------------
@@ -135,7 +135,7 @@
  * &lt;/requestHandler&gt;</pre>
  */
 public class SwitchQParserPlugin extends QParserPlugin {
-  public static String NAME = "switch";
+public static final String NAME = "switch";
 
   /** 
    * Used as both a local params key to find the "default" if no
---------------
-------------
@@ -58,7 +58,7 @@
       writer.addDocument(doc);
     }
     indexReader = SlowCompositeReaderWrapper.wrap(writer.getReader());
-    writer.close();
+writer.shutdown();
     indexSearcher = newSearcher(indexReader);
     indexSearcher.setSimilarity(new DefaultSimilarity());
   }
---------------
-------------
@@ -60,7 +60,7 @@
 
   public void testStopList() throws IOException {
     CharArraySet stopWordsSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("good", "test", "analyzer"), false);
-    StopAnalyzer newStop = new StopAnalyzer(Version.LUCENE_40, stopWordsSet);
+StopAnalyzer newStop = new StopAnalyzer(TEST_VERSION_CURRENT, stopWordsSet);
     try (TokenStream stream = newStop.tokenStream("test", "This is a good test of the english stop analyzer")) {
       assertNotNull(stream);
       CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
---------------
-------------
@@ -55,7 +55,7 @@
         for (int i = 0; i < ROWS_PER_SSTABLE; i++) {
             DecoratedKey key = Util.dk(String.valueOf(i));
             RowMutation rm = new RowMutation(TABLE1, key.key);
-            rm.add(new QueryPath("Super3", ByteBufferUtil.bytes("sc"), ByteBuffer.wrap(String.valueOf(i).getBytes())), ByteBuffer.wrap(new byte[ROWS_PER_SSTABLE * 10 - i * 2]), i);
+rm.add(new QueryPath("Super3", ByteBufferUtil.bytes("sc"), ByteBufferUtil.bytes(String.valueOf(i))), ByteBuffer.wrap(new byte[ROWS_PER_SSTABLE * 10 - i * 2]), i);
             rm.apply();
             inserted.add(key);
         }
---------------
-------------
@@ -217,7 +217,7 @@
             String value = URLDecoder.decode( kv.substring( idx+1 ), charset);
             MultiMapSolrParams.addParam( name, value, map );
           } else {
-            String name = URLDecoder.decode( kv, "UTF-8" );
+String name = URLDecoder.decode( kv, charset );
             MultiMapSolrParams.addParam( name, "", map );
           }
         }
---------------
-------------
@@ -137,7 +137,7 @@
   protected static void assertHits(Searcher s, Query query,
       final String description, final String[] expectedIds,
       final float[] expectedScores) throws IOException {
-    QueryUtils.check(query, s);
+QueryUtils.check(random, query, s);
     
     final float tolerance = 1e-5f;
     
---------------
-------------
@@ -33,7 +33,7 @@
 
   @Test
   public void testLegacy() throws Exception {
-    IndexWriter writer = ((DirectUpdateHandler2)h.getCore().getUpdateHandler()).getIndexWriterProvider().getIndexWriter(h.getCore());
+IndexWriter writer = ((DirectUpdateHandler2)h.getCore().getUpdateHandler()).getSolrCoreState().getIndexWriter(h.getCore());
     assertTrue(writer.getConfig().getMergePolicy().getClass().getName().equals(LogDocMergePolicy.class.getName()));
     assertTrue(writer.getConfig().getMergeScheduler().getClass().getName().equals(SerialMergeScheduler.class.getName()));
   }
---------------
-------------
@@ -62,7 +62,7 @@
 
   private static boolean disableLocks = false;
 
-  private static boolean DEFAULT_DO_SYNC = true;
+private static boolean DEFAULT_DO_SYNC = false;
 
   // True if we should call sync() before closing a file.
   // This improves chances that index will still be
---------------
-------------
@@ -57,7 +57,7 @@
         assertTrue(expected.contains(data.get(index)));
       }
 
-      return new Leaf(-1);
+return new Leaf(Double.NaN);
     }
   }
 
---------------
-------------
@@ -97,7 +97,7 @@
     // omitNorms is true
     for (FieldInfo fi : reader.getFieldInfos()) {
       if (fi.isIndexed()) {
-        assertTrue(fi.omitsNorms() == (reader.normValues(fi.name) == null));
+assertTrue(fi.omitsNorms() == (reader.getNormValues(fi.name) == null));
       }
     }
     reader.close();
---------------
-------------
@@ -158,7 +158,7 @@
       for (Weight w : weights) {
         // we will advance() subscorers
         Scorer subScorer = w.scorer(context, true, false, acceptDocs);
-        if (subScorer != null && subScorer.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
+if (subScorer != null) {
           scorers[idx++] = subScorer;
         }
       }
---------------
-------------
@@ -558,7 +558,7 @@
          //that is obtained from the Blob after
          //the update.
          ByteArrayInputStream str_is = new ByteArrayInputStream
-                 (str.getBytes());
+(str.getBytes("US-ASCII"));
          
          //create the empty Clob.
          Clob clob = getConnection().createClob();
---------------
-------------
@@ -62,7 +62,7 @@
     String thirdLocaleParam = random().nextBoolean() ? "" : (", '" + Locale.getDefault() + "'");
     StringBuilder sb = new StringBuilder();
     sb.append("<dataConfig> \n");
-    sb.append("<dataSource name=\"hsqldb\" driver=\"org.hsqldb.jdbcDriver\" url=\"jdbc:hsqldb:mem:.\" /> \n");
+sb.append("<dataSource name=\"hsqldb\" driver=\"${dataimporter.request.dots.in.hsqldb.driver}\" url=\"jdbc:hsqldb:mem:.\" /> \n");
     sb.append("<document name=\"TestEvaluators\"> \n");
     sb.append("<entity name=\"FIRST\" processor=\"SqlEntityProcessor\" dataSource=\"hsqldb\" ");
     sb.append(" query=\"" +
---------------
-------------
@@ -209,7 +209,7 @@
 	 */
 	protected void addDatabase(Database d)
 	{
-		dbtable.put(d.dbName, d);
+dbtable.put(d.getDatabaseName(), d);
 	}
 
 	/**
---------------
-------------
@@ -669,7 +669,7 @@
     public void stringField(FieldInfo fieldInfo, String value) throws IOException {
       assert currentField >= 0;
       StringBuilder builder = builders[currentField];
-      if (builder.length() > 0) {
+if (builder.length() > 0 && builder.length() < maxLength) {
         builder.append(' '); // for the offset gap, TODO: make this configurable
       }
       if (builder.length() + value.length() > maxLength) {
---------------
-------------
@@ -65,7 +65,7 @@
   
   public synchronized void  execute() throws Exception {
     if (executed) {
-      throw new Exception("Benchmark was already executed");
+throw new IllegalStateException("Benchmark was already executed");
     }
     executed = true;
     algorithm.execute();
---------------
-------------
@@ -31,7 +31,7 @@
 import java.util.HashMap;
 import org.apache.thrift.TEnum;
 
-public enum IndexType implements TEnum {
+public enum IndexType implements org.apache.thrift.TEnum {
   KEYS(0);
 
   private final int value;
---------------
-------------
@@ -1361,7 +1361,7 @@
         s.executeUpdate("CREATE TABLE APP.NAMES(ID INT, NAME VARCHAR(30))");
 
         
-        s.executeUpdate("CREATE TRIGGER  APP.MYTRIG AFTER DELETE ON APP.TAB REFERENCING OLD_TABLE AS OLDROWS FOR EACH STATEMENT INSERT INTO APP.LOG(i,name,deltime) SELECT OLDROWS.I, NAMES.NAME, CURRENT_TIMESTAMP FROM --DERBY-PROPERTIES joinOrder=FIXED\n NAMES, OLDROWS --DERBY-PROEPERTIES joinStrategy = NESTEDLOOP\n WHERE (OLDROWS.i = NAMES.ID) AND (1 = 1)");
+s.executeUpdate("CREATE TRIGGER  APP.MYTRIG AFTER DELETE ON APP.TAB REFERENCING OLD_TABLE AS OLDROWS FOR EACH STATEMENT INSERT INTO APP.LOG(i,name,deltime) SELECT OLDROWS.I, NAMES.NAME, CURRENT_TIMESTAMP FROM --DERBY-PROPERTIES joinOrder=FIXED\n NAMES, OLDROWS --DERBY-PROPERTIES joinStrategy = NESTEDLOOP\n WHERE (OLDROWS.i = NAMES.ID) AND (1 = 1)");
         
         s.executeUpdate("insert into APP.tab values(1)");
         s.executeUpdate("insert into APP.tab values(2)");
---------------
-------------
@@ -27,7 +27,7 @@
 import java.security.AccessController;
 import java.security.PrivilegedExceptionAction;
 
-import org.apache.derby.shared.common.sanity.SanityManager;
+import org.apache.derby.iapi.services.sanity.SanityManager;
 
 /**
 	This class is used to determine which Java specification Derby will run at.
---------------
-------------
@@ -272,7 +272,7 @@
         case 0: queryShape = randomPoint(); break;
         case 1:case 2:case 3:
           if (!indexedAtLeastOneShapePair) { // avoids ShapePair.relate(ShapePair), which isn't reliable
-            queryShape = randomShapePairRect(biasContains);
+queryShape = randomShapePairRect(!biasContains);//invert biasContains for query side
             break;
           }
         default: queryShape = randomRectangle();
---------------
-------------
@@ -43,7 +43,7 @@
 {
   /** The author's email address */
   @Id
-  @Column(nullable = false, unique = true)
+@Column(nullable = false)
   private String email;
   
   /** The author's full name */
---------------
-------------
@@ -29,7 +29,7 @@
 import java.io.IOException;
 import java.io.UnsupportedEncodingException;
 
-import org.apache.derby.iapi.services.sanity.SanityManager;
+import org.apache.derby.shared.common.sanity.SanityManager;
 
 /**
  * Create an encoded stream from a <code>Reader</code>.
---------------
-------------
@@ -48,7 +48,7 @@
 
   @Override
   int getStreamCount() {
-    if (fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS)
+if (indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS)
       return 1;
     else
       return 2;
---------------
-------------
@@ -88,7 +88,7 @@
              "org.apache.hadoop.io.serializer.JavaSerialization,org.apache.hadoop.io.serializer.WritableSerialization");
     // Dont ever forget this. People should keep track of how hadoop conf parameters and make or break a piece of code
 
-    FileSystem dfs = FileSystem.get(conf);
+FileSystem dfs = FileSystem.get(outPath.toUri(), conf);
     if (dfs.exists(outPath))
       dfs.delete(outPath, true);
 
---------------
-------------
@@ -1382,7 +1382,7 @@
                 keys.add(sample);
         }
         FBUtilities.sortSampledKeys(keys, range);
-        int splits = keys.size() * SSTableReader.indexInterval() / keysPerSplit;
+int splits = keys.size() * DatabaseDescriptor.getIndexInterval() / keysPerSplit;
 
         if (keys.size() >= splits)
         {
---------------
-------------
@@ -76,7 +76,7 @@
     LOG.info("init: " + config);
     String name = super.init(config, core);
     threshold = config.get(THRESHOLD_TOKEN_FREQUENCY) == null ? 0.0f
-            : (Float) config.get(THRESHOLD_TOKEN_FREQUENCY);
+: Float.valueOf((String)config.get(THRESHOLD_TOKEN_FREQUENCY));
     sourceLocation = (String) config.get(LOCATION);
     field = (String)config.get(FIELD);
     lookupImpl = (String)config.get(LOOKUP_IMPL);
---------------
-------------
@@ -83,7 +83,7 @@
         suite.addTest(ScrollCursors2Test.suite());
         suite.addTest(NullIfTest.suite());
         suite.addTest(InListMultiProbeTest.suite());
-        suite.addTest(SecurityPolicyReloadingTest.suite());
+//suite.addTest(SecurityPolicyReloadingTest.suite());
         suite.addTest(CurrentOfTest.suite());
         suite.addTest(UnaryArithmeticParameterTest.suite());
         suite.addTest(HoldCursorTest.suite());
---------------
-------------
@@ -154,7 +154,7 @@
         BufferedReader bufReader = new BufferedReader(new InputStreamReader(
                 new FileInputStream(filepath)), 16 * 1024 * 1024);
         String line = null;
-        String delimiter_ = new String(",");
+String delimiter_ = ",";
         RowMutation rm = null;
         Map<String, RowMutation> rms = new HashMap<String, RowMutation>();
         if(importer_.columnFamily.delimiter != null)
---------------
-------------
@@ -97,7 +97,7 @@
     // omitNorms is true
     for (FieldInfo fi : reader.getFieldInfos()) {
       if (fi.isIndexed()) {
-        assertTrue(fi.omitsNorms() == (reader.normValues(fi.name) == null));
+assertTrue(fi.omitsNorms() == (reader.getNormValues(fi.name) == null));
       }
     }
     reader.close();
---------------
-------------
@@ -1,6 +1,6 @@
 /*
 
-   Derby - Class org.apache.derbyTesting.unitTests.junit.FormatableBitSet
+Derby - Class org.apache.derbyTesting.unitTests.junit.FormatableBitSetTest
 
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -64,7 +64,7 @@
 
   public void testStopList() throws IOException {
     CharArraySet stopWordsSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("good", "test", "analyzer"), false);
-    StopAnalyzer newStop = new StopAnalyzer(Version.LUCENE_40, stopWordsSet);
+StopAnalyzer newStop = new StopAnalyzer(TEST_VERSION_CURRENT, stopWordsSet);
     TokenStream stream = newStop.tokenStream("test", "This is a good test of the english stop analyzer");
     try {
       assertNotNull(stream);
---------------
-------------
@@ -202,7 +202,7 @@
 
         protected IColumn getReduced()
         {
-            ColumnFamily purged = PrecompactedRow.removeDeletedAndOldShards(shouldPurge, controller, container);
+ColumnFamily purged = PrecompactedRow.removeDeletedAndOldShards(key, shouldPurge, controller, container);
             if (purged == null || !purged.iterator().hasNext())
             {
                 container.clear();
---------------
-------------
@@ -120,7 +120,7 @@
   
   /** Removes all entries from the PriorityQueue. */
   public final void clear() {
-    for (int i = 0; i < size; i++)
+for (int i = 0; i <= size; i++)
       heap[i] = null;
     size = 0;
   }
---------------
-------------
@@ -24,7 +24,7 @@
 import java.io.Externalizable;
 
 /**
-  Cloudscape interface for creating a stored form for
+Derby interface for creating a stored form for
   an object and re-constructing an equivalent object
   from this stored form. The object which creates the
   stored form and the re-constructed object need not be
---------------
-------------
@@ -84,7 +84,7 @@
   public void setField(String name, Object value) 
   {
     if( value instanceof Object[] ) {
-      value = Arrays.asList( (Object[])value );
+value = new ArrayList(Arrays.asList( (Object[])value ));
     }
     else if( value instanceof Collection ) {
       // nothing
---------------
-------------
@@ -286,7 +286,7 @@
     return Collections.unmodifiableSet(_deploymentImportPackage);
   }
 
-  public Collection<Filter> getDeployedServiceImport() throws InvalidAttributeException
+public Collection<Filter> getDeployedServiceImport()
   {
     return Collections.unmodifiableCollection(_deployedImportService);
   }
---------------
-------------
@@ -223,7 +223,7 @@
 
         // Check that the second insert did went in
         ColumnFamily cf = cfs.getColumnFamily(QueryFilter.getIdentityFilter(key, new QueryPath(cfName)));
-        assert cf.getColumnCount() == 10;
+assertEquals(10, cf.getColumnCount());
         for (IColumn c : cf)
             assert !c.isMarkedForDelete();
     }
---------------
-------------
@@ -264,7 +264,7 @@
 	*/
 	String DEADLOCK = "40001";
 	String LOCK_TIMEOUT = "40XL1";
-    String LOCK_TIMEOUT_LOG = "40XL2";
+String LOCK_TIMEOUT_LOG = "40XL1.T.1";
 
 	/*
 	** Store - access.protocol.Interface statement exceptions
---------------
-------------
@@ -221,7 +221,7 @@
         SegmentReader segmentReader = (SegmentReader) reader;
         for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {
           FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);
-          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name));
+fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);
         }
       } else {
         addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);
---------------
-------------
@@ -306,7 +306,7 @@
   }
 
   private static float explainToleranceDelta(float f1, float f2) {
-    return Math.max(f1, f2) * EXPLAIN_SCORE_TOLERANCE_DELTA;
+return Math.max(Math.abs(f1), Math.abs(f2)) * EXPLAIN_SCORE_TOLERANCE_DELTA;
   }
 
   /** 
---------------
-------------
@@ -49,7 +49,7 @@
             Row.serializer().serialize(row, dob);
         }
         byte[] data = Arrays.copyOf(dob.getData(), dob.getLength());
-        return originalMessage.getReply(FBUtilities.getLocalAddress(), data);
+return originalMessage.getReply(FBUtilities.getLocalAddress(), data, originalMessage.getVersion());
     }
 
     @Override
---------------
-------------
@@ -557,7 +557,7 @@
         if (options.contains(Option.REUSE_ENUMS) && random().nextInt(10) < 9) {
           prevDocsEnum = threadState.reuseDocsEnum;
         }
-        threadState.reuseDocsEnum = termsEnum.docs(liveDocs, prevDocsEnum, doCheckFreqs ? DocsEnum.FLAG_FREQS : 0);
+threadState.reuseDocsEnum = termsEnum.docs(liveDocs, prevDocsEnum, doCheckFreqs);
         docsEnum = threadState.reuseDocsEnum;
         docsAndPositionsEnum = null;
       }
---------------
-------------
@@ -31,7 +31,7 @@
 	
 	private static final String ALREADY_CLOSED_ERR_MESSAGE = 
             SqlException.getMessageUtil().getTextMessage(
-                MessageId.CONN_ALREADY_CLOSED);
+MessageId.OBJECT_CLOSED);
 	
 	private boolean closed;
 	
---------------
-------------
@@ -82,7 +82,7 @@
     }
     
     public static Test suite () {
-        return TestConfiguration.embeddedSuite (
+return TestConfiguration.defaultSuite (
                 BlobUpdateableStreamTest.class);
     }
     
---------------
-------------
@@ -39,7 +39,7 @@
     if (!store && !index)
       throw new IllegalArgumentException("field must be indexed or stored");
 
-    FieldType fieldType = new FieldType(DoubleField.TYPE);
+FieldType fieldType = new FieldType(DoubleField.TYPE_NOT_STORED);
     fieldType.setStored(store);
     fieldType.setIndexed(index);
     fieldType.setNumericPrecisionStep(precisionStep);
---------------
-------------
@@ -166,7 +166,7 @@
    * modify it.
    */
 
-  public Set<String> files() throws IOException {
+public Set<String> files() {
     if (setFiles == null) {
       throw new IllegalStateException("files were not computed yet");
     }
---------------
-------------
@@ -180,7 +180,7 @@
 
   private ZkCmdExecutor cmdExecutor;
 
-  private Aliases aliases = new Aliases();
+private volatile Aliases aliases = new Aliases();
 
   private volatile boolean closed = false;
 
---------------
-------------
@@ -41,5 +41,5 @@
      * @return object representing the owner of the compatibility space, or
      * <code>null</code> if no owner has been specified.
      */
-    Object getOwner();
+LockOwner getOwner();
 }
---------------
-------------
@@ -81,7 +81,7 @@
       BytesRef b = te.next();
       assertNotNull(b);
       assertEquals(t, b.utf8ToString());
-      DocsEnum td = _TestUtil.docs(random(), te, liveDocs, null, 0);
+DocsEnum td = _TestUtil.docs(random(), te, liveDocs, null, false);
       assertTrue(td.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
       assertEquals(0, td.docID());
       assertEquals(td.nextDoc(), DocIdSetIterator.NO_MORE_DOCS);
---------------
-------------
@@ -80,7 +80,7 @@
     writer.addDocument(doc);
     reader = writer.getReader();
     searcher = newSearcher(reader);
-    writer.close();
+writer.shutdown();
   }
 
   @Override
---------------
-------------
@@ -136,7 +136,7 @@
     }
   }
 
-  private DataModel buildModel() throws IOException {
+protected DataModel buildModel() throws IOException {
     FileLineIterator iterator = new FileLineIterator(dataFile, false);
     String firstLine = iterator.peek();
     while (firstLine.length() == 0 || firstLine.charAt(0) == COMMENT_CHAR) {
---------------
-------------
@@ -32,7 +32,7 @@
 	private static final String DEFAULT_FILENAME = "protocol.tests";
 	
 	// constructor
-	private testProtocol() {}
+public testProtocol() {}
 	
 
 	/**
---------------
-------------
@@ -1144,7 +1144,7 @@
     {
 
         // write the format id of this conglomerate
-        FormatIdUtil.writeFormatIdInteger(out, this.getTypeFormatId());
+FormatIdUtil.writeFormatIdInteger(out, conglom_format_id);
 
 		out.writeInt((int) id.getSegmentId());
         out.writeLong(id.getContainerId());
---------------
-------------
@@ -58,7 +58,7 @@
 
     public Writer(Directory dir, String id, Comparator<BytesRef> comp,
         Counter bytesUsed, IOContext context, boolean fasterButMoreRam) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context, fasterButMoreRam);
+super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context, fasterButMoreRam, Type.BYTES_FIXED_SORTED);
       this.comp = comp;
     }
 
---------------
-------------
@@ -568,7 +568,7 @@
   }
 
   @Override
-  public final int freq() throws IOException {
+public final int freq() {
     return _freq;
   }
 
---------------
-------------
@@ -343,7 +343,7 @@
 
         ColumnFamilyStore cfs = table.getColumnFamilyStore("Indexed2");
         ColumnDefinition old = cfs.metadata.getColumn_metadata().get(ByteBufferUtil.bytes("birthdate"));
-        ColumnDefinition cd = new ColumnDefinition(old.name, old.validator, IndexType.KEYS, "birthdate_index");
+ColumnDefinition cd = new ColumnDefinition(old.name, old.getValidator(), IndexType.KEYS, "birthdate_index");
         Future<?> future = cfs.addIndex(cd);
         future.get();
         // we had a bug (CASSANDRA-2244) where index would get created but not flushed -- check for that
---------------
-------------
@@ -42,7 +42,7 @@
 		more beyond the limit will result in an EOFException
 
 		@exception IOException IOException from some underlying stream
-		@exception EOFException The set limit would exceed
+@exception java.io.EOFException The set limit would exceed
 		the available data in the stream.
 	*/
 	public void setLimit(int length)
---------------
-------------
@@ -92,7 +92,7 @@
       assert pos < nextPos;
 
       // Cannot read from already freed past:
-      assert nextPos - pos <= count;
+assert nextPos - pos <= count: "nextPos=" + nextPos + " pos=" + pos + " count=" + count;
 
       final int index = getIndex(pos);
       return buffer[index];
---------------
-------------
@@ -279,7 +279,7 @@
       assertTrue(ts.incrementToken());
       termAtt.fillBytesRef();
       // ensure we make a copy of the actual bytes too
-      map.put(term, new BytesRef(bytes));
+map.put(term, BytesRef.deepCopyOf(bytes));
     }
     
     Thread threads[] = new Thread[numThreads];
---------------
-------------
@@ -53,6 +53,6 @@
         ColumnFamily retrieved = store.getColumnFamily(QueryFilter.getIdentityFilter(dk, new QueryPath("Standard1", null, "Column1".getBytes())));
         assert retrieved.isMarkedForDelete();
         assertNull(retrieved.getColumn("Column1".getBytes()));
-        assertNull(ColumnFamilyStore.removeDeleted(retrieved, Integer.MAX_VALUE));
+assertNull(Util.cloneAndRemoveDeleted(retrieved, Integer.MAX_VALUE));
     }
 }
---------------
-------------
@@ -312,7 +312,7 @@
                     (
                      cf,
                      aggInfo.getAggregateName(),
-                     aggInfo.getResultDescription().getColumnInfo()[ 0 ].getType()
+aggInfo.getResultDescription().getColumnInfo( 0 ).getType()
                      );
 
 			} catch (Exception e)
---------------
-------------
@@ -309,7 +309,7 @@
     type.setStoreTermVectors(true);
     type.freeze();
     String[] terms = { "org", "apache", "lucene"};
-    int iters = atLeast(1000);
+int iters = 1000; // don't let it go too big, or jenkins will stack overflow: atLeast(1000);
     StringBuilder builder = new StringBuilder();
     for (int i = 0; i < iters; i++) {
       builder.append(terms[random().nextInt(terms.length)]).append(" ");
---------------
-------------
@@ -53,7 +53,7 @@
   private SolrCore solrCore;
   private SimpleFSLockFactory lockFactory;
   
-  public SnapShooter(SolrCore core, String location) throws IOException {
+public SnapShooter(SolrCore core, String location) {
     solrCore = core;
     if (location == null) snapDir = core.getDataDir();
     else  {
---------------
-------------
@@ -191,7 +191,7 @@
       }
 
       @Override
-      public boolean incrementToken() throws IOException {
+public boolean incrementToken() {
         if (currentToken >= tokens.length) {
           return false;
         }
---------------
-------------
@@ -246,7 +246,7 @@
 {
   private final HttpServletRequest req;
   
-  public HttpRequestContentStream( HttpServletRequest req ) throws IOException {
+public HttpRequestContentStream( HttpServletRequest req ) {
     this.req = req;
     
     contentType = req.getContentType();
---------------
-------------
@@ -584,7 +584,7 @@
       fields.put(field, perDocs.docValues(field));
     }
 
-    @Override
+//@Override -- not until Java 1.6
     public void close() throws IOException {
       // nothing to do here
     }
---------------
-------------
@@ -268,7 +268,7 @@
                     {
                         // assuming version here. We've gone to lengths to make sure what gets written to the CL is in
                         // the current version.  so do make sure the CL is drained prior to upgrading a node.
-                        rm = RowMutation.serializer().deserialize(new DataInputStream(bufIn), MessagingService.version_);
+rm = RowMutation.serializer().deserialize(new DataInputStream(bufIn), MessagingService.version_, false);
                     }
                     catch (UnserializableColumnFamilyException ex)
                     {
---------------
-------------
@@ -28,7 +28,7 @@
 import java.util.Set;
 import java.util.TreeMap;
 
-public class DummyOutputCollector<K extends WritableComparable<?>, V extends Writable>
+public class DummyOutputCollector<K extends WritableComparable, V extends Writable>
     implements OutputCollector<K, V> {
 
   private final Map<String, List<V>> data = new TreeMap<String, List<V>>();
---------------
-------------
@@ -104,7 +104,7 @@
     {
         try
         {
-            output.write(bb.array(), 0, bb.limit());
+output.write(bb.array(), bb.position()+bb.arrayOffset(), bb.limit()+bb.arrayOffset());
             if (queue.peek() == null)
             {
                 output.flush();
---------------
-------------
@@ -273,7 +273,7 @@
 
 	protected void pushDataDictionaryContext(ContextManager cm) {
 		// we make sure there is a data dictionary context in place.
-		dd.pushDataDictionaryContext(cm, false);
+dd.pushDataDictionaryContext(cm);
 	}
 
 	/*
---------------
-------------
@@ -1534,7 +1534,7 @@
         // bulk insert with replace to empty table/one index from an empty file 
         // import empty_file.dat into EMPTY_TABLE 
         doImportTable(
-                "APP", "EMPTY_TABLE", emptyFileName, "|", "``", null, 1);
+"APP", "EMPTY_TABLE", emptyFileName, "|", "`", null, 1);
 
         commit();
 
---------------
-------------
@@ -182,7 +182,7 @@
           boolean continueRegen = regenerator.regenerateItem(searcher,
               this, old, itemsArr[i].getKey(), itemsArr[i].getValue());
           if (!continueRegen) break;
-        } catch (Throwable e) {
+} catch (Exception e) {
           SolrException.log(log, "Error during auto-warming of key:" + itemsArr[i].getKey(), e);
         }
       }
---------------
-------------
@@ -71,7 +71,7 @@
    * @param extension -- extension of the filename (including .)
    * @param gen -- generation
    */
-  public static final String fileNameFromGeneration(String base, String extension, long gen) {
+static final String fileNameFromGeneration(String base, String extension, long gen) {
     if (gen == -1) {
       return null;
     } else if (gen == 0) {
---------------
-------------
@@ -313,7 +313,7 @@
         AbstractType columnComparator = (rowPartitioner instanceof OrderPreservingPartitioner || rowPartitioner instanceof ByteOrderedPartitioner)
                                         ? BytesType.instance
                                         : new LocalByPartionerType(StorageService.getPartitioner());
-        final CFMetaData indexedCfMetadata = CFMetaData.newIndexMetadata(table.name, columnFamily, info, columnComparator);
+final CFMetaData indexedCfMetadata = CFMetaData.newIndexMetadata(metadata, info, columnComparator);
         ColumnFamilyStore indexedCfs = ColumnFamilyStore.createColumnFamilyStore(table,
                                                                                  indexedCfMetadata.cfName,
                                                                                  new LocalPartitioner(metadata.getColumn_metadata().get(info.name).validator),
---------------
-------------
@@ -2743,7 +2743,7 @@
     // Tests that if FSDir is opened w/ a NoLockFactory (or SingleInstanceLF),
     // then IndexWriter ctor succeeds. Previously (LUCENE-2386) it failed 
     // when listAll() was called in IndexFileDeleter.
-    Directory dir = newFSDirectory(new File(TEMP_DIR, "emptyFSDirNoLock"), NoLockFactory.getNoLockFactory());
+Directory dir = newFSDirectory(_TestUtil.getTempDir("emptyFSDirNoLock"), NoLockFactory.getNoLockFactory());
     new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT))).close();
     dir.close();
   }
---------------
-------------
@@ -327,7 +327,7 @@
         }
         TestCase.assertTrue(
             q+": multi valued explanation description=\""+descr
-            +"\" must be 'max of plus x times others' or end with 'prodoct of'"
++"\" must be 'max of plus x times others' or end with 'product of'"
             +" or 'sum of:' or 'max of:' - "+expl,
             productOf || sumOf || maxOf || maxTimesOthers);
         float sum = 0;
---------------
-------------
@@ -703,7 +703,7 @@
 			}
 			// For normal selects we are done, but procedures might
 			// have more resultSets
-		}while (isCallable && getMoreResults(JDBC30Translation.KEEP_CURRENT_RESULT));
+}while (isCallable && getMoreResults(Statement.KEEP_CURRENT_RESULT));
 
 		return hasResultSet;
 
---------------
-------------
@@ -70,7 +70,7 @@
         requestedDefault = null;
     }
     
-    private void setDefault(final TimeZone tz) throws SecurityException{
+public static void setDefault(final TimeZone tz) {
         if (tz== null) {
             throw new IllegalArgumentException("tz cannot be <null>");
         }
---------------
-------------
@@ -162,7 +162,7 @@
           // getNumPreferenceForItemSQL
           "SELECT COUNT(1) FROM " + preferenceTable + " WHERE " + itemIDColumn + "=?",
           // getNumPreferenceForItemsSQL
-          "SELECT COUNT(1) FROM " + preferenceTable + " tp1 INNER JOIN " + preferenceColumn + " tp2 " +
+"SELECT COUNT(1) FROM " + preferenceTable + " tp1 INNER JOIN " + preferenceTable + " tp2 " +
           "ON (tp1." + userIDColumn + "=tp2." + userIDColumn + ") " +
           "WHERE tp1." + itemIDColumn + "=? and tp2." + itemIDColumn + "=?");
   }
---------------
-------------
@@ -33,7 +33,7 @@
  * a statement trigger.  It is instantiated at execution
  * time.  There is one per statement trigger.
  */
-class StatementTriggerExecutor extends GenericTriggerExecutor
+public class StatementTriggerExecutor extends GenericTriggerExecutor
 {
 	/**
 	 * Constructor
---------------
-------------
@@ -76,7 +76,7 @@
   private final static int TERM_DOC_FREQ_RAND = 20;
 
   @BeforeClass
-  public static void beforeClass() throws Exception {
+public static void beforeClass() {
     NUM_TEST_ITER = atLeast(20);
   }
 
---------------
-------------
@@ -48,7 +48,7 @@
     {
         TestSuite suite = new TestSuite("errorcode Test");
         
-        suite.addTest(TestConfiguration.embeddedSuite(ErrorCodeTest.class));
+suite.addTest(TestConfiguration.defaultSuite(ErrorCodeTest.class));
         
         return new LocaleTestSetup(suite, Locale.ENGLISH);
     }
---------------
-------------
@@ -1453,7 +1453,7 @@
                 }
                 catch (Throwable th)
                 {
-                    logger_.error(th);
+logger_.error("error closing " + ci, th);
                 }
             }
 
---------------
-------------
@@ -359,7 +359,7 @@
                   + CheckHits.topdocsString(top1,0,0)
                   + CheckHits.topdocsString(top2,0,0)
                   + "for query:" + q2.toString(),
-                  score, otherScore, 1.0e-6f);
+score, otherScore, CheckHits.explainToleranceDelta(score, otherScore));
             }
           }
 
---------------
-------------
@@ -202,7 +202,7 @@
 
           for (int a=1; a<aliases.size(); a++) {
             core.open();
-            register(aliases.get(i), core, false);
+register(aliases.get(a), core, false);
           }
 
           register(name, core, false);
---------------
-------------
@@ -957,7 +957,7 @@
                     SQLState.BLOB_NONPOSITIVE_LENGTH,
                     new Long(length));
         }
-        if (length > (this.length() - pos)) {
+if (length > (this.length() - (pos -1))) {
             throw Util.generateCsSQLException(
                     SQLState.POS_AND_LENGTH_GREATER_THAN_LOB,
                     new Long(pos), new Long(length));
---------------
-------------
@@ -100,7 +100,7 @@
       docsReader = wrappedPostingsBaseFormat.postingsReaderBase(state);
       pulsingReader = new PulsingPostingsReader(docsReader);
       FieldsProducer ret = new BlockTreeTermsReader(
-                                                    state.dir, state.fieldInfos, state.segmentInfo,
+state.directory, state.fieldInfos, state.segmentInfo,
                                                     pulsingReader,
                                                     state.context,
                                                     state.segmentSuffix,
---------------
-------------
@@ -95,7 +95,7 @@
         if (zkStateReader == null) {
           try {
             ZkStateReader zk = new ZkStateReader(zkHost, zkConnectTimeout,
-                zkClientTimeout, true);
+zkClientTimeout);
             zk.createClusterStateWatchersAndUpdate();
             zkStateReader = zk;
           } catch (InterruptedException e) {
---------------
-------------
@@ -55,7 +55,7 @@
         }
         while (true)
         {
-            Future<Integer> ft = MinorCompactionManager.instance().submit(store);
+Future<Integer> ft = CompactionManager.instance().submit(store);
             if (ft.get() == 0)
                 break;
         }
---------------
-------------
@@ -92,7 +92,7 @@
 
         // load the cache from disk.  unregister the old mbean so we can recreate a new CFS object.
         // but don't invalidate() the old CFS, which would nuke the data we want to try to load
-        store.invalidate(); // unregistering old MBean to test how key cache will be loaded
+store.unregisterMBean();
         ColumnFamilyStore newStore = ColumnFamilyStore.createColumnFamilyStore(Table.open(TABLE1), COLUMN_FAMILY3);
         assertEquals(100, newStore.getKeyCacheSize());
 
---------------
-------------
@@ -368,7 +368,7 @@
       thread.join();
     }
 
-    writer.close();
+writer.shutdown();
     reader.close();
     dir.close();
   }
---------------
-------------
@@ -59,7 +59,7 @@
     sfq = new SpanNotQuery(include, sfq);
     assertEquals(1, searcher.search(sfq, 10).totalHits);
     
-    writer.close();
+writer.shutdown();
     reader.close();
     dir.close();
   }
---------------
-------------
@@ -850,7 +850,7 @@
     }
     c.launchThreads(-1);
 
-    Thread.sleep(500);
+Thread.sleep(_TestUtil.nextInt(random, 10, 500));
 
     // Close w/o first stopping/joining the threads
     if (VERBOSE) {
---------------
-------------
@@ -823,7 +823,7 @@
 	{
         bindDateTimeArg( rightOperand, 2);
         bindDateTimeArg( receiver, 3);
-        setType(DataTypeDescriptor.getBuiltInDataTypeDescriptor( Types.INTEGER));
+setType(DataTypeDescriptor.getBuiltInDataTypeDescriptor( Types.BIGINT));
         return this;
     } // End of timestampDiffBind
 
---------------
-------------
@@ -29,7 +29,7 @@
 
     public String  hostName;       // cassandra server name
     public int     thriftPort;     // cassandra server's thrift port
-    public boolean framed = false; // cassandra server's framed transport 
+public boolean framed = true; // cassandra server's framed transport
     public boolean debug = false;  // print stack traces when errors occur in the CLI
     public String  username;       // cassandra login name (if SimpleAuthenticator is used)
     public String  password;       // cassandra login password (if SimpleAuthenticator is used)
---------------
-------------
@@ -68,6 +68,6 @@
 
   @Override
   public String toString() {
-    return name;
+return "PostingsFormat(name=" + name + ")";
   }
 }
---------------
-------------
@@ -701,7 +701,7 @@
         try
         {
             oldCfm.apply(cf_def);
-            UpdateColumnFamily update = new UpdateColumnFamily(CFMetaData.convertToThrift(cf_def));
+UpdateColumnFamily update = new UpdateColumnFamily(cf_def);
             applyMigrationOnStage(update);
             return DatabaseDescriptor.getDefsVersion().toString();
         }
---------------
-------------
@@ -200,7 +200,7 @@
         Table systemTable = Table.open(Table.SYSTEM_TABLE);
         for (String tableName : DatabaseDescriptor.getTables())
         {
-            ColumnFamily hintedColumnFamily = systemTable.get(tableName, HINTS_CF);
+ColumnFamily hintedColumnFamily = ColumnFamilyStore.removeDeleted(systemTable.get(tableName, HINTS_CF), Integer.MAX_VALUE);
             if (hintedColumnFamily == null)
             {
                 continue;
---------------
-------------
@@ -70,7 +70,7 @@
   private Directory getIndex()
   throws IOException {
           RAMDirectory indexStore = new RAMDirectory ();
-          IndexWriter writer = new IndexWriter (indexStore, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+IndexWriter writer = new IndexWriter (indexStore, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
           RandomGen random = new RandomGen(newRandom());
           for (int i=0; i<INDEX_SIZE; ++i) { // don't decrease; if to low the problem doesn't show up
           Document doc = new Document();
---------------
-------------
@@ -187,7 +187,7 @@
   private void assertOrdinalsExist(String field, IndexReader ir) throws IOException {
     for (AtomicReaderContext context : ir.leaves()) {
       AtomicReader r = context.reader();
-      if (r.docValues(field) != null) {
+if (r.getBinaryDocValues(field) != null) {
         return; // not all segments must have this DocValues
       }
     }
---------------
-------------
@@ -116,7 +116,7 @@
             ExecutorService executor = agentContext.getRegistrationExecutor();
             executor.submit(new Runnable() {
                 public void run() {
-                    agentContext.unregisterMBean(getName());
+agentContext.unregisterMBean(AbstractCompendiumHandler.this);
                 }
             });
             trackedId = null;
---------------
-------------
@@ -83,7 +83,7 @@
     public void testSmallTokenInStream() throws Exception {
       input = new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader("abc de fgh"));
       NGramTokenFilter filter = new NGramTokenFilter(input, 3, 3);
-      assertTokenStreamContents(filter, new String[]{"abc","fgh"}, new int[]{0,0}, new int[]{3,3});
+assertTokenStreamContents(filter, new String[]{"abc","fgh"}, new int[]{0,7}, new int[]{3,10});
     }
     
     public void testReset() throws Exception {
---------------
-------------
@@ -32,7 +32,7 @@
     	ByteArrayOutputStream bos = new ByteArrayOutputStream();
         DataOutputStream dos = new DataOutputStream( bos );
         TouchMessage.serializer().serialize(touchMessage, dos);
-        Message message = new Message(StorageService.getLocalStorageEndPoint(), StorageService.readStage_, StorageService.touchVerbHandler_, new Object[]{bos.toByteArray()});         
+Message message = new Message(StorageService.getLocalStorageEndPoint(), StorageService.readStage_, StorageService.touchVerbHandler_, bos.toByteArray());
         return message;
     }
     
---------------
-------------
@@ -157,7 +157,7 @@
 
     return (IndexReader) new SegmentInfos.FindSegmentsFile(directory) {
 
-      public Object doBody(String segmentFileName) throws CorruptIndexException, IOException {
+protected Object doBody(String segmentFileName) throws CorruptIndexException, IOException {
 
         SegmentInfos infos = new SegmentInfos();
         infos.read(directory, segmentFileName);
---------------
-------------
@@ -28,7 +28,7 @@
 
   @Override
   public void testSize() {
-    assertEquals("size", 7, getTestVector().getNumNondefaultElements());
+assertEquals("size", 3, getTestVector().getNumNondefaultElements());
   }
 
   @Override
---------------
-------------
@@ -217,7 +217,7 @@
                 callCount++;
                 assertEquals(Stage.MISC, msg.getMessageType());
                 // simulate a response from remote server
-                Message response = msg.getReply(FBUtilities.getLocalAddress(), new byte[]{ });
+Message response = msg.getReply(FBUtilities.getLocalAddress(), new byte[]{ }, msg.getVersion());
                 MessagingService.instance().sendOneWay(response, FBUtilities.getLocalAddress());
                 return null;
             }
---------------
-------------
@@ -134,7 +134,7 @@
             RowMutation rowMutation = new RowMutation(table, key);
             rowMutation.add(diffCf);
             RowMutationMessage rowMutationMessage = new RowMutationMessage(rowMutation);
-            ReadRepairManager.instance().schedule(endPoints.get(i), rowMutationMessage);
+ReadRepairManager.instance.schedule(endPoints.get(i), rowMutationMessage);
         }
     }
 
---------------
-------------
@@ -1082,7 +1082,7 @@
         assert tableName != null;
         CFMetaData cfmd = getCFMetaData(tableName, cfName);
         if (cfmd == null)
-            throw new NullPointerException("Unknown ColumnFamily " + cfName + " in keyspace " + tableName);
+throw new IllegalArgumentException("Unknown ColumnFamily " + cfName + " in keyspace " + tableName);
         return cfmd.comparator;
     }
 
---------------
-------------
@@ -87,7 +87,7 @@
 
     // Delete the input if it already exists
     if (fs.exists(inpath)) {
-      FileUtil.fullyDelete(fs, inpath);
+fs.delete(inpath, true);
     }
 
     fs.mkdirs(inpath);
---------------
-------------
@@ -170,7 +170,7 @@
     
   public void testPhrasePrefixWithBooleanQuery() throws IOException {
     RAMDirectory indexStore = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(indexStore, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT, Collections.emptySet()), true, IndexWriter.MaxFieldLength.LIMITED);
+IndexWriter writer = new IndexWriter(indexStore, new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet()), true, IndexWriter.MaxFieldLength.LIMITED);
     add("This is a test", "object", writer);
     add("a note", "note", writer);
     writer.close();
---------------
-------------
@@ -178,7 +178,7 @@
         
         // service: ref=foo, no componentId set. So using it to test getComponentIdsByType.
         String[] serviceComponentIds = metadataProxy.getComponentIdsByType(sampleBlueprintContainerServiceId, BlueprintMetadataMBean.SERVICE_METADATA);
-        assertEquals("There should be only one service component in this sample", 1, serviceComponentIds.length);
+assertEquals("There should be two service components in this sample", 2, serviceComponentIds.length);
         
         MapEntryValidator mev = new MapEntryValidator();
         mev.setKeyValueValidator(new ValueValidator("key"), new ValueValidator("value"));
---------------
-------------
@@ -302,7 +302,7 @@
            + raf.toString());
       
       int nrBuffers = (int) (length / maxBufSize);
-      if (((long) nrBuffers * maxBufSize) < length) nrBuffers++;
+if (((long) nrBuffers * maxBufSize) <= length) nrBuffers++;
       
       this.buffers = new ByteBuffer[nrBuffers];
       this.bufSizes = new int[nrBuffers];
---------------
-------------
@@ -35,7 +35,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    configuration = new Configuration();
+configuration = getConfiguration();
     output = getTestTempDirPath();
   }
 
---------------
-------------
@@ -103,7 +103,7 @@
   
   @Test
   public void testBasic() throws Exception {
-    Replicator replicator = new HttpReplicator("localhost", port, ReplicationService.REPLICATION_CONTEXT + "/s1", 
+Replicator replicator = new HttpReplicator("127.0.0.1", port, ReplicationService.REPLICATION_CONTEXT + "/s1",
         getClientConnectionManager());
     ReplicationClient client = new ReplicationClient(replicator, new IndexReplicationHandler(handlerIndexDir, null), 
         new PerSessionDirectoryFactory(clientWorkDir));
---------------
-------------
@@ -48,5 +48,5 @@
 
   public abstract PostingsWriterBase postingsWriterBase(SegmentWriteState state) throws IOException;
 
-  public abstract void files(Directory dir, SegmentInfo segmentInfo, int codecID, Set<String> files) throws IOException;
+public abstract void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException;
 }
---------------
-------------
@@ -63,7 +63,7 @@
             return null;
         }
 
-        ColumnFamily filteredColumnFamily = new ColumnFamily(cfName, cf.type());
+ColumnFamily filteredColumnFamily = cf.cloneMeShallow();
 
         Collection<IColumn> columns = cf.getAllColumns();
         for (IColumn c : columns)
---------------
-------------
@@ -1074,7 +1074,7 @@
 
     final float getFloatFromDouble(double source) throws SqlException {
         if (Configuration.rangeCheckCrossConverters &&
-                (source > Float.MAX_VALUE || source < -Float.MAX_VALUE)) {
+Float.isInfinite((float)source)) {
             throw new LossOfPrecisionConversionException(agent_.logWriter_, String.valueOf(source));
         }
 
---------------
-------------
@@ -108,7 +108,7 @@
       };
       ScoreDoc[] hits = null;
 
-      QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, "contents", analyzer);
+QueryParser parser = new QueryParser(TEST_VERSION_CURRENT, "contents", analyzer);
       parser.setPhraseSlop(4);
       for (int j = 0; j < queries.length; j++) {
         Query query = parser.parse(queries[j]);
---------------
-------------
@@ -117,7 +117,7 @@
 		@param in			optional data
 
 		@exception IOException Can be thrown by any of the methods of ObjectInput.
-		@exception StandardException Standard Cloudscape policy.
+@exception StandardException Standard Derby error policy.
 
 		@see ContainerOperation#generateUndo
 	 */
---------------
-------------
@@ -83,7 +83,7 @@
         for ( int i = 0; i < size; ++i )
         {
             DataOutputBuffer buffer = buffers.get(i);
-            String file = args[1] + System.getProperty("file.separator") + "Bloom-Filter-" + i + ".dat";
+String file = args[1] + File.separator + "Bloom-Filter-" + i + ".dat";
             RandomAccessFile raf = new RandomAccessFile(file, "rw");
             raf.write(buffer.getData(), 0, buffer.getLength());
             raf.close();
---------------
-------------
@@ -297,7 +297,7 @@
       iw.addDocument(doc);
     }
     iw.forceMerge(1);
-    iw.close();
+iw.shutdown();
     dir.close();
     indexDir.mkdirs();
     spellchecker.add(AbstractLuceneSpellChecker.INDEX_DIR, indexDir.getAbsolutePath());
---------------
-------------
@@ -114,7 +114,7 @@
     public void reloadConfiguration() throws ConfigurationException
     {
         hostProperties = resourceToProperties(RACK_PROPERTY_FILENAME);
-        invalidateCachedSnitchValues();
+clearEndpointCache();
     }
 
     public static Properties resourceToProperties(String filename) throws ConfigurationException
---------------
-------------
@@ -1258,7 +1258,7 @@
     /***  REMOVED -YCS
     if (defaultFieldType != null) return new SchemaField(fieldName,defaultFieldType);
     ***/
-    throw new SolrException( SolrException.ErrorCode.BAD_REQUEST,"undefined field "+fieldName);
+throw new SolrException( SolrException.ErrorCode.BAD_REQUEST,"undefined field: \""+fieldName+"\"");
   }
 
   /**
---------------
-------------
@@ -83,7 +83,7 @@
       pending.cancel(true);
       pending = null;
     }
-    scheduler.shutdown();
+scheduler.shutdownNow();
   }
   
   /** schedule individual commits */
---------------
-------------
@@ -1316,7 +1316,7 @@
         retry  = true;
       }
       cnt++;
-      if (cnt > 10) break;
+if (cnt > 20) break;
       Thread.sleep(2000);
     } while (retry);
   }
---------------
-------------
@@ -88,7 +88,7 @@
     doc.add(new FacetField("Publish Date", "1999", "5", "5"));
     indexWriter.addDocument(config.build(taxoWriter, doc));
     
-    indexWriter.close();
+indexWriter.shutdown();
     taxoWriter.close();
   }
 
---------------
-------------
@@ -198,7 +198,7 @@
 
       if (options.extendedResults == true && reader != null && field != null) {
         term = term.createTerm(tokenText);
-        result.add(token, reader.docFreq(term));
+result.addFrequency(token, reader.docFreq(term));
         int countLimit = Math.min(options.count, suggestions.length);
         if(countLimit>0)
         {
---------------
-------------
@@ -66,7 +66,7 @@
             throw new AssertionError(ex);
         }
 
-        return resolver.isDataPresent() ? resolver.resolve() : null;
+return resolver.getMessageCount() > 0 ? resolver.resolve() : null;
     }
 
     public void response(Message message)
---------------
-------------
@@ -709,7 +709,7 @@
 		out.flush();
 		
 	    }catch(IOException e){
-		agent.markCommunicationsFailure ("DDMWriter.writeScalarStream()",
+agent.markCommunicationsFailure (e,"DDMWriter.writeScalarStream()",
 						 "",
 						 e.getMessage(),
 						 "*");
---------------
-------------
@@ -26,7 +26,7 @@
  * @lucene.internal
  */
 public final class CharsRef implements Comparable<CharsRef>, CharSequence, Cloneable {
-  private static final char[] EMPTY_CHARS = new char[0];
+public static final char[] EMPTY_CHARS = new char[0];
   public char[] chars;
   public int offset;
   public int length;
---------------
-------------
@@ -540,7 +540,7 @@
           Object output = run(fst, term, null);
 
           assertNotNull("term " + inputToString(inputMode, term) + " is not accepted", output);
-          assertEquals(output, pair.output);
+assertEquals(pair.output, output);
 
           // verify enum's next
           IntsRefFSTEnum.InputOutput<T> t = fstEnum.next();
---------------
-------------
@@ -61,7 +61,7 @@
   }
 
   @Override
-  public List<AtomicReaderContext> leaves() {
+public List<AtomicReaderContext> leaves() throws UnsupportedOperationException {
     if (!isTopLevel)
       throw new UnsupportedOperationException("This is not a top-level context.");
     assert leaves != null;
---------------
-------------
@@ -98,7 +98,7 @@
 		}
 	}
 	
-	class DataRepairHandler implements IAsyncCallback, ICacheExpungeHook<String, String>
+static class DataRepairHandler implements IAsyncCallback, ICacheExpungeHook<String, String>
 	{
 		private List<Message> responses_ = new ArrayList<Message>();
 		private IResponseResolver<Row> readResponseResolver_;
---------------
-------------
@@ -71,7 +71,7 @@
     if ( sf.multiValued() ) {
       throw new SolrException
         (SolrException.ErrorCode.SERVER_ERROR, 
-         errPrefix + " (" + VERSION_FIELD + " is not multiValued");
+errPrefix + " (" + VERSION_FIELD + " is multiValued");
     }
     
     return sf;
---------------
-------------
@@ -52,7 +52,7 @@
     {
         stages.put(MUTATION_STAGE, multiThreadedStage(MUTATION_STAGE, getConcurrentWriters()));
         stages.put(READ_STAGE, multiThreadedStage(READ_STAGE, getConcurrentReaders()));
-        stages.put(RESPONSE_STAGE, multiThreadedStage("RESPONSE-STAGE", Math.max(2, Runtime.getRuntime().availableProcessors())));
+stages.put(RESPONSE_STAGE, multiThreadedStage(RESPONSE_STAGE, Math.max(2, Runtime.getRuntime().availableProcessors())));
         // the rest are all single-threaded
         stages.put(STREAM_STAGE, new JMXEnabledThreadPoolExecutor(STREAM_STAGE));
         stages.put(GOSSIP_STAGE, new JMXEnabledThreadPoolExecutor("GMFD"));
---------------
-------------
@@ -303,7 +303,7 @@
         private IColumn unthriftifySuper(SuperColumn super_column)
         {
             ClockType clockType = ClockType.Timestamp; // TODO generalize
-            AbstractReconciler reconciler = new TimestampReconciler(); // TODO generalize
+AbstractReconciler reconciler = TimestampReconciler.instance; // TODO generalize
             org.apache.cassandra.db.SuperColumn sc = new org.apache.cassandra.db.SuperColumn(super_column.name, subComparator, clockType, reconciler);
             for (Column column : super_column.columns)
             {
---------------
-------------
@@ -57,7 +57,7 @@
     //Constant for special upgrade testing with both upgrade and create 
     // set. We just test this with one version in the interest of time
     // DERBY-4913
-    public static int VERSION_10_3_3_0_OFFSET = 7;
+public static int[] VERSION_10_3_3_0=  new int[] {10,3,3,0};
     /**
      * <p>
      * Get an array of versions supported by this platform.
---------------
-------------
@@ -129,7 +129,7 @@
     @Override
     public void setNextReader(AtomicReaderContext context)
         throws IOException {
-      this.docBase = docBase;
+docBase = context.docBase;
       other.setNextReader(context);
     }
 
---------------
-------------
@@ -74,7 +74,7 @@
       FixedBitSet visited = new FixedBitSet(ir.maxDoc());
       TermsEnum te = terms.iterator(null);
       while (te.next() != null) {
-        DocsEnum de = _TestUtil.docs(random(), te, null, null, 0);
+DocsEnum de = _TestUtil.docs(random(), te, null, null, false);
         while (de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
           visited.set(de.docID());
         }
---------------
-------------
@@ -617,7 +617,7 @@
     int NUM_DOCS = atLeast(10);
     for (int i = 0; i < NUM_DOCS; i++) {
       // must be > 4096 so it spans multiple chunks
-      int termCount = atLeast(5000);
+int termCount = _TestUtil.nextInt(random, 4097, 8200);
 
       List<String> doc = new ArrayList<String>();
 
---------------
-------------
@@ -156,7 +156,7 @@
 		@param in			optional data
 
 		@exception IOException Can be thrown by any of the methods of ObjectInput.
-		@exception StandardException Standard Cloudscape policy.
+@exception StandardException Standard Derby policy.
 
 	 */
 	public final void doMe(Transaction xact, LogInstant instant, LimitObjectInput in) 
---------------
-------------
@@ -163,7 +163,7 @@
                                               RAMDirectory directory,
                                               boolean createNew,
                                               int startingId) throws IOException {
-    IndexWriter writer = new IndexWriter( directory, new IndexWriterConfig(Version.LUCENE_41,new StandardAnalyzer(Version.LUCENE_41)));
+IndexWriter writer = new IndexWriter( directory, new IndexWriterConfig(Version.LUCENE_42,new StandardAnalyzer(Version.LUCENE_42)));
         
     try {
       for (int i = 0; i < DOCS.length; i++) {
---------------
-------------
@@ -31,7 +31,7 @@
 import java.util.HashMap;
 import org.apache.thrift.TEnum;
 
-public enum CqlResultType implements TEnum {
+public enum CqlResultType implements org.apache.thrift.TEnum {
   ROWS(1),
   VOID(2),
   INT(3);
---------------
-------------
@@ -53,7 +53,7 @@
         super.bytesPerChar = BYTES_PER_CHAR;
         EmbedStatement embStmt = (EmbedStatement)createStatement();
         EmbedConnection embCon =(EmbedConnection)getConnection();
-        iClob = new TemporaryClob(embCon.getDBName(), embStmt);
+iClob = new TemporaryClob(embStmt);
         transferData(
             new LoopingAlphabetReader(CLOBLENGTH, CharAlphabet.cjkSubset()),
             iClob.getWriter(1L),
---------------
-------------
@@ -194,7 +194,7 @@
   }
 
   @Override
-  public float score() throws IOException {
+public float score() {
     return docScorer.score(docID, freq);
   }
 
---------------
-------------
@@ -59,7 +59,7 @@
       System.out.println("TEST: NUM_DOCS=" + NUM_DOCS);
     }
 
-    MockDirectoryWrapper dir = newDirectory();
+BaseDirectoryWrapper dir = newDirectory();
     dir.setCheckIndexOnClose(false); // we use a custom codec provider
     IndexWriter w = new IndexWriter(
         dir,
---------------
-------------
@@ -243,7 +243,7 @@
       if (staticStats.contains(attribute) && attribute != null
               && attribute.length() > 0) {
         try {
-          String getter = "get" + attribute.substring(0, 1).toUpperCase()
+String getter = "get" + attribute.substring(0, 1).toUpperCase(Locale.ENGLISH)
                   + attribute.substring(1);
           Method meth = infoBean.getClass().getMethod(getter);
           val = meth.invoke(infoBean);
---------------
-------------
@@ -40,7 +40,7 @@
     }
 
     @Override
-    public void merge(MergePolicy.OneMerge merge) throws CorruptIndexException, IOException {
+public void merge(MergePolicy.OneMerge merge) throws IOException {
       if (merge.maxNumSegments != -1 && (first || merge.segments.size() == 1)) {
         first = false;
         if (VERBOSE) {
---------------
-------------
@@ -182,7 +182,7 @@
                 assert commitLogSync_ == CommitLogSync.periodic;
                 try
                 {
-                    commitLogSyncBatchMS_ = Double.valueOf(xmlUtils.getNodeValue("/Storage/CommitLogSyncPeriodInMS"));
+commitLogSyncPeriodMS_ = Integer.valueOf(xmlUtils.getNodeValue("/Storage/CommitLogSyncPeriodInMS"));
                 }
                 catch (Exception e)
                 {
---------------
-------------
@@ -48,7 +48,7 @@
 
   @Before
   public void before() throws IOException {
-    configuration = new Configuration();
+configuration = getConfiguration();
     seqFilesOutputPath = new Path(getTestTempDirPath(), "seqfiles");
 
     lucene2Seq = new SequenceFilesFromLuceneStorage();
---------------
-------------
@@ -101,7 +101,7 @@
       }
       debugEnabled = StrUtils.parseBool((String)initArgs.get(ENABLE_DEBUG), true);
       importer = new DataImporter(core, myName);         
-    } catch (Throwable e) {
+} catch (Exception e) {
       LOG.error( DataImporter.MSG.LOAD_EXP, e);
       throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, DataImporter.MSG.LOAD_EXP, e);
     }
---------------
-------------
@@ -432,7 +432,7 @@
 
           final int endPos = pos + len;
           while (pos < endPos) {
-            code = BytesRef.HASH_PRIME * code + bytes[pos++];
+code = 31 * code + bytes[pos++];
           }
         } else {
           code = bytesStart[e0];
---------------
-------------
@@ -500,7 +500,7 @@
       TermsEnum termsEnum = terms.iterator(null);
       DocsEnum docs = null;
       while(termsEnum.next() != null) {
-        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, true);
+docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);
         while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
           totalTokenCount2 += docs.freq();
         }
---------------
-------------
@@ -814,7 +814,7 @@
         EndPointState localState = endPointStateMap_.get(localEndPoint_);
         if ( localState == null )
         {
-            HeartBeatState hbState = new HeartBeatState(generationNbr, 0);
+HeartBeatState hbState = new HeartBeatState(generationNbr);
             localState = new EndPointState(hbState);
             localState.isAlive(true);
             localState.isAGossiper(true);
---------------
-------------
@@ -172,7 +172,7 @@
     final DocIdSetIterator docIdSetIterator;
     final float theScore;
 
-    public ConstantScorer(DocIdSetIterator docIdSetIterator, Weight w, float theScore) throws IOException {
+public ConstantScorer(DocIdSetIterator docIdSetIterator, Weight w, float theScore) {
       super(w);
       this.theScore = theScore;
       this.docIdSetIterator = docIdSetIterator;
---------------
-------------
@@ -226,7 +226,7 @@
                                       new BytesRef("aaa"),
                                       MultiFields.getLiveDocs(reader),
                                       null,
-                                      false);
+0);
       int count = 0;
       while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
         count++;
---------------
-------------
@@ -100,7 +100,7 @@
     }
   }
 
-  private final void add(String name, boolean isIndexed) {
+final void add(String name, boolean isIndexed) {
     FieldInfo fi = fieldInfo(name);
     if (fi == null)
       addInternal(name, isIndexed);
---------------
-------------
@@ -70,7 +70,7 @@
   @Test
   public void testLuceneEncoding() throws Exception {
     LuceneTextValueEncoder enc = new LuceneTextValueEncoder("text");
-    enc.setAnalyzer(new WhitespaceAnalyzer(Version.LUCENE_41));
+enc.setAnalyzer(new WhitespaceAnalyzer(Version.LUCENE_42));
     Vector v1 = new DenseVector(200);
     enc.addToVector("test1 and more", v1);
     enc.flush(1, v1);
---------------
-------------
@@ -485,7 +485,7 @@
         {
             ColumnFamilyStore cfStore = columnFamilyStores_.get( columnFamily );
             if ( cfStore != null )
-                MinorCompactionManager.instance().submitMajor(cfStore, 0);
+CompactionManager.instance().submitMajor(cfStore, 0);
         }
     }
 
---------------
-------------
@@ -773,7 +773,7 @@
     private boolean isCryptoBoot(Properties p)
         throws SQLException
     {
-        return (isTrue(p, Attribute.DATA_ENCRYPTION) ||
+return (vetTrue(p, Attribute.DATA_ENCRYPTION) ||
                 vetTrue(p, Attribute.DECRYPT_DATABASE) ||
                 isSet(p, Attribute.NEW_BOOT_PASSWORD) ||
                 isSet(p, Attribute.NEW_CRYPTO_EXTERNAL_KEY));
---------------
-------------
@@ -477,7 +477,7 @@
      public void run() {
        try {
          Document doc = new Document();
-         Field field = newField("field", "testData", TextField.TYPE_STORED);
+Field field = newTextField("field", "testData", Field.Store.YES);
          doc.add(field);
          IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(
              TEST_VERSION_CURRENT, new MockAnalyzer(random())));
---------------
-------------
@@ -43,7 +43,7 @@
   /** Creates a new ElisionFilterFactory */
   public ElisionFilterFactory(Map<String,String> args) {
     super(args);
-    articlesFile = args.remove("articles");
+articlesFile = get(args, "articles");
     ignoreCase = getBoolean(args, "ignoreCase", false);
     if (!args.isEmpty()) {
       throw new IllegalArgumentException("Unknown parameters: " + args);
---------------
-------------
@@ -224,7 +224,7 @@
 
   @Override
   public Query getHighlightQuery() throws ParseException {
-    return parsedUserQuery;
+return parsedUserQuery == null ? altUserQuery : parsedUserQuery;
   }
 
   public void addDebugInfo(NamedList<Object> debugInfo) {
---------------
-------------
@@ -54,7 +54,7 @@
         }
 
         throw connChild.newSQLException
-            ( SQLState.DATA_TYPE_NOT_SUPPORTED, sqlType.toString() );
+( SQLState.DATA_TYPE_NOT_SUPPORTED, sqlType );
     }
     
 }    
---------------
-------------
@@ -333,7 +333,7 @@
     Configuration conf = new Configuration();
     MeanShiftCanopyDriver.run(conf, testdata, output, measure, kernelProfile, 2.1, 1.0, 0.001, 10, false, true, true);
     int numIterations = 10;
-    Path clustersIn = new Path(output, "clusters-7-final");
+Path clustersIn = new Path(output, "clusters-8-final");
     RepresentativePointsDriver.run(conf, clustersIn, new Path(output, "clusteredPoints"), output, measure,
         numIterations, true);
     //printRepPoints(numIterations);
---------------
-------------
@@ -37,7 +37,7 @@
       
         Iterator<Class<?>> it = classes.iterator();
         while (it.hasNext()) {
-            if (it.next().isInterface()) it.remove();
+if (!!!it.next().isInterface()) it.remove();
         }
         
         return Proxy.newProxyInstance(new BundleToClassLoaderAdapter(bundle), classes.toArray(new Class[classes.size()]),
---------------
-------------
@@ -432,7 +432,7 @@
 
   private static final class DocsQueue extends PriorityQueue<DocsAndPositionsEnum> {
     DocsQueue(List<DocsAndPositionsEnum> docsEnums) throws IOException {
-      initialize(docsEnums.size());
+super(docsEnums.size());
 
       Iterator<DocsAndPositionsEnum> i = docsEnums.iterator();
       while (i.hasNext()) {
---------------
-------------
@@ -57,7 +57,7 @@
     addDoc(writer, "admin", "020", "20050101", "Maybe");
     addDoc(writer, "admin guest", "030", "20050101", "N");
     reader = SlowCompositeReaderWrapper.wrap(writer.getReader());
-    writer.close();
+writer.shutdown();
   }
 
   @Override
---------------
-------------
@@ -148,7 +148,7 @@
     {
         public void doVerb(Message message)
         {
-            Message reply = message.getReply(FBUtilities.getLocalAddress(), new byte[] {(byte)(isMoveable_.get() ? 1 : 0)});
+Message reply = message.getInternalReply(new byte[] {(byte)(isMoveable_.get() ? 1 : 0)});
             MessagingService.instance.sendOneWay(reply, message.getFrom());
             if ( isMoveable_.get() )
             {
---------------
-------------
@@ -39,7 +39,7 @@
     return new PersianNormalizationFilter(input);
   }
   
-  @Override
+//@Override
   public Object getMultiTermComponent() {
     return this;
   }
---------------
-------------
@@ -317,7 +317,7 @@
         cancel(key_);
         pendingWrites_.clear();
         if (pool_ != null)
-            pool_.destroy(this);
+pool_.reset();
     }
     
     private void cancel(SelectionKey key)
---------------
-------------
@@ -82,7 +82,7 @@
      * A wrapper class that holds two key parameters for a Bloom Filter: the
      * number of hash functions used, and the number of buckets per element used.
      */
-    public static final class BloomSpecification {
+public static class BloomSpecification {
         final int K; // number of hash functions.
         final int bucketsPerElement;
 
---------------
-------------
@@ -62,7 +62,7 @@
      * The maximum length in bytes for strings sent by {@code writeLDString()},
      * which is the maximum unsigned integer value that fits in two bytes.
      */
-    private final static int MAX_VARCHAR_BYTE_LENGTH = 0xFFFF;
+final static int MAX_VARCHAR_BYTE_LENGTH = 0xFFFF;
 
     /**
      * Output buffer.
---------------
-------------
@@ -74,7 +74,7 @@
 	private AccountDataBeanImpl account;
 
 	@ManyToOne(fetch = FetchType.EAGER)
-	@JoinColumn(name = "QUOTE_SYMBOL")
+@JoinColumn(name = "QUOTE_SYMBOL", columnDefinition="VARCHAR(250)")
 	private QuoteDataBeanImpl quote;
 
 	public HoldingDataBeanImpl() {
---------------
-------------
@@ -48,7 +48,7 @@
         d.add(new TextField(fieldName, docs[j], Field.Store.NO));
         writer.addDocument(d);
       }
-      writer.close();
+writer.shutdown();
     } catch (java.io.IOException ioe) {
       throw new Error(ioe);
     }
---------------
-------------
@@ -652,7 +652,7 @@
         sb.append("\t" + "maxQueryTerms  : " + maxQueryTerms + "\n");
         sb.append("\t" + "minWordLen     : " + minWordLen + "\n");
         sb.append("\t" + "maxWordLen     : " + maxWordLen + "\n");
-        sb.append("\t" + "fieldNames     : \"");
+sb.append("\t" + "fieldNames     : ");
         String delim = "";
         for (int i = 0; i < fieldNames.length; i++) {
             String fieldName = fieldNames[i];
---------------
-------------
@@ -52,7 +52,7 @@
     // this allows to easily identify a matching (exact) phrase 
     // when all PhrasePositions have exactly the same position.
     for (int i = 0; i < postings.length; i++) {
-      PhrasePositions pp = new PhrasePositions(postings[i].postings, postings[i].position);
+PhrasePositions pp = new PhrasePositions(postings[i].postings, postings[i].position, i);
       if (last != null) {			  // add next to end of list
         last.next = pp;
       } else {
---------------
-------------
@@ -101,7 +101,7 @@
   /** Default set of articles for ElisionFilter */
   public static final CharArraySet DEFAULT_ARTICLES = CharArraySet.unmodifiableSet(
       new CharArraySet(Version.LUCENE_CURRENT, Arrays.asList(
-          "l", "m", "t", "qu", "n", "s", "j"), true));
+"l", "m", "t", "qu", "n", "s", "j", "d", "c", "jusqu", "quoiqu", "lorsqu", "puisqu"), true));
   
   /**
    * Contains words that should be indexed but not stemmed.
---------------
-------------
@@ -138,7 +138,7 @@
     }
     
     if (this.cores == null) {
-      ((HttpServletResponse)response).sendError( 403, "Server is shutting down" );
+((HttpServletResponse)response).sendError( 503, "Server is shutting down" );
       return;
     }
     CoreContainer cores = this.cores;
---------------
-------------
@@ -75,7 +75,7 @@
         ColumnFamily.serializer().serializeWithIndexes(cf, buffer);               
         entries.put(ByteBufferUtil.bytes("k3"), ByteBuffer.wrap(Arrays.copyOf(buffer.getData(), buffer.getLength())));
         
-        SSTableReader orig = SSTableUtils.writeRawSSTable("Keyspace1", "Indexed1", entries);        
+SSTableReader orig = SSTableUtils.prepare().cf("Indexed1").writeRaw(entries);
         // whack the index to trigger the recover
         FileUtils.deleteWithConfirm(orig.descriptor.filenameFor(Component.PRIMARY_INDEX));
         FileUtils.deleteWithConfirm(orig.descriptor.filenameFor(Component.FILTER));
---------------
-------------
@@ -154,7 +154,7 @@
       doc.add(newTextField("role", docsContent[i].role, Field.Store.YES));
       w.addDocument(doc);
     }
-    w.close();
+w.shutdown();
     reader = DirectoryReader.open(rd);
     searcher = newSearcher(reader);
   }
---------------
-------------
@@ -74,7 +74,7 @@
           .getLocalizedMessage(QueryParserMessages.NODE_ACTION_NOT_SUPPORTED));
     }
 
-    for (QueryNode child : getChildren()) {
+for (QueryNode child : children) {
       add(child);
     }
 
---------------
-------------
@@ -3020,7 +3020,7 @@
         this.cKey = other.cKey;
         this.stream = other.stream;
         this._clobValue = other._clobValue;
-        this.localeFinder = localeFinder;
+this.localeFinder = other.localeFinder;
     }
 
     /**
---------------
-------------
@@ -3217,7 +3217,7 @@
             if (numBytes >= 0) {
                 byte[] readBytes = new byte[numBytes];
                 System.arraycopy(value, 0, readBytes, 0, numBytes);
-                valueString = new String(readBytes);
+valueString = new String(readBytes, "US-ASCII");
                 assertEquals("FAIL - wrong substring value",
                         valueString, subStr);
             } else {
---------------
-------------
@@ -32,7 +32,7 @@
   private final DocsAndFreqs lead;
 
   ConjunctionTermScorer(Weight weight, float coord,
-      DocsAndFreqs[] docsAndFreqs) throws IOException {
+DocsAndFreqs[] docsAndFreqs) {
     super(weight);
     this.coord = coord;
     this.docsAndFreqs = docsAndFreqs;
---------------
-------------
@@ -60,7 +60,7 @@
 
 public final class MessagingService implements MessagingServiceMBean
 {
-    public static final int version_ = 1;
+public static final int version_ = 2;
     //TODO: make this parameter dynamic somehow.  Not sure if config is appropriate.
     private SerializerType serializerType_ = SerializerType.BINARY;
 
---------------
-------------
@@ -139,7 +139,7 @@
    * Wraps the Reader with {@link PersianCharFilter}
    */
   @Override
-  protected Reader initReader(Reader reader) {
+protected Reader initReader(String fieldName, Reader reader) {
     return matchVersion.onOrAfter(Version.LUCENE_31) ? 
        new PersianCharFilter(CharReader.get(reader)) :
        reader;
---------------
-------------
@@ -125,7 +125,7 @@
    *  term.  This will return null if the field or term does
    *  not exist. */
   public static DocsEnum getTermDocsEnum(IndexReader r, Bits liveDocs, String field, BytesRef term) throws IOException {
-    return getTermDocsEnum(r, liveDocs, field, term);
+return getTermDocsEnum(r, liveDocs, field, term, DocsEnum.FLAG_FREQS);
   }
   
   /** Returns {@link DocsEnum} for the specified field &
---------------
-------------
@@ -35,7 +35,7 @@
 
   private boolean indexed;
   private boolean stored;
-  private boolean tokenized;
+private boolean tokenized = true;
   private boolean storeTermVectors;
   private boolean storeTermVectorOffsets;
   private boolean storeTermVectorPositions;
---------------
-------------
@@ -130,7 +130,7 @@
   // nocommit
   private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene41");
 
-  private final NormsFormat simpleNormsFormat = new Lucene41SimpleNormsFormat();
+private final NormsFormat simpleNormsFormat = new Lucene41NormsFormat();
 
   @Override
   public NormsFormat normsFormat() {
---------------
-------------
@@ -125,7 +125,7 @@
             CliMain.processStatement(statement);
             String result = outStream.toString();
             // System.out.println("Result:\n" + result);
-            assertEquals("", errStream.toString());
+assertEquals(errStream.toString() + " processing " + statement, "", errStream.toString());
             if (statement.startsWith("drop ") || statement.startsWith("create ") || statement.startsWith("update "))
             {
                 assertTrue(result.matches("(.{8})-(.{4})-(.{4})-(.{4})-(.{12})\n"));
---------------
-------------
@@ -81,7 +81,7 @@
             IColumn column = columns.get(columnName.getBytes());
             if (column == null)
                 return;
-            String value = ByteBufferUtil.string(column.value(), Charsets.UTF_8);
+String value = ByteBufferUtil.string(column.value());
             logger.debug("read " + key + ":" + value + " from " + context.getInputSplit());
 
             StringTokenizer itr = new StringTokenizer(value);
---------------
-------------
@@ -21,7 +21,7 @@
 
 final class PhraseQueue extends PriorityQueue<PhrasePositions> {
   PhraseQueue(int size) {
-    initialize(size);
+super(size);
   }
 
   @Override
---------------
-------------
@@ -595,7 +595,7 @@
       if (freq > tiq.minFreq) {
         UnicodeUtil.UTF8toUTF16(text, spare);
         String t = spare.toString();
-        tiq.distinctTerms = new Long(terms.getUniqueTermCount()).intValue();
+tiq.distinctTerms = new Long(terms.size()).intValue();
 
         tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));
         if (tiq.size() > numTerms) { // if tiq full
---------------
-------------
@@ -206,7 +206,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, new MockPayloadAnalyzer());
     Document doc = new Document();
     doc.add(new TextField("content", new StringReader(
-        "a a b c d e a f g h i j a b k k"), Field.Store.NO));
+"a a b c d e a f g h i j a b k k")));
     writer.addDocument(doc);
 
     final IndexReader readerFromWriter = writer.getReader();
---------------
-------------
@@ -39,7 +39,7 @@
   @Override
   public int run(String[] args) throws Exception {
     InformationGain job = new InformationGain();
-    ToolRunner.run(job, args);
+ToolRunner.run(getConf(), job, args);
     informationGain = job.getInformationGain();
     entropy = job.getEntropy();
     informationGainRatio = informationGain / entropy;
---------------
-------------
@@ -48,7 +48,7 @@
       int high = lookup.length-1;
 
       while (low <= high) {
-        int mid = (low + high) >> 1;
+int mid = (low + high) >>> 1;
         int cmp = lookup[mid].compareTo(key);
 
         if (cmp < 0)
---------------
-------------
@@ -26,7 +26,7 @@
  <P>
  This interface is used in the column SYS.SYSSTATISTICS.STATISTICS. It
  encapsulates information collected by the UPDATE STATISTICS command
- and is used internally by the Cloudscape optimizer to estimate cost 
+and is used internally by the Derby optimizer to estimate cost
  and selectivity of different query plans.
  <p>
 */
---------------
-------------
@@ -250,7 +250,7 @@
       
       for (int i = 1; i <= numSlices; i++) {
         for (int j = 1; j <= repFactor; j++) {
-          String nodeName = nodeList.get(((i - 1) + (j - 1)) % nodeList.size());
+String nodeName = nodeList.get((repFactor * (i - 1) + (j - 1)) % nodeList.size());
           String sliceName = "shard" + i;
           String shardName = collectionName + "_" + sliceName + "_replica" + j;
           log.info("Creating shard " + shardName + " as part of slice "
---------------
-------------
@@ -265,7 +265,7 @@
   @Override
   public void addTo(Vector v) {
     if (v.size() != size()) {
-      throw new CardinalityException();
+throw new CardinalityException(size(), v.size());
     }
     values.forEachPair(new AddToVector(v));
   }
---------------
-------------
@@ -439,7 +439,7 @@
 			default : return String.valueOf(jdbcType);
 		}
 	}
-	  public static String jdbcNameFromJdbc(int jdbcType) {
+public static String getNameFromJdbcType(int jdbcType) {
 		switch (jdbcType) {
 			case Types.BIT 		:  return "Types.BIT";
 			case JDBC30Translation.SQL_TYPES_BOOLEAN  : return "Types.BOOLEAN";
---------------
-------------
@@ -58,7 +58,7 @@
 
     private void insertRow(String key) throws IOException
     {
-        RowMutation rm = new RowMutation("Keyspace1", key);
+RowMutation rm = new RowMutation("Keyspace1", key.getBytes());
         ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
         cf.addColumn(column("col1", "val1", 1L));
         rm.add(cf);
---------------
-------------
@@ -2777,7 +2777,7 @@
           count++;
         }
       }
-      assertTrue("flush happened too quickly during " + (doIndexing ? "indexing" : "deleting") + " count=" + count, count > 1500);
+assertTrue("flush happened too quickly during " + (doIndexing ? "indexing" : "deleting") + " count=" + count, count > 3000);
     }
     w.close();
     dir.close();
---------------
-------------
@@ -267,7 +267,7 @@
           if (isBinaryField) {
             if (destinationField.getType() instanceof BinaryField) {
               BinaryField binaryField = (BinaryField) destinationField.getType();
-              binaryField.createField(destinationField, v, boost);
+f = binaryField.createField(destinationField, v, boost);
             }
           } else {
             f = destinationField.createField(cf.getLimitedValue(val), boost);
---------------
-------------
@@ -1139,7 +1139,7 @@
         params.set(FILE, fileName);
       }
       if (useInternal) {
-        params.set(COMPRESSION, "internal"); 
+params.set(COMPRESSION, "true");
       }
       //use checksum
       if (this.includeChecksum) {
---------------
-------------
@@ -92,7 +92,7 @@
 
     Option delimiterOpt = obuilder.withLongName("delimiter").withRequired(false).withArgument(
       abuilder.withName("delimiter").withMinimum(1).withMaximum(1).create()).withDescription(
-      "The delimiter for outputing the dictionary").withShortName("l").create();
+"The delimiter for outputting the dictionary").withShortName("l").create();
 
     Option powerOpt = obuilder.withLongName("norm").withRequired(false).withArgument(
       abuilder.withName("norm").withMinimum(1).withMaximum(1).create()).withDescription(
---------------
-------------
@@ -77,7 +77,7 @@
 
   @Override
   public double distance(double centroidLengthSquare, Vector centroid, Vector v) {
-    if (centroid.size() != centroid.size()) {
+if (centroid.size() != v.size()) {
       throw new CardinalityException();
     }
 
---------------
-------------
@@ -196,7 +196,7 @@
 
               @Override
               public boolean isIndexTerm(BytesRef term, TermStats stats) {
-                return rand.nextInt(gap) == 17;
+return rand.nextInt(gap) == gap/2;
               }
 
               @Override
---------------
-------------
@@ -995,7 +995,7 @@
      */
     public void addSavedEndpoint(InetAddress ep)
     {
-        if (ep == FBUtilities.getBroadcastAddress())
+if (ep == FBUtilities.getLocalAddress())
         {
             logger.debug("Attempt to add self as saved endpoint");
             return;
---------------
-------------
@@ -122,7 +122,7 @@
         }
       }
       if(weave)
-        bytes = WovenProxyGenerator.getWovenProxy(bytes, className, this);
+bytes = WovenProxyGenerator.getWovenProxy(bytes, this);
       
       return defineClass(className, bytes, 0, bytes.length);
     }
---------------
-------------
@@ -255,7 +255,7 @@
         {
             StorageService ss = StorageService.instance;
             String tokenString = StorageService.getPartitioner().getTokenFactory().toString(ss.getBootstrapToken());
-            Message response = message.getInternalReply(tokenString.getBytes(Charsets.UTF_8));
+Message response = message.getInternalReply(tokenString.getBytes(Charsets.UTF_8), message.getVersion());
             MessagingService.instance().sendOneWay(response, message.getFrom());
         }
     }
---------------
-------------
@@ -150,7 +150,7 @@
   public void testRepresentativePoints() throws Exception {
     ClusteringTestUtils.writePointsToFile(referenceData, new Path(testdata, "file1"), fs, conf);
     DistanceMeasure measure = new EuclideanDistanceMeasure();
-    Configuration conf = new Configuration();
+Configuration conf = getConfiguration();
     // run using MR reference point calculation
     CanopyDriver.run(conf, testdata, output, measure, 3.1, 1.1, true, 0.0, true);
     int numIterations = 2;
---------------
-------------
@@ -69,7 +69,7 @@
     if (input.incrementToken()) {
       payloadAttr.setPayload(new BytesRef(("pos: " + pos).getBytes()));
       int posIncr;
-      if (i % 2 == 1) {
+if (pos == 0 || i % 2 == 1) {
         posIncr = 1;
       } else {
         posIncr = 0;
---------------
-------------
@@ -59,7 +59,7 @@
     {
       _compositeBundle.start(Bundle.START_ACTIVATION_POLICY);
   
-      _packageAdminTracker = new ServiceTracker(_compositeBundle.getBundleContext(),
+_packageAdminTracker = new ServiceTracker(_compositeBundle.getCompositeFramework().getBundleContext(),
           PackageAdmin.class.getName(), null);
       _packageAdminTracker.open();
     }
---------------
-------------
@@ -59,7 +59,7 @@
         assert three.equals(source);
 
         InetAddress myEndpoint = InetAddress.getByName("127.0.0.1");
-        Range range3 = ss.getPrimaryRangeForEndPoint(three);
+Range range3 = ss.getPrimaryRangeForEndpoint(three);
         Token fakeToken = ((IPartitioner)StorageService.getPartitioner()).midpoint(range3.left, range3.right);
         assert range3.contains(fakeToken);
         ss.onChange(myEndpoint, StorageService.MOVE_STATE, new ApplicationState(StorageService.STATE_BOOTSTRAPPING + StorageService.Delimiter + ss.getPartitioner().getTokenFactory().toString(fakeToken)));
---------------
-------------
@@ -69,7 +69,7 @@
  */
 public class SimpleQParserPlugin extends QParserPlugin {
   /** The name that can be used to specify this plugin should be used to parse the query. */
-  public static String NAME = "simple";
+public static final String NAME = "simple";
 
   /** Map of string operators to their int counterparts in SimpleQueryParser. */
   private static final Map<String, Integer> OPERATORS = new HashMap<String, Integer>();
---------------
-------------
@@ -36,7 +36,7 @@
     for (int i = 0; i < numDocs; i++) {
       Document doc = new Document();
       if (withID) {
-        doc.add(new Field("id", "" + i, StringField.TYPE_UNSTORED));
+doc.add(new StringField("id", "" + i, Field.Store.NO));
       }
       writer.addDocument(doc);
     }
---------------
-------------
@@ -85,7 +85,7 @@
 	 * Simple text indicating any limits execeeded while generating
 	 * the class file.
 	 */
-	private String limitMsg;
+String limitMsg;
 	
 	//
 	// ClassBuilder interface
---------------
-------------
@@ -88,7 +88,7 @@
     private final static ReentrantLock lock_ = new ReentrantLock();
     private static Map<String, TcpConnectionManager> poolTable_ = new Hashtable<String, TcpConnectionManager>();
     
-    private static boolean bShutdown_ = false;
+private static volatile boolean bShutdown_ = false;
     
     private static Logger logger_ = Logger.getLogger(MessagingService.class);
     
---------------
-------------
@@ -202,7 +202,7 @@
                                                           ClockType.Timestamp,
                                                           columnComparator,
                                                           null,
-                                                          new TimestampReconciler(),
+TimestampReconciler.instance,
                                                           "",
                                                           0,
                                                           false,
---------------
-------------
@@ -180,7 +180,7 @@
 
       private char[] buffer = new char[0];
 
-      private LuceneStemmerAdapter() throws Exception {
+private LuceneStemmerAdapter() {
         delegate = new org.apache.lucene.analysis.ar.ArabicStemmer();
         normalizer = new org.apache.lucene.analysis.ar.ArabicNormalizer();
       }
---------------
-------------
@@ -20,7 +20,7 @@
 import org.apache.aries.application.modelling.standalone.OfflineModellingFactory;
 import org.apache.aries.mocks.BundleContextMock;
 import org.apache.aries.util.filesystem.FileSystem;
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
---------------
-------------
@@ -49,7 +49,7 @@
   }
 
   public QueryMaker getQueryMaker() {
-    return getRunData().getSearchQueryMaker();
+return getRunData().getQueryMaker(this);
   }
 
 
---------------
-------------
@@ -242,7 +242,7 @@
 
     public void delete(ColumnFamily cf2)
     {
-        FBUtilities.atomicSetMax(localDeletionTime, cf2.getLocalDeletionTime());
+FBUtilities.atomicSetMax(localDeletionTime, cf2.getLocalDeletionTime()); // do this first so we won't have a column that's "deleted" but has no local deletion time
         FBUtilities.atomicSetMax(markedForDeleteAt, cf2.getMarkedForDeleteAt());
     }
 
---------------
-------------
@@ -42,7 +42,7 @@
    * @param pattern
    *          the pattern to apply to the incoming term buffer
    **/
-  protected PatternKeywordMarkerFilter(TokenStream in, Pattern pattern) {
+public PatternKeywordMarkerFilter(TokenStream in, Pattern pattern) {
     super(in);
     this.matcher = pattern.matcher("");
   }
---------------
-------------
@@ -177,7 +177,7 @@
         info.add( "uptime", execute( "uptime" ) );
       }
     }
-    catch( Throwable ex ) {
+catch( Exception ex ) {
       ex.printStackTrace();
     } 
     return info;
---------------
-------------
@@ -346,7 +346,7 @@
   }
 
 
-  @Test
+@Test @Ignore("Please fix me!")
   public void testClientErrorOnMalformedNumbers() throws Exception {
 
     final String BAD_VALUE = "NOT_A_NUMBER";
---------------
-------------
@@ -112,7 +112,7 @@
       if (numHits > 0) {
         if (withCollector() == false) {
           if (sort != null) {
-            Weight w = q.weight(searcher);
+Weight w = searcher.createNormalizedWeight(q);
             TopFieldCollector collector = TopFieldCollector.create(sort, numHits,
                                                                    true, withScore(),
                                                                    withMaxScore(),
---------------
-------------
@@ -1393,7 +1393,7 @@
       return ((TermPositions)current).nextPosition();
     }
     
-    public int getPayloadLength() {
+public int getPayloadLength() throws IOException {
       return ((TermPositions)current).getPayloadLength();
     }
      
---------------
-------------
@@ -3835,7 +3835,7 @@
         long rowToFetch = getRowUncast() - absolutePosition_;
 
         // if rowToFetch is zero, already positioned on the current row
-        if (rowToFetch != 0 || cursorUnpositionedOnServer_) {
+if (rowToFetch != 0) {
             writePositioningFetch_((generatedSection_ == null) ? statement_.section_ : generatedSection_,
                     scrollOrientation_relative__,
                     rowToFetch);
---------------
-------------
@@ -387,7 +387,7 @@
             logger_.info("Loading persisted ring state");
             for (Map.Entry<Token, InetAddress> entry : SystemTable.loadTokens().entrySet())
             {
-                if (entry.getValue() == FBUtilities.getBroadcastAddress())
+if (entry.getValue() == FBUtilities.getLocalAddress())
                 {
                     // entry has been mistakenly added, delete it
                     SystemTable.removeToken(entry.getKey());
---------------
-------------
@@ -102,7 +102,7 @@
     w.addDocument(doc);
 
     IndexSearcher indexSearcher = newSearcher(w.getReader());
-    w.close();
+w.shutdown();
 
     AbstractAllGroupsCollector<?> allGroupsCollector = createRandomCollector(groupField);
     indexSearcher.search(new TermQuery(new Term("content", "random")), allGroupsCollector);
---------------
-------------
@@ -310,7 +310,7 @@
         }
       } else {
         assert sameTermInfo(ti, tiOrd, enumerator);
-        assert (int) enumerator.position == tiOrd.termOrd;
+assert enumerator.position == tiOrd.termOrd;
       }
     } else {
       ti = null;
---------------
-------------
@@ -51,6 +51,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new RomanianAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new RomanianAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -26,7 +26,7 @@
  * <br>Example: <code>{!func}log(foo)</code>
  */
 public class FunctionQParserPlugin extends QParserPlugin {
-  public static String NAME = "func";
+public static final String NAME = "func";
 
   @Override
   public void init(NamedList args) {
---------------
-------------
@@ -460,7 +460,7 @@
 	public void boot(boolean create, Properties startParams) 
 			throws StandardException
 	{
-		softwareVersion = new DD_Version(this, DataDictionary.DD_VERSION_DERBY_10_2);
+softwareVersion = new DD_Version(this, DataDictionary.DD_VERSION_DERBY_10_3);
 
 		/* There is a bootstrapping problem here. We would like to use
          * a language connection context to find the name of the system and default
---------------
-------------
@@ -322,7 +322,7 @@
      * @param onDiskType The object read that represents the type.
      * @return A type descriptor.
      */
-    private static TypeDescriptor getStoredType(Object onDiskType)
+public static TypeDescriptor getStoredType(Object onDiskType)
     {
         if (onDiskType instanceof OldRoutineType)
             return ((OldRoutineType) onDiskType).getCatalogType();
---------------
-------------
@@ -777,7 +777,7 @@
           DirectoryReader ir1 = DirectoryReader.open(this);
           int numDocs1 = ir1.numDocs();
           ir1.close();
-          new IndexWriter(this, new IndexWriterConfig(LuceneTestCase.TEST_VERSION_CURRENT, null)).close();
+new IndexWriter(this, new IndexWriterConfig(LuceneTestCase.TEST_VERSION_CURRENT, null)).shutdown();
           DirectoryReader ir2 = DirectoryReader.open(this);
           int numDocs2 = ir2.numDocs();
           ir2.close();
---------------
-------------
@@ -31,7 +31,7 @@
     FieldInfos fieldInfos = new FieldInfos();
     fieldInfos.add(testDoc);
     //Since the complement is stored as well in the fields map
-    assertTrue(fieldInfos.size() == 7); //this is 7 b/c we are using the no-arg constructor
+assertTrue(fieldInfos.size() == 6); //this is 6 b/c we are using the no-arg constructor
     RAMDirectory dir = new RAMDirectory();
     String name = "testFile";
     IndexOutput output = dir.createOutput(name);
---------------
-------------
@@ -78,7 +78,7 @@
 	                  {{"","APP","TSYN","SYNONYM","",null,null,null,null,null}});
 
 		rs = dmd.getTables( "%", "%", "%", new String[] {"SYSTEM TABLE"});
-		assertEquals(19, JDBC.assertDrainResults(rs));
+assertEquals(20, JDBC.assertDrainResults(rs));
 		s.executeUpdate("DROP VIEW APP.V");
 		s.executeUpdate("DROP TABLE APP.TAB");
 		s.executeUpdate("DROP SYNONYM APP.TSYN");
---------------
-------------
@@ -1738,7 +1738,7 @@
         for (String table : DatabaseDescriptor.getNonSystemTables())
         {
             // if the replication factor is 1 the data is lost so we shouldn't wait for confirmation
-            if (DatabaseDescriptor.getReplicationFactor(table) == 1)
+if (Table.open(table).getReplicationStrategy().getReplicationFactor() == 1)
                 continue;
 
             // get all ranges that change ownership (that is, a node needs
---------------
-------------
@@ -59,7 +59,7 @@
     ll        = EasyMock.createMock(LLCallback.class);
     cl        = new LLCallback() {
       @Override
-      public double logLikelihoodRatio(int k11, int k12, int k21, int k22) {
+public double logLikelihoodRatio(long k11, long k12, long k21, long k22) {
         log.info("k11:{} k12:{} k21:{} k22:{}", new Object[] {k11, k12, k21, k22});
         return LogLikelihood.logLikelihoodRatio(k11, k12, k21, k22);
       }
---------------
-------------
@@ -4057,7 +4057,7 @@
 		ResultColumn	rc = (ResultColumn) nodeFactory.getNode
 			(
 				C_NodeTypes.RESULT_COLUMN,
-				null,
+columnName,
 				nodeFactory.getNode
 				(
 					C_NodeTypes.COLUMN_REFERENCE,
---------------
-------------
@@ -154,7 +154,7 @@
    *          offset in input arrays where partition starts
    */
   protected boolean isSelfPartition (int ordinal, FacetArrays facetArrays, int offset) {
-    int partitionSize = facetArrays.getArraysLength();
+int partitionSize = facetArrays.arrayLength;
     return ordinal / partitionSize == offset / partitionSize;
   }
 
---------------
-------------
@@ -75,7 +75,7 @@
         writer.addDocument(d);
       }
     } finally {
-      writer.close();
+writer.shutdown();
     }
   }
 
---------------
-------------
@@ -1705,7 +1705,7 @@
     {
         sb.append(NEWLINE + TAB + TAB + "{");
 
-        final AbstractType comparator = getFormatType((cfDef.column_type == "Super")
+final AbstractType comparator = getFormatType(cfDef.column_type.equals("Super")
                                                         ? cfDef.subcomparator_type
                                                         : cfDef.comparator_type);
         sb.append("column_name : '" + CliUtils.escapeSQLString(comparator.getString(colDef.name)) + "'," + NEWLINE);
---------------
-------------
@@ -60,7 +60,7 @@
         riw.deleteDocuments(new Term("id", Integer.toString(i)));
       }
     }
-    riw.close();
+riw.shutdown();
     checkHeaders(dir);
     dir.close();
   }
---------------
-------------
@@ -86,7 +86,7 @@
     doc.add(new FloatAssociationFacetField(0.34f, "genre", "software"));
     indexWriter.addDocument(config.build(taxoWriter, doc));
 
-    indexWriter.close();
+indexWriter.shutdown();
     taxoWriter.close();
   }
 
---------------
-------------
@@ -118,7 +118,7 @@
       writer.addDocument(doc);
     }
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
 
     searcher = newSearcher(reader);
     searcher.setSimilarity(similarity);
---------------
-------------
@@ -48,7 +48,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -529,7 +529,7 @@
 
     //Initialize JMX
     if (config.jmxConfig.enabled) {
-      infoRegistry = new JmxMonitoredMap<String, SolrInfoMBean>(name, config.jmxConfig);
+infoRegistry = new JmxMonitoredMap<String, SolrInfoMBean>(name, String.valueOf(this.hashCode()), config.jmxConfig);
     } else  {
       log.info("JMX monitoring not detected for core: " + name);
       infoRegistry = new ConcurrentHashMap<String, SolrInfoMBean>();
---------------
-------------
@@ -89,7 +89,7 @@
     reader = writer.getReader();
     searcher1 = newSearcher(reader);
     searcher2 = newSearcher(reader);
-    writer.close();
+writer.shutdown();
   }
   
   @Override
---------------
-------------
@@ -51,7 +51,7 @@
   }
 
   public UAX29URLEmailTokenizer create(Reader input) {
-    UAX29URLEmailTokenizer tokenizer = new UAX29URLEmailTokenizer(input); 
+UAX29URLEmailTokenizer tokenizer = new UAX29URLEmailTokenizer(luceneMatchVersion, input);
     tokenizer.setMaxTokenLength(maxTokenLength);
     return tokenizer;
   }
---------------
-------------
@@ -78,7 +78,7 @@
         public int compare(String o1, String o2)
         {
             IPartitioner p = StorageService.getPartitioner();
-            return p.getDecoratedKeyComparator().compare(p.decorateKey(o1), p.decorateKey(o2));
+return p.decorateKey(o1).compareTo(p.decorateKey(o2));
         }
     };
 
---------------
-------------
@@ -90,7 +90,7 @@
                             {1, 0, 1, 5}  // worst of
     };
     
-    Configuration config = new Configuration();
+Configuration config = getConfiguration();
     config.set(LLRReducer.NGRAM_TOTAL, "7");
     EasyMock.expect(context.getConfiguration()).andReturn(config);
     
---------------
-------------
@@ -129,7 +129,7 @@
 
   private final static class FieldMergeQueue extends PriorityQueue<FieldsEnumWithSlice> {
     FieldMergeQueue(int size) {
-      initialize(size);
+super(size);
     }
 
     @Override
---------------
-------------
@@ -147,7 +147,7 @@
     if (tempDir == null)
       throw new IOException("java.io.tmpdir undefined, cannot run test");
     File indexDir = new File(tempDir, "lucenetestindex");
-    Directory rd = FSDirectory.getDirectory(indexDir);
+Directory rd = FSDirectory.getDirectory(indexDir, null, false);
     IndexThread.id = 0;
     IndexThread.idStack.clear();
     IndexModifier index = new IndexModifier(rd, new StandardAnalyzer(), create);
---------------
-------------
@@ -438,7 +438,7 @@
                         (
                          "p",
                          "The following tables list <i>SQLStates</i> for exceptions. Exceptions " +
-                         "that begin with an <i>X</i> are specific to <ph conref=\"refconrefs.dita#prod/productshortname\"></ph>."
+"that begin with an <i>X</i> are specific to <ph conref=\"../conrefs.dita#prod/productshortname\"></ph>."
                          );
                 }
                 ditaWriter.endTag();
---------------
-------------
@@ -21,7 +21,7 @@
 
 final class SegmentTermEnum extends TermEnum implements Cloneable {
   private InputStream input;
-  private FieldInfos fieldInfos;
+FieldInfos fieldInfos;
   long size;
   long position = -1;
 
---------------
-------------
@@ -957,7 +957,7 @@
               int docId = 12;
               for(int i=0;i<13;i++) {
                 reader.deleteDocument(docId);
-                reader.setNorm(docId, "contents", (float) 2.0);
+reader.setNorm(docId, "content", (float) 2.0);
                 docId += 12;
               }
             }
---------------
-------------
@@ -76,7 +76,7 @@
         Long serviceId = (Long) reference.getProperty(Constants.SERVICE_ID);
         //API stipulates versions for compendium services with static ObjectName
         //This shouldn't happen but added as a consistency check
-        if (getTrackingCount() > 0) {
+if (trackedId != null) {
             String serviceDescription = (String) ((reference.getProperty(Constants.SERVICE_DESCRIPTION) != null) ? 
                     reference.getProperty(Constants.SERVICE_DESCRIPTION) : reference.getProperty(Constants.OBJECTCLASS));
             logger.log(LogService.LOG_WARNING, "Detected secondary ServiceReference for [" + serviceDescription
---------------
-------------
@@ -553,7 +553,7 @@
       expected = new String[] {"_0.cfs",
                                "_0_1.del",
                                "_0_1.s" + contentFieldIndex,
-                               "segments_3",
+"segments_2",
                                "segments.gen"};
 
       String[] actual = dir.listAll();
---------------
-------------
@@ -378,7 +378,7 @@
         submitUserDefined(cfs, descriptors, getDefaultGcBefore(cfs));
     }
 
-    Future<Object> submitUserDefined(final ColumnFamilyStore cfs, final Collection<Descriptor> dataFiles, final int gcBefore)
+public Future<Object> submitUserDefined(final ColumnFamilyStore cfs, final Collection<Descriptor> dataFiles, final int gcBefore)
     {
         Callable<Object> callable = new Callable<Object>()
         {
---------------
-------------
@@ -62,7 +62,7 @@
     dvField.setFloatValue(4f); // boost x4
     iw.addDocument(doc);
     IndexReader ir = iw.getReader();
-    iw.close();
+iw.shutdown();
     
     // no boosting
     IndexSearcher searcher1 = newSearcher(ir, false);
---------------
-------------
@@ -324,7 +324,7 @@
         // and make it a non-distributed request.
         String ourSlice = cloudDescriptor.getShardId();
         String ourCollection = cloudDescriptor.getCollectionName();
-        if (rb.slices.length == 1
+if (rb.slices.length == 1 && rb.slices[0] != null
             && ( rb.slices[0].equals(ourSlice) || rb.slices[0].equals(ourCollection + "_" + ourSlice) )  // handle the <collection>_<slice> format
             && ZkStateReader.ACTIVE.equals(cloudDescriptor.getLastPublished()) )
         {
---------------
-------------
@@ -97,7 +97,7 @@
   /** Removes all (key,value) associations from the receiver. Implicitly calls <tt>trimToSize()</tt>. */
   @Override
   public void clear() {
-    Arrays.fill(this.state, 0, state.length - 1, FREE);
+Arrays.fill(this.state, FREE);
     distinct = 0;
     freeEntries = table.length; // delta
     trimToSize();
---------------
-------------
@@ -116,7 +116,7 @@
       CharacterRunAutomaton runAutomaton = new CharacterRunAutomaton(automaton);
       CharsRef utf16 = new CharsRef(10);
 
-      private SimpleAutomatonTermsEnum(TermsEnum tenum) throws IOException {
+private SimpleAutomatonTermsEnum(TermsEnum tenum) {
         super(tenum);
         setInitialSeekTerm(new BytesRef(""));
       }
---------------
-------------
@@ -52,7 +52,7 @@
         // Add message ids that don't start with XJ here
         clientMessageIds.add(SQLState.NO_CURRENT_CONNECTION);
         clientMessageIds.add(SQLState.NOT_IMPLEMENTED);
-        clientMessageIds.add(SQLState.CANNOT_CLOSE_ACTIVE_XA_CONNECTION);
+clientMessageIds.add(SQLState.CANNOT_CLOSE_ACTIVE_CONNECTION);
         clientMessageIds.add(SQLState.XACT_SAVEPOINT_RELEASE_ROLLBACK_FAIL);
     }
 
---------------
-------------
@@ -70,7 +70,7 @@
   }
 
   public void testReverseDateSort() throws Exception {
-    IndexSearcher searcher = new IndexSearcher(directory);
+IndexSearcher searcher = new IndexSearcher(directory, true);
 
     // Create a Sort object.  reverse is set to true.
     // problem occurs only with SortField.AUTO:
---------------
-------------
@@ -57,7 +57,7 @@
   static class Writer extends DerefBytesWriterBase {
     public Writer(Directory dir, String id, Counter bytesUsed, IOContext context)
         throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context, Type.BYTES_VAR_DEREF);
       size = 0;
     }
     
---------------
-------------
@@ -143,7 +143,7 @@
   /* process a field value executing UIMA the CAS containing it as document text */
   private JCas processText(String textFieldValue) throws ResourceInitializationException,
           AnalysisEngineProcessException {
-    log.info(new StringBuffer("Analazying text").toString());
+log.info(new StringBuffer("Analyzing text").toString());
     /* get the UIMA analysis engine */
     AnalysisEngine ae = aeProvider.getAE();
 
---------------
-------------
@@ -92,7 +92,7 @@
     } else if (t.min >= UnicodeUtil.UNI_SUR_HIGH_START) {
       if (t.max > UnicodeUtil.UNI_SUR_LOW_END) {
         // after surrogates
-        code = 1+UnicodeUtil.UNI_SUR_LOW_END+r.nextInt(t.max-UnicodeUtil.UNI_SUR_LOW_END+1);
+code = 1+UnicodeUtil.UNI_SUR_LOW_END+r.nextInt(t.max-UnicodeUtil.UNI_SUR_LOW_END);
       } else {
         throw new IllegalArgumentException("transition accepts only surrogates: " + t);
       }
---------------
-------------
@@ -29,7 +29,7 @@
 {   
     @Test
     public void testMissingSubcolumn() {
-    	SuperColumn sc = new SuperColumn("sc1".getBytes(), LongType.instance, ClockType.Timestamp, new TimestampReconciler());
+SuperColumn sc = new SuperColumn("sc1".getBytes(), LongType.instance, ClockType.Timestamp, TimestampReconciler.instance);
     	sc.addColumn(new Column(getBytes(1), "value".getBytes(), new TimestampClock(1)));
     	assertNotNull(sc.getSubColumn(getBytes(1)));
     	assertNull(sc.getSubColumn(getBytes(2)));
---------------
-------------
@@ -468,7 +468,7 @@
 		}
 
 		boolean logBootTrace = PropertyUtil.getSystemBoolean(Property.LOG_BOOT_TRACE);
-		istream.println(LINE);
+logMsg(LINE);
 		logMsg("\n" + new Date() +
                 MessageService.getTextMessage(
                     MessageId.STORE_SHUTDOWN_MSG,
---------------
-------------
@@ -189,7 +189,7 @@
     private ServerSocket getServerSocket(InetAddress localEp) throws IOException, ConfigurationException
     {
         final ServerSocket ss;
-        if (DatabaseDescriptor.getEncryptionOptions().internode_encryption == EncryptionOptions.InternodeEncryption.all)
+if (DatabaseDescriptor.getEncryptionOptions() != null && DatabaseDescriptor.getEncryptionOptions().internode_encryption == EncryptionOptions.InternodeEncryption.all)
         {
             ss = SSLFactory.getServerSocket(DatabaseDescriptor.getEncryptionOptions(), localEp, DatabaseDescriptor.getStoragePort());
             // setReuseAddress happens in the factory.
---------------
-------------
@@ -52,7 +52,7 @@
     @Test
     public void testKSMetaDataSerialization() throws IOException 
     {
-        for (KSMetaData ksm : DatabaseDescriptor.tables_.values())
+for (KSMetaData ksm : DatabaseDescriptor.tables.values())
         {
             byte[] ser = KSMetaData.serialize(ksm);
             KSMetaData ksmDupe = KSMetaData.deserialize(new ByteArrayInputStream(ser));
---------------
-------------
@@ -202,7 +202,7 @@
     /** hook for JSVC */
     public void start()
     {
-        logger.info("Cassandra starting up...");
+logger.info("Listening for thrift clients...");
         Mx4jTool.maybeLoad();
         serverEngine.serve();
     }
---------------
-------------
@@ -80,7 +80,7 @@
 
             /* get the various column ranges we have to read */
             AbstractType comparator = ssTable.getColumnComparator();
-            SortedSet<IndexHelper.IndexInfo> ranges = new TreeSet<IndexHelper.IndexInfo>(IndexHelper.getComparator(comparator));
+SortedSet<IndexHelper.IndexInfo> ranges = new TreeSet<IndexHelper.IndexInfo>(IndexHelper.getComparator(comparator, false));
             for (byte[] name : filteredColumnNames)
             {
                 int index = IndexHelper.indexFor(name, indexList, comparator, false);
---------------
-------------
@@ -102,7 +102,7 @@
 
   /** Prints a user-readable version of this query. */
   public String toString (String s) {
-    return "filtered("+query.toString()+")";
+return "filtered("+query.toString(s)+")";
   }
 
   /** Returns true iff <code>o</code> is equal to this. */
---------------
-------------
@@ -436,7 +436,7 @@
         continue;
       
       // add new entry in PQ
-      st.term = new BytesRef(candidateTerm);
+st.term = BytesRef.deepCopyOf(candidateTerm);
       st.boost = boost;
       st.docfreq = df;
       st.termAsString = termAsString;
---------------
-------------
@@ -293,7 +293,7 @@
     Assert.assertTrue(runAndReturnSyserr().contains("NOTE: reproduce with:"));
   }
 
-  private String runAndReturnSyserr() throws Exception {
+private String runAndReturnSyserr() {
     JUnitCore.runClasses(Nested.class);
 
     String err = getSysErr();
---------------
-------------
@@ -300,7 +300,7 @@
       IndexInput in = null;
       try {
         in = cache.openInput(fileName, context);
-        in.copyBytes(out, in.length());
+out.copyBytes(in, in.length());
       } finally {
         IOUtils.close(in, out);
       }
---------------
-------------
@@ -1130,7 +1130,7 @@
           bottomSameReader = true;
           readerGen[bottomSlot] = currentReaderGen;
         } else {
-          final int index = termsIndex.lookupTerm(bottomValue, tempBR);
+final int index = termsIndex.lookupTerm(bottomValue);
           if (index < 0) {
             bottomOrd = -index - 2;
             bottomSameReader = false;
---------------
-------------
@@ -138,7 +138,7 @@
   }
 
   @Override
-  protected Analyzer getWrappedAnalyzer(String fieldName) {
+public final Analyzer getWrappedAnalyzer(String fieldName) {
     return delegate;
   }
 
---------------
-------------
@@ -86,7 +86,7 @@
     }
     writer.forceMerge(1);
     final DirectoryReader indexReader = writer.getReader();
-    writer.close();
+writer.shutdown();
 
     final AtomicReader reader = getOnlySegmentReader(indexReader);
     final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("parent", "true"))));
---------------
-------------
@@ -291,7 +291,7 @@
     );
     for (int i = 0; i < ALT_DOCS.length; i++) {
       Document doc = new Document();
-      doc.add(new Field("title", ALT_DOCS[i], TextField.TYPE_STORED));
+doc.add(new TextField("title", ALT_DOCS[i], Field.Store.YES));
       iw.addDocument(doc);
     }
     iw.forceMerge(1);
---------------
-------------
@@ -600,7 +600,7 @@
 
   public static void verifyEquals(Fields d1, Fields d2) throws IOException {
     if (d1 == null) {
-      assertTrue(d2 == null || d2.getUniqueFieldCount() == 0);
+assertTrue(d2 == null || d2.size() == 0);
       return;
     }
     assertTrue(d2 != null);
---------------
-------------
@@ -120,7 +120,7 @@
             return true;
 
     	/* Incoming port is assumed to be the Storage port. We need to change it to the control port */
-        EndPointState epState = Gossiper.instance.getEndPointStateForEndPoint(ep);
+EndPointState epState = Gossiper.instance.getEndpointStateForEndpoint(ep);
         return epState.isAlive();
     }
     
---------------
-------------
@@ -160,7 +160,7 @@
 
             /* Get serialized message to send to cluster */
             message = createMessage(Keyspace, key.toString(), CFName, columnFamilies);
-            for (EndPoint endpoint: StorageService.instance().getNStorageEndPoint(key.toString()))
+for (EndPoint endpoint: StorageService.instance().getReadStorageEndPoints(key.toString()))
             {
                 /* Send message to end point */
                 MessagingService.getMessagingInstance().sendOneWay(message, endpoint);
---------------
-------------
@@ -66,7 +66,7 @@
    */
   private static boolean isEquinox(String bundleClassName) 
   {
-    if (bundleClassName.startsWith("org.eclipse.equinox")) {
+if (bundleClassName.startsWith("org.eclipse.osgi")) {
       try {
         Class.forName("org.eclipse.osgi.framework.internal.core.BundleHost");
         return true;
---------------
-------------
@@ -94,7 +94,7 @@
   public void testSmallTokenInStream() throws Exception {
     input = new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader("abc de fgh"));
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.FRONT, 3, 3);
-    assertTokenStreamContents(tokenizer, new String[]{"abc","fgh"}, new int[]{0,0}, new int[]{3,3});
+assertTokenStreamContents(tokenizer, new String[]{"abc","fgh"}, new int[]{0,7}, new int[]{3,10});
   }
   
   public void testReset() throws Exception {
---------------
-------------
@@ -107,7 +107,7 @@
         // reduce rows from all sources into a single row
         ReducingIterator<IColumnIterator, Row> reduced = new ReducingIterator<IColumnIterator, Row>(collated)
         {
-            private final int gcBefore = (int) (System.currentTimeMillis() / 1000) - cfs.metadata.gcGraceSeconds;
+private final int gcBefore = (int) (System.currentTimeMillis() / 1000) - cfs.metadata.getGcGraceSeconds();
             private final List<IColumnIterator> colIters = new ArrayList<IColumnIterator>();
             private DecoratedKey key;
 
---------------
-------------
@@ -266,7 +266,7 @@
     }
   }
   
-  public DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException {
+public synchronized DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException {
     String[] tuple = parser.next();
     docData.clear();
     docData.setName(tuple[ID]);
---------------
-------------
@@ -258,7 +258,7 @@
     /**
      * Holds offset and length of the file chunk
      */
-    public class Chunk
+public static class Chunk
     {
         public final long offset;
         public final int length;
---------------
-------------
@@ -376,7 +376,7 @@
         ByteBuffer key = getKeyAsBytes(columnFamily, columnFamilySpec.getChild(1));
         int columnSpecCnt = CliCompiler.numColumnSpecifiers(columnFamilySpec);
         CfDef cfDef = getCfDef(columnFamily);
-        boolean isSuper = cfDef.comparator_type.equals("Super");
+boolean isSuper = cfDef.column_type.equals("Super");
         
         byte[] superColumnName = null;
         ByteBuffer columnName;
---------------
-------------
@@ -348,7 +348,7 @@
   }
 
   @Override
-  public void document(int docID, StoredFieldVisitor visitor) throws CorruptIndexException, IOException {
+public void document(int docID, StoredFieldVisitor visitor) throws IOException {
     ensureOpen();
     in.document(docID, visitor);
   }
---------------
-------------
@@ -85,7 +85,7 @@
       return lock.isLocked();
     }
 
-    public synchronized void release() {
+public synchronized void release() throws IOException {
       if (isLocked()) {
         verify((byte) 0);
         lock.release();
---------------
-------------
@@ -74,7 +74,7 @@
     conf.setOutputFormat(SequenceFileOutputFormat.class);
 
     client.setConf(conf);
-    FileSystem dfs = FileSystem.get(conf);
+FileSystem dfs = FileSystem.get(outPath.toUri(), conf);
     if (dfs.exists(outPath))
       dfs.delete(outPath, true);
     JobClient.runJob(conf);
---------------
-------------
@@ -158,7 +158,7 @@
     parsedUserQuery = null;
     String userQuery = getString();
     altUserQuery = null;
-    if( userQuery == null || userQuery.length() < 1 ) {
+if( userQuery == null || userQuery.trim().length() == 0 ) {
       // If no query is specified, we may have an alternate
       String altQ = solrParams.get( DisMaxParams.ALTQ );
       if (altQ != null) {
---------------
-------------
@@ -466,7 +466,7 @@
         if ( len < 0 ) {
             throw new SqlException(agent_.logWriter_,
                 new ClientMessageId(SQLState.BLOB_NONPOSITIVE_LENGTH),
-                new Integer(length));
+new Integer(len));
         }
         if (len == 0) {
             return 0;
---------------
-------------
@@ -66,6 +66,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new GreekAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new GreekAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -72,7 +72,7 @@
 	}
 	
 	/**
-		@exception StandardException Standard Cloudscape error policy
+@exception StandardException Standard Derby error policy
 	*/
 	protected void openCore() throws StandardException
 	{
---------------
-------------
@@ -35,7 +35,7 @@
     private static final Logger logger = LoggerFactory.getLogger(SizeTieredCompactionStrategy.class);
     protected static final long DEFAULT_MIN_SSTABLE_SIZE = 50L * 1024L * 1024L;
     protected static final String MIN_SSTABLE_SIZE_KEY = "min_sstable_size";
-    protected static long minSSTableSize;
+protected long minSSTableSize;
     protected volatile int estimatedRemainingTasks;
 
     public SizeTieredCompactionStrategy(ColumnFamilyStore cfs, Map<String, String> options)
---------------
-------------
@@ -1852,7 +1852,7 @@
     public DocsEnum reset(int[] docIDs, int[] freqs) {
       this.docIDs = docIDs;
       this.freqs = freqs;
-      upto = -1;
+docID = upto = -1;
       return this;
     }
 
---------------
-------------
@@ -30,7 +30,7 @@
 
     public CompressedSegmentedFile(String path, CompressionMetadata metadata)
     {
-        super(path, metadata.dataLength);
+super(path, metadata.dataLength, metadata.compressedFileLength);
         this.metadata = metadata;
     }
 
---------------
-------------
@@ -48,7 +48,7 @@
 		queryFactory = new QueryBuilderFactory();
 		queryFactory.addBuilder("TermQuery",new TermQueryBuilder());
 		queryFactory.addBuilder("BooleanQuery",new BooleanQueryBuilder(queryFactory));
-		queryFactory.addBuilder("UserQuery",new UserInputQueryBuilder(new QueryParser("contents", analyzer)));
+queryFactory.addBuilder("UserQuery",new UserInputQueryBuilder(parser));
 		queryFactory.addBuilder("FilteredQuery",new FilteredQueryBuilder(filterFactory,queryFactory));
 		queryFactory.addBuilder("ConstantScoreQuery",new ConstantScoreQueryBuilder(filterFactory));
 		
---------------
-------------
@@ -117,7 +117,7 @@
 
   protected synchronized ClassLoader getClassLoader(final Bundle clientBundle, Collection<Class<?>> classes) 
   {
-    if (clientBundle.getState() == Bundle.UNINSTALLED) {
+if (clientBundle != null && clientBundle.getState() == Bundle.UNINSTALLED) {
       throw new IllegalStateException(NLS.MESSAGES.getMessage("bundle.uninstalled", clientBundle.getSymbolicName(), clientBundle.getVersion(), clientBundle.getBundleId()));
     }
     
---------------
-------------
@@ -40,7 +40,7 @@
         
         try
         {
-            RowMutation rm = RowMutation.serializer().deserialize(new DataInputStream(buffer));
+RowMutation rm = RowMutation.serializer().deserialize(new DataInputStream(buffer), message.getVersion());
             rm.apply();
         }
         catch (IOException e)
---------------
-------------
@@ -350,7 +350,7 @@
                 Assert.assertFalse("query's last doc was "+ lastDoc[0] +" but skipTo("+(lastDoc[0]+1)+") got to "+scorer.docID(),more);
               }
             }
-            this.reader = reader;
+this.reader = lastReader[0] = reader;
             this.scorer = null;
             lastDoc[0] = -1;
           }
---------------
-------------
@@ -45,7 +45,7 @@
 
 
   public String getVersion() {
-    return "1";
+return "2";
   }
 
   public NamedList<Object> processResponse(Reader reader) {
---------------
-------------
@@ -96,7 +96,7 @@
 		}
 		
 		
-		MoreLikeThisQuery mlt=new MoreLikeThisQuery(DOMUtils.getText(e),fields,analyzer);
+MoreLikeThisQuery mlt=new MoreLikeThisQuery(DOMUtils.getText(e),fields,analyzer, fields[0]);
 		mlt.setMaxQueryTerms(DOMUtils.getAttribute(e,"maxQueryTerms",defaultMaxQueryTerms));
 		mlt.setMinTermFrequency(DOMUtils.getAttribute(e,"minTermFrequency",defaultMinTermFrequency));
 		mlt.setPercentTermsToMatch(DOMUtils.getAttribute(e,"percentTermsToMatch",defaultPercentTermsToMatch)/100);
---------------
-------------
@@ -21,7 +21,7 @@
 
 package org.apache.derby.iapi.services.io;
 /**
-  Cloudscape interface for identifying the format id for the
+Derby interface for identifying the format id for the
   stored form of an object. Objects of different classes may
   have the same format id if:
 
---------------
-------------
@@ -47,7 +47,7 @@
 
     public void applyModels() throws IOException
     {
-        String snapshotName = Table.getTimestampedSnapshotName(null);
+String snapshotName = Table.getTimestampedSnapshotName(name);
         CompactionManager.instance.getCompactionLock().lock();
         try
         {
---------------
-------------
@@ -64,7 +64,7 @@
         return memtable.getNamesIterator(cf, this);
     }
 
-    public ColumnIterator getSSTableColumnIterator(SSTableReader sstable) throws IOException
+public ColumnIterator getSSTableColumnIterator(SSTableReader sstable)
     {
         return new SSTableNamesIterator(sstable, key, columns);
     }
---------------
-------------
@@ -90,7 +90,7 @@
   
           public void writeRequest(OutputStream out) throws IOException {
             try {
-              OutputStreamWriter writer = new OutputStreamWriter( out );
+OutputStreamWriter writer = new OutputStreamWriter(out, "UTF-8");
               writer.append( "<stream>" ); // can be anything...
               UpdateRequest req = queue.poll( 250, TimeUnit.MILLISECONDS );
               while( req != null ) {
---------------
-------------
@@ -73,7 +73,7 @@
       MILLISECOND_FORMAT.setTimeZone(GMT);
     }
     
-    final Calendar calInstance = Calendar.getInstance(GMT);
+final Calendar calInstance = Calendar.getInstance(GMT, Locale.US);
   }
   
   private static final ThreadLocal<DateFormats> FORMATS = new ThreadLocal<DateFormats>() {
---------------
-------------
@@ -94,7 +94,7 @@
      */
 	public QueryTreeNode bind() throws StandardException
 	{
-        privileges = (PrivilegeNode) privileges.bind( new HashMap(), grantees);
+privileges = (PrivilegeNode) privileges.bind( new HashMap(), grantees, false);
         return this;
     } // end of bind
 
---------------
-------------
@@ -102,7 +102,7 @@
    */
   private StopFilter(Version matchVersion, boolean enablePositionIncrements, TokenStream input, Set<?> stopWords, boolean ignoreCase){
     super(input);
-    this.stopWords = CharArraySet.unmodifiableSet(new CharArraySet(matchVersion, stopWords, ignoreCase));
+this.stopWords = stopWords instanceof CharArraySet ? (CharArraySet)stopWords : new CharArraySet(matchVersion, stopWords, ignoreCase);
     this.enablePositionIncrements = enablePositionIncrements;
     termAtt = addAttribute(TermAttribute.class);
     posIncrAtt = addAttribute(PositionIncrementAttribute.class);
---------------
-------------
@@ -212,7 +212,7 @@
                 writer.append(partitioner.decorateKey(key), buffer);
             }
         }
-        SSTableReader ssTable = writer.closeAndOpenReader(DatabaseDescriptor.getKeysCachedFraction(table_));
+SSTableReader ssTable = writer.closeAndOpenReader();
         cfStore.onMemtableFlush(cLogCtx);
         cfStore.storeLocation(ssTable);
         buffer.close();
---------------
-------------
@@ -105,7 +105,7 @@
     private boolean ended;
     private final BytesRefFSTEnum<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> fstEnum;
 
-    public SimpleTextTermsEnum(FST<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> fst, IndexOptions indexOptions) throws IOException {
+public SimpleTextTermsEnum(FST<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> fst, IndexOptions indexOptions) {
       this.indexOptions = indexOptions;
       fstEnum = new BytesRefFSTEnum<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>>(fst);
     }
---------------
-------------
@@ -309,7 +309,7 @@
            + raf.toString());
       
       int nrBuffers = (int) (length / maxBufSize);
-      if (((long) nrBuffers * maxBufSize) < length) nrBuffers++;
+if (((long) nrBuffers * maxBufSize) <= length) nrBuffers++;
       
       this.buffers = new ByteBuffer[nrBuffers];
       this.bufSizes = new int[nrBuffers];
---------------
-------------
@@ -72,7 +72,7 @@
     doTest(true,true);
   }
   
-  private void doTest(boolean withCache, boolean plantWrongData) throws IOException, Exception {
+private void doTest(boolean withCache, boolean plantWrongData) throws Exception {
     Map<CategoryPath,Integer> truth = facetCountsTruth();
     CategoryPath cp = (CategoryPath) truth.keySet().toArray()[0]; // any category path will do for this test 
     CountFacetRequest frq = new CountFacetRequest(cp, 10);
---------------
-------------
@@ -300,7 +300,7 @@
         if (is == null)
             inFile = new BufferedReader(new FileReader(srcFile));
         else
-            inFile = new BufferedReader(new InputStreamReader(is));
+inFile = new BufferedReader(new InputStreamReader(is, "UTF-8"));
         outFile = new PrintWriter
         ( new BufferedWriter(new FileWriter(dstFile), 10000), true );
 
---------------
-------------
@@ -179,7 +179,7 @@
 	 *
 	 * @param nested true iff this is a nested data dictionary context.
 	 */
-	DataDictionaryContext pushDataDictionaryContext(ContextManager cm, boolean nested);
+DataDictionaryContext pushDataDictionaryContext(ContextManager cm);
 
 	/**
 	 * Clear all of the DataDictionary caches.
---------------
-------------
@@ -172,7 +172,7 @@
 
             sugword.string=hits.doc(i).get(F_WORD); // get orig word)
 
-            if (sugword.string==word) {
+if (sugword.string.equals(word)) {
                 continue; // don't suggest a word for itself, that would be silly
             }
 
---------------
-------------
@@ -26,7 +26,7 @@
 //
 // Reply implementations may update result set state via this interface.
 
-public interface ResultSetCallbackInterface {
+public interface ResultSetCallbackInterface extends UnitOfWorkListener {
     // The query was ended at the server because all rows have been retrieved.
     public void earlyCloseComplete(Sqlca sqlca);
 
---------------
-------------
@@ -222,7 +222,7 @@
           ZkController.downloadConfigDir(zkClient, confName, new File(confDir));
         } else if (line.getOptionValue(CMD).equals(LINKCONFIG)) {
           if (!line.hasOption(COLLECTION) || !line.hasOption(CONFNAME)) {
-            System.out.println("-" + CONFDIR + " and -" + CONFNAME
+System.out.println("-" + COLLECTION + " and -" + CONFNAME
                 + " are required for " + LINKCONFIG);
             System.exit(1);
           }
---------------
-------------
@@ -213,7 +213,7 @@
 		NodeFactory nodeFactory = getNodeFactory();
 		ContextManager cm = getContextManager();
 
-        QueryTreeNode trueNode = nodeFactory.getNode(
+QueryTreeNode trueNode = (QueryTreeNode) nodeFactory.getNode(
 											C_NodeTypes.BOOLEAN_CONSTANT_NODE,
 											Boolean.TRUE,
 											cm);
---------------
-------------
@@ -110,7 +110,7 @@
       NamedList fieldTerms = new NamedList();
       termsResult.add(field, fieldTerms);
 
-      Terms terms = lfields.terms(field);
+Terms terms = lfields == null ? null : lfields.terms(field);
       if (terms == null) {
         // no terms for this field
         continue;
---------------
-------------
@@ -655,7 +655,7 @@
   /** returns true if both sets have the same bits set */
   public boolean equals(Object o) {
     if (this == o) return true;
-    if (!(this instanceof OpenBitSet)) return false;
+if (!(o instanceof OpenBitSet)) return false;
     OpenBitSet a;
     OpenBitSet b = (OpenBitSet)o;
     // make a the larger set.
---------------
-------------
@@ -398,7 +398,7 @@
       writer.optimize();
       writer.close();
 
-      IndexReader reader = IndexReader.open(dir);
+IndexReader reader = IndexReader.open(dir, true);
 
       FaultyIndexInput.doFail = true;
 
---------------
-------------
@@ -66,7 +66,7 @@
     file.setLength(0);
   }
 
-  public void flushBuffer(byte[] src, int len) {
+public void flushBuffer(byte[] src, int len) throws IOException {
     byte[] buffer;
     int bufferPos = 0;
     while (bufferPos != len) {
---------------
-------------
@@ -122,7 +122,7 @@
   public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
     // we pull this before the seed intentionally: because its not consumed at runtime
     // (the skipInterval is written into postings header)
-    int skipInterval = _TestUtil.nextInt(seedRandom, 2, 64);
+int skipInterval = _TestUtil.nextInt(seedRandom, 2, 10);
     
     if (LuceneTestCase.VERBOSE) {
       System.out.println("MockRandomCodec: skipInterval=" + skipInterval);
---------------
-------------
@@ -147,7 +147,7 @@
     }
   }
 
-  private final void switchCurrentBuffer() throws IOException {
+private final void switchCurrentBuffer() {
     if (currentBufferIndex == file.numBuffers()) {
       currentBuffer = file.addBuffer(BUFFER_SIZE);
     } else {
---------------
-------------
@@ -962,7 +962,7 @@
   private void assertMatches(Searcher searcher, Query query, Sort sort,
       String expectedResult) throws IOException {
     //ScoreDoc[] result = searcher.search (query, null, 1000, sort).scoreDocs;
-    TopDocs hits = searcher.search (query, null, expectedResult.length(), sort);
+TopDocs hits = searcher.search (query, null, Math.max(1, expectedResult.length()), sort);
     ScoreDoc[] result = hits.scoreDocs;
     assertEquals(hits.totalHits, expectedResult.length());
     StringBuilder buff = new StringBuilder(10);
---------------
-------------
@@ -166,7 +166,7 @@
 
   /** @see #setFloorSegmentMB */
   public double getFloorSegmentMB() {
-    return floorSegmentBytes/1024*1024.;
+return floorSegmentBytes/(1024*1024.);
   }
 
   /** When forceMergeDeletes is called, we only merge away a
---------------
-------------
@@ -570,7 +570,7 @@
                             // we re-grab monitor on "this" (which recovery
                             // needs) and retry writeRAFHeader.
                             try {
-                                Thread.sleep(500); // 0.5s
+Thread.sleep(INTERRUPT_RETRY_SLEEP);
                             } catch (InterruptedException ee) {
                                 // This thread received an interrupt as
                                 // well, make a note.
---------------
-------------
@@ -40,7 +40,7 @@
 
     public static RangeCommand read(Message message) throws IOException
     {
-        byte[] bytes = (byte[]) message.getMessageBody()[0];
+byte[] bytes = message.getMessageBody();
         DataInputBuffer dib = new DataInputBuffer();
         dib.reset(bytes, bytes.length);
         return serializer.deserialize(new DataInputStream(dib));
---------------
-------------
@@ -118,7 +118,7 @@
         Object toReturn = null;
 
         // Added method to unwrap from the collaborator.
-        if (method.getName().equals("getWrappedObject")
+if (method.getName().equals("unwrapObject")
                 && method.getDeclaringClass() == WrapperedObject.class) {
             toReturn = object;
         } else
---------------
-------------
@@ -106,7 +106,7 @@
 
     public void doWork() throws Throwable {
       for (int i=0; i<100; i++)
-        (new IndexSearcher(directory)).close();
+(new IndexSearcher(directory, true)).close();
       count += 100;
     }
   }
---------------
-------------
@@ -193,7 +193,7 @@
             c2.close();
             fail();
         } catch (SQLException e) {
-            assertSQLState("58009", e);
+assertSQLState("8006", e);
         }
     }
 }
---------------
-------------
@@ -103,7 +103,7 @@
           newLength += bufferGrowthSize;
         }
         bytes = new byte[newLength];
-        System.arraycopy(tmp, 0, bytes, 0, pos);
+System.arraycopy(tmp, 0, bytes, 0, (maxRead >= pos) ? maxRead + 1 : pos);
       }
     }
 
---------------
-------------
@@ -178,7 +178,7 @@
     }
     writer.close();
 
-    IndexReader reader = IndexReader.open(dir);
+IndexReader reader = IndexReader.open(dir, false);
     reader.deleteDocuments(new Term("content", "aaa"));
     reader.close();
 
---------------
-------------
@@ -112,7 +112,7 @@
       writer.addDocument(new Document());
     }
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
   }
   
   @Override
---------------
-------------
@@ -40,7 +40,7 @@
       double t2 = Double.parseDouble(args[4]);
       double convergenceDelta = Double.parseDouble(args[5]);
       int maxIterations = Integer.parseInt(args[6]);
-      runJob(input, output, measureClass, convergenceDelta, t1, t2,
+runJob(input, output, measureClass, t1, t2, convergenceDelta,
           maxIterations);
     } else
       runJob("testdata", "output",
---------------
-------------
@@ -60,7 +60,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -163,7 +163,7 @@
         
         start = System.nanoTime();
         LOG.info("Optimizing Solr: Closing index writer");
-        writer.close();
+writer.shutdown();
         secs = (System.nanoTime() - start) / (float)(10^9);
         LOG.info("Optimizing Solr: Done closing index writer in {} secs", secs);
         context.setStatus("Done");
---------------
-------------
@@ -91,7 +91,7 @@
      return indexedId;
    }
 
-   public void setIndexedId(BytesRef idBytes) {
+public void setIndexedId(BytesRef indexedId) {
      this.indexedId = indexedId;
    }
 
---------------
-------------
@@ -347,7 +347,7 @@
     MockAnalyzer stdAnalyzer = new MockAnalyzer(random());
 
     public AnalyzerReturningNull() {
-      super(new PerFieldReuseStrategy());
+super(PER_FIELD_REUSE_STRATEGY);
     }
 
     @Override
---------------
-------------
@@ -76,7 +76,7 @@
         return new MappedFileDataInputMark(position);
     }
 
-    public int bytesPastMark(FileMark mark)
+public long bytesPastMark(FileMark mark)
     {
         assert mark instanceof MappedFileDataInputMark;
         assert position >= ((MappedFileDataInputMark) mark).position;
---------------
-------------
@@ -26,7 +26,7 @@
 import java.io.IOException;
 
 import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.io.SSTable;
+import org.apache.cassandra.io.sstable.SSTable;
 
 class PendingFile
 {
---------------
-------------
@@ -403,7 +403,7 @@
       }
 
       @Override
-      public NumericDocValues simpleNormValues(String field) {
+public NumericDocValues getNormValues(String field) {
         return null;
       }
 
---------------
-------------
@@ -99,7 +99,7 @@
 	}
 	
 	protected String getLocation() {
-		return provisionTo.getSubsystemId() + "@" + provisionTo.getSymbolicName() + "@" + ResourceHelper.getSymbolicNameAttribute(resource);
+return provisionTo.getLocation() + "!/" + ResourceHelper.getLocation(resource);
 	}
 	
 	protected boolean isContent() {
---------------
-------------
@@ -49,7 +49,7 @@
   @Override
   protected void setup(Context context) throws IOException, InterruptedException {
     super.setup(context);
-    Parameters params = Parameters.fromString(context.getConfiguration().get("pfp.parameters", ""));
+Parameters params = new Parameters(context.getConfiguration().get("pfp.parameters", ""));
     maxHeapSize = Integer.valueOf(params.get("maxHeapSize", "50"));
     
   }
---------------
-------------
@@ -75,7 +75,7 @@
     
     reader = iw.getReader();
     searcher = new IndexSearcher(reader);
-    iw.close();
+iw.shutdown();
   }
   
   @Override
---------------
-------------
@@ -130,7 +130,7 @@
       writer.addDocument(doc);
     }
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
 
     searcher = newSearcher(reader);
     searcher.setSimilarity(similarity);
---------------
-------------
@@ -67,7 +67,7 @@
 
 			if (firstTime) {
 				Logs.reportString("----------------------------------------------");
-				Logs.reportMessage("CSLOOK_Header", "jars");
+Logs.reportMessage("CSLOOK_JarsHeader");
 				Logs.reportMessage("CSLOOK_Jar_Note");
 				Logs.reportString("----------------------------------------------\n");
 			}
---------------
-------------
@@ -46,7 +46,7 @@
 
     // dataset
     // This is sensitive to the working directory where the test is run:
-    Path input = new Path("target/test-classes/wdbc");
+Path input = new Path(this.getClass().getResource("/wdbc/").getPath());
     CDMahoutEvaluator.initializeDataSet(input);
 
     // evaluate the rules
---------------
-------------
@@ -307,7 +307,7 @@
 
 		// In soft upgrade mode the plan may not be understand by this engine
 		// so force a recompile.
-		if (((DataDictionaryImpl) dd).readOnlyUpgrade) {
+if (dd.isReadOnlyUpgrade()) {
 			valid = false;
 		} else {
 			// 5th column is VALID (boolean)
---------------
-------------
@@ -1,6 +1,6 @@
 /*
  
-Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun
+Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun_Local_StateTest_part2
  
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -247,7 +247,7 @@
      * @lucene.internal */
     static final MergeState.CheckAbort NONE = new MergeState.CheckAbort(null, null) {
       @Override
-      public void work(double units) throws MergePolicy.MergeAbortedException {
+public void work(double units) {
         // do nothing
       }
     };
---------------
-------------
@@ -47,7 +47,7 @@
 
   An open b-tree contains fields and methods common to scans and controllers.
   <P>
-  <B>Concurrency Notes<\B>
+<B>Concurrency Notes</B>
   <P>
   An instance of an open b-tree is owned by a single context.  The b-tree code
   assumes that the context ensures that only one thread at a time is using
---------------
-------------
@@ -136,7 +136,7 @@
     List<VectorWritable> repPts = representativePoints.get(cI);
     GaussianAccumulator accumulator = new RunningSumsGaussianAccumulator();
     for (VectorWritable vw : repPts) {
-      accumulator.observe(vw.get(), 1);
+accumulator.observe(vw.get());
     }
     accumulator.compute();
     double d = accumulator.getAverageStd();
---------------
-------------
@@ -136,7 +136,7 @@
     String CONN_NETWORK_SERVER_CLASS_LOAD   = "J101"; // Cannot load the network server constructor
     String CONN_NETWORK_SERVER_START_EXCEPTION = "J102";
     String CONN_NETWORK_SERVER_SHUTDOWN_EXCEPTION = "J103";
-    String CONN_ALREADY_CLOSED                              = "J104";
+String OBJECT_CLOSED                                    = "J104";
     String CONN_PRECISION_TOO_LARGE                         = "J105";   
     //  following only used in text we print out - see client.am.SqlException:
     String BATCH_POSITION_ID                                = "J107";
---------------
-------------
@@ -154,7 +154,7 @@
 					associationState = TRO_FAIL;
 					if (SQLState.DEADLOCK.equals(se.getMessageId()))
 						rollbackOnlyCode = XAException.XA_RBDEADLOCK;
-					else if (SQLState.LOCK_TIMEOUT.equals(se.getMessageId()))
+else if (se.isLockTimeout())
 						rollbackOnlyCode = XAException.XA_RBTIMEOUT;					
 					else
 						rollbackOnlyCode = XAException.XA_RBOTHER;
---------------
-------------
@@ -28,7 +28,7 @@
 
 
   public enum CollectionAction {
-    CREATE, DELETE, RELOAD;
+CREATE, DELETE, RELOAD, SYNCSHARD;
     
     public static CollectionAction get( String p )
     {
---------------
-------------
@@ -40,7 +40,7 @@
   @Override
   public void setUp() throws Exception {
       super.setUp();
-      workDir = new File(TEMP_DIR, "TestMultiMMap");
+workDir = _TestUtil.getTempDir("TestMultiMMap");
       workDir.mkdirs();
   }
   
---------------
-------------
@@ -122,7 +122,7 @@
   // NOTE: we track per-segment version as a String with the "X.Y" format, e.g.
   // "4.0", "3.1", "3.0". Therefore when we change this constant, we should keep
   // the format.
-  public static final String LUCENE_MAIN_VERSION = ident("3.6.1");
+public static final String LUCENE_MAIN_VERSION = ident("3.6.2");
 
   public static final String LUCENE_VERSION;
   static {
---------------
-------------
@@ -202,7 +202,7 @@
     {
         assert cf != null;
         final boolean isSuper = cf.isSuper();
-        final Collection<IColumn> filteredColumns = filter.reversed ? filter.applyPredicate(cf.getReverseSortedColumns()) : filter.applyPredicate(cf.getSortedColumns());
+final Collection<IColumn> filteredColumns = filter.reversed ? cf.getReverseSortedColumns() : cf.getSortedColumns();
 
         // ok to not have subcolumnComparator since we won't be adding columns to this object
         IColumn startColumn = isSuper ? new SuperColumn(filter.start, null, cf.getClockType(), cf.getReconciler()) :  new Column(filter.start);
---------------
-------------
@@ -498,7 +498,7 @@
 
       final IndexReader reader = searcher.getIndexReader();
       if (reader.maxDoc() > 0) {
-        for (final AtomicReaderContext ctx : reader.getTopReaderContext().leaves()) {
+for (final AtomicReaderContext ctx : reader.leaves()) {
           Terms terms = ctx.reader().terms(F_WORD);
           if (terms != null)
             termsEnums.add(terms.iterator(null));
---------------
-------------
@@ -142,7 +142,7 @@
             if (termLevel == detailLevel || scanCell.isLeaf()) {
               //TODO should put more thought into implications of box vs point
               Shape cShape = termLevel == grid.getMaxLevels() ? scanCell.getCenter() : scanCell.getShape();
-              if(queryShape.relate(cShape, grid.getSpatialContext()) == SpatialRelation.DISJOINT)
+if(queryShape.relate(cShape) == SpatialRelation.DISJOINT)
                 continue;
 
               docsEnum = termsEnum.docs(acceptDocs, docsEnum, 0);
---------------
-------------
@@ -124,7 +124,7 @@
                 {
                     throw new RuntimeException(e);
                 }
-                StorageService.instance().updateTokenMetadataUnsafe(new BigIntegerToken(new BigInteger(values[0])), address);
+StorageService.instance().updateForeignTokenUnsafe(new BigIntegerToken(new BigInteger(values[0])), address);
             }
         }
         public void close()
---------------
-------------
@@ -160,7 +160,7 @@
     **/
     // System.out.println("segdels2:"+writer.docWriter.segmentDeletes.toString());
     //System.out.println("close");
-    writer.close();
+writer.shutdown();
     dir.close();
   }
 
---------------
-------------
@@ -89,7 +89,7 @@
     }
     
     reader.close();
-    writer.close();
+writer.shutdown();
     rmDir(dirPath);
   }
 
---------------
-------------
@@ -261,7 +261,7 @@
 
       while (termsEnum.next() != null) {
         String text = termsEnum.term().utf8ToString();
-        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, DocsEnum.FLAG_FREQS);
+docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);
         
         while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
           int docId = docs.docID();
---------------
-------------
@@ -280,7 +280,7 @@
   }
   
   @Override
-  protected boolean score(Collector collector, int max, int firstDocID) throws IOException {
+public boolean score(Collector collector, int max, int firstDocID) throws IOException {
     doc = firstDocID;
     collector.setScorer(this);
     while (doc < max) {
---------------
-------------
@@ -55,7 +55,7 @@
         this.range = range;
     }
 
-    public Message getMessage(int version)
+public Message getMessage(Integer version)
     {
         DataOutputBuffer dob = new DataOutputBuffer();
         try
---------------
-------------
@@ -53,7 +53,7 @@
       Split expected = ref.computeSplit(data, attr);
       Split actual = opt.computeSplit(data, attr);
 
-      assertEquals(expected.ig, actual.ig);
+assertEquals(expected.ig, actual.ig, 0.0000001);
       assertEquals(expected.split, actual.split);
     }
   }
---------------
-------------
@@ -109,7 +109,7 @@
 
     @Override
     public void setNextReader(AtomicReaderContext context) throws IOException {
-      fromDocTerms = FieldCache.DEFAULT.getTerms(context.reader(), field);
+fromDocTerms = FieldCache.DEFAULT.getTerms(context.reader(), field, false);
     }
   }
 
---------------
-------------
@@ -44,7 +44,7 @@
         // add data.  use relatively large values to force quick segment creation since we have a low flush threshold in the test config.
         for (int i = 0; i < 10; i++)
         {
-            rm = new RowMutation("Keyspace1", "key1");
+rm = new RowMutation("Keyspace1", "key1".getBytes());
             rm.add(new QueryPath("Standard1", null, "Column1".getBytes()), value, 0);
             rm.add(new QueryPath("Standard2", null, "Column1".getBytes()), value, 0);
             rm.apply();
---------------
-------------
@@ -38,7 +38,7 @@
  */
 public final class RecursiveBundleTracker  {
     private static final int COMPOSITE_BUNDLE_MASK =
-      Bundle.INSTALLED | Bundle.STARTING | Bundle.ACTIVE | Bundle.STOPPING;
+Bundle.INSTALLED | Bundle.RESOLVED | Bundle.STARTING | Bundle.ACTIVE | Bundle.STOPPING;
     
     private final BundleTracker tracker;
         
---------------
-------------
@@ -39,7 +39,7 @@
       }
       
       @Override
-      public void reset(Reader input) throws IOException {
+public void reset(Reader input) {
         delegate.reset(input);
       }
       
---------------
-------------
@@ -66,7 +66,7 @@
 
     final float[] scores = new float[4];
 
-    new IndexSearcher(store).search
+new IndexSearcher(store, true).search
       (new TermQuery(new Term("field", "word")),
        new Collector() {
          private int base = 0;
---------------
-------------
@@ -71,7 +71,7 @@
   }
 
   @Override
-  public void release() {
+public void close() {
     synchronized(locks) {
       locks.remove(lockName);
     }
---------------
-------------
@@ -45,7 +45,7 @@
       w.addDocument(doc);
     }
     IndexReader reader = w.getReader();
-    w.close();
+w.shutdown();
 
     IndexSearcher searcher = newSearcher(reader);
     int numDocs = reader.numDocs();
---------------
-------------
@@ -555,7 +555,7 @@
 			// invalidate all the procedures we need to indicate that
 			// any procedure we read off disk is automatically invalid,
 			// so we do not try to load the generated class.
-			bootingDictionary.readOnlyUpgrade = true;
+bootingDictionary.setReadOnlyUpgrade();
 		}
 
 		bootingDictionary.clearCaches();
---------------
-------------
@@ -378,7 +378,7 @@
         submitUserDefined(cfs, descriptors, getDefaultGcBefore(cfs));
     }
 
-    private Future<Object> submitUserDefined(final ColumnFamilyStore cfs, final Collection<Descriptor> dataFiles, final int gcBefore)
+Future<Object> submitUserDefined(final ColumnFamilyStore cfs, final Collection<Descriptor> dataFiles, final int gcBefore)
     {
         Callable<Object> callable = new Callable<Object>()
         {
---------------
-------------
@@ -24,7 +24,7 @@
 import java.util.zip.ZipFile;
 
 import org.apache.aries.util.filesystem.IFile;
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 
 public class NestedCloseableDirectory extends CloseableDirectory {
 
---------------
-------------
@@ -165,7 +165,7 @@
         }
 */
 
-        float avgPayloadScore = payloadScore / payloadsSeen;
+float avgPayloadScore =  (payloadsSeen > 0 ? (payloadScore / payloadsSeen) : 1);
         payloadBoost.setValue(avgPayloadScore);
         //GSI: I suppose we could toString the payload, but I don't think that would be a good idea 
         payloadBoost.setDescription("scorePayload(...)");
---------------
-------------
@@ -104,7 +104,7 @@
     doc.add(f1);
     doc.add(f2);
     w.addDocument(doc);
-    w.close();
+w.shutdown();
 
     IndexReader r = DirectoryReader.open(dir);
     Terms vector = r.getTermVectors(0).terms("field");
---------------
-------------
@@ -48,7 +48,7 @@
   public static Path[] listOutputFiles(FileSystem fs, Path outpath) throws IOException {
     Collection<Path> outpaths = Lists.newArrayList();
     for (FileStatus s : fs.listStatus(outpath, PathFilters.logsCRCFilter())) {
-      if (!s.isDir()) {
+if (!s.isDir() && !s.getPath().getName().startsWith("_")) {
         outpaths.add(s.getPath());
       }
     }
---------------
-------------
@@ -44,7 +44,7 @@
     }
 
     public TestBlueprintContainer(ComponentDefinitionRegistryImpl registry, ProxyManager proxyManager) throws Exception {
-        super(new TestBundleContext(), null, null, null, null, null, null, proxyManager);
+super(null, new TestBundleContext(), null, null, null, null, null, null, proxyManager);
         this.registry = registry;
         if (registry != null) {
             registry.registerComponentDefinition(new PassThroughMetadataImpl("blueprintContainer", this));
---------------
-------------
@@ -64,7 +64,7 @@
    * @param id the codec id within this segment
    * @param files the of files to add the codec files to.
    */
-  public abstract void files(Directory dir, SegmentInfo segmentInfo, int id, Set<String> files) throws IOException;
+public abstract void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException;
 
   @Override
   public String toString() {
---------------
-------------
@@ -728,7 +728,7 @@
 											null,
 											(TableDescriptor) null,
 											null,
-											0, 0, false);
+0, 0);
 			rc.setColumnDescriptor(null, cd);
 			rc.setVirtualColumnId(index + 1);
 		}
---------------
-------------
@@ -61,7 +61,7 @@
     this.semaphore.acquire();
   }
   
-  public int getMaxPermits() {
+public synchronized int getMaxPermits() {
     return maxPermits;
   }
   
---------------
-------------
@@ -60,7 +60,7 @@
     writer.addDocument(doc("solr", "solr is a very popular search server and is using lucene"));
     writer.addDocument(doc("nutch", "nutch is an internet search engine with web crawler and is using lucene and hadoop"));
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     searcher = newSearcher(reader);
   }
   
---------------
-------------
@@ -110,7 +110,7 @@
   }
   
   public DirectUpdateHandler2(SolrCore core, UpdateHandler updateHandler) {
-    super(core);
+super(core, updateHandler.getUpdateLog());
     solrCoreState = core.getSolrCoreState();
     
     UpdateHandlerInfo updateHandlerInfo = core.getSolrConfig()
---------------
-------------
@@ -178,7 +178,7 @@
     return simpleTag("optimize", args);
   }
 
-  private static String simpleTag(String tag, String... args) {
+public static String simpleTag(String tag, String... args) {
     try {
       StringWriter r = new StringWriter();
 
---------------
-------------
@@ -64,7 +64,7 @@
 
 			if (firstTime) {
 				Logs.reportString("----------------------------------------------");
-				Logs.reportMessage("CSLOOK_SchemasHeader");
+Logs.reportMessage("DBLOOK_SchemasHeader");
 				Logs.reportString("----------------------------------------------\n");
 			}
 
---------------
-------------
@@ -896,7 +896,7 @@
         //   w.close();
         // }
       } else {
-        assert sumTotalTermFreq == 0;
+assert sumTotalTermFreq == 0 || fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY && sumTotalTermFreq == -1;
         assert sumDocFreq == 0;
         assert docCount == 0;
       }
---------------
-------------
@@ -239,7 +239,7 @@
     if (directory.exists()) {
       String[] files = directory.list(IndexFileNameFilter.getFilter());            // clear old files
       if (files == null)
-        throw new IOException("Cannot read directory " + directory.getAbsolutePath());
+throw new IOException("cannot read directory " + directory.getAbsolutePath() + ": list() returned null");
       for (int i = 0; i < files.length; i++) {
         File file = new File(directory, files[i]);
         if (!file.delete())
---------------
-------------
@@ -39,7 +39,7 @@
     }
 
     public XAConnection getXAConnection() throws SQLException {
-        return getXAConnection(user, password);
+return getXAConnection(getUser(), getPassword());
     }
 
     public XAConnection getXAConnection(String user, String password) throws SQLException {
---------------
-------------
@@ -433,7 +433,7 @@
     private void doCleanupCompaction(ColumnFamilyStore cfs) throws IOException
     {
         Collection<SSTableReader> originalSSTables = cfs.getSSTables();
-        List<SSTableReader> sstables = doAntiCompaction(cfs, originalSSTables, StorageService.instance.getLocalRanges(), null);
+List<SSTableReader> sstables = doAntiCompaction(cfs, originalSSTables, StorageService.instance.getLocalRanges(cfs.getTable().name), null);
         if (!sstables.isEmpty())
         {
             cfs.replaceCompactedSSTables(originalSSTables, sstables);
---------------
-------------
@@ -249,7 +249,7 @@
         
         // Make a new dir that will enforce disk usage:
         MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));
-        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));
+writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy(false)));
         IOException err = null;
 
         MergeScheduler ms = writer.getConfig().getMergeScheduler();
---------------
-------------
@@ -185,7 +185,7 @@
         int iter = 0;
         while (iter < 50) {
             
-            traceit("-- " + iter++);
+println("-- " + iter++);
             
             // remove database directory so we can start fresh each time;
             // the memory leak also manifests when a different directory is
---------------
-------------
@@ -57,7 +57,7 @@
     }
     
     if (clusters.isEmpty()) {
-      throw new IllegalStateException("Cluster is empty!!!");
+throw new IllegalStateException("No clusters found. Check your -c path.");
     }
   }
 
---------------
-------------
@@ -95,7 +95,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -76,7 +76,7 @@
 		fast access to a list of candidate contexts. If one of the contexts has its activeThread
 		equal to the current thread then it is the current context manager.
 
-		If the thread has pushed multiple contexts (e.g. open a new non-nested Cloudscape connection
+If the thread has pushed multiple contexts (e.g. open a new non-nested Derby connection
 		from a server side method) then threadContextList will contain a Stack. The value for each cm
 		will be a push order, with higher numbers being more recently pushed.
 
---------------
-------------
@@ -56,7 +56,7 @@
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException
   {
-    final BinaryDocValues terms = cache.getTerms(readerContext.reader(), field, PackedInts.FAST);
+final BinaryDocValues terms = cache.getTerms(readerContext.reader(), field, false, PackedInts.FAST);
     final IndexReader top = ReaderUtil.getTopLevelContext(readerContext).reader();
     Terms t = MultiFields.getTerms(top, qfield);
     final TermsEnum termsEnum = t == null ? TermsEnum.EMPTY : t.iterator(null);
---------------
-------------
@@ -218,7 +218,7 @@
         writer.deleteDocuments(new Term(ID_FIELD, id.toString()));
       }
     }
-    writer.close();
+writer.shutdown();
   }
   
   @BeforeClass
---------------
-------------
@@ -1265,7 +1265,7 @@
                         for( Enumeration newKeys = otherList.keys(); newKeys.hasMoreElements() ;)
                         {
                             String key = (String) newKeys.nextElement();
-                            if( moduleList.contains( key))
+if (moduleList.containsKey(key))
                                 // RESOLVE how do we localize messages before we have finished initialization?
                                 report( "Ignored duplicate property " + key + " in " + modulesPropertiesURL.toString());
                             else
---------------
-------------
@@ -82,7 +82,7 @@
     writer.addDocument(new Document());
     writer.commit();
     IndexReader ir = writer.getReader();
-    writer.close();
+writer.shutdown();
     IndexSearcher searcher = newSearcher(ir);
     Weight fake = new TermQuery(new Term("fake", "weight")).createWeight(searcher);
     Scorer s = new SimpleScorer(fake);
---------------
-------------
@@ -44,7 +44,7 @@
     }
 
     public TestBlueprintContainer(ComponentDefinitionRegistryImpl registry, ProxyManager proxyManager) throws Exception {
-        super(new TestBundleContext(), null, null, null, null, null, proxyManager);
+super(new TestBundleContext(), null, null, null, null, null, null, proxyManager);
         this.registry = registry;
         if (registry != null) {
             registry.registerComponentDefinition(new PassThroughMetadataImpl("blueprintContainer", this));
---------------
-------------
@@ -79,7 +79,7 @@
       add(docText[i%docText.length], iw);
     }
     iw.close();
-    searcher = new IndexSearcher(directory);
+searcher = new IndexSearcher(directory, true);
 
     String qtxt = "one";
     for (int i = 0; i < docText.length; i++) {
---------------
-------------
@@ -269,7 +269,7 @@
 // TODO: this class could be created by wrapping
 // BlockTreeTermsDict around Lucene40PostingsBaseFormat; ie
 // we should not duplicate the code from that class here:
-public class Lucene40PostingsFormat extends PostingsFormat {
+public final class Lucene40PostingsFormat extends PostingsFormat {
 
   private final int minBlockSize;
   private final int maxBlockSize;
---------------
-------------
@@ -145,7 +145,7 @@
   }
 
   public void release() throws LockReleaseFailedException {
-    if (!lockFile.delete())
+if (lockFile.exists() && !lockFile.delete())
       throw new LockReleaseFailedException("failed to delete " + lockFile);
   }
 
---------------
-------------
@@ -57,7 +57,7 @@
         rm2 = serializeAndDeserializeReadMessage(rm);
         assert rm2.toString().equals(rm.toString());
 
-        rm = new SliceFromReadCommand("Table1", "row1", "foo", true, 2);
+rm = new SliceFromReadCommand("Table1", "row1", "foo", true, 0, 2);
         rm2 = serializeAndDeserializeReadMessage(rm);
         assert rm2.toString().equals(rm.toString());
         
---------------
-------------
@@ -30,7 +30,7 @@
 import java.util.HashMap;
 import java.util.Map;
 
-public class AbstractSerializationsTester extends SchemaLoader
+public class AbstractSerializationsTester extends CleanupHelper
 {
     protected static final String CUR_VER = System.getProperty("cassandra.version", "0.7");
     protected static final Map<String, Integer> VERSION_MAP = new HashMap<String, Integer> () 
---------------
-------------
@@ -1191,7 +1191,7 @@
     // refresh cluster state
     clusterState = zkStateReader.getClusterState();
     Slice tempSourceSlice = clusterState.getCollection(tempSourceCollectionName).getSlices().iterator().next();
-    Replica tempSourceLeader = clusterState.getLeader(tempSourceCollectionName, tempSourceSlice.getName());
+Replica tempSourceLeader = zkStateReader.getLeaderRetry(tempSourceCollectionName, tempSourceSlice.getName(), 60000);
 
     String tempCollectionReplica1 = tempSourceCollectionName + "_" + tempSourceSlice.getName() + "_replica1";
     String coreNodeName = waitForCoreNodeName(clusterState.getCollection(tempSourceCollectionName),
---------------
-------------
@@ -27,7 +27,7 @@
 
 import org.apache.derby.iapi.services.info.ProductGenusNames;
 import org.apache.derby.iapi.services.info.ProductVersionHolder;
-import org.apache.derby.shared.common.info.JVMInfo;
+import org.apache.derby.iapi.services.info.JVMInfo;
 public class Configuration {
 
 
---------------
-------------
@@ -41,7 +41,7 @@
  * <br>Example: <code>{!term f=weight}1.5</code>
  */
 public class TermQParserPlugin extends QParserPlugin {
-  public static String NAME = "term";
+public static final String NAME = "term";
 
   @Override
   public void init(NamedList args) {
---------------
-------------
@@ -34,7 +34,7 @@
 import org.apache.aries.util.filesystem.ICloseableDirectory;
 import org.apache.aries.util.filesystem.IDirectory;
 import org.apache.aries.util.filesystem.IFile;
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
---------------
-------------
@@ -189,7 +189,7 @@
     Directory directory;
 
     // First in a RAM directory:
-    directory = new MockDirectoryWrapper(new RAMDirectory());
+directory = new MockDirectoryWrapper(random, new RAMDirectory());
     runTest(directory);
     directory.close();
 
---------------
-------------
@@ -58,7 +58,7 @@
             writer.addDocument(doc);
         }
         writer.close();
-        indexSearcher = new IndexSearcher(directory);
+indexSearcher = new IndexSearcher(directory, false);
         indexReader = indexSearcher.getIndexReader();
 
 
---------------
-------------
@@ -243,7 +243,7 @@
       }
       //System.out.println("  skipFP=" + termState.skipFP);
     } else if (isFirstTerm) {
-      termState.skipFP = termState.bytesReader.readVLong();
+termState.skipFP = 0;
     }
   }
 
---------------
-------------
@@ -362,7 +362,7 @@
               "finishDocument".equals(trace[i].getMethodName())) {
             sawAbortOrFlushDoc = true;
           }
-          if ("merge".equals(trace[i])) {
+if ("merge".equals(trace[i].getMethodName())) {
             sawMerge = true;
           }
           if ("close".equals(trace[i].getMethodName())) {
---------------
-------------
@@ -281,7 +281,7 @@
     Field field = new Field(FIELD, content, fieldType);
     doc.add(field);
     writer.addDocument(doc);
-    writer.close();
+writer.shutdown();
     DirectoryReader ir = DirectoryReader.open(ramDir);
     IndexSearcher is = new IndexSearcher(ir);
       
---------------
-------------
@@ -492,7 +492,7 @@
         
         assertFalse(dmd.isCatalogAtStart()); 
         
-        assertFalse(dmd.locatorsUpdateCopy());
+assertTrue(dmd.locatorsUpdateCopy());
         
         assertTrue(dmd.usesLocalFilePerTable());
         assertTrue(dmd.usesLocalFiles());
---------------
-------------
@@ -81,7 +81,7 @@
   private void verifyDocFreq()
       throws IOException
   {
-      IndexReader reader = IndexReader.open(dir);
+IndexReader reader = IndexReader.open(dir, true);
       TermEnum termEnum = null;
 
     // create enumeration of all terms
---------------
-------------
@@ -231,7 +231,7 @@
     final int numDocumentsToIndex = 50 + random().nextInt(50);
     for (int i = 0; i < numThreads.length; i++) {
       AtomicInteger numDocs = new AtomicInteger(numDocumentsToIndex);
-      MockDirectoryWrapper dir = newDirectory();
+MockDirectoryWrapper dir = newMockDirectory();
       // mock a very slow harddisk sometimes here so that flushing is very slow
       dir.setThrottling(MockDirectoryWrapper.Throttling.SOMETIMES);
       IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT,
---------------
-------------
@@ -34,7 +34,7 @@
 		result.headers.put(ManifestVersionHeader.NAME, manifest.getManifestVersion());
 		Collection<Requirement> requirements = new ArrayList<Requirement>();
 		for (SubsystemContentHeader.Content content : manifest.getSubsystemContent().getContents()) {
-			Requirement requirement = OsgiIdentityRequirement.newInstance(content);
+Requirement requirement = new OsgiIdentityRequirement(content.getName(), content.getVersionRange(), content.getType(), false);
 			requirements.add(requirement);
 		}
 		// TODO This does not validate that all content bundles were found.
---------------
-------------
@@ -1680,7 +1680,7 @@
                  catch (SecurityException se)
                  {
                      throw StandardException.newException(
-                         SQLState.FILE_CANNOT_REMOVE_FILE, se, file, stub);
+SQLState.FILE_CANNOT_REMOVE_FILE, se, file, se.toString());
                  }
              }
 	
---------------
-------------
@@ -251,7 +251,7 @@
 
         message("  merge thread: done");
 
-      } catch (IOException exc) {
+} catch (Throwable exc) {
 
         if (merge != null) {
           merge.setException(exc);
---------------
-------------
@@ -39,7 +39,7 @@
  * 
  * @lucene.experimental
  */
-public class Floats {
+class Floats {
   
   protected static final String CODEC_NAME = "Floats";
   protected static final int VERSION_START = 0;
---------------
-------------
@@ -37,7 +37,7 @@
     }
   };
 
-  public static final Comparator termComparator = new Comparator() {
+public static final Comparator<Object> termComparator = new Comparator<Object>() {
     public int compare(Object o, Object o1) {
       return ((InstantiatedTerm)o).getTerm().compareTo((Term)o1);
     }
---------------
-------------
@@ -43,7 +43,7 @@
     @Test
     public void testTransferTable() throws Exception
     {
-        StorageService.instance().initServer();
+StorageService.instance.initServer();
 
         // write a temporary SSTable, but don't register it
         Set<String> content = new HashSet<String>();
---------------
-------------
@@ -227,7 +227,7 @@
     writer.addDocument(doc);
   }
 
-  private void checkInvariants(IndexWriter writer) throws IOException {
+private void checkInvariants(IndexWriter writer) {
     writer.waitForMerges();
     int maxBufferedDocs = writer.getConfig().getMaxBufferedDocs();
     int mergeFactor = ((LogMergePolicy) writer.getConfig().getMergePolicy()).getMergeFactor();
---------------
-------------
@@ -44,7 +44,7 @@
 
     // use as a kind of magic number to send ourselves a message indicating listening state
     protected static final int sentinelPort_ = 5555;
-    public static EndPoint sentinelLocalEndPoint_;
+protected static EndPoint sentinelLocalEndPoint_;
     
     static
     {
---------------
-------------
@@ -66,7 +66,7 @@
         try
         {
             String key = (String)(rowKey_.get());
-            ReadCommand readCommand = new SliceFromReadCommand(cfMetaData_.tableName, key, cfMetaData_.cfName, true, limit_);
+ReadCommand readCommand = new SliceFromReadCommand(cfMetaData_.tableName, key, cfMetaData_.cfName, true, offset_, limit_);
             row = StorageProxy.readProtocol(readCommand, StorageService.ConsistencyLevel.WEAK);
         }
         catch (Exception e)
---------------
-------------
@@ -301,7 +301,7 @@
 
     Directory dir = FSDirectory.open(tempIndexPath);
 
-    IndexWriterConfig iwc = new IndexWriterConfig(Version.LUCENE_46, indexAnalyzer);
+IndexWriterConfig iwc = new IndexWriterConfig(Version.LUCENE_CURRENT, indexAnalyzer);
     iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE);
     iwc.setRAMBufferSizeMB(ramBufferSizeMB);
     IndexWriter writer = new IndexWriter(dir, iwc);
---------------
-------------
@@ -18,7 +18,7 @@
 */
 package org.apache.cassandra.db;
 
-final class DBConstants
+class DBConstants
 {
 	public static final int boolSize_ = 1;
 	public static final int intSize_ = 4;
---------------
-------------
@@ -103,7 +103,7 @@
             return -1;
         }
         // We can only read as many bytes as there are in the stream.
-        int nonBlankLength = Math.min((int)remainingNonBlanks, length);
+int nonBlankLength = (int)Math.min(remainingNonBlanks, (long)length);
         fillBuffer(nonBlankLength);
         int read = 0;
         // Find position of next letter in the buffer.
---------------
-------------
@@ -308,7 +308,7 @@
       }
       result.append(userIDs[i]);
     }
-    if (result.length() > 3) {
+if (userIDs.length > 3) {
       result.append("...");
     }
     result.append(']');
---------------
-------------
@@ -64,7 +64,7 @@
       }
     }
     assertFalse(failed);
-    writer.close();
+writer.shutdown();
     dir.close();
 
   }
---------------
-------------
@@ -42,6 +42,6 @@
 
 public class Constants {
 
-  public static final String VERSION = "8.1.0";
+public static final String VERSION = "8.2.0";
 
 }
---------------
-------------
@@ -226,7 +226,7 @@
       private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
       
       @Override
-      public boolean incrementToken() throws IOException {
+public boolean incrementToken() {
         if (index == tokens.length) {
           return false;
         } else {
---------------
-------------
@@ -827,7 +827,7 @@
         props.put(ZkStateReader.CORE_NAME_PROP, cname);
         props.put(ZkStateReader.NODE_NAME_PROP, zkController.getNodeName());
         
-        boolean success = syncStrategy.sync(zkController, core, new ZkNodeProps(props));
+boolean success = syncStrategy.sync(zkController, core, new ZkNodeProps(props), true);
         // solrcloud_debug
         if (log.isDebugEnabled()) {
           try {
---------------
-------------
@@ -40,7 +40,7 @@
   }
   
   public void setGramSize(int gramSize) {
-    set("gramSize", Integer.toBinaryString(gramSize));
+set("gramSize", Integer.toString(gramSize));
   }
   
   public void setMinSupport(int minSupport) {
---------------
-------------
@@ -74,7 +74,7 @@
 
 			if (firstTime) {
 				Logs.reportString("----------------------------------------------");
-				Logs.reportMessage("CSLOOK_TriggersHeader");
+Logs.reportMessage("DBLOOK_TriggersHeader");
 				Logs.reportString("----------------------------------------------\n");
 			}
 
---------------
-------------
@@ -72,7 +72,7 @@
   private final int incs[] = new int[]{1, 0, 1, 0};
 
   @Override
-  public boolean incrementToken() throws IOException {
+public boolean incrementToken() {
     if (nextTokenIndex < tokenCount) {
       termAtt.setEmpty().append(terms[nextTokenIndex]);
       offsetAtt.setOffset(starts[nextTokenIndex], ends[nextTokenIndex]);
---------------
-------------
@@ -640,7 +640,7 @@
 
       // check if docs have been submitted since the commit started
       if( lastAddedTime > started ) {
-        if( docsSinceCommit > docsUpperBound ) {
+if( docsUpperBound > 0 && docsSinceCommit > docsUpperBound ) {
           pending = scheduler.schedule( this, 100, TimeUnit.MILLISECONDS );
         }
         else if( timeUpperBound > 0 ) {
---------------
-------------
@@ -121,7 +121,7 @@
                 break;
 
             // only count live columns towards the `count` criteria
-            if (!column.isMarkedForDelete()
+if (column.isLive()
                 && (!container.isMarkedForDelete()
                     || column.mostRecentLiveChangeAt() > container.getMarkedForDeleteAt()))
             {
---------------
-------------
@@ -127,7 +127,7 @@
     while (recommendationVectorIterator.hasNext()) {
       Vector.Element element = recommendationVectorIterator.next();
       int index = element.index();
-      if (userVector.get(index) != 0.0) {
+if (userVector.get(index) == 0.0) {
         if (topItems.size() < recommendationsPerUser) {
           indexItemIDMap.get(new IntWritable(index), itemID);
           topItems.add(new GenericRecommendedItem(itemID.get(), (float) element.get()));
---------------
-------------
@@ -43,7 +43,7 @@
 import org.slf4j.LoggerFactory;
 
 public class ZkTestServer {
-  public static final int TICK_TIME = 3000;
+public static final int TICK_TIME = 1000;
 
   private static Logger log = LoggerFactory.getLogger(ZkTestServer.class);
   
---------------
-------------
@@ -135,7 +135,7 @@
     System.out.println();
     sampleData = Lists.newArrayList();
     RAMDirectory directory = new RAMDirectory();
-    IndexWriter writer = new IndexWriter( directory, new IndexWriterConfig(Version.LUCENE_41,new StandardAnalyzer(Version.LUCENE_41)));
+IndexWriter writer = new IndexWriter( directory, new IndexWriterConfig(Version.LUCENE_42,new StandardAnalyzer(Version.LUCENE_42)));
 
     FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);
     customType.setStoreTermVectors(true);
---------------
-------------
@@ -57,7 +57,7 @@
 
         try
         {
-            GossipDigestAckMessage gDigestAckMessage = GossipDigestAckMessage.serializer().deserialize(dis);
+GossipDigestAckMessage gDigestAckMessage = GossipDigestAckMessage.serializer().deserialize(dis, message.getVersion());
             List<GossipDigest> gDigestList = gDigestAckMessage.getGossipDigestList();
             Map<InetAddress, EndpointState> epStateMap = gDigestAckMessage.getEndpointStateMap();
 
---------------
-------------
@@ -145,7 +145,7 @@
 
   private class SpanQueue extends PriorityQueue<Spans> {
     public SpanQueue(int size) {
-      initialize(size);
+super(size);
     }
 
     @Override
---------------
-------------
@@ -47,7 +47,7 @@
 
 		try {
 
-			createTestDatabase();
+createTestDatabase(dbCreationScript_1);
 
 			// Don't let error stream ruin the diff.
 			System.err.close();
---------------
-------------
@@ -110,7 +110,7 @@
 	 * and then it is turned off while we process the query underlying the view
 	 * v1.             
 	 */
-	boolean isPrivilegeCollectionRequired = true;
+private boolean isPrivilegeCollectionRequired = true;
 
 	/**
 	 * Set the ContextManager for this node.
---------------
-------------
@@ -113,7 +113,7 @@
                 {
                     MessageProducer prod = new MessageProducer()
                     {
-                        public Message getMessage(int version) throws IOException
+public Message getMessage(Integer version) throws IOException
                         {
                             return makeGossipDigestSynMessage(gDigests, version);
                         }
---------------
-------------
@@ -74,7 +74,7 @@
         private boolean moreInclude = true;
 
         private Spans excludeSpans = exclude.getSpans(reader);
-        private boolean moreExclude = true;
+private boolean moreExclude = excludeSpans.next();
 
         public boolean next() throws IOException {
           if (moreInclude)                        // move to next include
---------------
-------------
@@ -86,7 +86,7 @@
     assertTrue(deleteReader != null);
     assertTrue(deleteReader.numDocs() == 1);
     deleteReader.deleteDocument(0);
-    assertTrue(deleteReader.isDeleted(0) == true);
+assertTrue(deleteReader.getDeletedDocs().get(0));
     assertTrue(deleteReader.hasDeletions() == true);
     assertTrue(deleteReader.numDocs() == 0);
     deleteReader.close();
---------------
-------------
@@ -34,7 +34,7 @@
     for (int i = 0; i < probes; i++) {
       int n = hashForProbe(originalForm, data.size(), name, i);
       if(isTraceEnabled()){
-        trace(name, n);                
+trace((String) null, n);
       }
       data.set(n, data.get(n) + getWeight(originalForm,weight));
     }
---------------
-------------
@@ -150,7 +150,7 @@
         builder = getQueryBuilder(clazz);
 
         if (builder == null) {
-          Class<?>[] classes = node.getClass().getInterfaces();
+Class<?>[] classes = clazz.getInterfaces();
 
           for (Class<?> actualClass : classes) {
             builder = getQueryBuilder(actualClass);
---------------
-------------
@@ -157,7 +157,7 @@
         builder = getQueryBuilder(clazz);
 
         if (builder == null) {
-          Class<?>[] classes = node.getClass().getInterfaces();
+Class<?>[] classes = clazz.getInterfaces();
 
           for (Class<?> actualClass : classes) {
             builder = getQueryBuilder(actualClass);
---------------
-------------
@@ -38,7 +38,7 @@
     protected UpdateColumnFamily() { }
     
     /** assumes validation has already happened. That is, replacing oldCfm with newCfm is neither illegal or totally whackass. */
-    public UpdateColumnFamily(org.apache.cassandra.db.migration.avro.CfDef cf_def) throws ConfigurationException, IOException
+public UpdateColumnFamily(org.apache.cassandra.avro.CfDef cf_def) throws ConfigurationException, IOException
     {
         super(UUIDGen.makeType1UUIDFromHost(FBUtilities.getLocalAddress()), DatabaseDescriptor.getDefsVersion());
         
---------------
-------------
@@ -167,7 +167,7 @@
   /**
    * (re)Builds the spelling index.  May be a NOOP if the implementation doesn't require building, or can't be rebuilt.
    */
-  public abstract void build(SolrCore core, SolrIndexSearcher searcher);
+public abstract void build(SolrCore core, SolrIndexSearcher searcher) throws IOException;
   
   /**
    * Get the value of {@link SpellingParams#SPELLCHECK_ACCURACY} if supported.  
---------------
-------------
@@ -54,7 +54,7 @@
     field.setStringValue("jumps over extremely very lazy broxn dog");
     iw.addDocument(doc);
     reader = iw.getReader();
-    iw.close();
+iw.shutdown();
     searcher = newSearcher(reader);
   }
   
---------------
-------------
@@ -174,7 +174,7 @@
     return doReopen(false);
   }
     
-  protected IndexReader doReopen(boolean doClone) throws CorruptIndexException, IOException {
+private IndexReader doReopen(boolean doClone) throws CorruptIndexException, IOException {
     ensureOpen();
     
     boolean reopened = false;
---------------
-------------
@@ -135,7 +135,7 @@
         {
             // table.cf['key']
         	List<column_t> columns = new ArrayList<column_t>();
-      		columns = thriftClient_.get_slice(tableName, key, columnFamily, true, 1000000);
+columns = thriftClient_.get_slice(tableName, key, columnFamily, true, 0, 1000000);
             int size = columns.size();
             for (Iterator<column_t> colIter = columns.iterator(); colIter.hasNext(); )
             {
---------------
-------------
@@ -756,7 +756,7 @@
             // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?
             // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)
             // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?
-            docsEnum = termsEnum.docs(null, docsEnum, false);
+docsEnum = termsEnum.docs(null, docsEnum, 0);
             c=0;
 
             if (docsEnum instanceof MultiDocsEnum) {
---------------
-------------
@@ -80,7 +80,7 @@
 
     JobConf jobConf = new JobConf(RecommenderJob.class);
 
-    FileSystem fs = FileSystem.get(jobConf);
+FileSystem fs = FileSystem.get(outputPathPath.toUri(), jobConf);
     if (fs.exists(outputPathPath)) {
       fs.delete(outputPathPath, true);
     }
---------------
-------------
@@ -64,7 +64,7 @@
     //-----------------------event callback methods-------------------------------
 
     public void listenToUnitOfWork() {
-        agent_.connection_.CommitAndRollbackListeners_.add(this);
+agent_.connection_.CommitAndRollbackListeners_.put(this,null);
     }
 
     public void completeLocalCommit(java.util.Iterator listenerIterator) {
---------------
-------------
@@ -62,7 +62,7 @@
       mayMerge.set(segmentCount > maxNumSegments);
       writer.forceMerge(maxNumSegments);
     }
-    writer.close();
+writer.shutdown();
     dir.close();
   }
   
---------------
-------------
@@ -1174,7 +1174,7 @@
 				(rootConnection == this) && 
 				(!autoCommit && !transactionIsIdle())) {
 			throw newSQLException(
-				SQLState.LANG_INVALID_TRANSACTION_STATE);
+SQLState.CANNOT_CLOSE_ACTIVE_CONNECTION);
 		}
 		
 		close(exceptionClose);
---------------
-------------
@@ -40,7 +40,7 @@
         super.setUp();
 
         // create test index
-        final IndexWriter writer = new IndexWriter(mDirectory, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
+final IndexWriter writer = new IndexWriter(mDirectory, new StandardAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
         addDocument(writer, "A", "Should we, could we, would we?");
         addDocument(writer, "B", "It should.  Should it?");
         addDocument(writer, "C", "It shouldn't.");
---------------
-------------
@@ -723,7 +723,7 @@
       // KeepOnlyLastCommitDeleter:
       IndexFileDeleter deleter = new IndexFileDeleter(directory,
                                                       deletionPolicy == null ? new KeepOnlyLastCommitDeletionPolicy() : deletionPolicy,
-                                                      segmentInfos, null, codecs);
+segmentInfos, null, codecs, null);
       segmentInfos.updateGeneration(deleter.getLastSegmentInfos());
       segmentInfos.changed();
 
---------------
-------------
@@ -71,7 +71,7 @@
 public class TestReplicationHandler extends SolrTestCaseJ4 {
 
 
-  private static final String CONF_DIR = "." + File.separator + "solr"
+private static final String CONF_DIR = "solr"
       + File.separator + "collection1" + File.separator + "conf"
       + File.separator;
 
---------------
-------------
@@ -63,7 +63,7 @@
      */
     public static Test suite() {
         if (JDBC.vmSupportsJSR169()) {
-            return new TestSuite("empty: client not supported on JSR169");
+return new TestSuite("empty: client not supported on JSR169; procs use DriverMgr");
         }
         else {
             return TestConfiguration.defaultSuite(
---------------
-------------
@@ -201,7 +201,7 @@
    * Sugar for <code>.getIndexReader().document(docID, fieldsToLoad)</code>
    * @see IndexReader#document(int, Set) 
    */
-  public final StoredDocument document(int docID, Set<String> fieldsToLoad) throws IOException {
+public StoredDocument doc(int docID, Set<String> fieldsToLoad) throws IOException {
     return reader.document(docID, fieldsToLoad);
   }
 
---------------
-------------
@@ -64,7 +64,7 @@
 	
 	
 	/**
-		@exception StandardException Standard Cloudscape error policy
+@exception StandardException Standard Derby error policy
 	*/
 	protected void openCore() throws StandardException
 	{
---------------
-------------
@@ -40,7 +40,7 @@
     
     public void doVerb(Message message)
     {          
-        byte[] body = (byte[])message.getMessageBody()[0];
+byte[] body = message.getMessageBody();
         DataInputBuffer buffer = new DataInputBuffer();
         buffer.reset(body, body.length);        
         
---------------
-------------
@@ -160,7 +160,7 @@
     float boost = (float) dboost;
     IndexSearcher s = new IndexSearcher(dir, true);
     FieldScoreQuery qValSrc = new FieldScoreQuery(field,tp); // a query that would score by the field
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, TEXT_FIELD,anlzr); 
+QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, TEXT_FIELD,anlzr);
     String qtxt = "first aid text"; // from the doc texts in FunctionQuerySetup.
     
     // regular (boolean) query.
---------------
-------------
@@ -33,7 +33,7 @@
 import org.apache.derby.iapi.error.PublicAPI;
 
 /**
-  This utility class provides static methods for managing user authorization in a Cloudscape database.
+This utility class provides static methods for managing user authorization in a Derby database.
   
    <p>This class can only be used within an SQL-J statement, a Java procedure or a server side Java method.
    <p>This class can be accessed using the class alias <code> USERUTILITY </code> in SQL-J statements.
---------------
-------------
@@ -103,7 +103,7 @@
         {
             byte[] combined = HintedHandOffManager.makeCombinedName(rm.getTable(), cf.metadata().cfName);
             QueryPath path = new QueryPath(HintedHandOffManager.HINTS_CF, rm.key(), combined);
-            add(path, ArrayUtils.EMPTY_BYTE_ARRAY, new TimestampClock(System.currentTimeMillis()));
+add(path, ArrayUtils.EMPTY_BYTE_ARRAY, new TimestampClock(System.currentTimeMillis()), DatabaseDescriptor.getGcGraceInSeconds());
         }
     }
 
---------------
-------------
@@ -45,7 +45,7 @@
 
 public class LeveledManifest
 {
-    private static final Logger logger = LoggerFactory.getLogger(LeveledCompactionStrategy.class);
+private static final Logger logger = LoggerFactory.getLogger(LeveledManifest.class);
 
     public static final String EXTENSION = ".json";
 
---------------
-------------
@@ -76,7 +76,7 @@
         ColumnFamily.serializer().serializeWithIndexes(cf, buffer);               
         entries.put(ByteBufferUtil.bytes("k3"), ByteBuffer.wrap(Arrays.copyOf(buffer.getData(), buffer.getLength())));
         
-        SSTableReader orig = SSTableUtils.writeRawSSTable("Keyspace1", "Indexed1", entries);        
+SSTableReader orig = SSTableUtils.prepare().cf("Indexed1").writeRaw(entries);
         // whack the index to trigger the recover
         FileUtils.deleteWithConfirm(orig.descriptor.filenameFor(Component.PRIMARY_INDEX));
         FileUtils.deleteWithConfirm(orig.descriptor.filenameFor(Component.FILTER));
---------------
-------------
@@ -42,6 +42,6 @@
 
 public class Constants {
 
-  public static final String VERSION = "10.0.0";
+public static final String VERSION = "11.0.0";
 
 }
---------------
-------------
@@ -428,7 +428,7 @@
       if (last != null) {
         assertTrue(last.compareTo(cur) < 0);
       }
-      last = new BytesRef(cur);
+last = BytesRef.deepCopyOf(cur);
     } 
     // LUCENE-3314: the results after next() already returned null are undefined,
     // assertNull(termEnum.next());
---------------
-------------
@@ -50,7 +50,7 @@
         // Response is been managed by the map so the waitlist size really doesnt matter.
         super(blockFor, table);
         this.blockFor = new AtomicInteger(blockFor);
-        endpointsnitch = (DatacenterEndPointSnitch) DatabaseDescriptor.getEndPointSnitch();
+endpointsnitch = (DatacenterEndPointSnitch) DatabaseDescriptor.getEndpointSnitch();
         localEndpoint = FBUtilities.getLocalAddress();
     }
 
---------------
-------------
@@ -316,7 +316,7 @@
                     String[] peices = FBUtilities.strip(sourceFile.getName(), "-");
                     String newFileName = fileNames.get( peices[1] + "-" + peices[2] );
                     
-                    String file = new String(DatabaseDescriptor.getDataFileLocation() + System.getProperty("file.separator") + newFileName + "-Data.db");
+String file = DatabaseDescriptor.getDataFileLocation() + File.separator + newFileName + "-Data.db";
                     logger_.debug("Received Data from  : " + message.getFrom() + " " + streamContext.getTargetFile() + " " + file);
                     streamContext.setTargetFile(file);
                     addStreamContext(message.getFrom().getHost(), streamContext, streamStatus);                                            
---------------
-------------
@@ -44,7 +44,7 @@
 
   @Override
   public boolean equals(Object other) {
-    return other != null && VarLongWritable.class.equals(other.getClass()) && ((VarLongWritable) other).value == value;
+return other != null && getClass().equals(other.getClass()) && ((VarLongWritable) other).value == value;
   }
 
   @Override
---------------
-------------
@@ -521,7 +521,7 @@
 						bindTime - parseTime, //bind time
 						optimizeTime - bindTime, //optimize time
 						generateTime - optimizeTime, //generate time
-						getElapsedTimeMillis(beginTime),
+generateTime - beginTime, //total compile time
 						beginTimestamp,
 						endTimestamp);
 				}
---------------
-------------
@@ -87,7 +87,7 @@
 		System.out.println("creating");
 		s.executeUpdate("CREATE TABLE atable (a INT, b LONG VARCHAR FOR BIT DATA)");
 		conn.commit();
-		java.io.File file = new java.io.File("short.txt");
+java.io.File file = new java.io.File("short.utf");
 		int fileLength = (int) file.length();
 
 		// first, create an input stream
---------------
-------------
@@ -32,7 +32,7 @@
 public class RangeSliceVerbHandler implements IVerbHandler
 {
 
-    private static final Logger logger = LoggerFactory.getLogger(RangeSliceVerbHandler.class);
+private static final Logger logger = LoggerFactory.getLogger(IndexScanVerbHandler.class);
 
     public void doVerb(Message message)
     {
---------------
-------------
@@ -42,7 +42,7 @@
     for (int i = 0; i < probes; i++) {
       int n = hashForProbe(originalForm, data.size(), name, i);
       if(isTraceEnabled()){
-        trace(name.getBytes(Charsets.UTF_8), n);        
+trace((String) null, n);
       }
       data.set(n, data.get(n) + getWeight(originalForm,weight));
     }
---------------
-------------
@@ -100,7 +100,7 @@
 		out.println("        <TR>");
 		out.println("         <TD align=\"left\" class=\"topbardiv\" nowrap=\"\">");
 		out
-				.println("          <A href=\"http://aries.apache.org/aries/\" title=\"Apache Aries \">");
+.println("          <A href=\"http://aries.apache.org/\" title=\"Apache Aries \">");
 		out
 				.println("          <IMG border=\"0\" src=\"images/Arieslogo_Horizontal.gif\">");
 		out.println("          </A>");
---------------
-------------
@@ -60,7 +60,7 @@
     outputDir.delete();
     tmpDir = getTestTempDir("tmp");
 
-    conf = new Configuration();
+conf = getConfiguration();
     // reset as we run all tests in the same JVM
     SharingMapper.reset();
   }
---------------
-------------
@@ -68,7 +68,7 @@
     }
     
     // FIXME: could use method in ThriftValidation
-    static ColumnFamilyType validateColumnFamily(String keyspace, String columnFamily) throws InvalidRequestException
+public static ColumnFamilyType validateColumnFamily(String keyspace, String columnFamily) throws InvalidRequestException
     {
         if (columnFamily.isEmpty())
             throw newInvalidRequestException("non-empty columnfamily is required");
---------------
-------------
@@ -506,7 +506,7 @@
     SolrZkClient zkClient  = new SolrZkClient(address, TIMEOUT);
     ZkStateReader reader = new ZkStateReader(zkClient);
     LeaderElector overseerElector = new LeaderElector(zkClient);
-    ElectionContext ec = new OverseerElectionContext(address, zkClient, reader);
+ElectionContext ec = new OverseerElectionContext(address.replaceAll("/", "_"), zkClient, reader);
     overseerElector.setup(ec);
     overseerElector.joinElection(ec);
     return zkClient;
---------------
-------------
@@ -71,6 +71,6 @@
     @Override
     public String toString()
     {
-        return "#<User %s groups=%s>".format(username, groups);
+return String.format("#<User %s groups=%s>", username, groups);
     }
 }
---------------
-------------
@@ -48,7 +48,7 @@
         super(stream); 
         SocketChannel socketChannel = stream.getStream();
         InetSocketAddress remoteAddress = (InetSocketAddress)socketChannel.socket().getRemoteSocketAddress();
-        String remoteHost = remoteAddress.getHostName();        
+String remoteHost = remoteAddress.getAddress().getHostAddress();
         streamContext_ = StreamContextManager.getStreamContext(remoteHost);   
         streamStatus_ = StreamContextManager.getStreamStatus(remoteHost);
     }
---------------
-------------
@@ -312,7 +312,7 @@
   private class MyMergeScheduler extends MergeScheduler {
     @Override
     synchronized public void merge(IndexWriter writer)
-      throws CorruptIndexException, IOException {
+throws IOException {
 
       while(true) {
         MergePolicy.OneMerge merge = writer.getNextMerge();
---------------
-------------
@@ -685,7 +685,7 @@
                   throw new RuntimeException("term " + term + ": doc " + doc + ": pos " + pos + " < lastPos " + lastPos);
                 }
                 lastPos = pos;
-                if (postings.getPayloadLength() != 0) {
+if (postings.hasPayload()) {
                   postings.getPayload();
                 }
               }
---------------
-------------
@@ -98,7 +98,7 @@
     Reader reader = null;
     try {
       reader = IOUtils.getDecodingReader(aClass.getResourceAsStream(resource), IOUtils.CHARSET_UTF_8);
-      return WordlistLoader.getWordSet(reader, comment, new CharArraySet(Version.LUCENE_31, 16, ignoreCase));
+return WordlistLoader.getWordSet(reader, comment, new CharArraySet(Version.LUCENE_CURRENT, 16, ignoreCase));
     } finally {
       IOUtils.close(reader);
     }
---------------
-------------
@@ -130,7 +130,7 @@
     }
     r.close();
 
-    w.close();
+w.shutdown();
 
     dir.close();
   }
---------------
-------------
@@ -51,6 +51,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new BasqueAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new BasqueAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -53,7 +53,7 @@
     
     assertNull(MultiFields.getTermPositionsEnum(reader, null, "foo", new BytesRef("test")));
     
-    DocsEnum de = _TestUtil.docs(random(), reader, "foo", new BytesRef("test"), null, null, true);
+DocsEnum de = _TestUtil.docs(random(), reader, "foo", new BytesRef("test"), null, null, DocsEnum.FLAG_FREQS);
     while (de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
       assertEquals(2, de.freq());
     }
---------------
-------------
@@ -52,7 +52,7 @@
         super.bytesPerChar = BYTES_PER_CHAR;
         EmbedStatement embStmt = (EmbedStatement)createStatement();
         EmbedConnection embCon =(EmbedConnection)getConnection();
-        iClob = new TemporaryClob(embCon.getDBName(), embStmt);
+iClob = new TemporaryClob(embStmt);
         transferData(
             new LoopingAlphabetReader(CLOBLENGTH, CharAlphabet.cjkSubset()),
             iClob.getWriter(1L),
---------------
-------------
@@ -2720,7 +2720,7 @@
                     agent_.logWriter_.traceEntry(this, "setFetchSize", rows);
                 }
                 checkForClosedResultSet("setFetchSize");
-                if (rows < 0 || (maxRows_ != 0 && rows > maxRows_)) {
+if (rows < 0) {
                     throw new SqlException(agent_.logWriter_, 
                         new ClientMessageId(SQLState.INVALID_FETCH_SIZE),
                         rows).getSQLException();
---------------
-------------
@@ -33,7 +33,7 @@
         map.put("org.apache.cassandra.db.marshal.AsciiType", JdbcAscii.instance);
         map.put("org.apache.cassandra.db.marshal.BooleanType", JdbcBoolean.instance);
         map.put("org.apache.cassandra.db.marshal.BytesType", JdbcBytes.instance);
-        map.put("org.apache.cassandra.db.marshal.ColumnCounterType", JdbcCounterColumn.instance);
+map.put("org.apache.cassandra.db.marshal.CounterColumnType", JdbcCounterColumn.instance);
         map.put("org.apache.cassandra.db.marshal.DateType", JdbcDate.instance);
         map.put("org.apache.cassandra.db.marshal.DecimalType", JdbcDecimal.instance);
         map.put("org.apache.cassandra.db.marshal.DoubleType", JdbcDouble.instance);
---------------
-------------
@@ -822,7 +822,7 @@
      */
     private void schedule()
     {
-        requestScheduler.queue(Thread.currentThread(), clientState.getSchedulingId());
+requestScheduler.queue(Thread.currentThread(), clientState.getSchedulingValue());
     }
 
     /**
---------------
-------------
@@ -1053,7 +1053,7 @@
             if (r instanceof AtomicReader) {
               r = new FCInvisibleMultiReader(new AssertingAtomicReader((AtomicReader)r));
             } else if (r instanceof DirectoryReader) {
-              r = new FCInvisibleMultiReader(new AssertingDirectoryReader((DirectoryReader)r));
+r = new FCInvisibleMultiReader((DirectoryReader)r);
             }
             break;
           default:
---------------
-------------
@@ -481,7 +481,7 @@
 
   @Override
   public Query getHighlightQuery() throws ParseException {
-    return parsedUserQuery;
+return parsedUserQuery == null ? altUserQuery : parsedUserQuery;
   }
 
   public void addDebugInfo(NamedList<Object> debugInfo) {
---------------
-------------
@@ -26,7 +26,7 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.*;
+import static org.apache.lucene.analysis.VocabularyAssert.*;
 
 /**
  * Simple tests for {@link GermanMinimalStemFilter}
---------------
-------------
@@ -447,7 +447,7 @@
                     "segments_3",
                     "segments.gen"};
 
-        String[] actual = dir.list();
+String[] actual = dir.listAll();
         Arrays.sort(expected);
         Arrays.sort(actual);
         if (!Arrays.equals(expected, actual)) {
---------------
-------------
@@ -32,7 +32,7 @@
     this.text = text;
   }
 
-  public <T> T newInstance(String cname, Class<T> expectedType, String... subpackages) {
+public <T> T newInstance(String cname, Class<T> expectedType) {
     return null;
   }
 
---------------
-------------
@@ -1136,7 +1136,7 @@
         s.executeUpdate("CREATE TABLE APP.NAMES(ID INT, NAME VARCHAR(30))");
 
         
-        s.executeUpdate("CREATE TRIGGER  APP.MYTRIG AFTER DELETE ON APP.TAB REFERENCING OLD_TABLE AS OLDROWS FOR EACH STATEMENT INSERT INTO APP.LOG(i,name,deltime) SELECT OLDROWS.I, NAMES.NAME, CURRENT_TIMESTAMP FROM --DERBY-PROPERTIES joinOrder=FIXED\n NAMES, OLDROWS --DERBY-PROEPERTIES joinStrategy = NESTEDLOOP\n WHERE (OLDROWS.i = NAMES.ID) AND (1 = 1)");
+s.executeUpdate("CREATE TRIGGER  APP.MYTRIG AFTER DELETE ON APP.TAB REFERENCING OLD_TABLE AS OLDROWS FOR EACH STATEMENT INSERT INTO APP.LOG(i,name,deltime) SELECT OLDROWS.I, NAMES.NAME, CURRENT_TIMESTAMP FROM --DERBY-PROPERTIES joinOrder=FIXED\n NAMES, OLDROWS --DERBY-PROPERTIES joinStrategy = NESTEDLOOP\n WHERE (OLDROWS.i = NAMES.ID) AND (1 = 1)");
         
         s.executeUpdate("insert into APP.tab values(1)");
         s.executeUpdate("insert into APP.tab values(2)");
---------------
-------------
@@ -425,7 +425,7 @@
         binaryMemtable_.get().put(key, buffer);
     }
 
-    void forceFlush() throws IOException
+public void forceFlush()
     {
         memtable_.get().forceflush();
     }
---------------
-------------
@@ -39,7 +39,7 @@
   public static final class LookupPriorityQueue extends PriorityQueue<LookupResult> {
     
     public LookupPriorityQueue(int size) {
-      initialize(size);
+super(size);
     }
 
     @Override
---------------
-------------
@@ -52,6 +52,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new CzechAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new CzechAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -117,7 +117,7 @@
       double clVal = calculatePrior(next) * calculateLikelihood(tokenizedDoc, next);
       if (clVal > max) {
         max = clVal;
-        foundClass = next.clone();
+foundClass = BytesRef.deepCopyOf(next);
       }
     }
     return new ClassificationResult<BytesRef>(foundClass, max);
---------------
-------------
@@ -111,7 +111,7 @@
         return new TokenStreamComponents(tokenizer, new ReverseStringFilter(TEST_VERSION_CURRENT, tokenizer));
       }
     };
-    checkRandomData(random(), a, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -118,7 +118,7 @@
     idFieldType = idField!=null ? idField.getType() : null;
     parseEventListeners();
     initLog();
-    if (!core.getDirectoryFactory().isPersistent()) {
+if (!core.isReloaded() && !core.getDirectoryFactory().isPersistent()) {
       try {
         clearLog();
       } catch (IOException e) {
---------------
-------------
@@ -165,7 +165,7 @@
     FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
     FieldPhraseList fpl = new FieldPhraseList( stack, fq );
     assertEquals( 1, fpl.phraseList.size() );
-    assertEquals( "d(1.0)((6,7))", fpl.phraseList.get( 0 ).toString() );
+assertEquals( "d(1.0)((9,10))", fpl.phraseList.get( 0 ).toString() );
   }
   
   public void test1PhraseLongMV() throws Exception {
---------------
-------------
@@ -823,7 +823,7 @@
             // locks on the table, otherwise the purge will fail with a self
             // deadlock.
             nested_xact = (TransactionManager) 
-                xact_manager.startNestedUserTransaction(false);
+xact_manager.startNestedUserTransaction(false, true);
 
             // now open the table in a nested user transaction so that each
             // page worth of work can be committed after it is done.
---------------
-------------
@@ -295,7 +295,7 @@
                                 }
                             }
                         };
-                        timeoutFuture = executors.schedule(r, 10, TimeUnit.SECONDS);
+timeoutFuture = executors.schedule(r, timeout, TimeUnit.MILLISECONDS);
                         state = State.WaitForInitialReferences;
                         break;
                     case WaitForInitialReferences:
---------------
-------------
@@ -56,7 +56,7 @@
             0, // We don't care about the bloom filter
             metadata,
             StorageService.getPartitioner(),
-            ReplayPosition.NONE);
+SSTableMetadata.createCollector());
     }
 
     // find available generation and pick up filename from that
---------------
-------------
@@ -18,7 +18,7 @@
 
  */
 
-package org.apache.derby.iapi.reference;
+package org.apache.derby.shared.common.reference;
 
 /**
 	This class contains message identifiers for
---------------
-------------
@@ -121,7 +121,7 @@
             StreamOutSession outsession = StreamOutSession.create(tableName, dst, callback);
             StreamOut.transferSSTables(outsession, sstables, ranges, OperationType.AES);
             // request ranges from the remote node
-            StreamIn.requestRanges(dst, tableName, ranges, callback, OperationType.AES);
+StreamIn.requestRanges(dst, tableName, Collections.singleton(cfstore), ranges, callback, OperationType.AES);
         }
         catch(Exception e)
         {
---------------
-------------
@@ -73,7 +73,7 @@
 
 			if (firstTime) {
 				Logs.reportString("----------------------------------------------");
-				Logs.reportMessage("CSLOOK_Header", "triggers");
+Logs.reportMessage("CSLOOK_TriggersHeader");
 				Logs.reportString("----------------------------------------------\n");
 			}
 
---------------
-------------
@@ -112,7 +112,7 @@
         {10, 1, 3, 1}, // 10.1.3.1 (Jun 30, 2006 / SVN 417277)
         {10, 2, 1, 6}, // 10.2.1.6 (Oct 02, 2006 / SVN 452058)
         {10, 2, 2, 0}, // 10.2.2.0 (Dec 12, 2006 / SVN 485682)
-        {10, 3, 1, 4}, // 10.3.1.4 (Aug 30, 2007 / SVN 571336)
+{10, 3, 1, 4}, // 10.3.1.4 (Aug 1, 2007 / SVN 561794)
     };
 
     /**
---------------
-------------
@@ -84,7 +84,7 @@
 
     private final static String STORAGE_CONF_FILE = "cassandra.yaml";
 
-    private static final UUID INITIAL_VERSION = new UUID(4096, 0); // has type nibble set to 1, everything else to zero.
+public static final UUID INITIAL_VERSION = new UUID(4096, 0); // has type nibble set to 1, everything else to zero.
     private static UUID defsVersion = INITIAL_VERSION;
 
     /**
---------------
-------------
@@ -35,7 +35,7 @@
 
   private final String name;
   private final HashPartitioner.Range range;
-  private final Integer replicationFactor;
+private final Integer replicationFactor;      // FUTURE: optional per-slice override of the collection replicationFactor
   private final Map<String,Replica> replicas;
   private final Replica leader;
 
---------------
-------------
@@ -820,7 +820,7 @@
         riw.commit();
       }
     }
-    riw.close();
+riw.shutdown();
     checkFiles(dir);
     dir.close();
   }
---------------
-------------
@@ -226,7 +226,7 @@
   private Map<SegmentInfoPerCommit,Boolean> segmentsToMerge = new HashMap<SegmentInfoPerCommit,Boolean>();
   private int mergeMaxNumSegments;
 
-  protected Lock writeLock;
+private Lock writeLock;
 
   private volatile boolean closed;
   private volatile boolean closing;
---------------
-------------
@@ -64,7 +64,7 @@
  FilteredTermEnum enumeration. MultiTermQuery is not designed to be used by 
  itself. The reason being that it is not intialized with a FilteredTermEnum 
  enumeration. A FilteredTermEnum enumeration needs to be provided. For example,
- WildcardQuery and FuzzyQuery extends MultiTermQuery to provide WildcardTermEnum
+WildcardQuery and FuzzyQuery extend MultiTermQuery to provide WildcardTermEnum
  and FuzzyTermEnum respectively. */
 public class MultiTermQuery extends Query {
     private Term term;
---------------
-------------
@@ -48,7 +48,7 @@
     // Whenever IW opens readers, eg for merging, we have to
     // keep terms order in UTF16:
 
-    return new Lucene3xFields(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.termsIndexDivisor) {
+return new Lucene3xFields(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.termsIndexDivisor) {
       @Override
       protected boolean sortTermsByUnicode() {
         // We carefully peek into stack track above us: if
---------------
-------------
@@ -41,7 +41,7 @@
  *
  * @see org.apache.lucene.index.IndexDeletionPolicy
  */
-public class SolrDeletionPolicy implements IndexDeletionPolicy, NamedListInitializedPlugin {
+public class SolrDeletionPolicy extends IndexDeletionPolicy implements NamedListInitializedPlugin {
   public static Logger log = LoggerFactory.getLogger(SolrCore.class);
 
   private String maxCommitAge = null;
---------------
-------------
@@ -121,7 +121,7 @@
     assertNotNull(similar);
     assertEquals(2, similar.length);
     assertEquals(2, similar[0]);
-    assertEquals(4, similar[1]);
+assertEquals(3, similar[1]);
   }
 
   @Test
---------------
-------------
@@ -54,7 +54,7 @@
                 if ( (jvmName == null) || (jvmName.length()==0) )
                     jvmName = "jdk13";
                 else if (jvmName.startsWith("jdk13"))
-                    jvmName = "jdk31";
+jvmName = "jdk13";
 
 				javavm = jvm.getJvm(jvmName);
                 if (javaCmd != null)
---------------
-------------
@@ -56,7 +56,7 @@
     // this allows to easily identify a matching (exact) phrase 
     // when all PhrasePositions have exactly the same position.
     for (int i = 0; i < tps.length; i++) {
-      PhrasePositions pp = new PhrasePositions(tps[i], offsets[i]);
+PhrasePositions pp = new PhrasePositions(tps[i], offsets[i], i);
       if (last != null) {			  // add next to end of list
         last.next = pp;
       } else {
---------------
-------------
@@ -109,7 +109,7 @@
     this.docStoreIsCompoundFile = docStoreIsCompoundFile;
     this.hasProx = hasProx;
     delCount = 0;
-    assert docStoreOffset == -1 || docStoreSegment != null;
+assert docStoreOffset == -1 || docStoreSegment != null: "dso=" + docStoreOffset + " dss=" + docStoreSegment + " docCount=" + docCount;
   }
 
   /**
---------------
-------------
@@ -1352,7 +1352,7 @@
                 sessionState.out.printf("      Row cache size / save period in seconds: %s/%s%n", cf_def.row_cache_size, cf_def.row_cache_save_period_in_seconds);
                 sessionState.out.printf("      Key cache size / save period in seconds: %s/%s%n", cf_def.key_cache_size, cf_def.key_cache_save_period_in_seconds);
                 sessionState.out.printf("      Memtable thresholds: %s/%s/%s (millions of ops/minutes/MB)%n",
-                                cf_def.memtable_operations_in_millions, cf_def.memtable_throughput_in_mb, cf_def.memtable_flush_after_mins);
+cf_def.memtable_operations_in_millions, cf_def.memtable_flush_after_mins, cf_def.memtable_throughput_in_mb);
                 sessionState.out.printf("      GC grace seconds: %s%n", cf_def.gc_grace_seconds);
                 sessionState.out.printf("      Compaction min/max thresholds: %s/%s%n", cf_def.min_compaction_threshold, cf_def.max_compaction_threshold);
                 sessionState.out.printf("      Read repair chance: %s%n", cf_def.read_repair_chance);
---------------
-------------
@@ -60,7 +60,7 @@
         Range r = new Range(partitioner.getToken("0"), partitioner.getToken("zzzzzzz"));
         ranges.add(r);
 
-        boolean result = store.doAntiCompaction(ranges, new EndPoint("127.0.0.1", 9150), fileList);
+boolean result = store.forceCompaction(ranges, new EndPoint("127.0.0.1", 9150), 0, fileList);
 
         assertEquals(true, result); // some keys should have qualified
         assertEquals(true, fileList.size() >= 3); //Data, index, filter files
---------------
-------------
@@ -292,7 +292,7 @@
 		}
 		if (SanityManager.DEBUG)
 			trace("Ending connection thread");
-		server.getThreadList().removeElement(this);
+server.removeThread(this);
 
 	}
 	/**
---------------
-------------
@@ -381,7 +381,7 @@
 
       FastIDSet result = new FastIDSet();
       while (rs.next()) {
-        result.add(rs.getLong(1));
+result.add(rs.getLong(2));
       }
 
       if (result.isEmpty()) {
---------------
-------------
@@ -197,7 +197,7 @@
   }
 
   private void assertNoPrx(Directory dir) throws Throwable {
-    final String[] files = dir.list();
+final String[] files = dir.listAll();
     for(int i=0;i<files.length;i++)
       assertFalse(files[i].endsWith(".prx"));
   }
---------------
-------------
@@ -68,7 +68,7 @@
 		}
                 // need to make a guess so we copy text files to local encoding
                 // on non-ascii systems...
-                if ((fileName.indexOf("sql") > 0) || (fileName.indexOf("txt") > 0) || (fileName.indexOf(".view") > 0))
+if ((fileName.indexOf("sql") > 0) || (fileName.indexOf("txt") > 0) || (fileName.indexOf(".view") > 0) || (fileName.indexOf(".multi") > 0) || (fileName.indexOf(".properties") > 0))
                 {
                     BufferedReader inFile = new BufferedReader(new InputStreamReader(is, "UTF-8"));
                     PrintWriter pw = new PrintWriter
---------------
-------------
@@ -81,7 +81,7 @@
          return valueSource.getValues(context, readerContext).getRangeScorer(readerContext.reader(), lowerVal, upperVal, includeLower, includeUpper);
        }
        @Override
-       public Bits bits() throws IOException {
+public Bits bits() {
          return null;  // don't use random access
        }
      }, acceptDocs);
---------------
-------------
@@ -886,7 +886,7 @@
             // memtable keys: current and historical
             Iterator<Memtable> memtables = (Iterator<Memtable>) IteratorUtils.chainedIterator(
                     IteratorUtils.singletonIterator(cfs.getMemtable()),
-                    MemtableManager.instance().getUnflushedMemtables(cfName).iterator());
+ColumnFamilyStore.getUnflushedMemtables(cfName).iterator());
             while (memtables.hasNext())
             {
                 iterators.add(IteratorUtils.filteredIterator(memtables.next().sortedKeyIterator(), new Predicate()
---------------
-------------
@@ -61,7 +61,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    Configuration conf = new Configuration();
+Configuration conf = getConfiguration();
     fs = FileSystem.get(conf);
   }
 
---------------
-------------
@@ -41,7 +41,7 @@
   @Test
   public void testSFVW() throws Exception {
     Path path = getTestTempFilePath("sfvw");
-    Configuration conf = new Configuration();
+Configuration conf = getConfiguration();
     FileSystem fs = FileSystem.get(conf);
     SequenceFile.Writer seqWriter = new SequenceFile.Writer(fs, conf, path, LongWritable.class, VectorWritable.class);
     SequenceFileVectorWriter writer = new SequenceFileVectorWriter(seqWriter);
---------------
-------------
@@ -1293,7 +1293,7 @@
         assertTrue(reader.numDocs() > 0);
         SegmentInfos sis = new SegmentInfos();
         sis.read(dir);
-        for(AtomicReaderContext context : reader.getTopReaderContext().leaves()) {
+for(AtomicReaderContext context : reader.leaves()) {
           assertFalse(context.reader().getFieldInfos().hasVectors());
         }
         reader.close();
---------------
-------------
@@ -92,7 +92,7 @@
 
     Group group = gbuilder.withName("Options").withOption(seqOpt).withOption(outputOpt).withOption(
             dictTypeOpt).withOption(dictOpt).withOption(csvOpt).withOption(vectorAsKeyOpt).withOption(
-            printKeyOpt).withOption(sizeOpt).create();
+printKeyOpt).withOption(sizeOpt).withOption(helpOpt).create();
 
     try {
       Parser parser = new Parser();
---------------
-------------
@@ -180,7 +180,7 @@
         		{"XJ004","Database '{0}' not found.","40000"},
         		{"XJ015","Derby system shutdown.","50000"},
         		{"XJ028","The URL '{0}' is not properly formed.","40000"},
-        		{"XJ040","Failed to start database '{0}', see the next exception for details.","40000"},
+{"XJ040","Failed to start database '{0}' with class loader {1}, see the next exception for details.","40000"},
         		{"XJ041","Failed to create database '{0}', see the next exception for details.","40000"},
                 {"XJ048","Conflicting boot attributes specified: {0}","40000"},
         		{"XJ049","Conflicting create attributes specified.","40000"},
---------------
-------------
@@ -110,7 +110,7 @@
  *
  */
 public class DisMaxQParserPlugin extends QParserPlugin {
-  public static String NAME = "dismax";
+public static final String NAME = "dismax";
 
   @Override
   public void init(NamedList args) {
---------------
-------------
@@ -109,7 +109,7 @@
                                        new BytesRef("field"),
                                        MultiFields.getLiveDocs(mergedReader),
                                        null,
-                                       0);
+false);
     assertTrue(termDocs != null);
     assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
 
---------------
-------------
@@ -99,7 +99,7 @@
                    msg.contains(name));
       }
     } finally {
-      w.close();
+w.shutdown();
     }
   }
 }
---------------
-------------
@@ -250,7 +250,7 @@
             stream = new DataInputStream(new BufferedInputStream(new FileInputStream(descriptor.filenameFor(Component.FILTER))));
             if (descriptor.usesOldBloomFilter)
             {
-                bf = LegacyBloomFilter.serializer().deserialize(stream);
+bf = LegacyBloomFilter.serializer().deserialize(stream, 0); // version means nothing.
             }
             else
             {
---------------
-------------
@@ -109,7 +109,7 @@
     if (s.length() < DATE_LEN) {
       StringBuffer sb = new StringBuffer(s);
       while (sb.length() < DATE_LEN)
-        sb.insert(0, ' ');
+sb.insert(0, 0);
       s = sb.toString();
     }
 
---------------
-------------
@@ -270,7 +270,7 @@
     if (libDir != null) {
       File f = FileUtils.resolvePath(new File(dir), libDir);
       log.info("loading shared library: " + f.getAbsolutePath());
-      loader.addToClassLoader(libDir);
+loader.addToClassLoader(libDir, null, false);
       loader.reloadLuceneSPI();
     }
 
---------------
-------------
@@ -45,7 +45,7 @@
     blockShift = Integer.numberOfTrailingZeros(blockSize);
     blockMask = blockSize - 1;
     final int numBlocks = (int) (valueCount / blockSize) + (valueCount % blockSize == 0 ? 0 : 1);
-    if (numBlocks * blockSize < valueCount) {
+if ((long) numBlocks * blockSize < valueCount) {
       throw new IllegalArgumentException("valueCount is too large for this block size");
     }
     long[] minValues = null;
---------------
-------------
@@ -370,7 +370,7 @@
                 Assert.assertFalse("query's last doc was "+ lastDoc[0] +" but skipTo("+(lastDoc[0]+1)+") got to "+scorer.docID(),more);
               }
             }
-            this.reader = reader;
+this.reader = lastReader[0] = reader;
             this.scorer = null;
             lastDoc[0] = -1;
           }
---------------
-------------
@@ -96,7 +96,7 @@
 		}
 		
 		
-		MoreLikeThisQuery mlt=new MoreLikeThisQuery(DOMUtils.getText(e),fields,analyzer);
+MoreLikeThisQuery mlt=new MoreLikeThisQuery(DOMUtils.getText(e),fields,analyzer, fields[0]);
 		mlt.setMaxQueryTerms(DOMUtils.getAttribute(e,"maxQueryTerms",defaultMaxQueryTerms));
 		mlt.setMinTermFrequency(DOMUtils.getAttribute(e,"minTermFrequency",defaultMinTermFrequency));
 		mlt.setPercentTermsToMatch(DOMUtils.getAttribute(e,"percentTermsToMatch",defaultPercentTermsToMatch)/100);
---------------
-------------
@@ -33,7 +33,7 @@
 import org.apache.cassandra.utils.FBUtilities;
 
 
-public final class SuperColumn implements IColumn, IColumnContainer
+public class SuperColumn implements IColumn, IColumnContainer
 {
 	private static Logger logger_ = Logger.getLogger(SuperColumn.class);
 
---------------
-------------
@@ -538,7 +538,7 @@
               throw e;
           } else
             throw e;
-        } catch (Throwable t) {
+} catch (Exception t) {
           if (verboseDebug) {
             getDebugLogger().log(DIHLogLevels.ENTITY_EXCEPTION, epw.getEntity().getName(), t);
           }
---------------
-------------
@@ -34,7 +34,7 @@
 
   @Override
   public float tfn(BasicStats stats, float tf, float len) {
-    return (tf + mu * (stats.getTotalTermFreq() / (float)stats.getNumberOfFieldTokens())) / (len + mu) * mu;
+return (tf + mu * ((stats.getTotalTermFreq()+1F) / (stats.getNumberOfFieldTokens()+1F))) / (len + mu) * mu;
   }
 
   @Override
---------------
-------------
@@ -210,7 +210,7 @@
    */
   public boolean configFileExists(String collection, String fileName)
       throws KeeperException, InterruptedException {
-    Stat stat = zkClient.exists(CONFIGS_ZKNODE + "/" + fileName, null);
+Stat stat = zkClient.exists(CONFIGS_ZKNODE + "/" + collection + "/" + fileName, null);
     return stat != null;
   }
 
---------------
-------------
@@ -62,7 +62,7 @@
        super.setUp();
        File file = new File(System.getProperty("tempDir"), "testIndex");
        _TestUtil.rmDir(file);
-       dir = FSDirectory.getDirectory(file, null, false);
+dir = FSDirectory.getDirectory(file);
     }
 
 
---------------
-------------
@@ -428,7 +428,7 @@
     IndexReader reader = writer.getReader(applyAllDeletes);
 
     // If in fact no changes took place, return null:
-    if (reader.getVersion() == getVersion()) {
+if (reader.getVersion() == segmentInfos.getVersion()) {
       reader.decRef();
       return null;
     }
---------------
-------------
@@ -416,7 +416,7 @@
         {
             for (ByteBuffer col : del.predicate.column_names)
             {
-                if (del.super_column == null && DatabaseDescriptor.getColumnFamilyType(rm.getTable(), cfName).equals("Super"))
+if (del.super_column == null && DatabaseDescriptor.getColumnFamilyType(rm.getTable(), cfName) == ColumnFamilyType.Super)
                     rm.delete(new QueryPath(cfName, col.array()), del.timestamp);
                 else
                     rm.delete(new QueryPath(cfName, del.super_column.array(), col.array()), del.timestamp);
---------------
-------------
@@ -75,7 +75,7 @@
         "blueberry pizza",
     };
     directory = newDirectory();
-    RandomIndexWriter iw = new RandomIndexWriter(random, directory);
+RandomIndexWriter iw = new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));
     
     for (int i=0; i<N_DOCS; i++) {
       add(docText[i%docText.length], iw);
---------------
-------------
@@ -384,7 +384,7 @@
         synchronized (runners) {
           runner = runners.peek();
         }
-        if (runner == null)
+if (runner == null || scheduler.isTerminated())
           break;
         runner.runnerLock.lock();
         runner.runnerLock.unlock();
---------------
-------------
@@ -76,7 +76,7 @@
         log.warning("!!!! WARNING: best effort to remove " + dataDir.getAbsolutePath() + " FAILED !!!!!");
       }
     }
-    File persistedFile = new File("solr-persist.xml");
+File persistedFile = new File(getSolrHome() + File.separator + "solr-persist.xml");
     persistedFile.delete();
   }
 
---------------
-------------
@@ -236,7 +236,7 @@
             if (css_.batch)
                 System.exit(4);
         }
-        catch (Exception e)
+catch (Throwable e)
         {
             css_.err.println((e.getCause() == null) ? e.getMessage() : e.getCause().getMessage());
             
---------------
-------------
@@ -201,7 +201,7 @@
     int count = 0;
     while (count < numDocs) {
       final long offset;
-      final int docID = startDocID + count + 1;
+final int docID = docStoreOffset + startDocID + count + 1;
       assert docID <= numTotalDocs;
       if (docID < numTotalDocs) 
         offset = indexStream.readLong();
---------------
-------------
@@ -205,7 +205,7 @@
   /**
    * refills buffers with new data from the current token.
    */
-  private void refill() throws IOException {
+private void refill() {
     // compact buffers to keep them smallish if they become large
     // just a safety check, but technically we only need the last codepoint
     if (bufferLen > 64) {
---------------
-------------
@@ -197,7 +197,7 @@
         TestSuite suite = new TestSuite("LargeDataLocksTest");
         suite.addTest(baseSuite("LargeDataLocksTest:embedded"));
         // Disable for client until DERBY-2892 is fixed
-        //suite.addTest(TestConfiguration.clientServerDecorator(baseSuite("LargeDataLocksTest:client")));
+suite.addTest(TestConfiguration.clientServerDecorator(baseSuite("LargeDataLocksTest:client")));
         return suite;
 
     }
---------------
-------------
@@ -227,7 +227,7 @@
         rm.add(new QueryPath(SCHEMA_CF,
                              null,
                              DefsTable.DEFINITION_SCHEMA_COLUMN_NAME),
-                             org.apache.cassandra.avro.KsDef.SCHEMA$.toString().getBytes(UTF_8),
+org.apache.cassandra.config.avro.KsDef.SCHEMA$.toString().getBytes(UTF_8),
                              now);
         return rm;
     }
---------------
-------------
@@ -142,7 +142,7 @@
     assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("lazy")));
     assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("dog")));
     assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("the")));
-    DocsEnum de = te.docs(null, null, true);
+DocsEnum de = te.docs(null, null);
     assertTrue(de.advance(0) != DocIdSetIterator.NO_MORE_DOCS);
     assertEquals(2, de.freq());
     assertTrue(de.advance(1) != DocIdSetIterator.NO_MORE_DOCS);
---------------
-------------
@@ -57,7 +57,7 @@
     if (VERBOSE) {
       System.out.println("\nTEST: forceMergeDeletes2");
     }
-    tmp.setForceMergeDeletesPctAllowed(10.0);
+((TieredMergePolicy) w.getConfig().getMergePolicy()).setForceMergeDeletesPctAllowed(10.0);
     w.forceMergeDeletes();
     assertEquals(60, w.maxDoc());
     assertEquals(60, w.numDocs());
---------------
-------------
@@ -59,7 +59,7 @@
     outputDir = getTestTempDir("output");
     outputDir.delete();
 
-    conf = new Configuration();
+conf = getConfiguration();
   }
 
   private Document asDocument(String line) {
---------------
-------------
@@ -1138,7 +1138,7 @@
     assertTrue(new ThreadInterruptedException(new InterruptedException()).getCause() instanceof InterruptedException);
 
     // issue 300 interrupts to child thread
-    final int numInterrupts = atLeast(3000);
+final int numInterrupts = atLeast(300);
     int i = 0;
     while(i < numInterrupts) {
       // TODO: would be nice to also sometimes interrupt the
---------------
-------------
@@ -63,7 +63,7 @@
   }
   
   public void testRandomData() throws IOException {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -77,7 +77,7 @@
 	/**
 		Attempt to establish a database connection.
 
-		@param user the database user on whose behalf the Connection is being made
+@param username the database user on whose behalf the Connection is being made
 		@param password the user's password
 
 		@return a Connection to the database
---------------
-------------
@@ -158,7 +158,7 @@
 
   /** Returns byte usage of all buffers. */
   public long sizeInBytes() {
-    return file.numBuffers() * BUFFER_SIZE;
+return (long) file.numBuffers() * (long) BUFFER_SIZE;
   }
   
   @Override
---------------
-------------
@@ -213,7 +213,7 @@
           System.out.println("      " + Integer.toHexString(finalBuffer[i]&0xFF));
         }
       }
-      builder.add(text, new BytesRef(spare));
+builder.add(text, BytesRef.deepCopyOf(spare));
       termCount++;
     }
 
---------------
-------------
@@ -3793,7 +3793,7 @@
 		} 
 		catch (SQLException se)
 		{
-			skipRemainder(false);
+skipRemainder(true);
 			throw se;
 		}
 	}
---------------
-------------
@@ -420,7 +420,7 @@
    */ 
   @Override
   public MergeSpecification findForcedDeletesMerges(SegmentInfos segmentInfos)
-      throws CorruptIndexException, IOException {
+throws IOException {
     final List<SegmentInfoPerCommit> segments = segmentInfos.asList();
     final int numSegments = segments.size();
 
---------------
-------------
@@ -51,6 +51,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new TurkishAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new TurkishAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -652,7 +652,7 @@
     GroupingSpecification groupSpec = rb.getGroupingSpec();
     if (rb.mergedTopGroups.isEmpty()) {
       for (String field : groupSpec.getFields()) {
-        rb.mergedTopGroups.put(field, new TopGroups(null, null, 0, 0, new GroupDocs[]{}));
+rb.mergedTopGroups.put(field, new TopGroups(null, null, 0, 0, new GroupDocs[]{}, Float.NaN));
       }
       rb.resultIds = new HashMap<Object, ShardDoc>();
     }
---------------
-------------
@@ -69,7 +69,7 @@
     double longUpperRight = upperRight.getLng();
     double longLowerLeft = lowerLeft.getLng();
 
-    CartesianTierPlotter ctp = new CartesianTierPlotter( miles, projector, tierPrefix );
+CartesianTierPlotter ctp = new CartesianTierPlotter( miles, projector, tierPrefix, minTier, maxTier );
     Shape shape = new Shape(ctp.getTierLevelId());
 
     if (longUpperRight < longLowerLeft) { // Box cross the 180 meridian
---------------
-------------
@@ -58,7 +58,7 @@
       writer.addDocument(doc);
     }
     IndexReader reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     
     IndexSearcher searcher=newSearcher(reader);
     num = atLeast(50);
---------------
-------------
@@ -139,7 +139,7 @@
         //remember in setup a locator is already created
         //hence expected value is 2
         assertEquals("The locator values returned by " +
-            "SYSIBM.CLOBCREATELOCATOR() are incorrect", 4, locator);
+"SYSIBM.CLOBCREATELOCATOR() are incorrect", 2, locator);
         cs.close();
     }
 
---------------
-------------
@@ -32,7 +32,7 @@
 import org.apache.derby.client.am.Agent;
 import org.apache.derby.client.am.ClientMessageId;
 import org.apache.derby.client.am.SqlException;
-import org.apache.derby.iapi.services.sanity.SanityManager;
+import org.apache.derby.shared.common.sanity.SanityManager;
 import org.apache.derby.shared.common.reference.SQLState;
 
 public class Utf8CcsidManager extends CcsidManager {
---------------
-------------
@@ -143,7 +143,7 @@
         // by the test threads may cause the index statistics daemon worker
         // thread to be "starved". Add a timeout to give it a chance to do
         // what it has been told to do.
-        IndexStatsUtil stats = new IndexStatsUtil(getConnection(), 2000);
+IndexStatsUtil stats = new IndexStatsUtil(getConnection(), 5000);
         IdxStats[] myStats = stats.getStatsTable(TAB, 2);
         for (int i=0; i < myStats.length; i++) {
             IdxStats s = myStats[i];
---------------
-------------
@@ -530,7 +530,7 @@
     protected abstract void modifyIndex(int i) throws IOException;
   }
   
-  static class KeepAllCommits implements IndexDeletionPolicy {
+static class KeepAllCommits extends IndexDeletionPolicy {
     @Override
     public void onInit(List<? extends IndexCommit> commits) {
     }
---------------
-------------
@@ -36,7 +36,7 @@
     public void doVerb(Message msg)
     {
         StorageService.instance.confirmReplication(msg.getFrom());
-        Message response = msg.getInternalReply(ArrayUtils.EMPTY_BYTE_ARRAY);
+Message response = msg.getInternalReply(ArrayUtils.EMPTY_BYTE_ARRAY, msg.getVersion());
         if (logger.isDebugEnabled())
             logger.debug("Replying to " + msg.getMessageId() + "@" + msg.getFrom());
         MessagingService.instance().sendOneWay(response, msg.getFrom());
---------------
-------------
@@ -176,7 +176,7 @@
   private void handleCreateAction(SolrQueryRequest req,
       SolrQueryResponse rsp) throws InterruptedException, KeeperException {
     log.info("Creating Collection : " + req.getParamString());
-    Integer numReplicas = req.getParams().getInt(OverseerCollectionProcessor.REPLICATION_FACTOR, 0);
+Integer numReplicas = req.getParams().getInt(OverseerCollectionProcessor.REPLICATION_FACTOR, 1);
     String name = req.getParams().required().get("name");
     String configName = req.getParams().get("collection.configName");
     String numShards = req.getParams().get(OverseerCollectionProcessor.NUM_SLICES);
---------------
-------------
@@ -53,7 +53,7 @@
     }
     
     IndexReader ir = iw.getReader();
-    iw.close();
+iw.shutdown();
     
     IndexSearcher is = newSearcher(ir);
     
---------------
-------------
@@ -109,6 +109,6 @@
 
     public int compareTo(IteratingRow o)
     {
-        return partitioner.getDecoratedKeyComparator().compare(key, o.key);
+return key.compareTo(o.key);
     }
 }
---------------
-------------
@@ -60,7 +60,7 @@
         int b = -1;
         try
         {
-            if (endpointsnitch.isInSameDataCenter(localEndpoint, message.getFrom()))
+if (endpointsnitch.getDatacenter(localEndpoint).equals(endpointsnitch.getDatacenter(message.getFrom())))
             {
                 b = blockFor.decrementAndGet();
             }
---------------
-------------
@@ -633,7 +633,7 @@
     {
         arg_passer[0]        = rawData;
 
-        rawLength = in.readCloudscapeUTF(arg_passer);
+rawLength = in.readDerbyUTF(arg_passer);
 
         rawData = arg_passer[0];
 
---------------
-------------
@@ -311,7 +311,7 @@
         uniqueTerms.add(term);
         fieldTerms.add(new Term(field, term));
         Document doc = new Document();
-        doc.add(newField(field, term, StringField.TYPE_UNSTORED));
+doc.add(newStringField(field, term, Field.Store.NO));
         w.addDocument(doc);
       }
       uniqueTermCount += uniqueTerms.size();
---------------
-------------
@@ -546,7 +546,7 @@
     // we write here (e.g., to write parent+2), and need to do a workaround
     // in the reader (which knows that anyway only category 0 has a parent
     // -1).    
-    parentStream.set(parent + 1);
+parentStream.set(Math.max(parent+1, 1));
     Document d = new Document();
     d.add(parentStreamField);
 
---------------
-------------
@@ -555,7 +555,7 @@
     sb.append("commit=").append(commit == null ? "null" : commit).append("\n");
     sb.append("openMode=").append(getOpenMode()).append("\n");
     sb.append("similarity=").append(getSimilarity().getClass().getName()).append("\n");
-    sb.append("mergeScheduler=").append(getMergeScheduler().getClass().getName()).append("\n");
+sb.append("mergeScheduler=").append(getMergeScheduler()).append("\n");
     sb.append("default WRITE_LOCK_TIMEOUT=").append(IndexWriterConfig.WRITE_LOCK_TIMEOUT).append("\n");
     sb.append("writeLockTimeout=").append(getWriteLockTimeout()).append("\n");
     sb.append("codec=").append(getCodec()).append("\n");
---------------
-------------
@@ -141,7 +141,7 @@
       if (cmdLine.hasOption(inputOpt)) { // Lucene case
         File file = new File(cmdLine.getValue(inputOpt).toString());
         if (!file.isDirectory()) {
-          throw new IllegalArgumentException("Lucene directory: " + file.getName() + 
+throw new IllegalArgumentException("Lucene directory: " + file.getAbsolutePath() +
               " does not exist or is not a directory");
         }
 
---------------
-------------
@@ -195,7 +195,7 @@
 			if (value instanceof BigDecimal) return ((BigDecimal)value);
 
 			if (value instanceof Number)
-				return new BigDecimal(((Number) value).doubleValue());
+return new BigDecimal(Double.toString(((Number) value).doubleValue()));
 		}
 		return super.getBigDecimal();
 	}
---------------
-------------
@@ -59,7 +59,7 @@
       ts.reset();
       while (ts.incrementToken()) {
         termAtt.fillBytesRef();
-        term = new Term(fieldName, new BytesRef(bytes));
+term = new Term(fieldName, BytesRef.deepCopyOf(bytes));
         bq.add(new BooleanClause(new TermQuery(term), BooleanClause.Occur.SHOULD));
       }
       ts.end();
---------------
-------------
@@ -44,7 +44,7 @@
  */
 public final class CommonGramsFilter extends TokenFilter {
 
-  static final String GRAM_TYPE = "gram";
+public static final String GRAM_TYPE = "gram";
   private static final char SEPARATOR = '_';
 
   private final CharArraySet commonWords;
---------------
-------------
@@ -709,7 +709,7 @@
                     throw StandardException.newException(
                             SQLState.LANG_STREAMING_COLUMN_I_O_EXCEPTION, 
                             ioe, 
-                            "java.sql.String");
+String.class.getName());
                 }
             }
         }
---------------
-------------
@@ -242,7 +242,7 @@
         return conf.get(OUTPUT_COLUMNFAMILY_CONFIG);
     }
 
-    public static int getThriftPort(Configuration conf)
+public static int getRpcPort(Configuration conf)
     {
         String v = conf.get(THRIFT_PORT);
         return v == null ? DatabaseDescriptor.getRpcPort() : Integer.valueOf(v);
---------------
-------------
@@ -695,7 +695,7 @@
 			return 0;
 
 		try {
-			return (int) Double.doubleToLongBits(getDouble());
+return (int) getLong();
 		} catch (StandardException se)
 		{
 			return 0;
---------------
-------------
@@ -546,6 +546,6 @@
     public  static  String   varchar_String_String( Long a ) { return "-1"; }
     public  static  String   varchar_String_String( Float a ) { return "-1"; }
     public  static  String   varchar_String_String( Double a ) { return "-1"; }
-    //    public  static  String   varchar_String_String( Object a ) { return "-1"; }
+public  static  String   varchar_String_String( Object a ) { return "-1"; }
     
 }
---------------
-------------
@@ -96,7 +96,7 @@
             }
             catch (Exception e)
             {
-                throw new RuntimeException(e);
+logger_.error("Gossip error", e);
             }
         }
     }
---------------
-------------
@@ -136,7 +136,7 @@
                 if (cached != null)
                 {
                     QueryFilter keyFilter = new QueryFilter(key, filter.path, filter.filter);
-                    returnCF = cfs.filterColumnFamily(cached, keyFilter, CompactionManager.getDefaultGCBefore());
+returnCF = cfs.filterColumnFamily(cached, keyFilter, cfs.metadata.gcGraceSeconds);
                 }
                 else
                 {
---------------
-------------
@@ -124,7 +124,7 @@
         writer.setMaxBufferedDocs(2);
       }
 
-      IndexReader reader = IndexReader.open(directory);
+IndexReader reader = IndexReader.open(directory, true);
       assertTrue(reader.isOptimized());
       assertEquals(expectedDocCount, reader.numDocs());
       reader.close();
---------------
-------------
@@ -24,7 +24,7 @@
 import java.io.IOException;
 import java.util.*;
 
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.utils.ReducingIterator;
 import org.apache.cassandra.db.*;
 import org.apache.cassandra.db.marshal.AbstractType;
---------------
-------------
@@ -177,7 +177,7 @@
     this.numIndexEntries = (nIndexEntries >= 0) ? nIndexEntries : 0;
     long maxIndexEntry = maxHighValue + numValues - 1; // clear upper bits, set upper bits, start at zero
     this.nIndexEntryBits = (maxIndexEntry <= 0) ? 0
-                          : (64 - Long.numberOfLeadingZeros(maxIndexEntry - 1));
+: (64 - Long.numberOfLeadingZeros(maxIndexEntry));
     long numLongsForIndexBits = numLongsForBits(numIndexEntries * nIndexEntryBits);
     if (numLongsForIndexBits > Integer.MAX_VALUE) {
       throw new IllegalArgumentException("numLongsForIndexBits too large to index a long array: " + numLongsForIndexBits);
---------------
-------------
@@ -28,7 +28,7 @@
 
 public class DebuggableThreadPoolExecutor extends ThreadPoolExecutor
 {
-    protected static Logger logger = LoggerFactory.getLogger(JMXEnabledThreadPoolExecutor.class);
+protected static Logger logger = LoggerFactory.getLogger(DebuggableThreadPoolExecutor.class);
 
     public DebuggableThreadPoolExecutor(String threadPoolName, int priority)
     {
---------------
-------------
@@ -447,7 +447,7 @@
     String jobName = String.format("Writing final document/topic inference from %s to %s", corpus, output);
     log.info("About to run: {}", jobName);
 
-    Job job = prepareJob(corpus, outputPath, SequenceFileInputFormat.class, CVB0DocInferenceMapper.class,
+Job job = prepareJob(corpus, output, SequenceFileInputFormat.class, CVB0DocInferenceMapper.class,
         IntWritable.class, VectorWritable.class, SequenceFileOutputFormat.class, jobName);
 
     FileSystem fs = FileSystem.get(corpus.toUri(), conf);
---------------
-------------
@@ -234,7 +234,7 @@
         rm.add(new QueryPath(SCHEMA_CF,
                              null,
                              DefsTable.DEFINITION_SCHEMA_COLUMN_NAME),
-                             ByteBuffer.wrap(org.apache.cassandra.db.migration.avro.KsDef.SCHEMA$.toString().getBytes(UTF_8)),
+ByteBuffer.wrap(org.apache.cassandra.avro.KsDef.SCHEMA$.toString().getBytes(UTF_8)),
                              now);
         return rm;
     }
---------------
-------------
@@ -573,7 +573,7 @@
             // read response
             // TODO send more requests if we need to span multiple nodes
             // double the usual timeout since range requests are expensive
-            byte[] responseBody = (byte[])iar.get(2 * DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS)[0];
+byte[] responseBody = iar.get(2 * DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
             return RangeReply.read(responseBody).keys;
         }
         catch (Exception e)
---------------
-------------
@@ -722,7 +722,7 @@
       writer.close();
       dir.close();
       if (exception.get() != null) {
-        throw new AssertionError("One thread threw an exception", exception.get());
+throw new RuntimeException("One thread threw an exception", exception.get());
       }
     }
   }
---------------
-------------
@@ -328,7 +328,7 @@
 			)
 			{
 				if (server != null)
-					server.consoleExceptionPrint(e);
+server.consoleExceptionPrintTrace(e);
 				else
 					e.printStackTrace();  // default output stream is System.out
 			}
---------------
-------------
@@ -294,7 +294,7 @@
       bytes = new byte[length];
       in.read(bytes, 0, length);
     } else {
-      bytes = null;
+bytes = EMPTY_BYTES;
     }
   }
 }
---------------
-------------
@@ -135,7 +135,7 @@
     f = newField(ID_FIELD, id2String(scoreAndID), customType); // for debug purposes
     d.add(f);
 
-    FieldType customType2 = new FieldType(TextField.TYPE_UNSTORED);
+FieldType customType2 = new FieldType(TextField.TYPE_NOT_STORED);
     customType2.setOmitNorms(true);
     f = newField(TEXT_FIELD, "text of doc" + scoreAndID + textLine(i), customType2); // for regular search
     d.add(f);
---------------
-------------
@@ -578,7 +578,7 @@
 
       if (params.getBool(CoreAdminParams.DELETE_DATA_DIR, false)) {
         try {
-          core.getDirectoryFactory().remove(core.getDataDir());
+core.getDirectoryFactory().remove(core.getDataDir(), true);
         } catch (Exception e) {
           SolrException.log(log, "Failed to flag data dir for removal for core:"
                   + core.getName() + " dir:" + core.getDataDir());
---------------
-------------
@@ -109,7 +109,7 @@
     addOutputOption();
     addOption("recommenderClassName", "r", "Name of recommender class to instantiate");
     addOption("numRecommendations", "n", "Number of recommendations per user", "10");
-    addOption("usersFile", "u", "Number of recommendations per user", null);
+addOption("usersFile", "u", "File of users to recommend for", null);
     
     Map<String,String> parsedArgs = parseArguments(args);
     if (parsedArgs == null) {
---------------
-------------
@@ -998,7 +998,7 @@
     assertQ(req("id:42 AND subword:IBM's")
             ,"*[count(//doc)=1]"
             );
-    assertQ(req("id:42 AND subword:IBM'sx")
+assertQ(req("id:42 AND subword:\"IBM'sx\"")
             ,"*[count(//doc)=0]"
             );
 
---------------
-------------
@@ -337,7 +337,7 @@
 
         // perform import with column names specified in random order.
         doImportDataLobsFromExtFile(null, "BOOKS_IMP", "PIC, CONTENT, NAME, ID", 
-                                  "4, 3, 2, 1", fileName, null, null, null, 1);
+"4, 3, 2, 1", fileName, null, null, "8859_1", 1);
         verifyData("PIC, CONTENT, NAME, ID");
 
 	//DERBY-2925: need to delete export files first
---------------
-------------
@@ -39,7 +39,7 @@
     if (!store && !index)
       throw new IllegalArgumentException("field must be indexed or stored");
 
-    FieldType fieldType = new FieldType(DoubleField.TYPE);
+FieldType fieldType = new FieldType(DoubleField.TYPE_NOT_STORED);
     fieldType.setStored(store);
     fieldType.setIndexed(index);
     fieldType.setNumericPrecisionStep(precisionStep);
---------------
-------------
@@ -1341,7 +1341,7 @@
         {
             QueryFilter nameFilter = new NamesQueryFilter(filter.key, new QueryPath(columnFamily_), filter.path.superColumnName);
             ColumnFamily cf = getColumnFamily(nameFilter);
-            if (cf == null)
+if (cf == null || cf.getColumnCount() == 0)
                 return cf;
 
             assert cf.getSortedColumns().size() == 1;
---------------
-------------
@@ -207,7 +207,7 @@
       }
     }
     
-    public int size() throws IOException {
+public int size() {
       return delegateFieldsProducer.size();
     }
     
---------------
-------------
@@ -56,7 +56,7 @@
     
     CommitLogHeader()
     {
-        this(new HashMap<Integer, Integer>(), CFMetaData.getCfIdMap().size());
+this(new HashMap<Integer, Integer>(), CFMetaData.getCfToIdMap().size());
     }
     
     /*
---------------
-------------
@@ -3107,7 +3107,7 @@
     checkpoint();
 
     if (infoStream.isEnabled("IW")) {
-      infoStream.message("IW", "after commit: " + segString());
+infoStream.message("IW", "after commitMerge: " + segString());
     }
 
     if (merge.maxNumSegments != -1 && !dropSegment) {
---------------
-------------
@@ -55,7 +55,7 @@
     addDoc(writer, "lucene release");
 
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     searcher = newSearcher(reader);
   }
   
---------------
-------------
@@ -865,7 +865,7 @@
       assertTrue(failure.failOnCommit && failure.failOnDeleteFile);
       w.rollback();
       assertFalse(dir.fileExists("1.fnx"));
-      assertEquals(0, dir.listAll().length);
+// FIXME: on windows, this often fails! assertEquals(0, dir.listAll().length);
       dir.close();
     }
   }
---------------
-------------
@@ -59,7 +59,7 @@
 
     // Open index
     IndexReader reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     IndexSearcher searcher = newSearcher(reader);
 
     // Get ValueSource from FieldCache
---------------
-------------
@@ -579,7 +579,7 @@
 
     assertQueryEquals("drop AND stop AND roll", qpAnalyzer, "+drop +roll");
     assertQueryEquals("term phrase term", qpAnalyzer,
-        "term phrase1 phrase2 term");
+"term (phrase1 phrase2) term");
 
     assertQueryEquals("term AND NOT phrase term", qpAnalyzer,
         "+term -(phrase1 phrase2) term");
---------------
-------------
@@ -42,6 +42,6 @@
 
 public class Constants {
 
-  public static final String VERSION = "3.0.0";
+public static final String VERSION = "4.0.0";
 
 }
---------------
-------------
@@ -40,7 +40,7 @@
   protected void map(Text key, VectorWritable value, Context context)
       throws IOException, InterruptedException {
     if (!labelMap.containsKey(key.toString())) {
-      context.getCounter("NaiveBayes", "Skipped instance: not in label list");
+context.getCounter("NaiveBayes", "Skipped instance: not in label list").increment(1);
       return;
     }  
     int label = labelMap.get(key.toString());
---------------
-------------
@@ -134,7 +134,7 @@
 		boolean encryptDB = Boolean.valueOf(finfo.getProperty(Attribute.DATA_ENCRYPTION)).booleanValue();		
 		String encryptpassword = finfo.getProperty(Attribute.BOOT_PASSWORD);
 
-		if (dbname.length() == 0 || (encryptDB = true && encryptpassword == null)) {
+if (dbname.length() == 0 || (encryptDB && encryptpassword == null)) {
 
 			// with no database name we can have shutdown or a database name
 
---------------
-------------
@@ -88,7 +88,7 @@
    * Only implemented by primitive queries, which re-write to themselves.
    */
   public Weight createWeight(IndexSearcher searcher) throws IOException {
-    throw new UnsupportedOperationException();
+throw new UnsupportedOperationException("Query " + this + " does not implement createWeight");
   }
 
   /** Expert: called to re-write queries into primitive queries. For example,
---------------
-------------
@@ -84,7 +84,7 @@
     
     void addApplicationState(String key, ApplicationState appState)
     {
-        assert !StorageService.instance().isClientMode();
+assert !StorageService.instance.isClientMode();
         applicationState_.put(key, appState);        
     }
     
---------------
-------------
@@ -369,7 +369,7 @@
         // some other product's Driver might hijack our stored procedure.
         InternalDriver id = InternalDriver.activeDriver();
         if (id != null) { 
-            EmbedConnection conn = (EmbedConnection) id.connect("jdbc:default:connection", null);
+EmbedConnection conn = (EmbedConnection) id.connect( "jdbc:default:connection", null, 0 );
             if (conn != null)
                 return conn;
         }
---------------
-------------
@@ -1367,7 +1367,7 @@
                 connection_.completeTransactionStart();
             }
 
-            super.markResultSetsClosed();
+super.markResultSetsClosed(true); // true means remove from list of commit and rollback listeners
 
             switch (sqlMode_) {
             case isUpdate__:
---------------
-------------
@@ -91,7 +91,7 @@
     conf.set("io.serializations",
              "org.apache.hadoop.io.serializer.JavaSerialization,org.apache.hadoop.io.serializer.WritableSerialization");
     // Dont ever forget this. People should keep track of how hadoop conf parameters and make or break a piece of code
-    FileSystem dfs = FileSystem.get(conf);
+FileSystem dfs = FileSystem.get(outPath.toUri(), conf);
     if (dfs.exists(outPath))
       dfs.delete(outPath, true);
 
---------------
-------------
@@ -510,7 +510,7 @@
 		newStatement.setQueryTimeout(oldStatement.getQueryTimeout());
 	}
 
-	protected Statement getStatement() throws SQLException {
+public Statement getStatement() throws SQLException {
 		return control.getRealStatement();
 	}
 	protected final ResultSet wrapResultSet(ResultSet rs) {
---------------
-------------
@@ -36,7 +36,7 @@
   }
 
   @Override
-  FastIDSet doGetCandidateItems(long[] preferredItemIDs, DataModel dataModel) throws TasteException {
+protected FastIDSet doGetCandidateItems(long[] preferredItemIDs, DataModel dataModel) throws TasteException {
     FastIDSet candidateItemIDs = new FastIDSet();
     for (long itemID : preferredItemIDs) {
       candidateItemIDs.addAll(similarity.allSimilarItemIDs(itemID));
---------------
-------------
@@ -46,7 +46,7 @@
   public UpdateShardHandler(ConfigSolr cfg) {
     
     clientConnectionManager = new PoolingClientConnectionManager();
-    clientConnectionManager.setDefaultMaxPerRoute(cfg.getMaxUpdateConnections());
+clientConnectionManager.setMaxTotal(cfg.getMaxUpdateConnections());
     clientConnectionManager.setDefaultMaxPerRoute(cfg.getMaxUpdateConnectionsPerHost());
     
     
---------------
-------------
@@ -30,7 +30,7 @@
 public class TestCrash extends LuceneTestCase {
 
   private IndexWriter initIndex(Random random, boolean initialCommit) throws IOException {
-    return initIndex(random, newDirectory(random), initialCommit);
+return initIndex(random, newMockDirectory(random), initialCommit);
   }
 
   private IndexWriter initIndex(Random random, MockDirectoryWrapper dir, boolean initialCommit) throws IOException {
---------------
-------------
@@ -62,7 +62,7 @@
     {
         try
         {
-            long timeout = System.currentTimeMillis() - startTime + DatabaseDescriptor.getRpcTimeout();
+long timeout = DatabaseDescriptor.getRpcTimeout() - (System.currentTimeMillis() - startTime);
             boolean success;
             try
             {
---------------
-------------
@@ -68,7 +68,7 @@
 
         for (int i = 900; i < 1000; ++i)
         {
-            RowMutation rm = new RowMutation("Keyspace1", ByteBuffer.wrap(Integer.toString(i).getBytes()));
+RowMutation rm = new RowMutation("Keyspace1", ByteBufferUtil.bytes(Integer.toString(i)));
             for (int j = 0; j < 8; ++j)
             {
                 rm.add(new QueryPath("StandardLong1", null, getBytes(j * 2)), ByteBufferUtil.bytes("a"), j * 2);
---------------
-------------
@@ -151,7 +151,7 @@
 
     // do many small tests so the thread locals go away inbetween
     for (int i=0; i<100; i++) {
-      ir1 = IndexReader.open(dir1);
+ir1 = IndexReader.open(dir1, false);
       doTest(10,100);
     }
   }
---------------
-------------
@@ -126,7 +126,7 @@
     private BoostAttribute boostAtt;
     
     @Override
-    public void setNextEnum(TermsEnum termsEnum) throws IOException {
+public void setNextEnum(TermsEnum termsEnum) {
       this.termsEnum = termsEnum;
       this.boostAtt = termsEnum.attributes().addAttribute(BoostAttribute.class);
     }
---------------
-------------
@@ -698,7 +698,7 @@
     return newArr;
   }
 
-  static SimilarityFactory readSimilarity(ResourceLoader loader, Node node) {
+static SimilarityFactory readSimilarity(SolrResourceLoader loader, Node node) {
     if (node==null) {
       return null;
     } else {
---------------
-------------
@@ -63,7 +63,7 @@
     
     reader = writer.getReader();
     searcher = newSearcher(reader);
-    writer.close();
+writer.shutdown();
     if (VERBOSE) {
       System.out.println("TEST: setUp searcher=" + searcher);
     }
---------------
-------------
@@ -199,7 +199,7 @@
     bundleFramework = 
       _bundleFrameworkFactory.createBundleFramework(
           parentCtx, 
-          deploymentMF.getApplicationSymbolicName() + " " + deploymentMF.getApplicationVersion(), 
+app.getApplicationMetadata().getApplicationScope(),
           frameworkConfig, 
           frameworkBundleManifest);
 
---------------
-------------
@@ -63,7 +63,7 @@
       writer.deleteDocuments(new Term("id", "" + random().nextInt(numDocs)));
     }
     writer.forceMerge(1);
-    writer.close();
+writer.shutdown();
     
     ir = DirectoryReader.open(dir);
     assertSumDocFreq(ir);
---------------
-------------
@@ -63,7 +63,7 @@
          private int base = 0;
          private Scorer scorer;
          @Override
-         public void setScorer(Scorer scorer) throws IOException {
+public void setScorer(Scorer scorer) {
           this.scorer = scorer;
          }
          @Override
---------------
-------------
@@ -27,7 +27,7 @@
   private int outOf;
   private int reported;
 
-  Report (String text, int size, int reported, int outOf) {
+public Report (String text, int size, int reported, int outOf) {
     this.text = text;
     this.size = size;
     this.reported = reported;
---------------
-------------
@@ -37,7 +37,7 @@
     public void testGetNewNames() throws IOException
     {
         Descriptor desc = Descriptor.fromFilename(new File("Keyspace1", "Standard1-500-Data.db").toString());
-        PendingFile inContext = new PendingFile(null, desc, "Data.db", Arrays.asList(new Pair<Long,Long>(0L, 1L)));
+PendingFile inContext = new PendingFile(null, desc, "Data.db", Arrays.asList(new Pair<Long,Long>(0L, 1L)), OperationType.BOOTSTRAP);
 
         PendingFile outContext = StreamIn.getContextMapping(inContext);
         // filename and generation are expected to have changed
---------------
-------------
@@ -218,7 +218,7 @@
       current = top[0].current;
     }
 
-    private void pushTop() throws IOException {
+private void pushTop() {
       // call next() on each top, and put back into queue
       for (int i = 0; i < numTop; i++) {
         top[i].current = top[i].next();
---------------
-------------
@@ -34,7 +34,7 @@
         buffer_ = ByteBuffer.allocate(length_);
     }
 
-    public byte[] read() throws IOException, ReadNotCompleteException
+public byte[] read() throws IOException
     {          
         return doRead(buffer_);
     }
---------------
-------------
@@ -293,7 +293,7 @@
      */
     public IValidator getValidator(String table, String cf, InetAddress initiator, boolean major)
     {
-        if (!major || table.equals(Table.SYSTEM_TABLE) || table.equals(Table.DEFINITIONS))
+if (!major || table.equals(Table.SYSTEM_TABLE))
             return new NoopValidator();
         if (StorageService.instance.getTokenMetadata().sortedTokens().size()  < 1)
             // gossiper isn't started
---------------
-------------
@@ -38,7 +38,7 @@
 import org.apache.cassandra.utils.FBUtilities;
 
 
-public final class ColumnFamily implements IColumnContainer
+public class ColumnFamily implements IColumnContainer
 {
     /* The column serializer for this Column Family. Create based on config. */
     private static ColumnFamilySerializer serializer_ = new ColumnFamilySerializer();
---------------
-------------
@@ -455,7 +455,7 @@
       assert lastDeleteTerm == null || term.compareTo(lastDeleteTerm) > 0: "lastTerm=" + lastDeleteTerm + " vs term=" + term;
     }
     // TODO: we re-use term now in our merged iterable, but we shouldn't clone, instead copy for this assert
-    lastDeleteTerm = term == null ? null : new Term(term.field(), new BytesRef(term.bytes));
+lastDeleteTerm = term == null ? null : new Term(term.field(), BytesRef.deepCopyOf(term.bytes));
     return true;
   }
 
---------------
-------------
@@ -53,6 +53,6 @@
         ColumnFamily retrieved = store.getColumnFamily(QueryFilter.getIdentityFilter(dk, new QueryPath("Standard1", null, "Column1".getBytes())));
         assert retrieved.isMarkedForDelete();
         assertNull(retrieved.getColumn("Column1".getBytes()));
-        assertNull(ColumnFamilyStore.removeDeleted(retrieved, Integer.MAX_VALUE));
+assertNull(Util.cloneAndRemoveDeleted(retrieved, Integer.MAX_VALUE));
     }
 }
---------------
-------------
@@ -49,7 +49,7 @@
       bits.flip(idx, idx + 1);
     }
     
-    FixedBitSet verify = new FixedBitSet(bits);
+FixedBitSet verify = bits.clone();
 
     ScoredDocIDs scoredDocIDs = ScoredDocIdsUtils.createScoredDocIds(bits, n); 
 
---------------
-------------
@@ -796,7 +796,7 @@
                 iterators.add(iter);
             }
 
-            Comparator<IColumn> comparator = filter.getColumnComparator(getComparator());
+Comparator<IColumn> comparator = QueryFilter.getColumnComparator(getComparator());
             Iterator collated = IteratorUtils.collatedIterator(comparator, iterators);
             if (!collated.hasNext())
                 return null;
---------------
-------------
@@ -697,7 +697,7 @@
         
         try
         {
-            oldCfm.apply(cf_def);
+CFMetaData.applyImplicitDefaults(cf_def);
             UpdateColumnFamily update = new UpdateColumnFamily(cf_def);
             applyMigrationOnStage(update);
             return DatabaseDescriptor.getDefsVersion().toString();
---------------
-------------
@@ -76,7 +76,7 @@
         while (true)
         {
             offs += def.deflate(output.buffer, offs, output.buffer.length - offs);
-            if (def.needsInput())
+if (def.finished())
             {
                 return offs - outputOffset;
             }
---------------
-------------
@@ -71,7 +71,7 @@
 
         r = w.getReader();
         s = newSearcher(r);
-        w.close();
+w.shutdown();
 //System.out.println("Set up " + getName());
     }
     
---------------
-------------
@@ -183,7 +183,7 @@
     boolean converged = false;
 
     for (int iteration = 1; ((maxIterations < 1) || (iteration <= maxIterations)) && !converged; iteration++) {
-      log.info("Iteration {}", iteration);
+log.info("LDA Iteration {}", iteration);
       // point the output to a new directory per iteration
       Path stateOut = new Path(output, "state-" + iteration);
       double ll = runIteration(conf, input, stateIn, stateOut, numTopics, numWords, topicSmoothing);
---------------
-------------
@@ -127,7 +127,7 @@
     } else {
       in = new BufferedReader(new InputStreamReader(System.in, "UTF-8"));
     }
-      QueryParser parser = new QueryParser(field, analyzer);
+QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, field, analyzer);
     while (true) {
       if (queries == null)                        // prompt the user
         System.out.println("Enter query: ");
---------------
-------------
@@ -95,7 +95,7 @@
     private class SimplePrefixTermsEnum extends FilteredTermsEnum {
       private final BytesRef prefix;
 
-      private SimplePrefixTermsEnum(TermsEnum tenum, BytesRef prefix) throws IOException {
+private SimplePrefixTermsEnum(TermsEnum tenum, BytesRef prefix) {
         super(tenum);
         this.prefix = prefix;
         setInitialSeekTerm(new BytesRef(""));
---------------
-------------
@@ -146,7 +146,7 @@
 
             for (int i = 0; i < keyTokens.length; i++)
             {
-                Collection<InetAddress> endpoints = tmd.getWriteEndpoints(keyTokens[i], table, strategy.getNaturalEndpoints(keyTokens[i]));
+Collection<InetAddress> endpoints = tmd.getWriteEndpoints(keyTokens[i], table, strategy.getNaturalEndpoints(keyTokens[i])).left;
                 assertTrue(endpoints.size() >= replicationFactor);
 
                 for (int j = 0; j < replicationFactor; j++)
---------------
-------------
@@ -72,7 +72,7 @@
         }
         String url = "service:jmx:rmi:///jndi/rmi://:" + port + "/solrjmx";
         JmxConfiguration config = new JmxConfiguration(true, null, url);
-        monitoredMap = new JmxMonitoredMap<String, SolrInfoMBean>(null, config);
+monitoredMap = new JmxMonitoredMap<String, SolrInfoMBean>(null, "", config);
         JMXServiceURL u = new JMXServiceURL(url);
         connector = JMXConnectorFactory.connect(u);
         mbeanServer = connector.getMBeanServerConnection();
---------------
-------------
@@ -181,7 +181,7 @@
         while (low <= high)
         {
             mid = (low + high) >> 1;
-            if ((result = -compare(get(mid).name(), name)) > 0)
+if ((result = compare(name, get(mid).name())) > 0)
             {
                 low = mid + 1;
             }
---------------
-------------
@@ -138,7 +138,7 @@
 
   // LUCENE-1468
   public void testFSDirectoryFilter() throws IOException {
-    checkDirectoryFilter(FSDirectory.open(new File("test")));
+checkDirectoryFilter(FSDirectory.open(new File(System.getProperty("tempDir"),"test")));
   }
 
   // LUCENE-1468
---------------
-------------
@@ -78,7 +78,7 @@
         byte[] data = bos.toByteArray();
         if ( data.length > 0 )
         {  
-            logger_.debug("Size of Gossip packet " + data.length);
+logger_.trace("Size of Gossip packet " + data.length);
             byte[] protocol = BasicUtilities.intToByteArray(protocol_);
             ByteBuffer buffer = ByteBuffer.allocate(data.length + protocol.length);
             buffer.put( protocol );
---------------
-------------
@@ -115,7 +115,7 @@
         analyzer = fieldType.getQueryAnalyzer();
       } else {
         log.warning("No fieldType: " + fieldTypeName
-                + " found for dictionary: " + name);
++ " found for dictionary: " + name + ".  Using WhitespaceAnalzyer.");
         analyzer = new WhitespaceAnalyzer();
 
         // check if character encoding is defined
---------------
-------------
@@ -112,7 +112,7 @@
       success = true;
     } finally {
       if (success) {
-        IOUtils.close(w);
+w.shutdown();
       } else {
         IOUtils.closeWhileHandlingException(w);
       }
---------------
-------------
@@ -1117,7 +1117,7 @@
             if (filter.path.superColumnName != null)
             {
                 QueryFilter nameFilter = new NamesQueryFilter(filter.key, new QueryPath(columnFamily_), filter.path.superColumnName);
-                ColumnFamily cf = getColumnFamilyInternal(nameFilter, getDefaultGCBefore());
+ColumnFamily cf = getColumnFamilyInternal(nameFilter, gcBefore);
                 if (cf == null || cf.getColumnCount() == 0)
                     return cf;
 
---------------
-------------
@@ -3596,7 +3596,7 @@
     TermPositions tps = s.getIndexReader().termPositions(new Term("field", "a"));
     assertTrue(tps.next());
     assertEquals(1, tps.freq());
-    assertEquals(0, tps.nextPosition());
+assertEquals(-1, tps.nextPosition());
     w.close();
 
     assertTrue(_TestUtil.checkIndex(dir));
---------------
-------------
@@ -112,7 +112,7 @@
           writer.addDocument(d);
         }
         writer.forceMerge(1);
-        writer.close();
+writer.shutdown();
 
         dictionary = new HighFrequencyDictionary(DirectoryReader.open(ramDir),
                 WORD_FIELD_NAME, 0.0f);
---------------
-------------
@@ -90,7 +90,7 @@
 
     CDFitness f = (CDFitness) obj;
 
-    return tp == f.getFp() && fp == f.getFp() && tn == f.getTn() && fn == f.getTn();
+return tp == f.getTp() && fp == f.getFp() && tn == f.getTn() && fn == f.getFn();
   }
 
   @Override
---------------
-------------
@@ -52,7 +52,7 @@
 
     JobConf jobConf = new JobConf(SlopeOnePrefsToDiffsJob.class);
 
-    FileSystem fs = FileSystem.get(jobConf);
+FileSystem fs = FileSystem.get(outputPathPath.toUri(), jobConf);
     if (fs.exists(outputPathPath)) {
       fs.delete(outputPathPath, true);
     }
---------------
-------------
@@ -27,7 +27,7 @@
 import java.util.Collection;
 
 import org.apache.cassandra.io.ICompactSerializer2;
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.db.marshal.AbstractType;
 
 public class ColumnFamilySerializer implements ICompactSerializer2<ColumnFamily>
---------------
-------------
@@ -42,7 +42,7 @@
 
   @Override
   public PostingsReaderBase postingsReaderBase(SegmentReadState state) throws IOException {
-    return new Lucene40PostingsReader(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+return new Lucene40PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
   }
 
   @Override
---------------
-------------
@@ -103,7 +103,7 @@
     assert field != null;
     Similarity sim = previousMappings.get(field);
     if (sim == null) {
-      sim = knownSims.get(Math.abs(perFieldSeed ^ field.hashCode()) % knownSims.size());
+sim = knownSims.get(Math.max(0, Math.abs(perFieldSeed ^ field.hashCode())) % knownSims.size());
       previousMappings.put(field, sim);
     }
     return sim;
---------------
-------------
@@ -53,7 +53,7 @@
   /** Acts like LetterTokenizer. */
   // the ugly regex below is incomplete Unicode 5.2 [:Letter:]
   public static final CharacterRunAutomaton SIMPLE =
-    new CharacterRunAutomaton(new RegExp("[A-Za-zªµºÀ-ÖØ-öø-Ｚ]+").toAutomaton());
+new CharacterRunAutomaton(new RegExp("[A-Za-zªµºÀ-ÖØ-öø-ˁ]+").toAutomaton());
 
   private final CharacterRunAutomaton runAutomaton;
   private final boolean lowerCase;
---------------
-------------
@@ -76,7 +76,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), a, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -3428,7 +3428,7 @@
 					break;
 				case CodePoint.TRGDFTRT:
 					byte b = reader.readByte();
-					if (b == 0xF1)
+if (b == (byte)0xF1)
 						database.sendTRGDFTRT = true;
 					break;
 				//optional - not used in JCC so skip for now
---------------
-------------
@@ -1090,7 +1090,7 @@
   @Override
   public void setUp() throws Exception {
     super.setUp();
-    indexDir = new File(TEMP_DIR, "IndexReaderReopen");
+indexDir = _TestUtil.getTempDir("IndexReaderReopen");
   }
   
   public void testCloseOrig() throws Throwable {
---------------
-------------
@@ -257,7 +257,7 @@
 	protected void setCollationUsingCompilationSchema(int collationDerivation)
 	throws StandardException {
 		dataTypeServices.setCollationType(
-	    	     getSchemaDescriptor(null).getCollationType());
+getSchemaDescriptor(null, false).getCollationType());
 		dataTypeServices.setCollationDerivation(collationDerivation);
 	}
 
---------------
-------------
@@ -497,7 +497,7 @@
                 }
                 
                 parameterMetaData_.clientParamtertype_[parameterIndex - 1] = java.sql.Types.BIT;
-                setInput(parameterIndex, new Short((short) (x ? 1 : 0)));
+setInput(parameterIndex, Boolean.valueOf(x));
             }
         }
         catch ( SqlException se )
---------------
-------------
@@ -3239,7 +3239,7 @@
 				int jdbcColumnType = rsmd.getColumnType(index);
 
 				switch (jdbcColumnType) {
-				case org.apache.derby.iapi.reference.JDBC20Translation.SQL_TYPES_JAVA_OBJECT:
+case Types.JAVA_OBJECT:
 				case Types.OTHER:
 				{
 					cti = TypeId.getUserDefinedTypeId(rsmd.getColumnTypeName(index), false);
---------------
-------------
@@ -407,7 +407,7 @@
       // This is too long and ugly to put in. Besides, it varies.
       assertNotNull(desc.getProperty("solr.core.instanceDir"));
 
-      assertEquals("data/", desc.getProperty("solr.core.dataDir"));
+assertEquals("data" + File.separator, desc.getProperty("solr.core.dataDir"));
       assertEquals("solrconfig-minimal.xml", desc.getProperty("solr.core.configName"));
       assertEquals("schema-tiny.xml", desc.getProperty("solr.core.schemaName"));
       core1.close();
---------------
-------------
@@ -154,7 +154,7 @@
             // "hoist up" the requested data into a more recent sstable
             if (sstablesIterated >= cfs.getMinimumCompactionThreshold() && cfs.getCompactionStrategy() instanceof SizeTieredCompactionStrategy)
             {
-                RowMutation rm = new RowMutation(cfs.table.name, new Row(filter.key, returnCF));
+RowMutation rm = new RowMutation(cfs.table.name, new Row(filter.key, returnCF.cloneMe()));
                 try
                 {
                     rm.applyUnsafe(); // skipping commitlog is fine since we're just de-fragmenting existing data
---------------
-------------
@@ -1532,7 +1532,7 @@
             rows.add(new Row(key, getColumnFamily(filter)));
         }
 
-        return new RangeSliceReply(rows, rr.rangeCompletedLocally);
+return new RangeSliceReply(rows);
     }
 
     public AbstractType getComparator()
---------------
-------------
@@ -56,7 +56,7 @@
   
   @Override
   protected long size(SegmentInfo info) throws IOException {
-    long byteSize = info.sizeInBytes();
+long byteSize = info.sizeInBytes(true);
     float delRatio = (info.docCount <= 0 ? 0.0f : ((float)info.getDelCount() / (float)info.docCount));
     return (info.docCount <= 0 ?  byteSize : (long)((1.0f - delRatio) * byteSize));
   }
---------------
-------------
@@ -52,7 +52,7 @@
   protected SolrCore core;
   protected SolrRequestParsers parsers;
   protected boolean handleSelect = false;
-  protected String pathPrefix = null; // strip this from the begging of a path
+protected String pathPrefix = null; // strip this from the beginning of a path
   protected String abortErrorMessage = null;
   
   public void init(FilterConfig config) throws ServletException 
---------------
-------------
@@ -361,7 +361,7 @@
                     keyColumns,      
                     eliminateDuplicates,// remove duplicates?
                     -1,                 // RESOLVE - is there a row estimate?
-                    -1,                 // RESOLVE - when should it go to disk?
+maxCapacity,
                     initialCapacity,    // in memory Hashtable initial capacity
                     loadFactor,         // in memory Hashtable load factor
                     runTimeStatisticsOn,
---------------
-------------
@@ -248,7 +248,7 @@
         {
             int header = 0;
             if (isAlias)
-                header = 0x8000 | ((byte)comparatorName.charAt(0));
+header = 0x8000 | (((byte)comparatorName.charAt(0)) & 0xFF);
             else
                 header = comparatorName.length();
             putShortLength(bb, header);
---------------
-------------
@@ -36,7 +36,7 @@
  * @lucene.internal
  **/
 
-public final class FixedBitSet extends DocIdSet {
+public final class FixedBitSet extends DocIdSet implements Bits {
   private final long[] bits;
   private int numBits;
 
---------------
-------------
@@ -105,7 +105,7 @@
 			TermQuery tq = new TermQuery( new Term( field, word));
 			try
 			{
-				tmp.add( tq, false, false);
+tmp.add( tq, BooleanClause.Occur.SHOULD);
 			}
 			catch( BooleanQuery.TooManyClauses too)
 			{
---------------
-------------
@@ -137,7 +137,7 @@
   }
 
   public static BytesRef analyzeMultiTerm(String field, String part, Analyzer analyzerIn) {
-    if (part == null) return null;
+if (part == null || analyzerIn == null) return null;
 
     TokenStream source;
     try {
---------------
-------------
@@ -37,7 +37,7 @@
     // NOTE: if we see a fail on this test with "NestedPulsing" its because its 
     // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing 
     // for more details
-    final MockDirectoryWrapper dir = newDirectory();
+final MockDirectoryWrapper dir = newMockDirectory();
     final TieredMergePolicy tmp = new TieredMergePolicy();
     tmp.setMaxMergeAtOnce(2);
     final RandomIndexWriter w = new RandomIndexWriter(random(), dir,
---------------
-------------
@@ -36,7 +36,7 @@
 
     private AggregateConverter service;
 
-    protected void setUp() {
+protected void setUp() throws Exception {
         service = new AggregateConverter(new TestBlueprintContainer(null));
     }
 
---------------
-------------
@@ -380,7 +380,7 @@
     public long getPosition(DecoratedKey decoratedKey, Operator op)
     {
         // first, check bloom filter
-        if (op == Operator.EQ && !bf.isPresent(partitioner.convertToDiskFormat(decoratedKey)))
+if (op == Operator.EQ && !bf.isPresent(decoratedKey.key))
             return -1;
 
         // next, the key cache
---------------
-------------
@@ -39,7 +39,7 @@
 
         try
         {
-            RowMutation rm = RowMutation.serializer().deserialize(new DataInputStream(buffer));
+RowMutation rm = RowMutation.serializer().deserialize(new DataInputStream(buffer), message.getVersion());
             rm.applyBinary();
 
             WriteResponse response = new WriteResponse(rm.getTable(), rm.key(), true);
---------------
-------------
@@ -71,7 +71,7 @@
 		StandardAnalyzer analyzer = new StandardAnalyzer(new HashSet());
 		// TODO: something about lock timeouts and leftover locks.
 		IndexWriter writer = new IndexWriter(storeDirectory, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
-		IndexSearcher searcher = new IndexSearcher(storePathname);
+IndexSearcher searcher = new IndexSearcher(storeDirectory, true);
 		
 		for(int dx = 0; dx < 1000; dx ++) {
 			String f = randomField();
---------------
-------------
@@ -69,7 +69,7 @@
       PostingsWriterBase postingsWriter,
       Comparator<BytesRef> termComp) throws IOException
   {
-    final String termsFileName = IndexFileNames.segmentFileName(state.segmentName, "", TERMS_EXTENSION);
+final String termsFileName = IndexFileNames.segmentFileName(state.segmentName, state.codecId, TERMS_EXTENSION);
     this.termsIndexWriter = termsIndexWriter;
     this.termComp = termComp;
     out = state.directory.createOutput(termsFileName);
---------------
-------------
@@ -116,7 +116,7 @@
     //writer.optimize();
     writer.close();
 
-    IndexSearcher searcher = new IndexSearcher(directory);
+IndexSearcher searcher = new IndexSearcher(directory, true);
     searcher.setSimilarity(similarity);
     return searcher;
   }
---------------
-------------
@@ -261,7 +261,7 @@
     return this;
   }
 
-  public boolean setHighlightRequireFieldMatch() {
+public boolean getHighlightRequireFieldMatch() {
     return this.getBool(HighlightParams.FIELD_MATCH, false);
   }
 
---------------
-------------
@@ -1281,7 +1281,7 @@
       // sort the test methods first before shuffling them, so that the shuffle is consistent
       // across different implementations that might order the methods different originally.
       Collections.sort(testMethods, new Comparator<FrameworkMethod>() {
-        @Override
+/* not until java 6 @Override */
         public int compare(FrameworkMethod f1, FrameworkMethod f2) {
           return f1.getName().compareTo(f2.getName());
         }
---------------
-------------
@@ -150,7 +150,7 @@
         }
         doFail.set(this);
         final String id = ""+r.nextInt(50);
-        idField.setValue(id);
+idField.setStringValue(id);
         Term idTerm = new Term("id", id);
         try {
           if (r.nextBoolean()) {
---------------
-------------
@@ -35,7 +35,7 @@
     int textlen;
     
     public FuzzyTermEnum(IndexReader reader, Term term) throws IOException {
-        super(reader, term);
+super();
         searchTerm = term;
         field = searchTerm.field();
         text = searchTerm.text();
---------------
-------------
@@ -63,7 +63,7 @@
   }
 
   @Override
-  synchronized public void abort() {
+public void abort() {
     consumer.abort();
     if (nextTermsHash != null)
       nextTermsHash.abort();
---------------
-------------
@@ -82,7 +82,7 @@
     final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);
 
     SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,
-                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));
+MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));
     merger.add(reader1);
     merger.add(reader2);
     MergeState mergeState = merger.merge();
---------------
-------------
@@ -431,7 +431,7 @@
     {
         isBootstrapMode = true;
         SystemTable.updateToken(token); // DON'T use setToken, that makes us part of the ring locally which is incorrect until we are done bootstrapping
-        Gossiper.instance.addLocalApplicationState(ApplicationState.STATUS, valueFactory.normal(token));
+Gossiper.instance.addLocalApplicationState(ApplicationState.STATUS, valueFactory.bootstrapping(token));
         setMode("Joining: sleeping " + RING_DELAY + " ms for pending range setup", true);
         try
         {
---------------
-------------
@@ -390,7 +390,7 @@
       docsEnum = reader.termDocsEnum(reader.getLiveDocs(),
           term.field(),
           new BytesRef(term.text()),
-          false);
+0);
       if (docsEnum != null) {
         int docId;
         if ((docId = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
---------------
-------------
@@ -50,7 +50,7 @@
   public Query parse(QualityQuery qq) throws ParseException {
     QueryParser qp = queryParser.get();
     if (qp==null) {
-      qp = new QueryParser(indexField, new StandardAnalyzer(Version.LUCENE_CURRENT));
+qp = new QueryParser(Version.LUCENE_CURRENT, indexField, new StandardAnalyzer(Version.LUCENE_CURRENT));
       queryParser.set(qp);
     }
     return qp.parse(qq.getValue(qqName));
---------------
-------------
@@ -87,7 +87,7 @@
     doc.add(new FacetField("Publish Date", "1999", "5", "5"));
     indexWriter.addDocument(config.build(taxoWriter, doc));
     
-    indexWriter.close();
+indexWriter.shutdown();
     taxoWriter.close();
   }
 
---------------
-------------
@@ -243,7 +243,7 @@
           throw new CorruptIndexException("doc counts differ for segment " + segment + ": fieldsReader shows " + fieldsReaderOrig.size() + " but segmentInfo shows " + si.docCount);
         }
 
-        if (fieldInfos.hasVectors()) { // open term vector files only as needed
+if (si.getHasVectors()) { // open term vector files only as needed
           termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);
         }
       }
---------------
-------------
@@ -128,7 +128,7 @@
       return puller.pull((AtomicReader) reader, field);
     }
     assert reader instanceof CompositeReader;
-    final List<AtomicReaderContext> leaves = reader.getTopReaderContext().leaves();
+final List<AtomicReaderContext> leaves = reader.leaves();
     switch (leaves.size()) {
       case 0:
         // no fields
---------------
-------------
@@ -197,7 +197,7 @@
     @Override
     public SolrParams getParams() {
       ModifiableSolrParams params = (ModifiableSolrParams) super.getParams();
-      params.set( "collections", collection );
+params.set( "collections", aliasedCollections );
       return params;
     }
 
---------------
-------------
@@ -18,7 +18,7 @@
 
 package org.apache.cassandra.utils;
 
-public class Pair<T1, T2>
+public final class Pair<T1, T2>
 {
     public final T1 left;
     public final T2 right;
---------------
-------------
@@ -35,7 +35,7 @@
         buffer_ = ByteBuffer.allocate(4);
     }
 
-    public byte[] read() throws IOException, ReadNotCompleteException
+public byte[] read() throws IOException
     {        
         return doRead(buffer_);
     }
---------------
-------------
@@ -209,7 +209,7 @@
                                                            MeanShiftCanopy.class);
       try {
         for (VectorWritable value : new SequenceFileValueIterable<VectorWritable>(s.getPath(), conf)) {
-          writer.append(new Text(), new MeanShiftCanopy(value.get(), id++, measure));
+writer.append(new Text(), MeanShiftCanopy.initialCanopy(value.get(), id++, measure));
         }
       } finally {
         writer.close();
---------------
-------------
@@ -110,7 +110,7 @@
     // otherwise scores are different!
     searcher.setSimilarity(new DefaultSimilarity());
     
-    writer.close();
+writer.shutdown();
     String line;
     while ((line = reader.readLine()) != null) {
       String params[] = line.split(",");
---------------
-------------
@@ -55,7 +55,7 @@
 {
     public String asHex(String str)
     {
-        return bytesToHex(ByteBuffer.wrap(str.getBytes()));
+return bytesToHex(ByteBufferUtil.bytes(str));
     }
 
     @Test
---------------
-------------
@@ -64,7 +64,7 @@
       while (term != null) {
         T shape = readShape(term);
         if( shape != null ) {
-          docs = te.docs(null, docs, 0);
+docs = te.docs(null, docs, false);
           Integer docid = docs.nextDoc();
           while (docid != DocIdSetIterator.NO_MORE_DOCS) {
             idx.add( docid, shape );
---------------
-------------
@@ -57,6 +57,6 @@
     
     /** blast some random strings through the analyzer */
     public void testRandomStrings() throws Exception {
-      checkRandomData(random(), new RussianAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new RussianAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
     }
 }
---------------
-------------
@@ -41,7 +41,7 @@
 // TODO: if we create PulsingPostingsBaseFormat then we
 // can simplify this? note: I don't like the *BaseFormat
 // hierarchy, maybe we can clean that up...
-public class NestedPulsingPostingsFormat extends PostingsFormat {
+public final class NestedPulsingPostingsFormat extends PostingsFormat {
   public NestedPulsingPostingsFormat() {
     super("NestedPulsing");
   }
---------------
-------------
@@ -129,6 +129,6 @@
      * @return true if the server supports this
      */
     final boolean serverSupportsUDTs() {
-        return supportsSessionDataCaching_;
+return supportsUDTs_;
     }
 }
---------------
-------------
@@ -453,7 +453,7 @@
     assert Thread.holdsLock(writer);
 
     if (infoStream.isEnabled("IFD")) {
-      infoStream.message("IFD", "now checkpoint \"" + writer.segString(segmentInfos) + "\" [" + segmentInfos.size() + " segments " + "; isCommit = " + isCommit + "]");
+infoStream.message("IFD", "now checkpoint \"" + writer.segString(writer.toLiveInfos(segmentInfos)) + "\" [" + segmentInfos.size() + " segments " + "; isCommit = " + isCommit + "]");
     }
 
     // Try again now to delete any previously un-deletable
---------------
-------------
@@ -359,7 +359,7 @@
         // compact so we have a big row with more than the minimum index count
         if (cfStore.getSSTables().size() > 1)
         {
-            cfStore.doCompaction(2, cfStore.getSSTables().size());
+CompactionManager.instance.submitMajor(cfStore).get();
         }
         SSTableReader sstable = cfStore.getSSTables().iterator().next();
         DecoratedKey decKey = sstable.getPartitioner().decorateKey(key);
---------------
-------------
@@ -209,7 +209,7 @@
                 throw new ServiceUnavailableException("Service is unavailable", getOsgiFilter());
             }
             if (service == null) {
-                service = getBundleContextForServiceLookup().getService(reference);
+service = getServiceSecurely(reference);
             }
             return service;
         }
---------------
-------------
@@ -550,7 +550,7 @@
   private void verifyTermDocs(Directory dir, Term term, int numDocs)
       throws IOException {
     IndexReader reader = DirectoryReader.open(dir);
-    DocsEnum docsEnum = _TestUtil.docs(random(), reader, term.field, term.bytes, null, null, 0);
+DocsEnum docsEnum = _TestUtil.docs(random(), reader, term.field, term.bytes, null, null, false);
     int count = 0;
     while (docsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS)
       count++;
---------------
-------------
@@ -77,7 +77,7 @@
   public CharsRef indexedToReadable(BytesRef input, CharsRef charsRef) {
     // TODO: this could be more efficient, but the sortable types should be deprecated instead
     final char[] indexedToReadable = indexedToReadable(input.utf8ToChars(charsRef).toString()).toCharArray();
-    charsRef.copy(indexedToReadable, 0, indexedToReadable.length);
+charsRef.copyChars(indexedToReadable, 0, indexedToReadable.length);
     return charsRef;
   }
 
---------------
-------------
@@ -41,7 +41,7 @@
       iw.addDocument(doc);
     }
     ir = iw.getReader();
-    iw.close();
+iw.shutdown();
     is = newSearcher(ir);
   }
   
---------------
-------------
@@ -172,7 +172,7 @@
   public int hashCode() {
     int h = include.hashCode();
     h = (h<<1) | (h >>> 31);  // rotate left
-    h ^= include.hashCode();
+h ^= exclude.hashCode();
     h = (h<<1) | (h >>> 31);  // rotate left
     h ^= Float.floatToRawIntBits(getBoost());
     return h;
---------------
-------------
@@ -42,7 +42,7 @@
     return range != null && range.includes(hash);
   }
 
-  protected int sliceHash(String id, SolrInputDocument sdoc, SolrParams params) {
+public int sliceHash(String id, SolrInputDocument sdoc, SolrParams params) {
     return Hash.murmurhash3_x86_32(id, 0, id.length(), 0);
   }
 
---------------
-------------
@@ -575,7 +575,7 @@
         String keyspace = state().getKeyspace();
         state().hasColumnFamilyAccess(column_parent.column_family, Permission.READ);
 
-        CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_parent.column_family, false);
+CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_parent.column_family);
         ThriftValidation.validateColumnParent(metadata, column_parent);
         ThriftValidation.validatePredicate(metadata, column_parent, predicate);
         ThriftValidation.validateKeyRange(range);
---------------
-------------
@@ -63,7 +63,7 @@
     // Now search the index:
     IndexSearcher isearcher = new IndexSearcher(directory, true); // read-only=true
     // Parse a simple query that searches for "text":
-    QueryParser parser = new QueryParser("fieldname", analyzer);
+QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, "fieldname", analyzer);
     Query query = parser.parse("text");
     ScoreDoc[] hits = isearcher.search(query, null, 1000).scoreDocs;
     assertEquals(1, hits.length);
---------------
-------------
@@ -54,7 +54,7 @@
     	ByteArrayOutputStream bos = new ByteArrayOutputStream();
         DataOutputStream dos = new DataOutputStream( bos );
         ReadResponse.serializer().serialize(readResponse, dos);
-        Message message = new Message(StorageService.getLocalStorageEndPoint(), MessagingService.responseStage_, MessagingService.responseVerbHandler_, new Object[]{bos.toByteArray()});         
+Message message = new Message(StorageService.getLocalStorageEndPoint(), MessagingService.responseStage_, MessagingService.responseVerbHandler_, bos.toByteArray());
         return message;
     }
 	
---------------
-------------
@@ -152,7 +152,7 @@
     }
     List<Node> copy = new ArrayList<Node>(cells.size());//copy since cells contractually isn't modifiable
     for (Node cell : cells) {
-      SpatialRelation rel = cell.getShape().relate(shapeFilter, spatialPrefixTree.ctx);
+SpatialRelation rel = cell.getShape().relate(shapeFilter);
       if (rel == SpatialRelation.DISJOINT)
         continue;
       cell.shapeRel = rel;
---------------
-------------
@@ -24,7 +24,7 @@
 import java.nio.ByteBuffer;
 import java.util.UUID;
 
-public class JdbcUUID extends JdbcLong
+public class JdbcUUID extends AbstractJdbcUUID
 {
     public static final JdbcUUID instance = new JdbcUUID();
     
---------------
-------------
@@ -39,7 +39,7 @@
 import org.apache.aries.application.management.spi.convert.BundleConversion;
 import org.apache.aries.application.management.spi.runtime.LocalPlatform;
 import org.apache.aries.application.utils.AppConstants;
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
---------------
-------------
@@ -42,7 +42,7 @@
  * This is here just to test the core sep codec
  * classes.
  */
-public class MockSepPostingsFormat extends PostingsFormat {
+public final class MockSepPostingsFormat extends PostingsFormat {
 
   public MockSepPostingsFormat() {
     super("MockSep");
---------------
-------------
@@ -60,7 +60,7 @@
   }
   
   public void testBaseDir() throws Exception {
-    final File base = _TestUtil.getTempDir("fsResourceLoaderBase");
+final File base = _TestUtil.getTempDir("fsResourceLoaderBase").getAbsoluteFile();
     try {
       base.mkdirs();
       Writer os = new OutputStreamWriter(new FileOutputStream(new File(base, "template.txt")), IOUtils.CHARSET_UTF_8);
---------------
-------------
@@ -107,7 +107,7 @@
         if (logger.isDebugEnabled())
             logger.debug("Finished {}. Sending ack to {}", remoteFile, this);
 
-        Future future = CompactionManager.instance.submitSSTableBuild(localFile.desc);
+Future future = CompactionManager.instance.submitSSTableBuild(localFile.desc, remoteFile.type);
         buildFutures.add(future);
 
         files.remove(remoteFile);
---------------
-------------
@@ -71,7 +71,7 @@
           writer.updateDocument(idTerm, doc);
         } catch (RuntimeException re) {
           if (VERBOSE) {
-            System.out.println("EXC: ");
+System.out.println(Thread.currentThread().getName() + ": EXC: ");
             re.printStackTrace(System.out);
           }
           try {
---------------
-------------
@@ -119,7 +119,7 @@
     
     reader = writer.getReader();
     searcher = newSearcher(reader);
-    writer.close();
+writer.shutdown();
     
     sims = new ArrayList<>();
     for (BasicModel basicModel : BASIC_MODELS) {
---------------
-------------
@@ -247,7 +247,7 @@
     
     reader = writer.getReader();
     searcher = newSearcher(reader);
-    writer.close();
+writer.shutdown();
     
   }
   
---------------
-------------
@@ -52,7 +52,7 @@
  </ul>
  
  <p>IndexReader instances for indexes on disk are usually constructed
- with a call to one of the static <code>DirectoryReader,open()</code> methods,
+with a call to one of the static <code>DirectoryReader.open()</code> methods,
  e.g. {@link DirectoryReader#open(Directory)}. {@link DirectoryReader} implements
  the {@link CompositeReader} interface, it is not possible to directly get postings.
 
---------------
-------------
@@ -276,7 +276,7 @@
         try
         {
             /* Make message */
-            message = rm.makeRowMutationMessage(StorageService.Verb.BINARY);
+message = rm.makeRowMutationMessage(StorageService.Verb.BINARY, MessagingService.version_);
         }
         catch (IOException e)
         {
---------------
-------------
@@ -66,7 +66,7 @@
               reader.norms(query.getField()));
     }
 
-    class BoostingSpanScorer extends SpanScorer {
+protected class BoostingSpanScorer extends SpanScorer {
 
       //TODO: is this the best way to allocate this?
       byte[] payload = new byte[256];
---------------
-------------
@@ -207,7 +207,7 @@
         if (docFreq >= freqmin && docFreq <= freqmax) {
           // add the term to the list
           if (sort) {
-            queue.add(new CountPair<BytesRef, Integer>(new BytesRef(term), docFreq));
+queue.add(new CountPair<BytesRef, Integer>(BytesRef.deepCopyOf(term), docFreq));
           } else {
 
             // TODO: handle raw somehow
---------------
-------------
@@ -22,7 +22,7 @@
 package org.apache.derby.client.am;
 
 import java.sql.SQLWarning;
-import org.apache.derby.shared.common.info.JVMInfo;
+import org.apache.derby.iapi.services.info.JVMInfo;
 
 /**
  * This represents a warning versus a full exception.  As with
---------------
-------------
@@ -1,6 +1,6 @@
 /*
  
-Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun
+Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun_CleanUp
  
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -61,7 +61,7 @@
     static volatile boolean newReaderCalled = false;
 
     @Override
-    public DirectoryReader newReader(Directory indexDir) throws IOException {
+public DirectoryReader newReader(Directory indexDir, SolrCore core) throws IOException {
       TestIndexReaderFactory.newReaderCalled = true;
       return DirectoryReader.open(indexDir);
     }
---------------
-------------
@@ -112,7 +112,7 @@
         this.trailingBlanks = trailingBlanks;
         this.remainingNonBlanks = length - trailingBlanks;
         this.remainingBlanks = trailingBlanks;
-        this.alphabet = alphabet;
+this.alphabet = alphabet.getClone();
         fillBuffer(alphabet.charCount());
     }
 
---------------
-------------
@@ -91,7 +91,7 @@
   
   // should produce no exceptions
   public void testEmptyArraySort() {
-    List<Integer> list = Collections.emptyList();
+List<Integer> list = Arrays.asList(new Integer[0]);
     CollectionUtil.quickSort(list);
     CollectionUtil.mergeSort(list);
     CollectionUtil.insertionSort(list);
---------------
-------------
@@ -73,6 +73,6 @@
 
     public long maxTimestamp()
     {
-        throw new UnsupportedOperationException();
+return Long.MIN_VALUE;
     }
 }
---------------
-------------
@@ -222,7 +222,7 @@
         assertTrue("posLength must be >= 1", posLengthAtt.getPositionLength() >= 1);
       }
     }
-    assertFalse("TokenStream has more tokens than expected", ts.incrementToken());
+assertFalse("TokenStream has more tokens than expected (expected count=" + output.length + ")", ts.incrementToken());
     ts.end();
     if (finalOffset != null) {
       assertEquals("finalOffset ", finalOffset.intValue(), offsetAtt.endOffset());
---------------
-------------
@@ -130,7 +130,7 @@
       currentBuffer = (byte[]) file.buffers.get(currentBufferIndex);
     }
     bufferPosition = 0;
-    bufferStart = BUFFER_SIZE * currentBufferIndex;
+bufferStart = (long) BUFFER_SIZE * (long) currentBufferIndex;
     bufferLength = currentBuffer.length;
   }
 
---------------
-------------
@@ -100,7 +100,7 @@
         hostContext.append("_");
       }
       hostContext.append(_TestUtil.randomSimpleString(random(), 3));
-      if ( ! "/".equals(hostContext)) {
+if ( ! "/".equals(hostContext.toString())) {
         // if our random string is empty, this might add a trailing slash, 
         // but our code should be ok with that
         hostContext.append("/").append(_TestUtil.randomSimpleString(random(), 2));
---------------
-------------
@@ -390,7 +390,7 @@
       docsEnum = reader.termDocsEnum(reader.getLiveDocs(),
           term.field(),
           new BytesRef(term.text()),
-          0);
+false);
       if (docsEnum != null) {
         int docId;
         if ((docId = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
---------------
-------------
@@ -145,7 +145,7 @@
     samplingParams.setMaxSampleSize((int) (10000 * retryFactor));
     samplingParams.setOversampleFactor(5.0 * retryFactor);
 
-    samplingParams.setSampingThreshold(11000); //force sampling 
+samplingParams.setSamplingThreshold(11000); //force sampling
     Sampler sampler = useRandomSampler ? 
         new RandomSampler(samplingParams, new Random(random().nextLong())) :
           new RepeatableSampler(samplingParams);
---------------
-------------
@@ -50,7 +50,7 @@
 
     public final static SqlCode queuedXAError = new SqlCode(-4203);
 
-    public final static SqlCode disconnectError = new SqlCode(-4499);
+public final static SqlCode disconnectError = new SqlCode(40000);
 
     public final static SqlCode undefinedError = new SqlCode(-99999);
     
---------------
-------------
@@ -306,7 +306,7 @@
     conf.setOutputFormat(SequenceFileOutputFormat.class);
     
     conf.setMapOutputKeyClass(Text.class);
-    conf.setMapOutputValueClass(VectorWritable.class);
+conf.setMapOutputValueClass(Text.class);
     conf.setOutputKeyClass(Text.class);
     // the output is the cluster id
     conf.setOutputValueClass(Text.class);
---------------
-------------
@@ -190,7 +190,7 @@
           // so that the text of the file is tokenized and indexed, but not stored.
           // Note that FileReader expects the file to be in UTF-8 encoding.
           // If that's not the case searching for special characters will fail.
-          doc.add(new TextField("contents", new BufferedReader(new InputStreamReader(fis, "UTF-8")), Field.Store.NO));
+doc.add(new TextField("contents", new BufferedReader(new InputStreamReader(fis, "UTF-8"))));
 
           if (writer.getConfig().getOpenMode() == OpenMode.CREATE) {
             // New index, so we just add the document (no old document can be there):
---------------
-------------
@@ -28,7 +28,7 @@
 import java.io.EOFException;
 
 /**
-	Converts a stream containing the Cloudscape stored binary form
+Converts a stream containing the Derby stored binary form
 	to one that just contains the application's data.
 	Simply read and save the length information.
 */
---------------
-------------
@@ -104,7 +104,7 @@
     if (RUN_LENGTH != -1) {
       runLength = RUN_LENGTH;
     } else {
-      int[] runTimes = new int[] {5000,6000,10000,15000,15000,30000,30000,45000,90000,120000};
+int[] runTimes = new int[] {5000,6000,10000,25000,27000,30000,30000,45000,90000,120000};
       runLength = runTimes[random().nextInt(runTimes.length - 1)];
     }
     try {
---------------
-------------
@@ -59,7 +59,7 @@
       target.length = 0;
       return false;
     }
-    target.copy(s);
+target.copyChars(s);
     return true;
   };
 
---------------
-------------
@@ -81,7 +81,7 @@
       BytesRef b = te.next();
       assertNotNull(b);
       assertEquals(t, b.utf8ToString());
-      DocsEnum td = _TestUtil.docs(random(), te, liveDocs, null, false);
+DocsEnum td = _TestUtil.docs(random(), te, liveDocs, null, 0);
       assertTrue(td.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
       assertEquals(0, td.docID());
       assertEquals(td.nextDoc(), DocIdSetIterator.NO_MORE_DOCS);
---------------
-------------
@@ -163,7 +163,7 @@
       IndexInput bloomIn = null;
       boolean success = false;
       try {
-        bloomIn = state.dir.openInput(bloomFileName, state.context);
+bloomIn = state.directory.openInput(bloomFileName, state.context);
         CodecUtil.checkHeader(bloomIn, BLOOM_CODEC_NAME, BLOOM_CODEC_VERSION,
             BLOOM_CODEC_VERSION);
         // // Load the hash function used in the BloomFilter
---------------
-------------
@@ -53,7 +53,7 @@
     {
         stages.put(MUTATION_STAGE, multiThreadedStage(MUTATION_STAGE, getConcurrentWriters()));
         stages.put(READ_STAGE, multiThreadedStage(READ_STAGE, getConcurrentReaders()));
-        stages.put(RESPONSE_STAGE, multiThreadedStage("RESPONSE-STAGE", MessagingService.MESSAGE_DESERIALIZE_THREADS));
+stages.put(RESPONSE_STAGE, multiThreadedStage("RESPONSE-STAGE", Runtime.getRuntime().availableProcessors()));
         // the rest are all single-threaded
         stages.put(STREAM_STAGE, new JMXEnabledThreadPoolExecutor(STREAM_STAGE));
         stages.put(GOSSIP_STAGE, new JMXEnabledThreadPoolExecutor("GMFD"));
---------------
-------------
@@ -40,7 +40,7 @@
   public void testAltReaderUsed() throws Exception {
     IndexReaderFactory readerFactory = h.getCore().getIndexReaderFactory();
     assertNotNull("Factory is null", readerFactory);
-    assertTrue("readerFactory is not an instanceof " + AlternateIndexReaderTest.TestIndexReaderFactory.class, readerFactory instanceof StandardIndexReaderFactory);
+assertTrue("readerFactory is not an instanceof " + AlternateDirectoryTest.TestIndexReaderFactory.class, readerFactory instanceof StandardIndexReaderFactory);
     assertTrue("termInfoIndexDivisor not set to 12", readerFactory.getTermInfosIndexDivisor() == 12);
 
 
---------------
-------------
@@ -47,7 +47,7 @@
 		else if (EnumSet.of(State.INSTALL_FAILED, State.UNINSTALLING, State.UNINSTALLED).contains(state))
 			throw new IllegalStateException("Cannot stop from state " + state);
 		else if (EnumSet.of(State.INSTALLING, State.RESOLVING, State.STARTING, State.STOPPING).contains(state)) {
-			waitForStateChange();
+waitForStateChange(state);
 			return new StopAction(requestor, target, disableRootCheck).run();
 		}
 		target.setState(State.STOPPING);
---------------
-------------
@@ -944,7 +944,7 @@
       CodecProvider.getDefault().setDefaultFieldCodec("Standard");
     }
 
-    final LineFileDocs docs = new LineFileDocs(false);
+final LineFileDocs docs = new LineFileDocs(random);
     final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 100 : 1;
     final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(-1).setRAMBufferSizeMB(64);
     final File tempDir = _TestUtil.getTempDir("fstlines");
---------------
-------------
@@ -632,7 +632,7 @@
       randomIOExceptionRateOnOpen = 0.0;
       if (DirectoryReader.indexExists(this)) {
         if (LuceneTestCase.VERBOSE) {
-          System.out.println("\nNOTE: MockDirectoryWrapper: now crash");
+System.out.println("\nNOTE: MockDirectoryWrapper: now crush");
         }
         crash(); // corrupt any unsynced-files
         if (LuceneTestCase.VERBOSE) {
---------------
-------------
@@ -64,7 +64,7 @@
     final String codec = Codec.getDefault().getName();
     int num = codec.equals("Lucene3x") ? 200 * RANDOM_MULTIPLIER : atLeast(1000);
     for (int i = 0; i < num; i++) {
-      field.setValue(_TestUtil.randomUnicodeString(random, 10));
+field.setStringValue(_TestUtil.randomUnicodeString(random, 10));
       writer.addDocument(doc);
     }
     reader = writer.getReader();
---------------
-------------
@@ -97,7 +97,7 @@
         int totalReplicas = getReplicationFactor(table);
         Map<String, Integer> remainingReplicas = new HashMap<String, Integer>(datacenters.get(table));
         Map<String, Set<String>> dcUsedRacks = new HashMap<String, Set<String>>();
-        Set<InetAddress> endpoints = new HashSet<InetAddress>(totalReplicas);
+Set<InetAddress> endpoints = new LinkedHashSet<InetAddress>(totalReplicas);
 
         // first pass: only collect replicas on unique racks
         for (Iterator<Token> iter = TokenMetadata.ringIterator(tokenMetadata.sortedTokens(), searchToken);
---------------
-------------
@@ -42,6 +42,6 @@
 
 public class Constants {
 
-  public static final String VERSION = "12.1.0";
+public static final String VERSION = "13.0.0";
 
 }
---------------
-------------
@@ -94,7 +94,7 @@
         {
             int clen = Math.min(len, buffer.length);
             for (int i = 0; i < clen; i++) {
-                buffer[i] = (char) b[off + i];
+buffer[i] = (char)(b[off + i] & 0xff);
             }
             writer.write(buffer, 0, clen);
             off += clen;
---------------
-------------
@@ -27,7 +27,7 @@
 
 public class Job {
 
-  public static void main(String[] args) throws Exception {
+public static void main(String[] args) throws IOException {
     if (args.length == 7) {
       String input = args[0];
       String output = args[1];
---------------
-------------
@@ -514,7 +514,7 @@
 	    if (expectedString != null) {
 	        for (int i = 0; i < expectedString.length; i++) {
 	            assertTrue("Could not find expectedString:" +
-	                    expectedString + " in output:" + output,
+expectedString[i] + " in output:" + output,
 	                    output.indexOf(expectedString[i]) >= 0);
 	        }
 	    }
---------------
-------------
@@ -462,7 +462,7 @@
     }
   }
 
-  void incRef(List<String> files) throws IOException {
+void incRef(Collection<String> files) throws IOException {
     for(final String file : files) {
       incRef(file);
     }
---------------
-------------
@@ -104,7 +104,7 @@
       String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
       data = state.directory.openInput(dataName, state.context);
       final int version2 = CodecUtil.checkHeader(data, dataCodec, 
-                                                 Lucene45DocValuesFormat.VERSION_CURRENT,
+Lucene45DocValuesFormat.VERSION_START,
                                                  Lucene45DocValuesFormat.VERSION_CURRENT);
       if (version != version2) {
         throw new CorruptIndexException("Format versions mismatch");
---------------
-------------
@@ -200,7 +200,7 @@
 
 		@exception SQLException	thrown on failure
 	 */
-	protected void closeActions() throws SQLException {
+void closeActions() throws SQLException {
 
 		//we release the resource for preparedStatement
 		preparedStatement = null;
---------------
-------------
@@ -65,7 +65,7 @@
 
     writer.close();
 
-    searcher = new IndexSearcher(directory);
+searcher = new IndexSearcher(directory, true);
   }
   
   public void testTerm() throws Exception {
---------------
-------------
@@ -486,7 +486,7 @@
       fail("fake disk full IOExceptions not hit");
     } catch (IOException ioe) {
       // expected
-      assertTrue(ftdm.didFail1);
+assertTrue(ftdm.didFail1 || ftdm.didFail2);
     }
     _TestUtil.checkIndex(dir);
     ftdm.clearDoFail();
---------------
-------------
@@ -546,7 +546,7 @@
     // we write here (e.g., to write parent+2), and need to do a workaround
     // in the reader (which knows that anyway only category 0 has a parent
     // -1).    
-    parentStream.set(parent + 1);
+parentStream.set(Math.max(parent+1, 1));
     Document d = new Document();
     d.add(parentStreamField);
 
---------------
-------------
@@ -795,7 +795,7 @@
       // KeepOnlyLastCommitDeleter:
       IndexFileDeleter deleter = new IndexFileDeleter(directory,
                                                       deletionPolicy == null ? new KeepOnlyLastCommitDeletionPolicy() : deletionPolicy,
-                                                      segmentInfos, null);
+segmentInfos, null, null);
       segmentInfos.updateGeneration(deleter.getLastSegmentInfos());
       segmentInfos.changed();
 
---------------
-------------
@@ -152,7 +152,7 @@
                 // Add: MyClass.$$FCCL$$<classname>$<methodname>(<class>);                
                 if (ServiceLoader.class.getName().equals(wd.getClassName()) &&
                     "load".equals(wd.getMethodName()) &&
-                    Arrays.equals(new String [] {Class.class.getName()}, wd.getArgClasses())) {
+(wd.getArgClasses() == null || Arrays.equals(new String [] {Class.class.getName()}, wd.getArgClasses()))) {
                     // ServiceLoader.load() is a special case because it's a general-purpose service loader, 
                     // therefore, the target class it the class being passed in to the ServiceLoader.load() 
                     // call itself.
---------------
-------------
@@ -110,7 +110,7 @@
 
     writer.close();
 
-    IndexReader reader = new TestReader(IndexReader.open(directory));
+IndexReader reader = new TestReader(IndexReader.open(directory, true));
 
     assertTrue(reader.isOptimized());
     
---------------
-------------
@@ -1281,7 +1281,7 @@
                                                   a.getMaxDynamicResults());
 					}
                     
-                    resultsToWrap.finish(); // Don't need the result set any more
+resultsToWrap.close(); // Don't need the result set any more
 
                     // executeQuery() is not allowed if the statement
                     // doesn't return exactly one ResultSet.
---------------
-------------
@@ -93,7 +93,7 @@
                                            ChunkedWriter.class,
                                            Charset.class,
                                            FileSystem.class);
-        pathFilter = constructor.newInstance(conf, keyPrefix, options, writer, fs);
+pathFilter = constructor.newInstance(conf, keyPrefix, options, writer, charset, fs);
       }
       fs.listStatus(input, pathFilter);
     } finally {
---------------
-------------
@@ -71,7 +71,7 @@
 	}
 
 	/**
-		@exception StandardException Standard Cloudscape error policy
+@exception StandardException Standard Derby error policy
 	*/
 	protected void openCore() throws StandardException
 	{
---------------
-------------
@@ -49,7 +49,7 @@
     	ByteArrayOutputStream bos = new ByteArrayOutputStream();
         DataOutputStream dos = new DataOutputStream( bos );
         WriteResponse.serializer().serialize(writeResponseMessage, dos);
-        return original.getReply(FBUtilities.getLocalAddress(), bos.toByteArray());
+return original.getReply(FBUtilities.getLocalAddress(), bos.toByteArray(), original.getVersion());
     }
 
 	private final String table_;
---------------
-------------
@@ -881,7 +881,7 @@
   public int docId(AtomicReader reader, Term term) throws IOException {
     int docFreq = reader.docFreq(term);
     assertEquals(1, docFreq);
-    DocsEnum termDocsEnum = reader.termDocsEnum(null, term.field, term.bytes, false);
+DocsEnum termDocsEnum = reader.termDocsEnum(null, term.field, term.bytes, 0);
     int nextDoc = termDocsEnum.nextDoc();
     assertEquals(DocIdSetIterator.NO_MORE_DOCS, termDocsEnum.nextDoc());
     return nextDoc;
---------------
-------------
@@ -38,7 +38,7 @@
 
   @Test
   public void loadClass() throws Exception {
-    Class clz = DocBuilder.loadClass("RegexTransformer");
+Class clz = DocBuilder.loadClass("RegexTransformer", null);
     Assert.assertNotNull(clz);
   }
 
---------------
-------------
@@ -197,7 +197,7 @@
         int chksumChunkId = (int)(chksumChunks*(chunk - 1) + 1);
         do
         {            
-            int fId = SequenceFile.getFileId(filename_);               
+int fId = ChecksumManager.getFileId(filename_);
             switch( chksumOps )
             {
                 case LOG:                    
---------------
-------------
@@ -355,7 +355,7 @@
     {
         StringBuilder sb = new StringBuilder("ColumnFamily(");
         CFMetaData cfm = metadata();
-        sb.append(cfm == null ? "-deleted-" : cfm.cfName);
+sb.append(cfm == null ? "<anonymous>" : cfm.cfName);
 
         if (isMarkedForDelete())
             sb.append(" -deleted at " + getMarkedForDeleteAt() + "-");
---------------
-------------
@@ -387,7 +387,7 @@
   public static CharArraySet copy(final Set<?> set) {
     if(set == EMPTY_SET)
       return EMPTY_SET;
-    return (set instanceof CharArraySet) ? copy((CharArraySet) set) : copy(Version.LUCENE_30, set);
+return copy(Version.LUCENE_30, set);
   }
   
   /**
---------------
-------------
@@ -1293,7 +1293,7 @@
   // share the underlying postings data) will map to the
   // same entry in the FieldCache.  See LUCENE-1579.
   @Override
-  public final Object getFieldCacheKey() {
+public final Object getCoreCacheKey() {
     return core;
   }
   
---------------
-------------
@@ -87,7 +87,7 @@
             success = true;
 
         } finally {
-            if (! success) {
+if (! success && (stream != null)) {
                 try {
                     stream.close();
                 } catch (IOException e) { }
---------------
-------------
@@ -41,7 +41,7 @@
     @Override
     protected int getLevelForDistance(double degrees) {
       QuadPrefixTree grid = new QuadPrefixTree(ctx, MAX_LEVELS_POSSIBLE);
-      return grid.getLevelForDistance(degrees) + 1;//returns 1 greater
+return grid.getLevelForDistance(degrees);
     }
 
     @Override
---------------
-------------
@@ -166,7 +166,7 @@
         assert openedFiles.get(dataFileName) == null;
 
         long start = System.currentTimeMillis();
-        SSTableReader sstable = new SSTableReader(dataFileName, partitioner)
+SSTableReader sstable = new SSTableReader(dataFileName, partitioner);
         sstable.loadIndexFile();
         sstable.loadBloomFilter();
         if (cacheFraction > 0)
---------------
-------------
@@ -55,7 +55,7 @@
 
       @Override
       public boolean bytesVal(int doc, BytesRef target) {
-        target.copy(bytesRef);
+target.copyBytes(bytesRef);
         return true;
       }
 
---------------
-------------
@@ -1,6 +1,6 @@
 /*
 
-   Derby - Class com.ihost.cs.JBitSet
+Derby - Class org.apache.derby.iapi.util.JBitSet
 
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -153,7 +153,7 @@
         logger.info(String.format("Binding thrift service to %s:%s", listenAddr, listenPort));
 
         // Protocol factory
-        TProtocolFactory tProtocolFactory = new TBinaryProtocol.Factory(false, 
+TProtocolFactory tProtocolFactory = new TBinaryProtocol.Factory(true,
                                                                         true, 
                                                                         DatabaseDescriptor.getThriftMaxMessageLength());
         
---------------
-------------
@@ -67,7 +67,7 @@
     if (_exportServices == null) {
       _exportServices = getContentSetFromHeader (_attributes, Constants.EXPORT_SERVICE);
     }
-    return _exportPackages;
+return _exportServices;
   }
 
   public Map<String, String> getHeaders() {
---------------
-------------
@@ -95,7 +95,7 @@
                 // if keeper does not replace oldKeeper we must be sure to close it
                 synchronized (connectionUpdateLock) {
                   try {
-                    waitForConnected(SolrZkClient.DEFAULT_CLIENT_CONNECT_TIMEOUT);
+waitForConnected(Long.MAX_VALUE);
                   } catch (Exception e1) {
                     closeKeeper(keeper);
                     throw new RuntimeException(e1);
---------------
-------------
@@ -449,7 +449,7 @@
     }
     
     private void verifyCachedSchema(Connection c) throws SQLException {
-        if (c instanceof org.apache.derby.client.am.Connection) {
+if (usingDerbyNetClient()) {
             String cached =
                     ((org.apache.derby.client.am.Connection) c).
                     getCurrentSchemaName();
---------------
-------------
@@ -80,7 +80,7 @@
       writer.commit();
       assertEquals("wrong number of commits !", i + 1, DirectoryReader.listCommits(dir).size());
     }
-    writer.close();
+writer.shutdown();
     dir.close();
   }
   
---------------
-------------
@@ -105,7 +105,7 @@
 
         // add data
         rm = new RowMutation("Keyspace1", dk.key);
-        rm.add(new QueryPath("Standard1", null, "Column1".getBytes()), "abcd".getBytes(), 0);
+rm.add(new QueryPath("Standard1", null, "Column1".getBytes()), "abcd".getBytes(), new TimestampClock(0));
         rm.apply();
 
         ReadCommand command = new SliceByNamesReadCommand("Keyspace1", dk.key, new QueryPath("Standard1"), Arrays.asList("Column1".getBytes()));
---------------
-------------
@@ -316,7 +316,7 @@
                 try
                 {
                     // validate compaction strategy class
-                    CFMetaData.createCompactionSrategy(compactionStrategy);
+CFMetaData.createCompactionStrategy(compactionStrategy);
                 }
                 catch (ConfigurationException e)
                 {
---------------
-------------
@@ -171,7 +171,7 @@
 	<LI>The properties set of the service (i.e. that passed into Monitor.createPersistentService()
 	or Monitor.startService()).
 	<LI>The System (JVM) properties set (i.e. java.lang.System.getProperties()).
-	<LI>The application properties set (i.e. obtained from the cloudscape.properties file).
+<LI>The application properties set (i.e. obtained from the derby.properties file).
 	<LI>The default implementation properties set (i.e. obtained from the
 	/org/apache/derby/modules.properties resource).
 	</OL>
---------------
-------------
@@ -129,7 +129,7 @@
   protected void retryDelay(int attemptCount) {
     if (attemptCount > 0) {
       try {
-        Thread.sleep(Math.min(10000, attemptCount * retryDelay));
+Thread.sleep(Math.max(10000, attemptCount * retryDelay));
       } catch (InterruptedException e) {
         LOG.debug("Failed to sleep: " + e, e);
       }
---------------
-------------
@@ -425,7 +425,7 @@
     public ColumnFamilyStoreMBeanIterator(MBeanServerConnection mbeanServerConn)
     throws MalformedObjectNameException, NullPointerException, IOException
     {
-        ObjectName query = new ObjectName("org.apache.cassandra.db:type=ColumnFamilyStores,*");
+ObjectName query = new ObjectName("org.apache.cassandra.db:type=ColumnFamilies,*");
         resIter = mbeanServerConn.queryNames(query, null).iterator();
         this.mbeanServerConn = mbeanServerConn;
     }
---------------
-------------
@@ -62,7 +62,7 @@
 import org.apache.aries.application.utils.AppConstants;
 import org.apache.aries.application.utils.manifest.ContentFactory;
 import org.apache.aries.util.VersionRange;
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 import org.apache.aries.util.manifest.ManifestHeaderProcessor;
 import org.apache.felix.bundlerepository.Capability;
 import org.apache.felix.bundlerepository.DataModelHelper;
---------------
-------------
@@ -254,7 +254,7 @@
         {
             StorageService ss = StorageService.instance;
             String tokenString = StorageService.getPartitioner().getTokenFactory().toString(ss.getBootstrapToken());
-            Message response = message.getReply(FBUtilities.getLocalAddress(), tokenString.getBytes(Charsets.UTF_8));
+Message response = message.getInternalReply(tokenString.getBytes(Charsets.UTF_8));
             MessagingService.instance.sendOneWay(response, message.getFrom());
         }
     }
---------------
-------------
@@ -408,7 +408,7 @@
     int[] c = size();
     int[] o = other.size();
     if (c[COL] != o[ROW]) {
-      throw new CardinalityException();
+throw new CardinalityException(c[COL], o[ROW]);
     }
     Matrix result = like(c[ROW], o[COL]);
     for (int row = 0; row < c[ROW]; row++) {
---------------
-------------
@@ -25,7 +25,7 @@
 import java.util.jar.Attributes;
 import java.util.jar.Manifest;
 
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 
 public class EbaUnitTestUtils {
 
---------------
-------------
@@ -55,7 +55,7 @@
  * directly to the main Lucene index as opposed to adding to a separate smaller index.
  */
 public class DirectUpdateHandler2 extends UpdateHandler implements SolrCoreState.IndexWriterCloser {
-  protected SolrCoreState solrCoreState;
+protected final SolrCoreState solrCoreState;
   protected final Lock commitLock = new ReentrantLock();
 
   // stats
---------------
-------------
@@ -111,7 +111,7 @@
    * random. All positions for that number are saved up front and compared to
    * the enums positions.
    */
-  public void testRandomPositons() throws IOException {
+public void testRandomPositions() throws IOException {
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random, dir,
         newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(
---------------
-------------
@@ -58,7 +58,7 @@
         // the nightly runs.
         // suite.addTest(largeCodeGen.suite());
 
-		// suite.addTest(PrepareExecuteDDL.suite());
+suite.addTest(PrepareExecuteDDL.suite());
 		suite.addTest(LangScripts.suite());
         suite.addTest(GroupByExpressionTest.suite());
         suite.addTest(MathTrigFunctionsTest.suite());
---------------
-------------
@@ -67,6 +67,6 @@
     
     /** blast some random strings through the analyzer */
     public void testRandomStrings() throws Exception {
-      checkRandomData(random(), new RussianAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new RussianAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
     }
 }
---------------
-------------
@@ -99,7 +99,7 @@
   @Override
   public void configure(JobConf job) {
     try {
-      Parameters params = Parameters.fromString(job.get("bayes.parameters", ""));
+Parameters params = new Parameters(job.get("bayes.parameters", ""));
       if (params.get("dataSource").equals("hbase")) {
         useHbase = true;
       } else {
---------------
-------------
@@ -45,7 +45,7 @@
       private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
       
       @Override
-      public boolean incrementToken() throws IOException {
+public boolean incrementToken() {
         if (index == tokens.length) {
           return false;
         } else {
---------------
-------------
@@ -124,7 +124,7 @@
   
   private static class TermsDfQueue extends PriorityQueue<TermDf> {
     TermsDfQueue (int maxSize) {
-      initialize(maxSize);
+super(maxSize);
     }
     @Override
     protected boolean lessThan(TermDf tf1, TermDf tf2) {
---------------
-------------
@@ -91,7 +91,7 @@
             }
             catch (ExecutionException e)
             {
-                logger.error("Error in executor futuretask", e);
+Thread.getDefaultUncaughtExceptionHandler().uncaughtException(Thread.currentThread(), e);
             }
         }
 
---------------
-------------
@@ -57,6 +57,6 @@
         ColumnFamilyStore cfstore = Table.open(tablename).getColumnFamilyStore(cfname);
         RangeSliceReply rr = Util.getRangeSlice(cfstore);
         assert rr.rows.size() == 1;
-        assert rr.rows.get(0).key.equals("key");
+assert Arrays.equals(rr.rows.get(0).key.key, "key".getBytes());
     }
 }
---------------
-------------
@@ -168,7 +168,7 @@
 
   /** @see #setFloorSegmentMB */
   public double getFloorSegmentMB() {
-    return floorSegmentBytes/1024*1024.;
+return floorSegmentBytes/(1024*1024.);
   }
 
   /** When forceMergeDeletes is called, we only merge away a
---------------
-------------
@@ -285,7 +285,7 @@
 					(lastEndOffset < text.length()) 
 					&&
 //					and that text is not too large...
-					(text.length()< maxDocCharsToAnalyze)
+(text.length()<= maxDocCharsToAnalyze)
 				)				
 			{
 				//append it to the last fragment
---------------
-------------
@@ -42,7 +42,7 @@
  * Converted from ieptests.sql
  *
  */
-public final class ImportExportProcedureTest extends BaseJDBCTestCase {
+public class ImportExportProcedureTest extends BaseJDBCTestCase {
 
     /**
      * Public constructor required for running test as standalone JUnit.
---------------
-------------
@@ -66,7 +66,7 @@
 import org.apache.aries.application.utils.AppConstants;
 import org.apache.aries.application.utils.manifest.ContentFactory;
 import org.apache.aries.util.filesystem.FileSystem;
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 import org.apache.aries.util.manifest.ManifestHeaderProcessor;
 import org.osgi.framework.Constants;
 import org.osgi.framework.Filter;
---------------
-------------
@@ -328,7 +328,7 @@
      */
     public static void scrubDataDirectories(String table, String columnFamily)
     {
-        logger.info("Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)");
+logger.info("Removing compacted SSTable files from " + columnFamily + " (see http://wiki.apache.org/cassandra/MemtableSSTable)");
         for (Map.Entry<Descriptor,Set<Component>> sstableFiles : files(table, columnFamily, true, true).entrySet())
         {
             Descriptor desc = sstableFiles.getKey();
---------------
-------------
@@ -54,7 +54,7 @@
 				doAllTheWork();
 			} catch (IOException iex) {
 				//in case of ioexception, catch it and throw it as our own exception
-				throw LoadError.errorWritingData();
+throw LoadError.errorWritingData(iex);
 			}
 		} catch (Exception ex) {
 			throw LoadError.unexpectedError(ex);
---------------
-------------
@@ -1,6 +1,6 @@
 /*
  
-Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun
+Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun_Local_StateTest_part1_2
  
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -18,7 +18,7 @@
 
  */
 
-package org.apache.derby.tools.iapi;
+package org.apache.derby.iapi.tools;
 
 import java.io.IOException;
 import org.apache.derby.tools.dblook;
---------------
-------------
@@ -94,7 +94,7 @@
 	{
 		TypeId	operandType;
 
-		super.bindExpression(fromList, subqueryList,
+bindOperand(fromList, subqueryList,
 				aggregateVector);
 
 		/*
---------------
-------------
@@ -78,7 +78,7 @@
   @Override
   public void configure(JobConf job) {
     try {
-      Parameters params = Parameters.fromString(job.get("bayes.parameters", ""));
+Parameters params = new Parameters(job.get("bayes.parameters", ""));
       if (params.get("dataSource").equals("hbase")) {
         useHbase = true;
       } else {
---------------
-------------
@@ -42,6 +42,6 @@
 
 public class Constants {
 
-  public static final String VERSION = "15.0.0";
+public static final String VERSION = "16.0.0";
 
 }
---------------
-------------
@@ -71,7 +71,7 @@
 public class TestReplicationHandler extends SolrTestCaseJ4 {
 
 
-  private static final String CONF_DIR = "." + File.separator + "solr"
+private static final String CONF_DIR = "solr"
       + File.separator + "collection1" + File.separator + "conf"
       + File.separator;
 
---------------
-------------
@@ -33,7 +33,7 @@
    * constructors, typically implicit.) */
   protected SortedSetDocValues() {}
 
-  public static final long NO_MORE_ORDS = Long.MAX_VALUE;
+public static final long NO_MORE_ORDS = -1;
 
   /** 
    * Returns the next ordinal for the current document (previously
---------------
-------------
@@ -469,7 +469,7 @@
 
 		boolean logBootTrace = PropertyUtil.getSystemBoolean(Property.LOG_BOOT_TRACE);
 		logMsg(LINE);
-		logMsg("\n" + new Date() +
+logMsg(new Date() +
                 MessageService.getTextMessage(
                     MessageId.STORE_SHUTDOWN_MSG,
                     getIdentifier(),
---------------
-------------
@@ -405,7 +405,7 @@
     	// Section 2.4.1.4
     	if (forRollback)
     	{
-    		items[orderItemCount] = 2334432;
+items[orderItemCount - 1] = 2334432;
     	}
     	
         ops.newOrder(display, displayData,
---------------
-------------
@@ -51,7 +51,7 @@
     public static final String TABLE1 = "Keyspace1";
     public static final String CF1 = "Indexed1";
     public static final String CF2 = "Standard1";
-    public static final ByteBuffer COLUMN = ByteBuffer.wrap("birthdate".getBytes());
+public static final ByteBuffer COLUMN = ByteBufferUtil.bytes("birthdate");
     public static final ByteBuffer VALUE = ByteBuffer.allocate(8);
     static
     {
---------------
-------------
@@ -34,7 +34,7 @@
   public void testRollingUpdates() throws Exception {
     final MockDirectoryWrapper dir = newDirectory();
     dir.setCheckIndexOnClose(false); // we use a custom codec provider
-    final LineFileDocs docs = new LineFileDocs(random);
+final LineFileDocs docs = new LineFileDocs(random, defaultCodecSupportsDocValues());
 
     //provider.register(new MemoryCodec());
     if ( (!"Lucene3x".equals(Codec.getDefault().getName())) && random.nextBoolean()) {
---------------
-------------
@@ -314,7 +314,7 @@
 
         ColumnFamilyStore cfs = table.getColumnFamilyStore("Indexed2");
         ColumnDefinition old = cfs.metadata.getColumn_metadata().get(ByteBufferUtil.bytes("birthdate"));
-        ColumnDefinition cd = new ColumnDefinition(old.name, old.validator.getClass().getName(), IndexType.KEYS, "birthdate_index");
+ColumnDefinition cd = new ColumnDefinition(old.name, old.validator, IndexType.KEYS, "birthdate_index");
         Future<?> future = cfs.addIndex(cd);
         future.get();
         // we had a bug (CASSANDRA-2244) where index would get created but not flushed -- check for that
---------------
-------------
@@ -220,7 +220,7 @@
         termState.skipFP += termState.bytesReader.readVLong();
       }
     } else if (isFirstTerm) {
-      termState.skipFP = termState.bytesReader.readVLong();
+termState.skipFP = 0;
     }
   }
 
---------------
-------------
@@ -1665,7 +1665,7 @@
             {
             case Types.DECIMAL:
             case Types.NUMERIC:
-                maxScale = 32767; // 31; BUG DERBY-2262
+maxScale = 31; // Max Scale for Decimal & Numeric is 31: Derby-2262
                 break;
             case Types.TIMESTAMP:
                 maxScale = 6;
---------------
-------------
@@ -167,7 +167,7 @@
 			doImportFromFile(c, "extin/EndOfFile.txt" , "T4" , null , null , null, 0);
 		} catch (SQLException e) {
 			// DERBY-1440: JDBC 4 client driver doesn't include nested exception SQLStates
-			assertSQLState(JDBC.vmSupportsJDBC4() ? "38000" : "XIE0E", e);
+assertSQLState(JDBC.vmSupportsJDBC4() ? "XIE0R" : "XIE0E", e);
 		}
 	}
 	
---------------
-------------
@@ -117,7 +117,7 @@
                 long bytesRead = 0;
                 while (bytesRead < length)
                 {
-                    in.reset();
+in.reset(0);
                     key = SSTableReader.decodeKey(StorageService.getPartitioner(), localFile.desc, ByteBufferUtil.readWithShortLength(in));
                     long dataSize = SSTableReader.readRowSize(in, localFile.desc);
                     ColumnFamily cf = null;
---------------
-------------
@@ -207,7 +207,7 @@
                 double p = (double) k / (double) n; 
                 if (d<p) {
                     // Replace a random value from the sample with the new value
-                    int keyToReplace = Math.abs(r.nextInt())%k;                    
+int keyToReplace = r.nextInt(k);
                     sampledKeys.set(keyToReplace, key);
                 }
             }
---------------
-------------
@@ -68,7 +68,7 @@
   @Test
   public void baseUIMAAnalyzerIntegrationTest() throws Exception {
     Directory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_40, analyzer));
+IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
     // add the first doc
     Document doc = new Document();
     String dummyTitle = "this is a dummy title ";
---------------
-------------
@@ -1598,7 +1598,7 @@
             String apppropsjvmflags = ap.getProperty("jvmflags");
             if (apppropsjvmflags != null)
             {
-                if (jvmflags != null)
+if (jvmflags != null && jvmflags.length() > 0)
                     jvmflags = apppropsjvmflags + "^" + jvmflags;
                 else
                     jvmflags = apppropsjvmflags;
---------------
-------------
@@ -110,7 +110,7 @@
     tf.addTerm(new Term(fieldName, "content1"));
     
     MultiReader multi = new MultiReader(reader1, reader2);
-    for (AtomicReaderContext context : multi.getTopReaderContext().leaves()) {
+for (AtomicReaderContext context : multi.leaves()) {
       FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());
       assertTrue("Must be >= 0", bits.cardinality() >= 0);      
     }
---------------
-------------
@@ -132,7 +132,7 @@
 
   @Override
   public SortField getSortField(SchemaField field, boolean top) {
-    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, "Sorting not suported on PointType " + field.getName());
+throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, "Sorting not supported on PointType " + field.getName());
   }
 
   @Override
---------------
-------------
@@ -453,7 +453,7 @@
 
     // these queries should be exactly ordered and scores should exactly match
     query("q","*:*", "sort",i1+" desc");
-    query("q","*:*", "sort",i1+" desc", "fl","*,score");
+//query("q","*:*", "sort",i1+" desc", "fl","*,score");
     handle.put("maxScore", SKIPVAL);
     query("q","{!func}"+i1);// does not expect maxScore. So if it comes ,ignore it. NamedListCodec.writeSolrDocumentList()
     //is agnostic of request params.
---------------
-------------
@@ -230,7 +230,7 @@
 
           if (postingsEnum == null) {
             // term does exist, but has no positions
-            assert termsEnum.docs(liveDocs, null, 0) != null: "termstate found but no term exists in reader";
+assert termsEnum.docs(liveDocs, null, false) != null: "termstate found but no term exists in reader";
             throw new IllegalStateException("field \"" + term.field() + "\" was indexed without position data; cannot run PhraseQuery (term=" + term.text() + ")");
           }
 
---------------
-------------
@@ -97,7 +97,7 @@
                   false); // master server dead
 
         // Try to re-establish replication mode:
-        masterServer = startServer(masterJvmVersion, derbyMasterVersion,
+startServer(masterJvmVersion, derbyMasterVersion,
                 masterServerHost,
                 ALL_INTERFACES,
                 masterServerPort,
---------------
-------------
@@ -121,7 +121,7 @@
 
 		c3a.createStatement().execute("CREATE TABLE CLOSED.LOCKME(i int)");
 		
-		c3a.createStatement().execute("create procedure sleep(t INTEGER) dynamic result sets 0 language java external name 'java.lang.Thread.sleep' parameter style java");
+c3a.createStatement().execute("create procedure sleep(t BIGINT) dynamic result sets 0 language java external name 'java.lang.Thread.sleep' parameter style java");
 		c3a.setAutoCommit(false);
 		c3a.createStatement().execute("LOCK TABLE CLOSED.LOCKME IN SHARE MODE");
 		
---------------
-------------
@@ -224,7 +224,7 @@
                                       new BytesRef("aaa"),
                                       MultiFields.getLiveDocs(reader),
                                       null,
-                                      false);
+0);
       int count = 0;
       while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
         count++;
---------------
-------------
@@ -313,7 +313,7 @@
     onlyStored.setIndexed(false);
 
     final Field smallField = new Field("fld", randomByteArray(random().nextInt(10), 256), onlyStored);
-    final int numFields = atLeast(1000000);
+final int numFields = RandomInts.randomIntBetween(random(), 500000, 1000000);
     for (int i = 0; i < numFields; ++i) {
       bigDoc1.add(smallField);
     }
---------------
-------------
@@ -54,7 +54,7 @@
   static SolrRequestParsers parser;
 
   @AfterClass
-  public static void afterClass() throws Exception {
+public static void afterClass() {
     parser = null;
   }
   
---------------
-------------
@@ -239,7 +239,7 @@
     }
 
     public void testSetBufferSize() throws IOException {
-      File indexDir = new File(TEMP_DIR, "testSetBufferSize");
+File indexDir = _TestUtil.getTempDir("testSetBufferSize");
       MockFSDirectory dir = new MockFSDirectory(indexDir, random);
       try {
         IndexWriter writer = new IndexWriter(
---------------
-------------
@@ -120,7 +120,7 @@
     }
     reader = writer.getReader();
     searcher = newSearcher(reader);
-    writer.close();
+writer.shutdown();
   }
 
   @AfterClass
---------------
-------------
@@ -91,7 +91,7 @@
     reader = writer.getReader();
     searcher1 = newSearcher(reader);
     searcher2 = newSearcher(reader);
-    writer.close();
+writer.shutdown();
   }
   
   @Override
---------------
-------------
@@ -307,7 +307,7 @@
       System.out.println("TEST: done join [" + (System.currentTimeMillis()-t0) + " ms]; addCount=" + addCount + " delCount=" + delCount);
     }
     writer.commit();
-    assertEquals(addCount.get() - delCount.get(), writer.numDocs());
+assertEquals("index=" + writer.segString(), addCount.get() - delCount.get(), writer.numDocs());
       
     writer.close(false);
     dir.close();
---------------
-------------
@@ -278,7 +278,7 @@
    */
   public void emitPoint(Vector point, OutputCollector<Text, Text> collector)
       throws IOException {
-    collector.collect(new Text(formatCanopy(this)), new Text(point
+collector.collect(new Text(this.getIdentifier()), new Text(point
         .asFormatString()));
   }
 
---------------
-------------
@@ -707,7 +707,7 @@
                     SQLState.BLOB_NONPOSITIVE_LENGTH,
                     new Long(length));
         }
-        if (length > (this.length() - pos)) {
+if (length > (this.length() - (pos -1))) {
             throw Util.generateCsSQLException(
                     SQLState.POS_AND_LENGTH_GREATER_THAN_LOB,
                     new Long(pos), new Long(length));
---------------
-------------
@@ -81,7 +81,7 @@
    *  measure isn't stored by the codec. Note that, just like 
    *  other term measures, this measure does not take deleted 
    *  documents into account. */
-  public abstract long getUniqueTermCount() throws IOException;
+public abstract long size() throws IOException;
   
   /** Returns the sum of {@link TermsEnum#totalTermFreq} for
    *  all terms in this field, or -1 if this measure isn't
---------------
-------------
@@ -246,7 +246,7 @@
 
     Collection<SearchGroup> topSearchGroups = firstPassCollector.getTopGroups(groupOffset, fillSortFields);
     if (topSearchGroups == null) {
-      return new TopGroups(new SortField[0], new SortField[0], 0, 0, new GroupDocs[0]);
+return new TopGroups(new SortField[0], new SortField[0], 0, 0, new GroupDocs[0], Float.NaN);
     }
 
     int topNInsideGroup = groupDocsOffset + groupDocsLimit;
---------------
-------------
@@ -433,7 +433,7 @@
     
     FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(CP_A, NUM_CHILDREN_CP_A), 
         new CountFacetRequest(CP_B, NUM_CHILDREN_CP_B));
-    FacetsCollector fc = new CountingFacetsCollector(fsp , taxoReader, new FacetArrays(taxoReader.getSize()), true);
+FacetsCollector fc = new CountingFacetsCollector(fsp , taxoReader, new FacetArrays(taxoReader.getSize()));
     searcher.search(new MatchAllDocsQuery(), fc);
     
     List<FacetResult> facetResults = fc.getFacetResults();
---------------
-------------
@@ -598,7 +598,7 @@
           
     protected void setClientLocale(String locale)
     {
-        serverImpl.clientLocale = locale;
+serverImpl.setClientLocale( locale );
     }
 
     /**
---------------
-------------
@@ -178,7 +178,7 @@
    * Reverses the language of the given (non-singleton) automaton while returning
    * the set of new initial states.
    */
-  static Set<State> reverse(Automaton a) {
+public static Set<State> reverse(Automaton a) {
     a.expandSingleton();
     // reverse all edges
     HashMap<State, HashSet<Transition>> m = new HashMap<State, HashSet<Transition>>();
---------------
-------------
@@ -199,7 +199,7 @@
 			st.execute("drop table t1");
 		} catch (SQLException se) {}
 		try {
-			st.execute("drop procedure proc1");
+st.execute("drop procedure za");
 		} catch (SQLException se) {}
 
 		st.close();
---------------
-------------
@@ -316,7 +316,7 @@
         cancel(key_);
         pendingWrites_.clear();
         if (pool_ != null)
-            pool_.destroy(this);
+pool_.reset();
     }
     
     private void cancel(SelectionKey key)
---------------
-------------
@@ -119,7 +119,7 @@
 
     Path inputFile = getInputPath();
     Path outputPath = getOutputPath();
-    Path usersFile = hasOption("usersFile") ? inputFile : new Path(getOption("usersFile"));
+Path usersFile = hasOption("usersFile") ? new Path(getOption("usersFile")) : inputFile;
     
     String recommendClassName = getOption("recommenderClassName");
     int recommendationsPerUser = Integer.parseInt(getOption("numRecommendations"));
---------------
-------------
@@ -241,7 +241,7 @@
 
   @Override
   public Query getHighlightQuery() throws ParseException {
-    return parsedUserQuery;
+return parsedUserQuery == null ? altUserQuery : parsedUserQuery;
   }
 
   public void addDebugInfo(NamedList<Object> debugInfo) {
---------------
-------------
@@ -64,7 +64,7 @@
     bq.setMinimumNumberShouldMatch(1);
     try {
       
-      IndexSearcher searcher = new IndexSearcher(dir);
+IndexSearcher searcher = new IndexSearcher(dir, true);
       for (int i = 0; i < inOrder.length; i++) {
         TopDocsCollector tdc = TopScoreDocCollector.create(3, inOrder[i]);
         assertEquals("org.apache.lucene.search.TopScoreDocCollector$" + actualTSDCClass[i], tdc.getClass().getName());
---------------
-------------
@@ -60,7 +60,7 @@
             // load the next pair
             if (!reader.nextKeyValue())
                 return null;
-            String key = (String)reader.getCurrentKey();
+byte[] key = (byte[])reader.getCurrentKey();
             SortedMap<byte[],IColumn> cf = (SortedMap<byte[],IColumn>)reader.getCurrentValue();
             assert key != null && cf != null;
             
---------------
-------------
@@ -237,7 +237,7 @@
    * </p>
    */
   protected boolean isTokenChar(int c) {
-    throw new UnsupportedOperationException("since LUCENE_3_1 subclasses of CharTokenizer must implement isTokenChar(int)");
+throw new UnsupportedOperationException("since LUCENE_31 subclasses of CharTokenizer must implement isTokenChar(int)");
   }
 
   /**
---------------
-------------
@@ -63,7 +63,7 @@
         suite.addTest(CreateTableFromQueryTest.suite());
         suite.addTest(DatabaseClassLoadingTest.suite());
         suite.addTest(DynamicLikeOptimizationTest.suite());
-        suite.addTest(ExistsWithSetOpsTest.suite());
+suite.addTest(ExistsWithSubqueriesTest.suite());
         suite.addTest(GrantRevokeTest.suite());
         suite.addTest(GroupByExpressionTest.suite());
 		suite.addTest(LangScripts.suite());
---------------
-------------
@@ -25,7 +25,7 @@
 
   @Override
   public Sorter newSorter(Entry[] arr) {
-    return new ArrayTimSorter<Entry>(arr, ArrayUtil.<Entry>naturalComparator(), random().nextInt(arr.length));
+return new ArrayTimSorter<Entry>(arr, ArrayUtil.<Entry>naturalComparator(), _TestUtil.nextInt(random(), 0, arr.length));
   }
 
 }
---------------
-------------
@@ -37,7 +37,7 @@
  */
 
 @SuppressCodecs("Lucene3x")
-public class FacetTestCase extends LuceneTestCase {
+public abstract class FacetTestCase extends LuceneTestCase {
   
   private static final IntEncoder[] ENCODERS = new IntEncoder[] {
     new SortingIntEncoder(new UniqueValuesIntEncoder(new VInt8IntEncoder())),
---------------
-------------
@@ -1,6 +1,6 @@
 /*
  
-Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun
+Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun_Full
  
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -233,7 +233,7 @@
     if (discountOverlaps)
       numTerms = state.getLength() - state.getNumOverlap();
     else
-      numTerms = state.getLength() / state.getBoost();
+numTerms = state.getLength();
     return encodeNormValue(state.getBoost(), numTerms);
   }
   
---------------
-------------
@@ -310,7 +310,7 @@
 
 			System.out.println("\n\n " + sqe + sqe.getErrorCode() + " "
 					+ sqe.getSQLState());
-			if ((sqe.getErrorCode() == -4499)
+if ((sqe.getErrorCode() == 40000)
 					|| sqe.getSQLState().equalsIgnoreCase("08001")) {
 				System.out
 				.println("\n Unable to connect, test cannot proceed. Please verify if the Network Server is started on port 1900.");
---------------
-------------
@@ -90,7 +90,7 @@
       state.docWriter.removeOpenFile(state.docStoreSegmentName + "." + IndexFileNames.FIELDS_EXTENSION);
       state.docWriter.removeOpenFile(state.docStoreSegmentName + "." + IndexFileNames.FIELDS_INDEX_EXTENSION);
 
-      if (4+state.numDocsInStore*8 != state.directory.fileLength(state.docStoreSegmentName + "." + IndexFileNames.FIELDS_INDEX_EXTENSION))
+if (4+((long) state.numDocsInStore)*8 != state.directory.fileLength(state.docStoreSegmentName + "." + IndexFileNames.FIELDS_INDEX_EXTENSION))
         throw new RuntimeException("after flush: fdx size mismatch: " + state.numDocsInStore + " docs vs " + state.directory.fileLength(state.docStoreSegmentName + "." + IndexFileNames.FIELDS_INDEX_EXTENSION) + " length in bytes of " + state.docStoreSegmentName + "." + IndexFileNames.FIELDS_INDEX_EXTENSION);
     }
   }
---------------
-------------
@@ -135,7 +135,7 @@
     f = newField(ID_FIELD, id2String(scoreAndID), customType); // for debug purposes
     d.add(f);
 
-    FieldType customType2 = new FieldType(TextField.TYPE_UNSTORED);
+FieldType customType2 = new FieldType(TextField.TYPE_NOT_STORED);
     customType2.setOmitNorms(true);
     f = newField(TEXT_FIELD, "text of doc" + scoreAndID + textLine(i), customType2); // for regular search
     d.add(f);
---------------
-------------
@@ -26,7 +26,7 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.*;
+import static org.apache.lucene.analysis.VocabularyAssert.*;
 
 /**
  * Simple tests for {@link ItalianLightStemFilter}
---------------
-------------
@@ -53,7 +53,7 @@
 	EmbeddedDataSource automatically supports the correct JDBC specification version
 	for the Java Virtual Machine's environment.
 	<UL>
-	<LI> JDBC 3.0 - Java 2 - JDK 1.4
+<LI> JDBC 3.0 - Java 2 - JDK 1.4, J2SE 5.0
 	<LI> JDBC 2.0 - Java 2 - JDK 1.2,1.3
 	</UL>
 
---------------
-------------
@@ -135,7 +135,7 @@
     RAMDirectory rd = new RAMDirectory();
 
     //
-    IndexWriter writer = new IndexWriter(rd, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+IndexWriter writer = new IndexWriter(rd, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
     //
     Document d = new Document();
---------------
-------------
@@ -100,7 +100,7 @@
    */
   public static final Mode DEFAULT_MODE = Mode.SEARCH;
 
-  enum Type {
+public enum Type {
     KNOWN,
     UNKNOWN,
     USER
---------------
-------------
@@ -165,7 +165,7 @@
     String myURL = "";
 
     if (zkController != null) {
-      myURL = zkController.getZkServerAddress();
+myURL = zkController.getBaseUrl();
     }
 
     // TODO: core name turns up blank in many tests - find URL if cloud enabled?
---------------
-------------
@@ -3055,7 +3055,7 @@
     public void listenToUnitOfWork() {
         if (!listenToUnitOfWork_) {
             listenToUnitOfWork_ = true;
-            connection_.CommitAndRollbackListeners_.put(this,null);
+connection_.CommitAndRollbackListeners_.add(this);
         }
     }
 
---------------
-------------
@@ -106,7 +106,7 @@
       do {
         // System.out.println("  iter termCount=" + termCount + " term=" +
         // enumerator.term().toBytesString());
-        docsEnum = termsEnum.docs(acceptDocs, docsEnum, 0);
+docsEnum = termsEnum.docs(acceptDocs, docsEnum, false);
         int docid;
         while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
           bitSet.set(docid);
---------------
-------------
@@ -440,7 +440,7 @@
     pool.shutdown();
     pool.awaitTermination(Long.MAX_VALUE, TimeUnit.SECONDS);
     log.info("Finished StreamingKMeans");
-    SequenceFile.Writer writer = SequenceFile.createWriter(FileSystem.get(conf), conf, output, IntWritable.class,
+SequenceFile.Writer writer = SequenceFile.createWriter(FileSystem.get(conf), conf, new Path(output, "part-r-00000"), IntWritable.class,
         CentroidWritable.class);
     int numCentroids = 0;
     // Run BallKMeans on the intermediate centroids.
---------------
-------------
@@ -240,7 +240,7 @@
 	/**
 	 * Create CloudState from json string that is typically stored in zookeeper.
 	 */
-	public static CloudState load(byte[] bytes, Set<String> liveNodes) throws KeeperException, InterruptedException {
+public static CloudState load(byte[] bytes, Set<String> liveNodes) {
     if (bytes == null || bytes.length == 0) {
       return new CloudState(liveNodes, Collections.<String, Map<String,Slice>>emptyMap());
     }
---------------
-------------
@@ -120,7 +120,7 @@
     client.setConf(conf);
     try {
       JobClient.runJob(conf);
-      FileSystem fs = FileSystem.get(conf);
+FileSystem fs = FileSystem.get(outPath.toUri(), conf);
       return isConverged(clustersOut + "/part-00000", conf, fs);
     } catch (IOException e) {
       log.warn(e.toString(), e);
---------------
-------------
@@ -61,7 +61,7 @@
         return Memtable.getNamesIterator(key, cf, this);
     }
 
-    public IColumnIterator getSSTableColumnIterator(SSTableReader sstable, String key)
+public IColumnIterator getSSTableColumnIterator(SSTableReader sstable, DecoratedKey key)
     {
         return new SSTableNamesIterator(sstable, key, columns);
     }
---------------
-------------
@@ -74,7 +74,7 @@
     writer.addDocument(doc);
 
     writer.forceMerge(1);
-    writer.close();
+writer.shutdown();
   }
 
   @Override
---------------
-------------
@@ -26,7 +26,7 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.*;
+import static org.apache.lucene.analysis.VocabularyAssert.*;
 
 /**
  * Simple tests for {@link RussianLightStemFilter}
---------------
-------------
@@ -239,7 +239,7 @@
     }
 
     @Override
-    public void setNextReader(IndexReader reader, int docBase)
+public FieldComparator setNextReader(IndexReader reader, int docBase)
         throws IOException {
       throw new UnsupportedOperationException(UNSUPPORTED_MSG);
     }
---------------
-------------
@@ -808,7 +808,7 @@
   private long seed;
   
   private static final Random seedRand = new Random();
-  protected static final Random random = new Random();
+protected static final Random random = new Random(0);
 
   private String name = "<unknown>";
   
---------------
-------------
@@ -76,7 +76,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), a, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -45,7 +45,7 @@
     {
         // clean up commitlog
         String[] directoryNames = {
-                DatabaseDescriptor.getLogFileLocation(),
+DatabaseDescriptor.getCommitLogLocation(),
         };
         for (String dirName : directoryNames)
         {
---------------
-------------
@@ -201,7 +201,7 @@
       seekOpt = true;
     }
 
-    lastSeekScratch.copy(term);
+lastSeekScratch.copyBytes(term);
     lastSeek = lastSeekScratch;
 
     for(int i=0;i<numSubs;i++) {
---------------
-------------
@@ -52,7 +52,7 @@
  
  <p>IndexReader instances for indexes on disk are usually constructed
  with a call to one of the static <code>DirectoryReader.open()</code> methods,
- e.g. {@link DirectoryReader#open(Directory)}. {@link DirectoryReader} implements
+e.g. {@link DirectoryReader#open(org.apache.lucene.store.Directory)}. {@link DirectoryReader} implements
  the {@link CompositeReader} interface, it is not possible to directly get postings.
 
  <p> For efficiency, in this API documents are often referred to via
---------------
-------------
@@ -106,7 +106,7 @@
         DataInputStream stream = new DataInputStream(ByteBufferUtil.inputStream(bytes));
 
         return useOldBuffer
-                ? LegacyBloomFilter.serializer().deserialize(stream)
+? LegacyBloomFilter.serializer().deserialize(stream, 0) // version means nothing there.
                 : BloomFilter.serializer().deserialize(stream);
     }
 
---------------
-------------
@@ -194,7 +194,7 @@
     try {
       doMaybeRefresh();
     } finally {
-      refreshLock.lock();
+refreshLock.unlock();
     }
   }
 
---------------
-------------
@@ -179,7 +179,7 @@
 
         public int compareTo(KeyPosition kp)
         {
-            return partitioner.getDecoratedKeyComparator().compare(key, kp.key);
+return key.compareTo(kp.key);
         }
 
         public String toString()
---------------
-------------
@@ -59,7 +59,7 @@
 	/**
 		Create a new DaemonService with the default daemon timer delay.
 
-		@exception StandardException Standard cloudscape error policy
+@exception StandardException Standard Derby error policy
 	 */
 	public DaemonService createNewDaemon(String name) throws StandardException;
 }
---------------
-------------
@@ -55,7 +55,7 @@
 import com.google.common.collect.Iterators;
 import com.google.common.base.Predicate;
 
-public final class ColumnFamilyStore implements ColumnFamilyStoreMBean
+public class ColumnFamilyStore implements ColumnFamilyStoreMBean
 {
     private static Logger logger_ = Logger.getLogger(ColumnFamilyStore.class);
 
---------------
-------------
@@ -63,7 +63,7 @@
         }
         
         
-        return String.valueOf(bytes.getLong(bytes.position()+bytes.arrayOffset()));
+return String.valueOf(bytes.getLong(bytes.position()));
     }
 
     public ByteBuffer fromString(String source)
---------------
-------------
@@ -126,7 +126,7 @@
         {
             Table table = Table.open(readCommand.table);
             Row row = readCommand.getRow(table);
-            if (null == row)
+if (row == null || row.cf == null)
                 continue;
             AbstractType defaultValidator = row.cf.metadata().getDefaultValidator();
             if (defaultValidator.isCommutative())
---------------
-------------
@@ -125,7 +125,7 @@
           final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(
               "OutOfMemoryError likely caused by the Sun VM Bug described in "
               + "https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize "
-              + "with a a value smaller than the current chunks size (" + chunkSize + ")");
++ "with a value smaller than the current chunks size (" + chunkSize + ")");
           outOfMemoryError.initCause(e);
           throw outOfMemoryError;
         }
---------------
-------------
@@ -132,7 +132,7 @@
   
   public static void purgeFieldCache(IndexReader r) throws IOException {
     // this is just a hack, to get an atomic reader that contains all subreaders for insanity checks
-    FieldCache.DEFAULT.purge(SlowCompositeReaderWrapper.wrap(r));
+FieldCache.DEFAULT.purgeByCacheKey(SlowCompositeReaderWrapper.wrap(r).getCoreCacheKey());
   }
   
   /** This is a MultiReader that can be used for randomly wrapping other readers
---------------
-------------
@@ -30,7 +30,7 @@
 		if (EnumSet.of(State.UNINSTALLED).contains(state))
 			return null;
 		else if (EnumSet.of(State.INSTALL_FAILED, State.INSTALLING, State.RESOLVING, State.STARTING, State.STOPPING, State.UNINSTALLING).contains(state)) {
-			waitForStateChange();
+waitForStateChange(state);
 			target.uninstall();
 		}
 		else if (state.equals(State.ACTIVE)) {
---------------
-------------
@@ -514,7 +514,7 @@
             systemMeta.cfMetaData.put(HintedHandOffManager.HINTS_CF, new CFMetaData(Table.SYSTEM_TABLE,
                                                                                     HintedHandOffManager.HINTS_CF,
                                                                                     "Super",
-                                                                                    new UTF8Type(),
+new BytesType(),
                                                                                     new BytesType(),
                                                                                     "hinted handoff data",
                                                                                     0.0,
---------------
-------------
@@ -50,7 +50,7 @@
     doc.add(newTextField(FN, "the quick brown fox jumps over the lazy dog", Field.Store.NO));
     writer.addDocument(doc);
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     searcher = newSearcher(reader);
   }
 
---------------
-------------
@@ -413,7 +413,7 @@
         Test test = new SupportFilesSetup(suite, new String[] {
                 "functionTests/testData/ResultSetStream/littleclob.utf",
                 "functionTests/testData/ResultSetStream/short.utf",
-                "functionTests/testData/ResultSetStream/resultsetstream.gif" });
+"functionTests/testData/ResultSetStream/resultsetStream.gif" });
 
         return new CleanDatabaseTestSetup(test) {
             protected void decorateSQL(Statement s) throws SQLException {
---------------
-------------
@@ -141,7 +141,7 @@
         zkController.getZkStateReader());
     this.zkController = zkController;
     this.cc = cc;
-    syncStrategy = new SyncStrategy(cc.getUpdateShardHandler());
+syncStrategy = new SyncStrategy(cc);
   }
   
   @Override
---------------
-------------
@@ -103,7 +103,7 @@
     private Collection<HoldingDataBeanImpl> holdings;
     
     @OneToOne(fetch=FetchType.LAZY)
-    @JoinColumn(name="PROFILE_USERID")
+@JoinColumn(name="PROFILE_USERID", columnDefinition="VARCHAR(250)")
     private AccountProfileDataBeanImpl profile;
 
     /* Accessor methods for relationship fields are only included for the AccountProfile profileID */
---------------
-------------
@@ -51,6 +51,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new SpanishAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new SpanishAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -343,7 +343,7 @@
 		// and re-use it. This way, the column alteration only changes the
 		// aspects of the autoincrement settings that it intends to change,
 		// and does not lose the other aspecs.
-		if (defaultNode == null)
+if (keepCurrentDefault)
         { defaultInfo = (DefaultInfoImpl)cd.getDefaultInfo(); }
         else
         {
---------------
-------------
@@ -56,7 +56,7 @@
 
         // create test index
         mDirectory = new RAMDirectory();
-        final IndexWriter writer = new IndexWriter(mDirectory, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+final IndexWriter writer = new IndexWriter(mDirectory, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         addDocument(writer, "1", "I think it should work.");
         addDocument(writer, "2", "I think it should work.");
         addDocument(writer, "3", "I think it should work.");
---------------
-------------
@@ -126,7 +126,7 @@
         FileUtils.fullyDelete(indexDir);
       }
       indexDir.mkdirs();
-      final boolean doSync = config.get("fsdirectory.dosync", true);
+final boolean doSync = config.get("fsdirectory.dosync", false);
       directory = FSDirectory.getDirectory(indexDir, null, doSync);
     } else {
       directory = new RAMDirectory();
---------------
-------------
@@ -201,7 +201,7 @@
     
     IntPairWritable key = new IntPairWritable();
     DoubleWritable value = new DoubleWritable();
-    for (FileStatus status : fs.globStatus(new Path(dir, "*"))) {
+for (FileStatus status : fs.globStatus(new Path(dir, "part-*"))) {
       Path path = status.getPath();
       SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, job);
       while (reader.next(key, value)) {
---------------
-------------
@@ -336,7 +336,7 @@
   @Override
   public final String toString(String f) {
     StringBuilder buffer = new StringBuilder();
-    if (!field.equals(f)) {
+if (field == null || !field.equals(f)) {
       buffer.append(field);
       buffer.append(":");
     }
---------------
-------------
@@ -535,7 +535,7 @@
     SolrQueryRequest req = new LocalSolrQueryRequest(solrCore,
         new ModifiableSolrParams());
     // reboot the writer on the new index and get a new searcher
-    solrCore.getUpdateHandler().newIndexWriter();
+solrCore.getUpdateHandler().newIndexWriter(true);
     
     try {
       // first try to open an NRT searcher so that the new 
---------------
-------------
@@ -56,7 +56,7 @@
   private final FieldInfos fieldInfos; // unread
 
   public FixedGapTermsIndexWriter(SegmentWriteState state) throws IOException {
-    final String indexFileName = IndexFileNames.segmentFileName(state.segmentName, state.formatId, TERMS_INDEX_EXTENSION);
+final String indexFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_INDEX_EXTENSION);
     termIndexInterval = state.termIndexInterval;
     out = state.directory.createOutput(indexFileName, state.context);
     boolean success = false;
---------------
-------------
@@ -622,7 +622,7 @@
     for(File file : files) {
       if (!file.getName().startsWith(".")) {
         if (!file.isDirectory()) {
-          zkClient.setData(zkPath + "/" + file.getName(), file);
+zkClient.makePath(zkPath + "/" + file.getName(), file);
         } else {
           uploadToZK(file, zkPath + "/" + file.getName());
         }
---------------
-------------
@@ -209,7 +209,7 @@
   }
 
   private String readFully(Reader stream) throws IOException {
-    StringBuffer buffer = new StringBuffer();
+StringBuilder buffer = new StringBuilder();
     int ch;
     while ((ch = stream.read()) != -1) {
       buffer.append((char) ch);
---------------
-------------
@@ -103,7 +103,7 @@
       Collection<String> files = indexCommit.getFileNames();
       FileCopier fileCopier = new FileCopier();
       
-      Directory dir = solrCore.getDirectoryFactory().get(solrCore.getNewIndexDir(), DirContext.DEFAULT, solrCore.getSolrConfig().indexConfig.lockType);
+Directory dir = solrCore.getDirectoryFactory().get(solrCore.getIndexDir(), DirContext.DEFAULT, solrCore.getSolrConfig().indexConfig.lockType);
       try {
         fileCopier.copyFiles(dir, files, snapShotDir);
       } finally {
---------------
-------------
@@ -39,7 +39,7 @@
     {
         if ( args.length != 2 )
         {
-            System.out.println("Usage : java com.facebook.infrastructure.tools.ThreadListBuilder <directory containing files to be processed> <directory to dump the bloom filter in.>");
+System.out.println("Usage : java org.apache.cassandra.tools.ThreadListBuilder <directory containing files to be processed> <directory to dump the bloom filter in.>");
             System.exit(1);
         }
         
---------------
-------------
@@ -343,7 +343,7 @@
 
         ColumnFamilyStore cfs = table.getColumnFamilyStore("Indexed2");
         ColumnDefinition old = cfs.metadata.getColumn_metadata().get(ByteBufferUtil.bytes("birthdate"));
-        ColumnDefinition cd = new ColumnDefinition(old.name, old.validator.getClass().getName(), IndexType.KEYS, "birthdate_index");
+ColumnDefinition cd = new ColumnDefinition(old.name, old.getValidator().getClass().getName(), IndexType.KEYS, "birthdate_index");
         Future<?> future = cfs.addIndex(cd);
         future.get();
         // we had a bug (CASSANDRA-2244) where index would get created but not flushed -- check for that
---------------
-------------
@@ -44,7 +44,7 @@
 
   private static class LogCallback implements PredictionCallback {
   
-    private final Logger log;
+private static Logger log;
   
     private LogCallback(Logger log) {
       this.log = log;
---------------
-------------
@@ -89,7 +89,7 @@
         }
         
         public void postCallWithException(ComponentMetadata cm, Method m,
-                Exception ex, Object preCallToken) throws Throwable {
+Throwable ex, Object preCallToken) throws Throwable {
             
             if(!isIgnorableMethod(m))
                 interceptorLog.add("POSTCALLEXCEPTION["+ex.toString()+"]:"+preCallToken);
---------------
-------------
@@ -91,7 +91,7 @@
         if (liveDocs == null || liveDocs.get(i)) {
           long val = docValues.get(i);
           minValue = Math.min(val, minValue);
-          maxValue = Math.min(val, maxValue);
+maxValue = Math.max(val, maxValue);
         }
         mergeState.checkAbort.work(300);
       }
---------------
-------------
@@ -78,7 +78,7 @@
   @Override
   public void configure(JobConf job) {
     try {
-      Parameters params = Parameters.fromString(job.get("bayes.parameters", ""));
+Parameters params = new Parameters(job.get("bayes.parameters", ""));
       if (params.get("dataSource").equals("hbase")) {
         useHbase = true;
       } else {
---------------
-------------
@@ -242,7 +242,7 @@
                     {
                         // direct write to local DC
                         assert message.getHeader(RowMutation.FORWARD_HEADER) == null;
-                        MessagingService.instance().sendOneWay(message, target);
+MessagingService.instance().sendOneWay(message, destination);
                     }
                     else
                     {
---------------
-------------
@@ -41,7 +41,7 @@
   private SegmentTermEnum indexEnum;
 
   TermInfosReader(Directory dir, String seg, FieldInfos fis)
-       throws IOException {
+throws CorruptIndexException, IOException {
     directory = dir;
     segment = seg;
     fieldInfos = fis;
---------------
-------------
@@ -281,7 +281,7 @@
                 List<DecoratedKey> keys = new ArrayList<DecoratedKey>();
                 for (DecoratedKey sample : cfs.keySamples(request.range))
                 {
-                    assert request.range.contains(sample.token);
+assert request.range.contains(sample.token): "Token " + sample.token + " is not within range " + request.range;
                     keys.add(sample);
                 }
 
---------------
-------------
@@ -39,7 +39,7 @@
     "a c e a b c"
   };
 
-  SingleFieldTestDb db1 = new SingleFieldTestDb(docs1, fieldName);
+SingleFieldTestDb db1 = new SingleFieldTestDb(random, docs1, fieldName);
 
   public void normalTest1(String query, int[] expdnrs) throws Exception {
     BooleanQueryTst bqt = new BooleanQueryTst( query, expdnrs, db1, fieldName, this,
---------------
-------------
@@ -349,7 +349,7 @@
         assertEquals(new String(cfres.getColumn("col1992").value()), "vvvvvvvvvvvvvvvv1992");
     }
 
-    private void assertColumns(ColumnFamily columnFamily, String... columnNames)
+public static void assertColumns(ColumnFamily columnFamily, String... columnNames)
     {
         assertNotNull(columnFamily);
         SortedSet<IColumn> columns = columnFamily.getAllColumns();
---------------
-------------
@@ -729,7 +729,7 @@
     if (r.nextBoolean()) {
       if (rarely(r)) {
         // crazy value
-        c.setTermIndexInterval(random.nextBoolean() ? _TestUtil.nextInt(r, 1, 31) : _TestUtil.nextInt(r, 129, 1000));
+c.setTermIndexInterval(r.nextBoolean() ? _TestUtil.nextInt(r, 1, 31) : _TestUtil.nextInt(r, 129, 1000));
       } else {
         // reasonable value
         c.setTermIndexInterval(_TestUtil.nextInt(r, 32, 128));
---------------
-------------
@@ -1309,7 +1309,7 @@
   private void addPropertyParams(ZkNodeProps message, ModifiableSolrParams params) {
     // Now add the property.key=value pairs
     for (String key : message.keySet()) {
-      if (key.indexOf(COLL_PROP_PREFIX) != -1) {
+if (key.startsWith(COLL_PROP_PREFIX)) {
         params.set(key, message.getStr(key));
       }
     }
---------------
-------------
@@ -37,7 +37,7 @@
 	EmbeddedConnectionPoolDataSource automatically supports the correct JDBC specification version
 	for the Java Virtual Machine's environment.
 	<UL>
-	<LI> JDBC 3.0 - Java 2 - JDK 1.4
+<LI> JDBC 3.0 - Java 2 - JDK 1.4, J2SE 5.0
 	<LI> JDBC 2.0 - Java 2 - JDK 1.2,1.3
 	</UL>
 
---------------
-------------
@@ -26,7 +26,7 @@
  * the default deletion policy.
  */
 
-public final class KeepOnlyLastCommitDeletionPolicy implements IndexDeletionPolicy {
+public final class KeepOnlyLastCommitDeletionPolicy extends IndexDeletionPolicy {
 
   /** Sole constructor. */
   public KeepOnlyLastCommitDeletionPolicy() {
---------------
-------------
@@ -45,7 +45,7 @@
       }
 
       @Override
-      protected Reader initReader(Reader reader) {
+protected Reader initReader(String fieldName, Reader reader) {
         return new HTMLStripCharFilter(CharReader.get(reader));
       }
     };
---------------
-------------
@@ -180,7 +180,7 @@
 		// current plans using "this" node as the key.  If needed, we'll
 		// then make the call to revert the plans in OptimizerImpl's
 		// getNextDecoratedPermutation() method.
-		addOrLoadBestPlanMapping(true, this);
+updateBestPlanMap(ADD_PLAN, this);
 
 		/*
 		** RESOLVE: Most types of Optimizables only implement estimateCost(),
---------------
-------------
@@ -48,7 +48,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -116,7 +116,7 @@
     stress test.
   */
   public void runStressTest(Directory directory, boolean autoCommit, MergeScheduler mergeScheduler) throws Exception {
-    IndexWriter modifier = new IndexWriter(directory, autoCommit, ANALYZER, true, IndexWriter.MaxFieldLength.LIMITED);
+IndexWriter modifier = new IndexWriter(directory, autoCommit, ANALYZER, true);
 
     modifier.setMaxBufferedDocs(10);
 
---------------
-------------
@@ -91,7 +91,7 @@
   //TODO: remove when we don't have to support old indexes anymore that had this field
   private int hasVectors = CHECK_FIELDINFO;
   //TODO: remove when we don't have to support old indexes anymore that had this field
-  private int hasProx = CHECK_FIELDINFO;     // True if this segment has any fields with omitTermFreqAndPositions==false
+private int hasProx = CHECK_FIELDINFO;     // True if this segment has any fields with positional information
 
   
   private FieldInfos fieldInfos;
---------------
-------------
@@ -76,7 +76,7 @@
 
     Sort sort = new Sort(new SortField(DATE_TIME_FIELD, SortField.STRING, true));
 
-    QueryParser queryParser = new QueryParser(Version.LUCENE_CURRENT, TEXT_FIELD, new WhitespaceAnalyzer());
+QueryParser queryParser = new QueryParser(TEST_VERSION_CURRENT, TEXT_FIELD, new WhitespaceAnalyzer());
     Query query = queryParser.parse("Document");
 
     // Execute the search and process the search results.
---------------
-------------
@@ -648,7 +648,7 @@
           final Position posData2 = positions.get(pos2);
           for(int idx=0;idx<posData2.count;idx++) {
             //System.out.println("    idx=" + idx + " cost=" + cost);
-            final int cost = posData.costs[idx];
+final int cost = posData2.costs[idx];
             if (cost < leastCost) {
               leastCost = cost;
               leastIDX = idx;
---------------
-------------
@@ -2930,7 +2930,7 @@
                     }
 						
 					if (checkpointInstant == LogCounter.INVALID_LOG_INSTANT &&
-										getMirrorControlFileName().exists())
+privExists(getMirrorControlFileName()))
                     {
 						checkpointInstant =
                             readControlFile(
---------------
-------------
@@ -226,7 +226,7 @@
   private Map<SegmentInfoPerCommit,Boolean> segmentsToMerge = new HashMap<SegmentInfoPerCommit,Boolean>();
   private int mergeMaxNumSegments;
 
-  private Lock writeLock;
+protected Lock writeLock;
 
   private volatile boolean closed;
   private volatile boolean closing;
---------------
-------------
@@ -63,7 +63,7 @@
         return Memtable.getSliceIterator(key, cf, this, comparator);
     }
 
-    public IColumnIterator getSSTableColumnIterator(SSTableReader sstable, String key)
+public IColumnIterator getSSTableColumnIterator(SSTableReader sstable, DecoratedKey key)
     {
         return new SSTableSliceIterator(sstable, key, start, finish, getPredicate(), reversed);
     }
---------------
-------------
@@ -73,7 +73,7 @@
      * This method changes the ports of the endpoints from
      * the control port to the storage ports.
     */
-    protected void retrofitPorts(List<EndPoint> eps)
+public void retrofitPorts(List<EndPoint> eps)
     {
         for ( EndPoint ep : eps )
         {
---------------
-------------
@@ -654,7 +654,7 @@
         }
       }
       docs.add(doc);
-      f.setValue(sb.toString());
+f.setStringValue(sb.toString());
       w.addDocument(d);
     }
 
---------------
-------------
@@ -36,7 +36,7 @@
                            Iterable<Cooccurrence> cooccurrences,
                            double weightOfVectorA,
                            double weightOfVectorB,
-                           int numberOfColumns) {
+long numberOfColumns) {
     return AbstractDistributedVectorSimilarity.countElements(cooccurrences);
   }
 }
---------------
-------------
@@ -103,7 +103,7 @@
         {
             byte[] combined = HintedHandOffManager.makeCombinedName(rm.getTable(), cf.metadata().cfName);
             QueryPath path = new QueryPath(HintedHandOffManager.HINTS_CF, rm.key(), combined);
-            add(path, ArrayUtils.EMPTY_BYTE_ARRAY, new TimestampClock(System.currentTimeMillis()), DatabaseDescriptor.getGcGraceInSeconds());
+add(path, ArrayUtils.EMPTY_BYTE_ARRAY, new TimestampClock(System.currentTimeMillis()), cf.metadata().gcGraceSeconds);
         }
     }
 
---------------
-------------
@@ -38,7 +38,7 @@
 import org.apache.aries.util.filesystem.FileSystem;
 import org.apache.aries.util.filesystem.IDirectory;
 import org.apache.aries.util.filesystem.IFile;
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 import org.junit.AfterClass;
 import org.junit.Test;
 
---------------
-------------
@@ -72,7 +72,7 @@
     
     reader = writer.getReader();
     searcher = newSearcher(reader);
-    writer.close();
+writer.shutdown();
   }
   
   @Override
---------------
-------------
@@ -64,7 +64,7 @@
     writer.commit();
     writer.close();
     AtomicReader open = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
-    NumericDocValues norms = open.simpleNormValues(floatTestField);
+NumericDocValues norms = open.getNormValues(floatTestField);
     assertNotNull(norms);
     for (int i = 0; i < open.maxDoc(); i++) {
       StoredDocument document = open.document(i);
---------------
-------------
@@ -140,7 +140,7 @@
     }
 
     public InputStream getStream() throws IOException {
-      return new ByteArrayInputStream( str.getBytes() );
+return new ByteArrayInputStream( str.getBytes(DEFAULT_CHARSET) );
     }
 
     /**
---------------
-------------
@@ -1074,7 +1074,7 @@
 
     final float getFloatFromDouble(double source) throws SqlException {
         if (Configuration.rangeCheckCrossConverters &&
-                (source > Float.MAX_VALUE || source < -Float.MAX_VALUE)) {
+Float.isInfinite((float)source)) {
             throw new LossOfPrecisionConversionException(agent_.logWriter_, String.valueOf(source));
         }
 
---------------
-------------
@@ -194,7 +194,7 @@
 
     final int delta = position - lastPosition;
     
-    assert delta > 0 || position == 0: "position=" + position + " lastPosition=" + lastPosition;            // not quite right (if pos=0 is repeated twice we don't catch it)
+assert delta >= 0: "position=" + position + " lastPosition=" + lastPosition;
 
     lastPosition = position;
 
---------------
-------------
@@ -62,7 +62,7 @@
         buckets.incrementAndGet(index);
     }
 
-    public long[] get(Boolean reset)
+public long[] get(boolean reset)
     {
         long[] rv = new long[numBuckets];
         for (int i = 0; i < numBuckets; i++)
---------------
-------------
@@ -35,7 +35,7 @@
   /**
    * Initialize this factory via a set of key-value pairs.
    */
-  protected KoreanFilterFactory(Map<String, String> args) {
+public KoreanFilterFactory(Map<String, String> args) {
     super(args);
     init(args);
   }
---------------
-------------
@@ -73,7 +73,7 @@
         providerBundleTracker.open();
 
         consumerBundleTracker = new BundleTracker(context,
-                Bundle.INSTALLED, new ConsumerBundleTrackerCustomizer(this, consumerHeaderName));
+Bundle.INSTALLED | Bundle.RESOLVED | Bundle.STARTING | Bundle.ACTIVE, new ConsumerBundleTrackerCustomizer(this, consumerHeaderName));
         consumerBundleTracker.open();
 
         for (Bundle bundle : context.getBundles()) {
---------------
-------------
@@ -40,7 +40,7 @@
 
   @Override
   protected void map(Text labelText, VectorWritable instance, Context ctx) throws IOException, InterruptedException {
-    String label = labelText.toString();
+String label = labelText.toString().split("/")[1];
     if (labelIndex.containsKey(label)) {
       ctx.write(new IntWritable(labelIndex.get(label)), instance);
     } else {
---------------
-------------
@@ -67,7 +67,7 @@
     addDocument(writer, "3", "I think it should work.");
     addDocument(writer, "4", "I think it should work.");
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     searcher = newSearcher(reader);
     searcher.setSimilarity(new DefaultSimilarity());
   }
---------------
-------------
@@ -18,7 +18,7 @@
 */
 package org.apache.cassandra.db;
 
-class DBConstants
+public class DBConstants
 {
 	public static final int boolSize_ = 1;
 	public static final int intSize_ = 4;
---------------
-------------
@@ -28,7 +28,7 @@
 
   @Override
   public void setUp() throws Exception {
-    LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE = true;
+LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true;
     super.setUp();
   }
 
---------------
-------------
@@ -224,7 +224,7 @@
     public static IStemmer createStemmer() {
       try {
         return new LuceneStemmerAdapter();
-      } catch (Throwable e) {
+} catch (Exception e) {
         return IdentityStemmer.INSTANCE;
       }
     }
---------------
-------------
@@ -205,6 +205,6 @@
         }
       }
     }
-    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());
+finish(indexOptions == IndexOptions.DOCS_ONLY ? -1 : sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());
   }
 }
---------------
-------------
@@ -55,7 +55,7 @@
   }
 
   @Override
-  public void release() {
+public void close() {
   }
 
   @Override
---------------
-------------
@@ -22,7 +22,7 @@
             Table table = Table.open(command.table);
             keys = table.getKeyRange(command.startWith, command.stopAt, command.maxResults);
         }
-        catch (IOException e)
+catch (Exception e)
         {
             throw new RuntimeException(e);
         }
---------------
-------------
@@ -102,7 +102,7 @@
 
       if (obtained) {
         System.out.print("l");
-        l.release();
+l.close();
       }
       Thread.sleep(sleepTimeMS);
     }
---------------
-------------
@@ -62,7 +62,7 @@
 /** A Query that matches documents containing a term.
   This may be combined with other terms with a {@link BooleanQuery}.
   */
-final public class TermQuery extends Query {
+public class TermQuery extends Query {
   private Term term;
   private float idf = 0.0f;
   private float weight = 0.0f;
---------------
-------------
@@ -42,7 +42,7 @@
         }
         for (int i = from; i < to; i++)
         {
-            hitsDocuments[i] = hits.doc(i));
+hitsDocuments[i] = hits.doc(i);
         }
     }
 
---------------
-------------
@@ -427,7 +427,7 @@
   private final static class TermMergeQueue extends PriorityQueue<TermsEnumWithSlice> {
     Comparator<BytesRef> termComp;
     TermMergeQueue(int size) {
-      initialize(size);
+super(size);
     }
 
     @Override
---------------
-------------
@@ -64,7 +64,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -89,7 +89,7 @@
       private Scorer scorer;
       
       @Override
-      public void setScorer(Scorer scorer) throws IOException {
+public void setScorer(Scorer scorer) {
         this.scorer = scorer;
       }
       
---------------
-------------
@@ -49,7 +49,7 @@
 import org.apache.aries.application.modelling.internal.MessageUtil;
 import org.apache.aries.util.filesystem.IDirectory;
 import org.apache.aries.util.filesystem.IFile;
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 import org.apache.aries.util.manifest.BundleManifest;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
---------------
-------------
@@ -161,7 +161,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new DutchAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new DutchAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
   
 }
---------------
-------------
@@ -325,7 +325,7 @@
     }
 
     public void _testStressLocks(LockFactory lockFactory, String indexDirName) throws IOException {
-        FSDirectory fs1 = FSDirectory.getDirectory(indexDirName, lockFactory, false);
+FSDirectory fs1 = FSDirectory.getDirectory(indexDirName, lockFactory);
 
         // First create a 1 doc index:
         IndexWriter w = new IndexWriter(fs1, new WhitespaceAnalyzer(), true);
---------------
-------------
@@ -24,7 +24,7 @@
 
   @Override
   protected Codec getCodec() {
-    return new Lucene40Codec();
+return new Lucene40RWCodec();
   }
   
 }
---------------
-------------
@@ -177,7 +177,7 @@
 
   /** Returns byte usage of all buffers. */
   public long sizeInBytes() {
-    return file.numBuffers() * BUFFER_SIZE;
+return (long) file.numBuffers() * (long) BUFFER_SIZE;
   }
   
   @Override
---------------
-------------
@@ -57,7 +57,7 @@
      * returns an iterator that returns columns from the given SSTable
      * matching the Filter criteria in sorted order.
      */
-    public abstract ColumnIterator getSSTableColumnIterator(SSTableReader sstable) throws IOException;
+public abstract ColumnIterator getSSTableColumnIterator(SSTableReader sstable);
 
     /**
      * collects columns from reducedColumns into returnCF.  Termination is determined
---------------
-------------
@@ -162,7 +162,7 @@
 	*/
 
     /**
-    Close the conglomerate controller
+Close the conglomerate controller.
 	<p>
 	Any changes to this method will probably have to be reflected in close as 
     well.
---------------
-------------
@@ -479,7 +479,7 @@
 	/**
 		Set my transaction identifier.
 	*/
-	public void setTransactionId(GlobalTransactionId extid, TransactionId localid) {
+void setTransactionId(GlobalTransactionId extid, TransactionId localid) {
 
 		if (SanityManager.DEBUG) {
 
---------------
-------------
@@ -87,7 +87,7 @@
       writer.addDocument(doc);
     }
     
-    writer.close();
+writer.shutdown();
     searchers = Collections.synchronizedList(new ArrayList<IndexSearcher>());
     // create the spellChecker
     spellindex = newDirectory();
---------------
-------------
@@ -81,7 +81,7 @@
                                           FloatWritable.class,
                                           SlopeOneDiffsToAveragesReducer.class,
                                           EntityEntityWritable.class,
-                                          FloatWritable.class,
+FullRunningAverageAndStdDevWritable.class,
                                           TextOutputFormat.class);
       FileOutputFormat.setOutputCompressorClass(diffsToAveragesJob, GzipCodec.class);
       diffsToAveragesJob.waitForCompletion(true);
---------------
-------------
@@ -447,7 +447,7 @@
         schemaFile = new File(solrLoader.getInstanceDir() + "conf" + File.separator + dcore.getSchemaName());
       }
       if(schemaFile. exists()){
-        String key = schemaFile.getAbsolutePath()+":"+new SimpleDateFormat("yyyyMMddhhmmss", Locale.US).format(new Date(schemaFile.lastModified()));
+String key = schemaFile.getAbsolutePath()+":"+new SimpleDateFormat("yyyyMMddHHmmss", Locale.US).format(new Date(schemaFile.lastModified()));
         schema = indexSchemaCache.get(key);
         if(schema == null){
           log.info("creating new schema object for core: " + dcore.name);
---------------
-------------
@@ -117,7 +117,7 @@
     indexWriter.addDocument(newSampleDocument(
         20, ctx.makePoint(0.1,0.1), ctx.makePoint(0, 0)));
 
-    indexWriter.close();
+indexWriter.shutdown();
   }
 
   private Document newSampleDocument(int id, Shape... shapes) {
---------------
-------------
@@ -945,7 +945,7 @@
       }
     }
     
-    SolrCore newCore = core.reload(solrLoader);
+SolrCore newCore = core.reload(solrLoader, core);
     // keep core to orig name link
     String origName = coreToOrigName.remove(core);
     if (origName != null) {
---------------
-------------
@@ -57,7 +57,7 @@
       writer.addDocument(doc);
     }
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
   }
   
   @Override
---------------
-------------
@@ -226,7 +226,7 @@
             }
 
             // Create a BufferedReader to read the list of tests to run
-            runlistFile = new BufferedReader(new InputStreamReader(is));
+runlistFile = new BufferedReader(new InputStreamReader(is, "UTF-8"));
             if (runlistFile == null)
             {
                 System.out.println("The suite runall file could not be read.");
---------------
-------------
@@ -884,7 +884,7 @@
   public int docId(AtomicReader reader, Term term) throws IOException {
     int docFreq = reader.docFreq(term);
     assertEquals(1, docFreq);
-    DocsEnum termDocsEnum = reader.termDocsEnum(null, term.field, term.bytes, false);
+DocsEnum termDocsEnum = reader.termDocsEnum(null, term.field, term.bytes, 0);
     int nextDoc = termDocsEnum.nextDoc();
     assertEquals(DocIdSetIterator.NO_MORE_DOCS, termDocsEnum.nextDoc());
     return nextDoc;
---------------
-------------
@@ -1618,7 +1618,7 @@
 		** This ProjectRestrictNode is not a No-Op if it does any
 		** restriction.
 		*/
-		if ( (restriction != null) ||
+if ( (restriction != null) || (constantRestriction != null) ||
 			 (restrictionList != null && restrictionList.size() > 0) )
 		{
 			return false;
---------------
-------------
@@ -35,7 +35,7 @@
  * name passed in when creating this decorator.
  *
  */
-class DropDatabaseSetup extends TestSetup {
+class DropDatabaseSetup extends BaseTestSetup {
 
     final String logicalDBName;
     DropDatabaseSetup(Test test, String logicalDBName) {
---------------
-------------
@@ -144,7 +144,7 @@
 
         for (InetAddress host : hosts)
         {
-            Message msg = new Message(host, StorageService.Verb.REPLICATION_FINISHED, new byte[0]);
+Message msg = new Message(host, StorageService.Verb.REPLICATION_FINISHED, new byte[0], MessagingService.version_);
             MessagingService.instance().sendRR(msg, FBUtilities.getLocalAddress());
         }
 
---------------
-------------
@@ -396,7 +396,7 @@
             if (DatabaseDescriptor.getNonSystemTables().size() > 0)
             {
                 bootstrap(token);
-                assert !isBootstrapMode; // bootstrap will block until finishec
+assert !isBootstrapMode; // bootstrap will block until finished
             }
             else
             {
---------------
-------------
@@ -112,7 +112,7 @@
       }
     }
 
-    TokenStream tokenStream = tfac.create(tokenizerChain.initReader(new StringReader(value)));
+TokenStream tokenStream = tfac.create(tokenizerChain.initReader(null, new StringReader(value)));
     List<AttributeSource> tokens = analyzeTokenStream(tokenStream);
 
     namedList.add(tokenStream.getClass().getName(), convertTokensToNamedLists(tokens, context));
---------------
-------------
@@ -382,7 +382,7 @@
         {
             // We want to pass this down to RunTest so it will
             // run an individual test with jvmflags like -nojit
-            jvmProps.addElement("jvmflags=" + '"' + jvmflags + '"');
+jvmProps.addElement("jvmflags=" + jvmflags);
         }
 
         if ( (timeout != null) && (timeout.length()>0) )
---------------
-------------
@@ -84,7 +84,7 @@
 
     @Override
     public TermsEnum intersect(CompiledAutomaton automaton, BytesRef bytes) throws IOException {
-      TermsEnum termsEnum = super.intersect(automaton, bytes);
+TermsEnum termsEnum = in.intersect(automaton, bytes);
       assert termsEnum != null;
       assert bytes == null || bytes.isValid();
       return new AssertingTermsEnum(termsEnum);
---------------
-------------
@@ -52,7 +52,7 @@
   final PostingsReaderBase wrappedPostingsReader;
   int maxPositions;
 
-  public PulsingPostingsReader(PostingsReaderBase wrappedPostingsReader) throws IOException {
+public PulsingPostingsReader(PostingsReaderBase wrappedPostingsReader) {
     this.wrappedPostingsReader = wrappedPostingsReader;
   }
 
---------------
-------------
@@ -385,7 +385,7 @@
                 throw new SQLSyntaxErrorException(String.format(BAD_FETCH_DIR, direction));
             fetchDirection = direction;
         }
-        throw new SQLSyntaxErrorException(String.format(BAD_FETCH_DIR, direction));
+else throw new SQLSyntaxErrorException(String.format(BAD_FETCH_DIR, direction));
     }
 
 
---------------
-------------
@@ -324,7 +324,7 @@
 
     @Override
     public double similarity(int rowA, int rowB, Iterable<Cooccurrence> cooccurrences, double weightOfVectorA,
-        double weightOfVectorB, int numberOfRows) {
+double weightOfVectorB, long numberOfRows) {
       if (rowA == rowB) {
         return Double.NaN;
       }
---------------
-------------
@@ -80,7 +80,7 @@
         } else if (sqlState.startsWith(SQLState.INTEGRITY_VIOLATION_PREFIX)) {
             ex = new SQLIntegrityConstraintViolationException(message, sqlState,
                     severity, t);
-        } else if (sqlState.startsWith(SQLState.AUTHORIZATION_PREFIX)) {
+} else if (sqlState.startsWith(SQLState.AUTHORIZATION_SPEC_PREFIX)) {
             ex = new SQLInvalidAuthorizationSpecException(message, sqlState,
                     severity, t);
         }        
---------------
-------------
@@ -38,6 +38,6 @@
    * @return valid input stream or null if this converter does not support conversion of
    *         this artifact type.  
    */
-  public InputStream convert (IDirectory parentEba, IFile fileInEba) throws ConversionException;
+public BundleConversion convert (IDirectory parentEba, IFile fileInEba) throws ConversionException;
 
 }
---------------
-------------
@@ -80,7 +80,7 @@
  * </ul>
  * @lucene.experimental
  */
-public class BloomFilteringPostingsFormat extends PostingsFormat {
+public final class BloomFilteringPostingsFormat extends PostingsFormat {
   
   public static final String BLOOM_CODEC_NAME = "BloomFilter";
   public static final int BLOOM_CODEC_VERSION = 1;
---------------
-------------
@@ -64,7 +64,7 @@
 
     private final Set<BlueprintListener> listeners = new CopyOnWriteArraySet<BlueprintListener>();
     private final Map<Bundle, BlueprintEvent> states = new ConcurrentHashMap<Bundle, BlueprintEvent>();
-    private final ExecutorService executor = Executors.newSingleThreadExecutor();
+private final ExecutorService executor = Executors.newSingleThreadExecutor(new BlueprintThreadFactory("Blueprint Event Dispatcher"));
     private final ExecutorService sharedExecutor;
     private final EventAdminListener eventAdminListener;
     private final ServiceTracker containerListenerTracker;
---------------
-------------
@@ -1360,7 +1360,7 @@
     String TYPE_MISMATCH = "XJ020.S";
     String INVALID_JDBCTYPE = "XJ021.S";
     String SET_STREAM_FAILURE = "XJ022.S";
-    String SET_STREAM_INSUFFICIENT_DATA = "XJ023.S";
+String SET_STREAM_INEXACT_LENGTH_DATA = "XJ023.S";
     String SET_UNICODE_INVALID_LENGTH = "XJ024.S";
     String NEGATIVE_STREAM_LENGTH = "XJ025.S";
     String NO_AUTO_COMMIT_ON = "XJ030.S";
---------------
-------------
@@ -189,7 +189,7 @@
         out[i] = Double.parseDouble(externalVal.substring(start, end));
         start = idx + 1;
         end = externalVal.indexOf(',', start);
-	idex = end;
+idx = end;
         if (end == -1) {
           end = externalVal.length();
         }
---------------
-------------
@@ -162,7 +162,7 @@
                 STDev = Float.parseFloat(cmd.getOptionValue("s"));
 
             if (cmd.hasOption("r"))
-                random = Boolean.parseBoolean(cmd.getOptionValue("r"));
+random = true;
 
             if (cmd.hasOption("f"))
             {
---------------
-------------
@@ -52,7 +52,7 @@
     writer.addDocument(d2);
 
     IndexReader reader = writer.getReader();
-    writer.close();
+writer.shutdown();
 
     final float[] scores = new float[4];
 
---------------
-------------
@@ -141,7 +141,7 @@
    */
   public abstract void newIndexWriter() throws IOException;
 
-  public abstract SolrCoreState getIndexWriterProvider();
+public abstract SolrCoreState getSolrCoreState();
 
   public abstract int addDoc(AddUpdateCommand cmd) throws IOException;
   public abstract void delete(DeleteUpdateCommand cmd) throws IOException;
---------------
-------------
@@ -36,7 +36,7 @@
  * 
  * @lucene.experimental
  */
-public final class Ints {
+final class Ints {
   protected static final String CODEC_NAME = "Ints";
   protected static final int VERSION_START = 0;
   protected static final int VERSION_CURRENT = VERSION_START;
---------------
-------------
@@ -58,7 +58,7 @@
     }
     IndexReader r = w.getReader();
     //System.out.println("numDocs=" + r.numDocs());
-    w.close();
+w.shutdown();
 
     final IndexSearcher s = newSearcher(r);
     Terms terms = MultiFields.getFields(r).terms("body");
---------------
-------------
@@ -1367,7 +1367,7 @@
   }
 
   /** Expert */
-  public Object getFieldCacheKey() {
+public Object getCoreCacheKey() {
     return this;
   }
 
---------------
-------------
@@ -51,7 +51,7 @@
 
   /** Must fully consume state, since after this call that
    *  TermState may be reused. */
-  public abstract DocsEnum docs(FieldInfo fieldInfo, BlockTermState state, Bits skipDocs, DocsEnum reuse, int flags) throws IOException;
+public abstract DocsEnum docs(FieldInfo fieldInfo, BlockTermState state, Bits skipDocs, DocsEnum reuse, boolean needsFreqs) throws IOException;
 
   /** Must fully consume state, since after this call that
    *  TermState may be reused. */
---------------
-------------
@@ -226,7 +226,7 @@
   }
 
   /** Closes the store to future operations, releasing associated memory. */
-  public final void close() {
+public void close() {
     fileMap = null;
   }
 
---------------
-------------
@@ -537,7 +537,7 @@
 			// DERBY-4059
 			// if this is a lock timeout just return null.
 			// otherwise throw the exception
-			if (!se.getSQLState().equals(SQLState.LOCK_TIMEOUT)) {
+if (!se.isLockTimeout()) {
 				throw se;
 			}
 		}
---------------
-------------
@@ -135,7 +135,7 @@
 
   private void getServers() throws Exception {
     jetty.start();
-    url = "http" + (isSSLMode() ? "s" : "") + "://127.0.0.1:" + jetty.getLocalPort() + "/solr/";
+url = buildUrl(jetty.getLocalPort(), "/solr/");
 
     // Mostly to keep annoying logging messages from being sent out all the time.
 
---------------
-------------
@@ -24,7 +24,7 @@
 import java.util.TreeMap;
 
 import org.apache.derby.client.resources.ResourceKeys;
-import org.apache.derby.shared.common.info.JVMInfo;
+import org.apache.derby.iapi.services.info.JVMInfo;
 import org.apache.derby.shared.common.i18n.MessageUtil;
 import org.apache.derby.shared.common.error.ExceptionUtil;
 
---------------
-------------
@@ -209,7 +209,7 @@
   }
 
   /** Creates a new, empty file in the directory with the given name. Returns a stream writing this file. */
-  public IndexOutput createOutput(String name) {
+public IndexOutput createOutput(String name) throws IOException {
     ensureOpen();
     RAMFile file = new RAMFile(this);
     synchronized (this) {
---------------
-------------
@@ -219,7 +219,7 @@
         if (isStandard)
             startIColumn = new Column(filter.start);
         else
-            startIColumn = new SuperColumn(filter.start, null); // ok to not have subcolumnComparator since we won't be adding columns to this object
+startIColumn = new SuperColumn(filter.start, null, cf.getClockType()); // ok to not have subcolumnComparator since we won't be adding columns to this object
 
         // can't use a ColumnComparatorFactory comparator since those compare on both name and time (and thus will fail to match
         // our dummy column, since the time there is arbitrary).
---------------
-------------
@@ -60,7 +60,7 @@
     {
         RowMutation rm = new RowMutation("Keyspace1", key.getBytes());
         ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
-        cf.addColumn(column("col1", "val1", 1L));
+cf.addColumn(column("col1", "val1", new TimestampClock(1L)));
         rm.add(cf);
         rm.apply();
     }
---------------
-------------
@@ -27,7 +27,7 @@
 
 import org.apache.cassandra.io.ICompactSerializer;
 
-class BitSetSerializer
+public class BitSetSerializer
 {
     public static void serialize(BitSet bs, DataOutputStream dos) throws IOException
     {
---------------
-------------
@@ -46,7 +46,7 @@
         ByteArrayInputStream bufIn = new ByteArrayInputStream(body);
         try
         {
-            StreamRequestMessage srm = StreamRequestMessage.serializer().deserialize(new DataInputStream(bufIn));
+StreamRequestMessage srm = StreamRequestMessage.serializer().deserialize(new DataInputStream(bufIn), message.getVersion());
             if (logger.isDebugEnabled())
                 logger.debug(srm.toString());
 
---------------
-------------
@@ -164,7 +164,7 @@
         long endFreeMemory = Runtime.getRuntime().freeMemory();
         
         long lossage = startFreeMemory - endFreeMemory;
-        assertTrue("We lost: "+lossage, lossage < 10000000);
+assertTrue("We lost: "+lossage, lossage < 20000000);
     }
 
     @org.ops4j.pax.exam.junit.Configuration
---------------
-------------
@@ -45,7 +45,7 @@
 
 
 public class JoinQParserPlugin extends QParserPlugin {
-  public static String NAME = "join";
+public static final String NAME = "join";
 
   @Override
   public void init(NamedList args) {
---------------
-------------
@@ -119,7 +119,7 @@
     // this will grow into the returned IntermediateFacetResult
     IntToObjectMap<AACO> AACOsOfOnePartition = new IntToObjectMap<AACO>();
 
-    int partitionSize = arrays.getArraysLength(); // all partitions, except, possibly, the last,
+int partitionSize = arrays.arrayLength; // all partitions, except, possibly, the last,
     // have the same length. Hence modulo is OK.
 
     int depth = facetRequest.getDepth();
---------------
-------------
@@ -2184,7 +2184,7 @@
         
         securityOption = "-noSecurityManager";
         
-        String workingDirName = masterDatabasePath +FS+ masterDbSubPath;
+String workingDirName = masterDatabasePath +FS+ dbSubDirPath;
         
         final String[] commandElements = {ReplicationRun.getMasterJavaExecutableName()
                 , " -Dderby.system.home=" + workingDirName
---------------
-------------
@@ -465,7 +465,7 @@
     }
   }
 
-  void incRef(List<String> files) throws IOException {
+void incRef(Collection<String> files) throws IOException {
     for(final String file : files) {
       incRef(file);
     }
---------------
-------------
@@ -814,7 +814,7 @@
             for (org.apache.cassandra.thrift.ColumnDef cdef : def.getColumn_metadata())
             {
                 org.apache.cassandra.db.migration.avro.ColumnDef tdef = new org.apache.cassandra.db.migration.avro.ColumnDef();
-                tdef.name = ByteBufferUtil.clone(cdef.BufferForName());
+tdef.name = ByteBufferUtil.clone(cdef.bufferForName());
                 tdef.validation_class = cdef.getValidation_class();
                 tdef.index_name = cdef.getIndex_name();
                 tdef.index_type = cdef.getIndex_type() == null ? null : org.apache.cassandra.db.migration.avro.IndexType.valueOf(cdef.getIndex_type().name());
---------------
-------------
@@ -159,7 +159,7 @@
 
         @param t                        Transaction to associate lock with.
         @param record                   Record to lock.
-        @param lockForInsertPreviouskey Lock is for a previous key of a insert.
+@param lockForPreviousKey       Lock is for a previous key of a insert.
         @param waitForLock              Should lock request wait until granted?
 
 		@return true if the lock was obtained, false if it wasn't. 
---------------
-------------
@@ -63,7 +63,7 @@
   public void setContext( TransformContext context ) {
     try {
       IndexReader reader = qparser.getReq().getSearcher().getIndexReader();
-      readerContexts = reader.getTopReaderContext().leaves();
+readerContexts = reader.leaves();
       docValuesArr = new FunctionValues[readerContexts.size()];
 
       searcher = qparser.getReq().getSearcher();
---------------
-------------
@@ -404,7 +404,7 @@
 
   public boolean hasNorms() {
     for (FieldInfo fi : this) {
-      if (fi.isIndexed && !fi.omitNorms) {
+if (fi.normsPresent()) {
         return true;
       }
     }
---------------
-------------
@@ -233,7 +233,7 @@
     if (discountOverlaps)
       numTerms = state.getLength() - state.getNumOverlap();
     else
-      numTerms = state.getLength() / state.getBoost();
+numTerms = state.getLength();
     return encodeNormValue(state.getBoost(), numTerms);
   }
   
---------------
-------------
@@ -73,7 +73,7 @@
     doc.add(new TextField("content", "a sentence which contains no test", Field.Store.YES));
     writer.addDocument(doc);
 
-    writer.close();
+writer.shutdown();
 
     reader = DirectoryReader.open(directory);
     searcher = newSearcher(reader);
---------------
-------------
@@ -75,7 +75,7 @@
 
 			if (firstTime) {
 				Logs.reportString("----------------------------------------------");
-				Logs.reportMessage("CSLOOK_Header", "stored procedures");
+Logs.reportMessage("CSLOOK_StoredProcHeader");
 				Logs.reportString("----------------------------------------------\n");
 			}
 
---------------
-------------
@@ -76,7 +76,7 @@
         return instance_;
     }
 
-    private static boolean sendMessage(String endpointAddress, String key) throws DigestMismatchException, TimeoutException, IOException
+private static boolean sendMessage(String endpointAddress, String key) throws DigestMismatchException, TimeoutException, IOException, InvalidRequestException
     {
         EndPoint endPoint = new EndPoint(endpointAddress, DatabaseDescriptor.getStoragePort());
         if (!FailureDetector.instance().isAlive(endPoint))
---------------
-------------
@@ -615,7 +615,7 @@
      */
     private void schedule()
     {
-        requestScheduler.queue(Thread.currentThread(), clientState.getSchedulingId());
+requestScheduler.queue(Thread.currentThread(), clientState.getSchedulingValue());
     }
 
     /**
---------------
-------------
@@ -53,7 +53,7 @@
       FieldDateResolutionMapAttribute dateResMapAttr = this.config
           .addAttribute(FieldDateResolutionMapAttribute.class);
       dateRes = dateResMapAttr.getFieldDateResolutionMap().get(
-          fieldConfig.getFieldName().toString());
+fieldConfig.getField());
     }
 
     if (dateRes == null) {
---------------
-------------
@@ -70,7 +70,7 @@
     IndexWriter writer = new IndexWriter(target, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
     reader = SortingAtomicReader.wrap(reader, sorter);
     writer.addIndexes(reader);
-    writer.close();
+writer.shutdown();
     reader.close();
     dir.close();
     
---------------
-------------
@@ -658,7 +658,7 @@
                     allKeys.remove(allKeys.size() - 1);
                     allKeys.addAll(rangeKeys);
                 }
-                else if (rangeKeys.size() > 0)
+else if (rangeKeys != null && rangeKeys.size() > 0)
                 {
                     allKeys.addAll(rangeKeys);
                 }
---------------
-------------
@@ -91,7 +91,7 @@
   
   // should produce no exceptions
   public void testEmptyArraySort() {
-    List<Integer> list = Collections.emptyList();
+List<Integer> list = Arrays.asList(new Integer[0]);
     CollectionUtil.quickSort(list);
     CollectionUtil.mergeSort(list);
     CollectionUtil.insertionSort(list);
---------------
-------------
@@ -60,7 +60,7 @@
   }
   
   public void testBaseDir() throws Exception {
-    final File base = _TestUtil.getTempDir("fsResourceLoaderBase");
+final File base = _TestUtil.getTempDir("fsResourceLoaderBase").getAbsoluteFile();
     try {
       base.mkdirs();
       Writer os = new OutputStreamWriter(new FileOutputStream(new File(base, "template.txt")), IOUtils.CHARSET_UTF_8);
---------------
-------------
@@ -492,7 +492,7 @@
     ToolRunner.run(conf, new MeanShiftCanopyDriver(), args);
     Path outPart = new Path(output, "clusters-3-final/part-r-00000");
     long count = HadoopUtil.countRecords(outPart, conf);
-    assertEquals("count", 3, count);
+assertEquals("count", 4, count);
     Iterator<?> iterator = new SequenceFileValueIterator<Writable>(outPart,
         true, conf);
     while (iterator.hasNext()) {
---------------
-------------
@@ -82,7 +82,7 @@
         br.copyBytes(term.bytes());
         assert termsEnum != null;
         if (termsEnum.seekCeil(br) == TermsEnum.SeekStatus.FOUND) {
-          docs = termsEnum.docs(acceptDocs, docs, 0);
+docs = termsEnum.docs(acceptDocs, docs, false);
           while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
             result.set(docs.docID());
           }
---------------
-------------
@@ -204,7 +204,7 @@
     } else {
       res = entries.keySet().toArray(new String[entries.size()]);
       // Add the segment name
-      String seg = fileName.substring(0, fileName.indexOf('.'));
+String seg = IndexFileNames.parseSegmentName(fileName);
       for (int i = 0; i < res.length; i++) {
         res[i] = seg + res[i];
       }
---------------
-------------
@@ -176,7 +176,7 @@
         // assess the substitute password to be legitimate for Derby's
         // BUILTIN authentication scheme/provider.
         if ((clientSecurityMechanism =
-                info.getProperty(Attribute.CLIENT_SECURITY_MECHANISM)) != null)
+info.getProperty(Attribute.DRDA_SECMEC)) != null)
         {
             secMec = Integer.parseInt(clientSecurityMechanism);
         }
---------------
-------------
@@ -303,7 +303,7 @@
         int p = 0;
         
         @Override
-        public boolean incrementToken() throws IOException {
+public boolean incrementToken() {
           if( p >= tokens.length ) return false;
           clearAttributes();
           tokens[p++].copyTo(reusableToken);
---------------
-------------
@@ -158,7 +158,7 @@
    * @see #baselineTf
    */
   @Override
-  public float tf(int freq) {
+public float tf(float freq) {
     return baselineTf(freq);
   }
   
---------------
-------------
@@ -116,7 +116,7 @@
         MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
         try
         {
-            mbs.registerMBean(this, new ObjectName("org.apache.cassandra.concurrent:type=MESSAGING-SERVICE-POOL"));
+mbs.registerMBean(this, new ObjectName("org.apache.cassandra.net:type=MessagingService"));
         }
         catch (Exception e)
         {
---------------
-------------
@@ -61,7 +61,7 @@
       transactions.add(Arrays.asList("D", "A", "C", "E", "B"));
       transactions.add(Arrays.asList("C", "A", "B", "E"));
       transactions.add(Arrays.asList("B", "A", "D"));
-      transactions.add(Arrays.asList("D"));
+transactions.add(Arrays.asList("D", "D", "", "D", "D"));
       transactions.add(Arrays.asList("D", "B"));
       transactions.add(Arrays.asList("A", "D", "E"));
       transactions.add(Arrays.asList("B", "C"));
---------------
-------------
@@ -49,7 +49,7 @@
 		super(name);
 	}
 
-	public static Test suite() {
+public static Test suite() throws Exception {
 
 		TestSuite suite = new TestSuite("lang");
         
---------------
-------------
@@ -921,7 +921,7 @@
       System.err.println("###### Only in " + bName + ": " + onlyInB);
     }
 
-    onlyInA.addAll(b);
+onlyInA.addAll(onlyInB);
     return onlyInA;
   }
 
---------------
-------------
@@ -75,7 +75,7 @@
     List<SegToken> result = new ArrayList<SegToken>();
     int s = -1, count = 0, size = tokenListTable.size();
     List<SegToken> tokenList;
-    short index = 0;
+int index = 0;
     while (count < size) {
       if (isStartExist(s)) {
         tokenList = tokenListTable.get(s);
---------------
-------------
@@ -89,7 +89,7 @@
   }
   
   private void checkHits(Query query, int[] results) throws IOException {
-    CheckHits.checkHits(query, field, searcher, results);
+CheckHits.checkHits(random, query, field, searcher, results);
   }
   
   private void orderedSlopTest3SQ(
---------------
-------------
@@ -107,7 +107,7 @@
     private Element el;
 
     private NonZeroIterator() {
-      it = vector.iterator();
+it = vector.iterateNonZero();
       buffer();
     }
 
---------------
-------------
@@ -86,7 +86,7 @@
         while (reducedColumns.hasNext())
         {
             IColumn column = reducedColumns.next();
-            if (!column.isMarkedForDelete() || column.getLocalDeletionTime() > gcBefore)
+if (QueryFilter.isRelevant(column, container, gcBefore))
                 container.addColumn(column);
         }
     }
---------------
-------------
@@ -49,7 +49,7 @@
         {
             String columnName = ("C" + Integer.toString(i));
             ByteBuffer columnValue = values.get(i % values.size());
-            columns.add(new Column(ByteBufferUtil.bytes(columnName), columnValue, System.currentTimeMillis()));
+columns.add(new Column(ByteBufferUtil.bytes(columnName)).setValue(columnValue).setTimestamp(System.currentTimeMillis()));
         }
 
         if (session.getColumnFamilyType() == ColumnFamilyType.Super)
---------------
-------------
@@ -39,7 +39,7 @@
 */
 public abstract class AbstractStrategy implements IReplicaPlacementStrategy
 {
-    protected static Logger logger_ = Logger.getLogger(AbstractStrategy.class);
+protected static final Logger logger_ = Logger.getLogger(AbstractStrategy.class);
 
     protected TokenMetadata tokenMetadata_;
     protected IPartitioner partitioner_;
---------------
-------------
@@ -6913,7 +6913,7 @@
 	private boolean readBoolean(int codepoint) throws DRDAProtocolException
 	{
 		checkLength(codepoint, 1);
-		int val = reader.readByte();
+byte val = reader.readByte();
 		if (val == CodePoint.TRUE)
 			return true;
 		else if (val == CodePoint.FALSE)
---------------
-------------
@@ -389,7 +389,7 @@
       gotExpectedException = true;
     }
     assertTrue("Should have gotten NoMoreDataException!", gotExpectedException);
-    assertEquals("Wrong number of documents created by osurce!",5,n);
+assertEquals("Wrong number of documents created by source!",5,n);
     assertTrue("Did not see all types!",unseenTypes.isEmpty());
   }
 
---------------
-------------
@@ -996,7 +996,7 @@
                     SQLState.BLOB_NONPOSITIVE_LENGTH,
                     new Long(length));
         }
-        if (length > (this.length() - pos)) {
+if (length > (this.length() - (pos -1))) {
             throw Util.generateCsSQLException(
                     SQLState.POS_AND_LENGTH_GREATER_THAN_LOB,
                     new Long(pos), new Long(length));
---------------
-------------
@@ -217,7 +217,7 @@
 	protected final CallableStatement getCallableStatement() throws SQLException {
 		return control.getRealCallableStatement();
 	}
-	protected final PreparedStatement getPreparedStatement() throws SQLException {
+public final PreparedStatement getPreparedStatement() throws SQLException {
 		return getCallableStatement();
 	}
 	/**
---------------
-------------
@@ -22,7 +22,7 @@
 import java.util.Map;
 import java.util.TreeMap;
 
-import org.apache.lucene.codecs.DocValuesReaderBase;
+import org.apache.lucene.codecs.lucene40.values.DocValuesReaderBase;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.util.IOUtils;
---------------
-------------
@@ -209,7 +209,7 @@
     w.addDocument(document);
 
     IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(w.w, false));
-    w.close();
+w.shutdown();
     Filter parentFilter = new QueryWrapperFilter(new TermQuery(new Term("__type", "parent")));
     Filter childFilter = new QueryWrapperFilter(new PrefixQuery(new Term("field2")));
     ToParentBlockJoinQuery query = new ToParentBlockJoinQuery(
---------------
-------------
@@ -195,7 +195,7 @@
       System.out.println("TEST: full merge");
       w.forceMerge(1);
       System.out.println("TEST: close writer");
-      w.close();
+w.shutdown();
     }
 
     System.out.println("TEST: open reader");
---------------
-------------
@@ -64,7 +64,7 @@
       
   /** Creates a new ThaiTokenizer, supplying the AttributeFactory */
   public ThaiTokenizer(AttributeFactory factory) {
-    super((BreakIterator)sentenceProto.clone());
+super(factory, (BreakIterator)sentenceProto.clone());
     if (!DBBI_AVAILABLE) {
       throw new UnsupportedOperationException("This JRE does not have support for Thai segmentation");
     }
---------------
-------------
@@ -126,7 +126,7 @@
       String vocabCountString = job.get("cnaivebayes.vocabCount", stringifier.toString(vocabCount));
       vocabCount = stringifier.fromString(vocabCountString);
       
-      Parameters params = Parameters.fromString(job.get("bayes.parameters", ""));
+Parameters params = new Parameters(job.get("bayes.parameters", ""));
       alphaI = Double.valueOf(params.get("alpha_i", "1.0"));
       
     } catch (IOException ex) {
---------------
-------------
@@ -377,7 +377,7 @@
         cfg = new ConfigSolrXmlBackCompat(loader, null, is, null, false);
         this.cfg = new ConfigSolrXmlBackCompat(loader, (ConfigSolrXmlBackCompat)cfg);
       } else {
-        cfg = new SolrProperties(this, is, fileName);
+cfg = new SolrProperties(this, loader, is, fileName);
         this.cfg = new SolrProperties(this, loader, (SolrProperties)cfg);
       }
     } catch (Exception e) {
---------------
-------------
@@ -78,7 +78,7 @@
       disconnectedTimer = null;
     }
     if (!isClosed) {
-      disconnectedTimer = new Timer();
+disconnectedTimer = new Timer(true);
       disconnectedTimer.schedule(new TimerTask() {
         
         @Override
---------------
-------------
@@ -3803,7 +3803,7 @@
 		} 
 		catch (SQLException se)
 		{
-			skipRemainder(false);
+skipRemainder(true);
 			throw se;
 		}
 	}
---------------
-------------
@@ -669,7 +669,7 @@
         // validate
         if (!cf_def.keyspace.toString().equals(ksName))
             throw new ConfigurationException(String.format("Keyspace mismatch (found %s; expected %s)",
-                                                           cf_def.keyspace, tableName));
+cf_def.keyspace, ksName));
         if (!cf_def.name.toString().equals(cfName))
             throw new ConfigurationException(String.format("Column family mismatch (found %s; expected %s)",
                                                            cf_def.name, cfName));
---------------
-------------
@@ -351,7 +351,7 @@
                 ColumnFamilyStore cfs = columnFamilyStores.get(columnFamily.id());
                 if (cfs == null)
                 {
-                    logger.error("Attempting to mutate non-existant column family " + columnFamily.name());
+logger.error("Attempting to mutate non-existant column family " + columnFamily.id());
                 }
                 else
                 {
---------------
-------------
@@ -40,7 +40,7 @@
     }
 
     @Override
-    public void merge(MergePolicy.OneMerge merge) throws CorruptIndexException, IOException {
+public void merge(MergePolicy.OneMerge merge) throws IOException {
       if (merge.maxNumSegments != -1 && (first || merge.segments.size() == 1)) {
         first = false;
         if (VERBOSE) {
---------------
-------------
@@ -179,7 +179,7 @@
   
   public static long getTotalTermFreq(IndexReader reader, final String field, final BytesRef termText) throws Exception {   
     long totalTF = 0L;
-    for (final AtomicReaderContext ctx : reader.getTopReaderContext().leaves()) {
+for (final AtomicReaderContext ctx : reader.leaves()) {
       AtomicReader r = ctx.reader();
       Bits liveDocs = r.getLiveDocs();
       if (liveDocs == null) {
---------------
-------------
@@ -26,7 +26,7 @@
  * <br>Example: <code>{!lucenePlusSort}myfield:foo +bar -baz;price asc</code>
  */
 public class OldLuceneQParserPlugin extends QParserPlugin {
-  public static String NAME = "lucenePlusSort";
+public static final String NAME = "lucenePlusSort";
 
   @Override
   public void init(NamedList args) {
---------------
-------------
@@ -349,7 +349,7 @@
     // test ensureCapacityWords
     int numWords = random().nextInt(10) + 2; // make sure we grow the array (at least 128 bits)
     bits.ensureCapacityWords(numWords);
-    bit = _TestUtil.nextInt(random(), 128, numWords << 6); // pick a higher bit than 128, but still within range
+bit = _TestUtil.nextInt(random(), 127, (numWords << 6)-1); // pick a bit >= to 128, but still within range
     bits.fastSet(bit);
     assertTrue(bits.fastGet(bit));
     bits.fastClear(bit);
---------------
-------------
@@ -172,7 +172,7 @@
     try {
       return (Query)super.clone();
     } catch (CloneNotSupportedException e) {
-      throw new RuntimeException(e);
+throw new RuntimeException("Clone not supported: " + e.getMessage());
     }
   }
 }
---------------
-------------
@@ -64,7 +64,7 @@
         ByteBuffer array = guidAsBytes();
         
         StringBuilder sb = new StringBuilder();
-        for (int j = array.position()+array.arrayOffset(); j < array.limit(); ++j) {
+for (int j = array.position()+array.arrayOffset(); j < array.limit()+array.arrayOffset(); ++j) {
             int b = array.array()[j] & 0xFF;
             if (b < 0x10) sb.append('0');
             sb.append(Integer.toHexString(b));
---------------
-------------
@@ -29,7 +29,7 @@
 import java.util.zip.ZipEntry;
 import java.util.zip.ZipOutputStream;
 
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 import org.apache.aries.util.manifest.BundleManifest;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
---------------
-------------
@@ -729,7 +729,7 @@
     for (int i = 0; i < cnt; i++) {
       createCollection(collectionInfos, i,
           _TestUtil.nextInt(random(), 0, shardCount) + 1,
-          _TestUtil.nextInt(random(), 0, 5) + 1);
+_TestUtil.nextInt(random(), 0, 3) + 1);
     }
     
     Set<Entry<String,List<Integer>>> collectionInfosEntrySet = collectionInfos.entrySet();
---------------
-------------
@@ -507,7 +507,7 @@
                                     new BytesRef(t.text()),
                                     MultiFields.getLiveDocs(reader),
                                     null,
-                                    0);
+false);
 
     int count = 0;
     while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
---------------
-------------
@@ -215,7 +215,7 @@
             false);
     Map entityAttrs = createMap("name", "e",
             XPathEntityProcessor.USE_SOLR_ADD_SCHEMA, "true", "xsl", ""
-            + new File(tmpdir, "x.xsl").getAbsolutePath(), "url", "cd.xml");
++ new File(tmpdir, "x.xsl").toURI(), "url", "cd.xml");
     Context c = getContext(null,
             new VariableResolverImpl(), getDataSource(cdData), Context.FULL_DUMP, null, entityAttrs);
     XPathEntityProcessor xPathEntityProcessor = new XPathEntityProcessor();
---------------
-------------
@@ -418,7 +418,7 @@
                     modifiedRowCount > 1 ? (order >= lastOrder) :
                         (order > lastOrder);
                 assertTrue("matching triggers need to be fired in order creation:"
-                        +info, orderOk);
++info+". Triggers got fired in this order:"+TRIGGER_INFO.get().toString(), orderOk);
                 lastOrder = order;
                 continue;
             }
---------------
-------------
@@ -432,7 +432,7 @@
 
           final int endPos = pos + len;
           while (pos < endPos) {
-            code = BytesRef.HASH_PRIME * code + bytes[pos++];
+code = 31 * code + bytes[pos++];
           }
         } else {
           code = bytesStart[e0];
---------------
-------------
@@ -557,7 +557,7 @@
         if (options.contains(Option.REUSE_ENUMS) && random().nextInt(10) < 9) {
           prevDocsEnum = threadState.reuseDocsEnum;
         }
-        threadState.reuseDocsEnum = termsEnum.docs(liveDocs, prevDocsEnum, doCheckFreqs);
+threadState.reuseDocsEnum = termsEnum.docs(liveDocs, prevDocsEnum, doCheckFreqs ? DocsEnum.FLAG_FREQS : 0);
         docsEnum = threadState.reuseDocsEnum;
         docsAndPositionsEnum = null;
       }
---------------
-------------
@@ -82,7 +82,7 @@
    * <P>You can only use this method, if you keep the default
    * implementation of {@link #nextSeekTerm}.
    */
-  protected final void setInitialSeekTerm(BytesRef term) throws IOException {
+protected final void setInitialSeekTerm(BytesRef term) {
     this.initialSeekTerm = term;
   }
   
---------------
-------------
@@ -319,7 +319,7 @@
 							SYSVIEWS_COMPILATION_SCHEMAID,	// column number
 							0,					// precision
 							0,					// scale
-							false,				// nullability
+true,				// nullability
 							"CHAR",				// dataType
 							true,				// built-in type
 							36					// maxLength
---------------
-------------
@@ -56,7 +56,7 @@
   @Override
   public String getName()
   {
-    return "function("+name+")";
+return name;
   }
 
   @Override
---------------
-------------
@@ -659,7 +659,7 @@
   public static String updateJ(String json, SolrParams args) throws Exception {
     SolrCore core = h.getCore();
     DirectSolrConnection connection = new DirectSolrConnection(core);
-    SolrRequestHandler handler = core.getRequestHandler("/udate/json");
+SolrRequestHandler handler = core.getRequestHandler("/update/json");
     if (handler == null) {
       handler = new JsonUpdateRequestHandler();
       handler.init(null);
---------------
-------------
@@ -25,7 +25,7 @@
 import org.apache.derby.iapi.error.StandardException;
 import org.apache.derby.iapi.sql.Activation;
 import org.apache.derby.iapi.sql.execute.RunTimeStatistics;
-import org.apache.derby.impl.sql.execute.rts.ResultSetStatistics;
+import org.apache.derby.iapi.sql.execute.ResultSetStatistics;
 /**
  * Classes, which implement this interface have the ability to explain the
  * gathered ResultSetStatistics. A Visitor pattern is used to traverse the 
---------------
-------------
@@ -143,7 +143,7 @@
   }
 
   public void testBoostsSimple() throws Exception {
-    Map<CharSequence,Float> boosts = new HashMap<CharSequence,Float>();
+Map<String,Float> boosts = new HashMap<String,Float>();
     boosts.put("b", Float.valueOf(5));
     boosts.put("t", Float.valueOf(10));
     String[] fields = { "b", "t" };
---------------
-------------
@@ -60,7 +60,7 @@
     iw.addDocument(document);
     ir = iw.getReader();
     is = newSearcher(ir);
-    iw.close();
+iw.shutdown();
   }
   
   @AfterClass
---------------
-------------
@@ -47,7 +47,7 @@
         // the rest are all single-threaded
         stages.put(Stage.STREAM, new JMXEnabledThreadPoolExecutor(Stage.STREAM));
         stages.put(Stage.GOSSIP, new JMXEnabledThreadPoolExecutor(Stage.GOSSIP));
-        stages.put(Stage.ANTIENTROPY, new JMXEnabledThreadPoolExecutor(Stage.ANTIENTROPY));
+stages.put(Stage.ANTI_ENTROPY, new JMXEnabledThreadPoolExecutor(Stage.ANTI_ENTROPY));
         stages.put(Stage.MIGRATION, new JMXEnabledThreadPoolExecutor(Stage.MIGRATION));
         stages.put(Stage.MISC, new JMXEnabledThreadPoolExecutor(Stage.MISC));
     }
---------------
-------------
@@ -122,7 +122,7 @@
       replicationHandler.snapShootDetails = details;
       if (lock != null) {
         try {
-          lock.release();
+lock.close();
         } catch (IOException e) {
           LOG.error("Unable to release snapshoot lock: " + directoryName + ".lock");
         }
---------------
-------------
@@ -421,7 +421,7 @@
    */ 
   @Override
   public MergeSpecification findForcedDeletesMerges(SegmentInfos segmentInfos)
-      throws CorruptIndexException, IOException {
+throws IOException {
     final List<SegmentInfoPerCommit> segments = segmentInfos.asList();
     final int numSegments = segments.size();
 
---------------
-------------
@@ -1,4 +1,4 @@
-package org.apache.lucene.queryparser.flexible.core.builders;
+package org.apache.lucene.queryParser.core.builders;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
---------------
-------------
@@ -132,7 +132,7 @@
     tvf.writeVInt(suffix);
     tvf.writeBytes(term.bytes, term.offset + prefix, suffix);
     tvf.writeVInt(freq);
-    lastTerm.copy(term);
+lastTerm.copyBytes(term);
     lastPosition = lastOffset = 0;
     
     if (offsets && positions) {
---------------
-------------
@@ -61,7 +61,7 @@
             for (int i = 0; i < ROWS_PER_SSTABLE; i++) {
                 DecoratedKey key = Util.dk(String.valueOf(i % 2));
                 RowMutation rm = new RowMutation(TABLE1, key.key);
-                rm.add(new QueryPath("Standard1", null, ByteBuffer.wrap(String.valueOf(i / 2).getBytes())), ByteBufferUtil.EMPTY_BYTE_BUFFER, j * ROWS_PER_SSTABLE + i);
+rm.add(new QueryPath("Standard1", null, ByteBufferUtil.bytes(String.valueOf(i / 2))), ByteBufferUtil.EMPTY_BYTE_BUFFER, j * ROWS_PER_SSTABLE + i);
                 rm.apply();
                 inserted.add(key);
             }
---------------
-------------
@@ -90,7 +90,7 @@
      */
     public static Test suite()
     {
-        TestSuite suite = (TestSuite) TestConfiguration.embeddedSuite(AnsiSignaturesTest.class);
+TestSuite suite = (TestSuite) TestConfiguration.defaultSuite(AnsiSignaturesTest.class);
 
         return new CleanDatabaseTestSetup( suite );
     }
---------------
-------------
@@ -182,7 +182,7 @@
 				"undo Page is not an allocPage");
 		}
 
-		((AllocPage)undoPage).compressSpace(
+((AllocPage)undoPage).undoCompressSpace(
              CLRInstant, newHighestPage, num_pages_truncated);
 	}
 
---------------
-------------
@@ -169,7 +169,7 @@
 	/**
 		Check that there are not output parameters defined
 		by the parameter set. If there are unknown parameter
-		types they are forced to input types. i.e. Cloudscape static method
+types they are forced to input types. i.e. Derby static method
 		calls with parameters that are array.
 
 		@return true if a declared Java Procedure INOUT or OUT parameter is in the set, false otherwise.
---------------
-------------
@@ -48,7 +48,7 @@
 
         public void doVerb(Message message)
         {
-            byte[] body = (byte[])message.getMessageBody()[0];
+byte[] body = message.getMessageBody();
             DataInputBuffer bufIn = new DataInputBuffer();
             bufIn.reset(body, body.length);
 
---------------
-------------
@@ -849,7 +849,7 @@
     String[] startFiles = dir.listAll();
     SegmentInfos infos = new SegmentInfos();
     infos.read(dir);
-    new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null);
+new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null, null);
     String[] endFiles = dir.listAll();
     modifier.close();
     dir.close();
---------------
-------------
@@ -103,7 +103,7 @@
   
   @Test
   public void testBasic() throws Exception {
-    Replicator replicator = new HttpReplicator("localhost", port, ReplicationService.REPLICATION_CONTEXT + "/s1", 
+Replicator replicator = new HttpReplicator("127.0.0.1", port, ReplicationService.REPLICATION_CONTEXT + "/s1",
         getClientConnectionManager());
     ReplicationClient client = new ReplicationClient(replicator, new IndexReplicationHandler(handlerIndexDir, null), 
         new PerSessionDirectoryFactory(clientWorkDir));
---------------
-------------
@@ -54,7 +54,7 @@
 import org.apache.aries.util.filesystem.FileSystem;
 import org.apache.aries.util.filesystem.FileUtils;
 import org.apache.aries.util.filesystem.IDirectory;
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 import org.apache.felix.bundlerepository.Capability;
 import org.apache.felix.bundlerepository.Property;
 import org.apache.felix.bundlerepository.RepositoryAdmin;
---------------
-------------
@@ -53,7 +53,7 @@
 
     final long t0 = System.currentTimeMillis();
 
-    final LineFileDocs docs = new LineFileDocs(true);
+final LineFileDocs docs = new LineFileDocs(random);
     final File tempDir = _TestUtil.getTempDir("nrtopenfiles");
     final MockDirectoryWrapper dir = new MockDirectoryWrapper(random, FSDirectory.open(tempDir));
     final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());
---------------
-------------
@@ -237,7 +237,7 @@
     final Object empty = outputs.getNoOutput();
     final Builder<Object> builder = new Builder<Object>(
         FST.INPUT_TYPE.BYTE1, 0, 0, true, true, 
-        shareMaxTailLength, outputs, null, false);
+shareMaxTailLength, outputs, null, false, true);
     
     BytesRef scratch = new BytesRef();
     BytesRef entry;
---------------
-------------
@@ -52,7 +52,7 @@
     // get a private context that is used to rewrite, createWeight and score eventually
     assert context.reader.getTopReaderContext().isAtomic;
     final AtomicReaderContext privateContext = (AtomicReaderContext) context.reader.getTopReaderContext();
-    final Weight weight = query.weight(new IndexSearcher(privateContext));
+final Weight weight = new IndexSearcher(privateContext).createNormalizedWeight(query);
     return new DocIdSet() {
       @Override
       public DocIdSetIterator iterator() throws IOException {
---------------
-------------
@@ -1371,7 +1371,7 @@
     // now an ugly built of XML parsing to test the snippet is encoded OK
     DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();
     DocumentBuilder db = dbf.newDocumentBuilder();
-    org.w3c.dom.Document doc = db.parse(new ByteArrayInputStream(xhtml.getBytes()));
+org.w3c.dom.Document doc = db.parse(new ByteArrayInputStream(xhtml.getBytes("UTF-8")));
     Element root = doc.getDocumentElement();
     NodeList nodes = root.getElementsByTagName("body");
     Element body = (Element) nodes.item(0);
---------------
-------------
@@ -82,7 +82,7 @@
     /* if the size of columns or super-columns are more than this, indexing will kick in */
     public Integer column_index_size_in_kb = 64;
     public Integer in_memory_compaction_limit_in_mb = 256;
-    public Boolean compaction_multithreading = true;
+public Integer concurrent_compactors = Runtime.getRuntime().availableProcessors();
     public Integer compaction_throughput_mb_per_sec = 16;
     
     public String[] data_file_directories;
---------------
-------------
@@ -52,7 +52,7 @@
   public static final String KEY = "KEY";
   public static final String FIELD = "field";
   public static final QueryParser qp =
-    new QueryParser(Version.LUCENE_CURRENT, FIELD, new WhitespaceAnalyzer());
+new QueryParser(TEST_VERSION_CURRENT, FIELD, new WhitespaceAnalyzer());
 
   @Override
   public void tearDown() throws Exception {
---------------
-------------
@@ -249,7 +249,7 @@
   }
   
   public void testTokenAttributes() throws Exception {
-    TokenStream ts = a.tokenStream("dummy", new StringReader("This is a test"));
+TokenStream ts = a.tokenStream("dummy", "This is a test");
     ScriptAttribute scriptAtt = ts.addAttribute(ScriptAttribute.class);
     ts.reset();
     while (ts.incrementToken()) {
---------------
-------------
@@ -94,7 +94,7 @@
 
   private Object accumulateGuard;
 
-  private double complementThreshold;
+private double complementThreshold = DEFAULT_COMPLEMENT_THRESHOLD;
   
   public StandardFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader, 
       TaxonomyReader taxonomyReader) {
---------------
-------------
@@ -98,7 +98,7 @@
     
     for (final PerReaderTermState stat : termStats ) {
       final long totalTermFrequency = stat.totalTermFreq();
-      value += 1 / (mu * (totalTermFrequency+1L/(double)(sumOfTotalTermFreq+1L)));
+value += 1 / (mu * ((totalTermFrequency+1L)/(double)(sumOfTotalTermFreq+1L)));
       exp.append(" ");
       exp.append(totalTermFrequency);
     }
---------------
-------------
@@ -72,7 +72,7 @@
         Util.createInitialRing(ss, partitioner, endpointTokens, keyTokens, hosts, 6);
 
         MessagingService.instance().listen(FBUtilities.getLocalAddress());
-        Gossiper.instance.start(FBUtilities.getLocalAddress(), 1);
+Gossiper.instance.start(1);
         for (int i = 0; i < 6; i++)
         {
             Gossiper.instance.initializeNodeUnsafe(hosts.get(i), 1);
---------------
-------------
@@ -163,6 +163,6 @@
     FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
     SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
     sfb.setMultiValuedSeparator( '/' );
-    assertEquals( " b c//<b>d</b> e", sfb.createFragment( reader, 0, F, ffl ) );
+assertEquals( "//a b c//<b>d</b> e", sfb.createFragment( reader, 0, F, ffl ) );
   }
 }
---------------
-------------
@@ -890,7 +890,7 @@
         {
             if (!cf.getKeyspace().equals(ks_def.getName()))
             {
-                throw new InvalidRequestException("CsDef (" + cf.getName() +") had a keyspace definition that did not match KsDef");
+throw new InvalidRequestException("CfDef (" + cf.getName() +") had a keyspace definition that did not match KsDef");
             }
         }
 
---------------
-------------
@@ -154,7 +154,7 @@
   protected LogWatcher logging = null;
   private String zkHost;
   private Map<SolrCore,String> coreToOrigName = new ConcurrentHashMap<SolrCore,String>();
-  private String leaderVoteWait;
+private String leaderVoteWait = LEADER_VOTE_WAIT;
   private int coreLoadThreads;
   
   {
---------------
-------------
@@ -1045,7 +1045,7 @@
 
     String termImage=discardEscapeChar(term.image);
     if (wildcard) {
-      q = getWildcardQuery(qfield, termImage);
+q = getWildcardQuery(qfield, term.image);
     } else if (prefix) {
       q = getPrefixQuery(qfield,
           discardEscapeChar(term.image.substring
---------------
-------------
@@ -100,7 +100,7 @@
   @Override
   public void tearDown() throws Exception {
     reader.close();
-    iw.close();
+iw.shutdown();
     dir.close();
     super.tearDown();
   }
---------------
-------------
@@ -261,7 +261,7 @@
 
       while (termsEnum.next() != null) {
         String text = termsEnum.term().utf8ToString();
-        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);
+docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, DocsEnum.FLAG_FREQS);
         
         while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
           int docId = docs.docID();
---------------
-------------
@@ -37,7 +37,7 @@
  * {@link org.apache.lucene.queries.function.valuesource.ReciprocalFloatFunction}
  */
 public class BoostQParserPlugin extends QParserPlugin {
-  public static String NAME = "boost";
+public static final String NAME = "boost";
   public static String BOOSTFUNC = "b";
 
   @Override
---------------
-------------
@@ -54,7 +54,7 @@
     {
         try
         {
-            long timeout = System.currentTimeMillis() - startTime + DatabaseDescriptor.getRpcTimeout();
+long timeout = DatabaseDescriptor.getRpcTimeout() - (System.currentTimeMillis() - startTime);
             boolean success;
             try
             {
---------------
-------------
@@ -91,7 +91,7 @@
       }
 
       @Override
-      public float getValueForNormalization() throws IOException {
+public float getValueForNormalization() {
         throw new IllegalStateException("Weight already normalized.");
       }
 
---------------
-------------
@@ -1,6 +1,6 @@
 /*
  
-   Derby - Class BlobClobTestSetup
+Derby - Class org.apache.derbyTesting.functionTests.tests.jdbc4.BlobClobTestSetup
  
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -51,7 +51,7 @@
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
     Random random = random();
-    checkRandomData(random, analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random, analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -82,7 +82,7 @@
         StringBuilder builder = new StringBuilder();
         for (IColumn column : columns)
         {
-            builder.append(getString(column.name())).append(",");
+builder.append(column.getString(this)).append(",");
         }
         return builder.toString();
     }
---------------
-------------
@@ -77,7 +77,7 @@
     params.setMinSampleSize(2);
     params.setMaxSampleSize(50);
     params.setOversampleFactor(5);
-    params.setSampingThreshold(60);
+params.setSamplingThreshold(60);
     params.setSampleRatio(0.1);
     
     FacetResult res = searchWithFacets(r, tr, fsp, params);
---------------
-------------
@@ -908,7 +908,7 @@
       System.out.println("TEST: " + shardState.subSearchers.length + " shards: " + Arrays.toString(shardState.subSearchers));
     }
     // Run 1st pass collector to get top groups per shard
-    final Weight w = query.weight(topSearcher);
+final Weight w = topSearcher.createNormalizedWeight(query);
     final List<Collection<SearchGroup<String>>> shardGroups = new ArrayList<Collection<SearchGroup<String>>>();
     for(int shardIDX=0;shardIDX<shardState.subSearchers.length;shardIDX++) {
       final TermFirstPassGroupingCollector c = new TermFirstPassGroupingCollector("group", groupSort, groupOffset+topNGroups);
---------------
-------------
@@ -43,7 +43,7 @@
       this.output2 = output2;
     }
 
-    @Override @SuppressWarnings("unchecked")
+@Override @SuppressWarnings("rawtypes")
     public boolean equals(Object other) {
       if (other == this) {
         return true;
---------------
-------------
@@ -107,7 +107,7 @@
 
     log.info("Creating FileDataModel for file " + dataFile);
 
-    this.dataFile = dataFile;
+this.dataFile = dataFile.getAbsoluteFile();
     this.lastModified = dataFile.lastModified();
     this.reloadLock = new ReentrantLock();
   }
---------------
-------------
@@ -59,7 +59,7 @@
 
     SegmentInfos sis = new SegmentInfos();
     sis.read(dir);
-    double min = sis.info(0).sizeInBytes();
+double min = sis.info(0).sizeInBytes(true);
 
     conf = newWriterConfig();
     LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();
---------------
-------------
@@ -52,7 +52,7 @@
         epStateMap_ = epStateMap;
     }
         
-    Map<InetAddress, EndPointState> getEndPointStateMap()
+Map<InetAddress, EndPointState> getEndpointStateMap()
     {
          return epStateMap_;
     }
---------------
-------------
@@ -55,7 +55,7 @@
    *          length may be larger than the actual number of scorers.
    */
   public DisjunctionMaxScorer(Weight weight, float tieBreakerMultiplier,
-      Scorer[] subScorers, int numScorers) throws IOException {
+Scorer[] subScorers, int numScorers) {
     super(weight);
     this.tieBreakerMultiplier = tieBreakerMultiplier;
     // The passed subScorers array includes only scorers which have documents
---------------
-------------
@@ -378,7 +378,7 @@
         throw new IllegalArgumentException("field '" + field + "' was indexed without offsets, cannot highlight");
       }
       int end = dp.endOffset();
-      if (start > current.endOffset) {
+if (start >= current.endOffset) {
         if (current.startOffset >= 0) {
           // finalize current
           current.score *= scorer.norm(current.startOffset);
---------------
-------------
@@ -46,7 +46,7 @@
             store.forceBlockingFlush();
             assertEquals(inserted.size(), table.getColumnFamilyStore(columnFamilyName).getKeyRange("", "", 10000).keys.size());
         }
-        Future<Integer> ft = MinorCompactionManager.instance().submit(store, 2, 32);
+Future<Integer> ft = CompactionManager.instance().submit(store, 2, 32);
         ft.get();
         assertEquals(1, store.getSSTables().size());
         assertEquals(table.getColumnFamilyStore(columnFamilyName).getKeyRange("", "", 10000).keys.size(), inserted.size());
---------------
-------------
@@ -59,7 +59,7 @@
 import org.apache.lucene.store.FSDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.document.Document;
-import demo.FileDocument;
+import org.apache.lucene.FileDocument;
 
 import java.io.File;
 import java.util.Date;
---------------
-------------
@@ -65,7 +65,7 @@
  *
  * @author jamie
  */
-final class SQLDecimal extends NumberDataType implements VariableSizeDataValue
+public final class SQLDecimal extends NumberDataType implements VariableSizeDataValue
 {
 	static final BigDecimal ZERO = BigDecimal.valueOf(0L);
 	static final BigDecimal ONE = BigDecimal.valueOf(1L);
---------------
-------------
@@ -167,7 +167,7 @@
 
 			subqueryNode = (SubqueryNode) elementAt(index);
 
-			if (subqueryNode.getResultSet().referencesSessionSchema())
+if (subqueryNode.referencesSessionSchema())
 			{
 				return true;
 			}
---------------
-------------
@@ -64,7 +64,7 @@
       success = true;
     } finally {
       if (!success) {
-        IOUtils.closeWhileHandlingException(tvx, tvd, tvf);
+abort();
       }
     }
   }
---------------
-------------
@@ -669,7 +669,7 @@
     public void stringField(FieldInfo fieldInfo, String value) throws IOException {
       assert currentField >= 0;
       StringBuilder builder = builders[currentField];
-      if (builder.length() > 0) {
+if (builder.length() > 0 && builder.length() < maxLength) {
         builder.append(' '); // for the offset gap, TODO: make this configurable
       }
       if (builder.length() + value.length() > maxLength) {
---------------
-------------
@@ -453,7 +453,7 @@
         stop = StopCode.TRIVIAL;
       }
 
-      if (stop != StopCode.CONTINUE && stop.ordinal() != istop + 1) {
+if (stop != StopCode.CONTINUE && stop.ordinal() != istop) {
         throw new IllegalStateException(String.format("bad code match %d vs %d", istop, stop.ordinal()));
       }
 
---------------
-------------
@@ -1,7 +1,7 @@
 /*
  
    Derby - Class 
-       org.apache.derbyTesting.functionTests.lang.UpdatableResultSetTest
+org.apache.derbyTesting.functionTests.tests.lang.UpdatableResultSetTest
  
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -260,7 +260,7 @@
         IndexExpression expr = new IndexExpression(ByteBufferUtil.bytes("birthdate"), IndexOperator.EQ, ByteBufferUtil.bytes(1L));
         IndexClause clause = new IndexClause(Arrays.asList(expr), ByteBufferUtil.EMPTY_BYTE_BUFFER, 100);
         IPartitioner p = StorageService.getPartitioner();
-        Range range = new Range(p.getMinimumToken(), p.getMinimumToken());
+Range<RowPosition> range = Util.range("", "");
         List<Row> rows = indexedCFS.search(clause, range, new IdentityQueryFilter());
         assert rows.size() == 1;
     }
---------------
-------------
@@ -1048,7 +1048,7 @@
             if (r instanceof AtomicReader) {
               r = new FCInvisibleMultiReader(new AssertingAtomicReader((AtomicReader)r));
             } else if (r instanceof DirectoryReader) {
-              r = new FCInvisibleMultiReader(new AssertingDirectoryReader((DirectoryReader)r));
+r = new FCInvisibleMultiReader((DirectoryReader)r);
             }
             break;
           default:
---------------
-------------
@@ -49,7 +49,7 @@
         super(filename, partitioner);
         dataFile = new BufferedRandomAccessFile(path, "rw", (int)(DatabaseDescriptor.getFlushDataBufferSizeInMB() * 1024 * 1024));
         indexFile = new BufferedRandomAccessFile(indexFilename(), "rw", (int)(DatabaseDescriptor.getFlushIndexBufferSizeInMB() * 1024 * 1024));
-        bf = new BloomFilter((int)keyCount, 15); // TODO fix long -> int cast
+bf = BloomFilter.getFilter(keyCount, 15);
     }
 
     private long beforeAppend(DecoratedKey decoratedKey) throws IOException
---------------
-------------
@@ -82,7 +82,7 @@
     doc.add(new SortedSetDocValuesFacetField("Publish Year", "1999"));
     indexWriter.addDocument(config.build(doc));
     
-    indexWriter.close();
+indexWriter.shutdown();
   }
 
   /** User runs a query and counts facets. */
---------------
-------------
@@ -269,7 +269,7 @@
      *         property is undefined.
      * @throws IOException if the operation failed
      */
-    String getProperty(String key);
+String getProperty(String key) throws IOException;
 
 	/**
      * Returns the bundles IDs that have non-current, in use bundle wirings. This
---------------
-------------
@@ -805,7 +805,7 @@
     try {
       core = coreContainer.getCore(cname);
       if (core != null) {
-        syncStrategy = new SyncStrategy();
+syncStrategy = new SyncStrategy(core.getCoreDescriptor().getCoreContainer().getUpdateShardHandler());
         
         Map<String,Object> props = new HashMap<String,Object>();
         props.put(ZkStateReader.BASE_URL_PROP, zkController.getBaseUrl());
---------------
-------------
@@ -44,7 +44,7 @@
   @Override
   public Filter makeFilter(SpatialArgs args) {
     final SpatialOperation op = args.getOperation();
-    if (! SpatialOperation.is(op, SpatialOperation.IsWithin, SpatialOperation.Intersects, SpatialOperation.BBoxWithin, SpatialOperation.BBoxIntersects))
+if (op != SpatialOperation.Intersects)
       throw new UnsupportedSpatialOperation(op);
 
     Shape shape = args.getShape();
---------------
-------------
@@ -167,7 +167,7 @@
 		assertSelectPrivilege(false, users[4], "s1", "t1", new String[] {"c3"});
 		assertSelectPrivilege(false, users[4], "s1", "t1", null);
 		assertUpdatePrivilege(false, users[4], "S1", "T1", new String[] {"C1"});
-		assertUpdatePrivilege(false, users[4], "S1", "T1", new String[] {"C2", "C3"});
+assertUpdatePrivilege(true, users[4], "S1", "T1", new String[] {"C2", "C3"});
 		assertReferencesPrivilege(true, users[4], "s1", "t1", new String[] {"c1","c2","c3"});
 		revoke("select(c1),update(c3,c2),references(c3,c1,c2)", "s1", "t1", users[4]);
 	}
---------------
-------------
@@ -67,7 +67,7 @@
 	public void init(
 							Object columnName,
 							Object tableName,
-				   			Object dts)
+Object dts) throws StandardException
 	{
 		this.columnName = (String) columnName;
 		this.tableName = (TableName) tableName;
---------------
-------------
@@ -984,7 +984,7 @@
 	String LANG_TOO_MANY_PARAMETERS_FOR_STORED_PROC                    = "54023";
 
 	//following 1 does not match the DB2 sql state, it is a Cloudscape specific behavior which is not compatible with DB2
-	String LANG_OPERATION_NOT_ALLOWED_ON_SESSION_SCHEMA_TABLES = "XCL478.S";
+String LANG_OPERATION_NOT_ALLOWED_ON_SESSION_SCHEMA_TABLES = "XCL51.S";
 
 	// org.apache.derby.impl.sql.execute.rts
 	String RTS_ATTACHED_TO											   = "43X00.U";
---------------
-------------
@@ -33,7 +33,7 @@
     private long capacity = 0;
     private HashMap<Integer,byte[]> singleBuffers = new HashMap<Integer,byte[]>();
     @Override
-    byte[] newBuffer(int size) {
+protected byte[] newBuffer(int size) {
       capacity += size;
       if (capacity <= MAX_VALUE) {
         // below maxint we reuse buffers
---------------
-------------
@@ -121,7 +121,7 @@
     final SpanOrQuery that = (SpanOrQuery) o;
 
     if (!clauses.equals(that.clauses)) return false;
-    if (!field.equals(that.field)) return false;
+if (!clauses.isEmpty() && !field.equals(that.field)) return false;
 
     return getBoost() == that.getBoost();
   }
---------------
-------------
@@ -370,7 +370,7 @@
       String[] expressions = new  String[2];
       String instHome = new File(solrHomeDirectory, "new_one").getAbsolutePath();
       expressions[0] = "/solr/cores/core[@name='new_one' and @instanceDir='" + instHome + "']";
-      expressions[1] = "/solr/cores/core[@name='new_two' and @instanceDir='new_two/']";
+expressions[1] = "/solr/cores/core[@name='new_two' and @instanceDir='new_two" + File.separator + "']";
 
       assertXmlFile(persistXml1, expressions);
 
---------------
-------------
@@ -2328,7 +2328,7 @@
             else
             {
                 sb.append(stub ? 'D' : 'C');
-                sb.append(containerId.getContainerId());
+sb.append(Long.toHexString(containerId.getContainerId()));
                 sb.append(".DAT");
             }
             return storageFactory.newStorageFile( sb.toString());
---------------
-------------
@@ -51,7 +51,7 @@
     // dataset
     // This is sensitive to the working directory where the test is run:
     FileSystem fs = FileSystem.get(new Configuration());
-    Path input = fs.makeQualified(new Path(Resources.getResource("wdbc").toString()));
+Path input = fs.makeQualified(new Path(Resources.getResource("wdbc").toURI()));
     CDMahoutEvaluator.initializeDataSet(input);
 
     // evaluate the rules
---------------
-------------
@@ -77,7 +77,7 @@
   }
 
   private final static class AtomicCounter extends Counter {
-    private AtomicLong count;
+private final AtomicLong count = new AtomicLong();
 
     @Override
     public long addAndGet(long delta) {
---------------
-------------
@@ -71,7 +71,7 @@
   @BeforeClass
   public static void beforeClass() throws Exception {
     // NOTE: turn off compound file, this test will open some index files directly.
-    LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE = true;
+LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true;
     IndexWriterConfig config = newIndexWriterConfig(TEST_VERSION_CURRENT, 
         new MockAnalyzer(random(), MockTokenizer.KEYWORD, false)).setUseCompoundFile(false);
     
---------------
-------------
@@ -447,7 +447,7 @@
   public void checkpoint(SegmentInfos segmentInfos, boolean isCommit) throws IOException {
 
     if (infoStream != null) {
-      message("now checkpoint \"" + segmentInfos + "\" [" + segmentInfos.size() + " segments " + "; isCommit = " + isCommit + "]");
+message("now checkpoint \"" + segmentInfos.toString(directory) + "\" [" + segmentInfos.size() + " segments " + "; isCommit = " + isCommit + "]");
     }
 
     // Try again now to delete any previously un-deletable
---------------
-------------
@@ -88,7 +88,7 @@
   
   public void testSpanNearQuery() throws Exception {
     SpanNearQuery q = makeQuery();
-    CheckHits.checkHits(q, FIELD, searcher, new int[] {0,1});
+CheckHits.checkHits(random, q, FIELD, searcher, new int[] {0,1});
   }
 
   public String s(Spans span) {
---------------
-------------
@@ -117,7 +117,7 @@
       CharacterRunAutomaton runAutomaton = new CharacterRunAutomaton(automaton);
       CharsRef utf16 = new CharsRef(10);
 
-      private SimpleAutomatonTermsEnum(TermsEnum tenum) throws IOException {
+private SimpleAutomatonTermsEnum(TermsEnum tenum) {
         super(tenum);
         setInitialSeekTerm(new BytesRef(""));
       }
---------------
-------------
@@ -57,7 +57,7 @@
     addDoc(writer, "jonny smith", "5");
     addDoc(writer, "johnathon smythe", "6");
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     searcher = newSearcher(reader);
   }
 
---------------
-------------
@@ -90,7 +90,7 @@
              "org.apache.hadoop.io.serializer.JavaSerialization,org.apache.hadoop.io.serializer.WritableSerialization");
     // Dont ever forget this. People should keep track of how hadoop conf parameters and make or break a piece of code
     
-    FileSystem dfs = FileSystem.get(conf);
+FileSystem dfs = FileSystem.get(outPath.toUri(), conf);
     if (dfs.exists(outPath))
       dfs.delete(outPath, true);
 
---------------
-------------
@@ -288,7 +288,7 @@
             // This is a substatement; for now, we do not set any timeout
             // for it. We might change this behaviour later, by linking
             // timeout to its parent statement's timeout settings.
-			rs = ps.execute(lcc, false, 0L);
+rs = ps.executeSubStatement(lcc, false, 0L);
 			ExecRow row = rs.getNextRow();
 			if (SanityManager.DEBUG)
 			{
---------------
-------------
@@ -118,7 +118,7 @@
 })
 public abstract class SolrTestCaseJ4 extends LuceneTestCase {
   private static String coreName = ConfigSolrXmlOld.DEFAULT_DEFAULT_CORE_NAME;
-  public static int DEFAULT_CONNECTION_TIMEOUT = 30000;  // default socket connection timeout in ms
+public static int DEFAULT_CONNECTION_TIMEOUT = 45000;  // default socket connection timeout in ms
 
   protected static volatile SSLConfig sslConfig = new SSLConfig();
 
---------------
-------------
@@ -405,7 +405,7 @@
             }
             catch (StandardException se)
             {
-                if ( !se.getMessageId().equals( SQLState.LOCK_TIMEOUT ) ) { throw se; }
+if ( !se.isLockTimeout() ) { throw se; }
             }
             finally
             {
---------------
-------------
@@ -257,7 +257,7 @@
         long sum = 0;
         for (SSTableReader sstable : sstables)
         {
-            sum += sstable.length();
+sum += sstable.onDiskLength();
         }
         return sum;
     }
---------------
-------------
@@ -26,7 +26,7 @@
 
 import org.apache.cassandra.utils.UUIDGen;
 
-public class JdbcLexicalUUID extends JdbcLong
+public class JdbcLexicalUUID extends AbstractJdbcUUID
 {
     public static final JdbcLexicalUUID instance = new JdbcLexicalUUID();
     
---------------
-------------
@@ -185,7 +185,7 @@
     } catch (SolrException ex) {
       SolrException.log(log, "Collection " + operation + " of " + operation
           + " failed");
-      results.add("Operation " + operation + " cause exception:", ex);
+results.add("Operation " + operation + " caused exception:", ex);
     } finally {
       return new OverseerSolrResponse(results);
     }
---------------
-------------
@@ -512,7 +512,7 @@
         // the file for subsequent deletion.
 
         if (infoStream != null) {
-          message("IndexFileDeleter: unable to remove file \"" + fileName + "\": " + e.toString() + "; Will re-try later.");
+message("unable to remove file \"" + fileName + "\": " + e.toString() + "; Will re-try later.");
         }
         if (deletable == null) {
           deletable = new ArrayList<String>();
---------------
-------------
@@ -79,7 +79,7 @@
             {
                 if ( !done_.get() )
                 {
-                    long overall_timeout = System.currentTimeMillis() - startTime_ + timeout;
+long overall_timeout = timeout - (System.currentTimeMillis() - startTime_);
                     if(overall_timeout > 0)
                         bVal = condition_.await(overall_timeout, TimeUnit.MILLISECONDS);
                     else
---------------
-------------
@@ -25,7 +25,7 @@
 
   @Override
   public Sorter newSorter(Entry[] arr) {
-    return new ArrayTimSorter<Entry>(arr, ArrayUtil.<Entry>naturalComparator(), random().nextInt(arr.length));
+return new ArrayTimSorter<Entry>(arr, ArrayUtil.<Entry>naturalComparator(), _TestUtil.nextInt(random(), 0, arr.length));
   }
 
 }
---------------
-------------
@@ -98,7 +98,7 @@
       // delete 1-100% of docs
       iw.deleteDocuments(new Term("title", terms[random().nextInt(terms.length)]));
     }
-    iw.close();
+iw.shutdown();
     dir.close(); // checkindex
   }
   
---------------
-------------
@@ -103,7 +103,7 @@
   // LUCENE-3849: make sure after .end() we see the "ending" posInc
   public void testEndStopword() throws Exception {
     CharArraySet stopSet = StopFilter.makeStopSet(TEST_VERSION_CURRENT, "of");
-    StopFilter stpf = new StopFilter(Version.LUCENE_40, new MockTokenizer(new StringReader("test of"), MockTokenizer.WHITESPACE, false), stopSet);
+StopFilter stpf = new StopFilter(TEST_VERSION_CURRENT, new MockTokenizer(new StringReader("test of"), MockTokenizer.WHITESPACE, false), stopSet);
     assertTokenStreamContents(stpf, new String[] { "test" },
                               new int[] {0},
                               new int[] {4},
---------------
-------------
@@ -290,7 +290,7 @@
 		{
 			mb.push(trimType);
 			leftOperand.generateExpression(acb, mb);
-			mb.upCast(leftInterfaceType);
+mb.cast(leftInterfaceType);
 
 			mb.getField(field);
 			nargs = 3;
---------------
-------------
@@ -81,7 +81,7 @@
 
     doc.add(idField);
     for(int i=0;i<NUM_DOCS;i++) {
-      idField.setValue(""+i);
+idField.setStringValue(""+i);
       w.addDocument(doc);
       if ((i+1)%10 == 0) {
         w.commit();
---------------
-------------
@@ -71,7 +71,7 @@
   @Override
   protected void setup(Context context) throws IOException, InterruptedException {
     super.setup(context);
-    Parameters params = Parameters.fromString(context.getConfiguration().get("job.parameters", ""));
+Parameters params = new Parameters(context.getConfiguration().get("job.parameters", ""));
     splitter = Pattern.compile(params.get("splitPattern", "[ \t]*\t[ \t]*"));
     
     int selectedFieldCount = Integer.valueOf(params.get("selectedFieldCount", "0"));
---------------
-------------
@@ -86,7 +86,7 @@
         try
         {
             if (ByteBufferUtil.string(name).equalsIgnoreCase(ByteBufferUtil.string(cf.keyAlias)))
-                return AsciiTerm.instance;
+return JdbcAscii.instance;
         }
         catch (CharacterCodingException e)
         {
---------------
-------------
@@ -115,7 +115,7 @@
 
     Directory fsDir;
     
-    public FaultyFSDirectory(File dir) throws IOException {
+public FaultyFSDirectory(File dir) {
       fsDir = newFSDirectory(dir);
       lockFactory = fsDir.getLockFactory();
     }
---------------
-------------
@@ -50,7 +50,7 @@
     protected Token process(Token t) throws IOException {
       if ("A".equals(new String(t.termBuffer(), 0, t.termLength())) && 
           "B".equals(new String(peek(1).termBuffer(), 0, peek(1).termLength())))
-        write(t);
+write((Token)t.clone());
       return t;
     }
   }
---------------
-------------
@@ -343,7 +343,7 @@
 
         current = newPosition;
 
-        if (newPosition >= bufferOffset + validBufferBytes || newPosition < bufferOffset)
+if (newPosition > (bufferOffset + validBufferBytes) || newPosition < bufferOffset)
             reBuffer(); // this will set bufferEnd for us
     }
 
---------------
-------------
@@ -47,7 +47,7 @@
     public void doVerb(Message message)
     {
         logger_.debug("Received a BootstrapMetadataMessage from " + message.getFrom());
-        byte[] body = (byte[])message.getMessageBody()[0];
+byte[] body = message.getMessageBody();
         DataInputBuffer bufIn = new DataInputBuffer();
         bufIn.reset(body, body.length);
         try
---------------
-------------
@@ -263,7 +263,7 @@
 
     SolrQueryRequest req = null;
     String[] snippetFieldAry = null;
-    if (produceSummary == true) {
+if (produceSummary) {
       highlighter = core.getHighlighter();
       if (highlighter != null){
         Map<String, Object> args = Maps.newHashMap();
---------------
-------------
@@ -31,7 +31,7 @@
 
   @BeforeClass
   public static void beforeTest() throws Exception {
-    initCore(EXAMPLE_CONFIG, EXAMPLE_SCHEMA);
+initCore(EXAMPLE_CONFIG, EXAMPLE_SCHEMA, EXAMPLE_HOME);
   }
 
   @Override
---------------
-------------
@@ -69,7 +69,7 @@
       // setup the server...
       String url = "http://localhost:"+port+context;
       CommonsHttpSolrServer s = new CommonsHttpSolrServer( url );
-      s.setConnectionTimeout(5);
+s.setConnectionTimeout(100); // 1/10th sec
       s.setDefaultMaxConnectionsPerHost(100);
       s.setMaxTotalConnections(100);
       return s;
---------------
-------------
@@ -207,7 +207,7 @@
                 double p = (double) k / (double) n; 
                 if (d<p) {
                     // Replace a random value from the sample with the new value
-                    int keyToReplace = Math.abs(r.nextInt())%k;                    
+int keyToReplace = r.nextInt(k);
                     sampledKeys.set(keyToReplace, key);
                 }
             }
---------------
-------------
@@ -145,7 +145,7 @@
                     Message hintedMessage = rm.makeRowMutationMessage();
                     hintedMessage.addHeader(RowMutation.HINT, target.getAddress());
                     if (logger.isDebugEnabled())
-                        logger.debug("insert writing key " + rm.key() + " to " + unhintedMessage.getMessageId() + "@" + hintedTarget + " for " + target);
+logger.debug("insert writing key " + rm.key() + " to " + hintedMessage.getMessageId() + "@" + hintedTarget + " for " + target);
                     MessagingService.instance().sendOneWay(hintedMessage, hintedTarget);
                 }
             }
---------------
-------------
@@ -412,7 +412,7 @@
 			String[] newList_a;
 			try {newList_a = IdUtil.parseIdList(value_s);}
 			catch (StandardException se) {
-				throw StandardException.newException(SQLState.AUTH_INVALID_AUTHORIZATION_PROPERTY, key,value_s,se);
+throw StandardException.newException(SQLState.AUTH_INVALID_AUTHORIZATION_PROPERTY, se, key,value_s);
 			}
 
 			/** Check the new list userIdList for duplicates. */
---------------
-------------
@@ -413,7 +413,7 @@
         } finally {
             conn_.pendingEndXACallinfoOffset_ = -1; // indicate no pending callinfo
         }
-        if (rc != XAResource.XA_OK) {
+if ((rc != XAResource.XA_OK ) && (rc != XAResource.XA_RDONLY)) {
             throwXAException(rc, false);
         }
         if (conn_.agent_.loggingEnabled()) {
---------------
-------------
@@ -132,7 +132,7 @@
         assert !columnFamilies_.isEmpty();
         logger_.info("Sorting " + this);
         List<DecoratedKey> keys = new ArrayList<DecoratedKey>(columnFamilies_.keySet());
-        Collections.sort(keys, partitioner_.getDecoratedKeyComparator());
+Collections.sort(keys);
         return keys;
     }
 
---------------
-------------
@@ -131,7 +131,7 @@
       }
 
       for (int d = 0; d < termCounts.length; d++) {
-        if (! reader.isDeleted(d)) {
+if (!delDocs.get(d)) {
           byte norm = Similarity.encodeNorm(sim.lengthNorm(fieldName, termCounts[d]));
           reader.setNorm(d, fieldName, norm);
         }
---------------
-------------
@@ -33,7 +33,7 @@
 
   @Override
   public String toString() {
-    return "<matchAllDocs field='*' term='*'>";
+return "<matchAllDocs field='*' term='*'/>";
   }
 
   public CharSequence toQueryString(EscapeQuerySyntax escapeSyntaxParser) {
---------------
-------------
@@ -121,7 +121,7 @@
       String configDir = core.getResourceLoader().getConfigDir();
       FileUtils.moveFile(new File(configDir, "stopwords.txt"), new File(configDir, "stopwords.txt.bak"));
       File file = new File(configDir, "stopwords.txt");
-      FileUtils.writeStringToFile(file, stopwords);
+FileUtils.writeStringToFile(file, stopwords, "UTF-8");
      
     } finally {
       core.close();
---------------
-------------
@@ -81,7 +81,7 @@
       len = buf.length;
     }
 
-    Field f = new org.apache.lucene.document.BinaryField(field.getName(), buf, offset, len);
+Field f = new org.apache.lucene.document.StoredField(field.getName(), buf, offset, len);
     f.setBoost(boost);
     return f;
   }
---------------
-------------
@@ -559,7 +559,7 @@
     public void addSSTable(SSTableReader sstable)
     {
         ssTables_.add(sstable);
-        CompactionManager.instance.submitMinor(this);
+CompactionManager.instance.submitMinorIfNeeded(this);
     }
 
     /*
---------------
-------------
@@ -37,7 +37,7 @@
   @Test
   public void testUnitVectorizerMapper() throws Exception {
     UnitVectorizerMapper mapper = new UnitVectorizerMapper();
-    Configuration conf = new Configuration();
+Configuration conf = getConfiguration();
     
     // set up the dummy writers
     DummyRecordWriter<IntWritable, VectorWritable> writer = new
---------------
-------------
@@ -57,7 +57,7 @@
  *
  *  NOTE: this codec sorts terms by reverse-unicode-order! */
 
-public class RAMOnlyPostingsFormat extends PostingsFormat {
+public final class RAMOnlyPostingsFormat extends PostingsFormat {
 
   // For fun, test that we can override how terms are
   // sorted, and basic things still work -- this comparator
---------------
-------------
@@ -61,7 +61,7 @@
       //  riw.deleteDocuments(new Term("id", Integer.toString(i)));
       // }
     }
-    riw.close();
+riw.shutdown();
     checkHeaders(dir);
     dir.close();
   }
---------------
-------------
@@ -2882,7 +2882,7 @@
 
   /** Returns true if there are changes that have not been committed */
   public final boolean hasUncommittedChanges() {
-    return changeCount != lastCommitChangeCount;
+return changeCount != lastCommitChangeCount || docWriter.anyChanges() || bufferedDeletesStream.any();
   }
 
   private final void commitInternal() throws IOException {
---------------
-------------
@@ -77,7 +77,7 @@
     		        if ( names_.contains(subColumn.name()) )
     		        {
     		            names_.remove(subColumn.name());
-    		            filteredSuperColumn.addColumn(subColumn.name(), subColumn);
+filteredSuperColumn.addColumn(subColumn);
     		        }
     				if( isDone() )
     				{
---------------
-------------
@@ -89,7 +89,7 @@
     conf.set("io.serializations",
              "org.apache.hadoop.io.serializer.JavaSerialization,org.apache.hadoop.io.serializer.WritableSerialization");
     // Dont ever forget this. People should keep track of how hadoop conf parameters and make or break a piece of code
-    FileSystem dfs = FileSystem.get(conf);
+FileSystem dfs = FileSystem.get(outPath.toUri(), conf);
     if (dfs.exists(outPath))
       dfs.delete(outPath, true);
 
---------------
-------------
@@ -179,7 +179,7 @@
   /** Returns {@link NumericDocValues} representing norms
    *  for this field, or null if no {@link NumericDocValues}
    *  were indexed. */
-  public abstract NumericDocValues simpleNormValues(String field) throws IOException;
+public abstract NumericDocValues getNormValues(String field) throws IOException;
 
   /**
    * Get the {@link FieldInfos} describing all fields in
---------------
-------------
@@ -149,7 +149,7 @@
       throw new IOException("tempDir undefined, cannot run test");
 
     String dirName = tempDir + "/luceneTestThreadedOptimize";
-    directory = FSDirectory.getDirectory(dirName);
+directory = FSDirectory.getDirectory(dirName, null, false);
     runTest(directory, false, null);
     runTest(directory, true, null);
     runTest(directory, false, new ConcurrentMergeScheduler());
---------------
-------------
@@ -419,7 +419,7 @@
    *@return The <code>String</code> that indexes the node argument.
    */
   protected String getKey(TSTNode node) {
-    StringBuffer getKeyBuffer = new StringBuffer();
+StringBuilder getKeyBuffer = new StringBuilder();
     getKeyBuffer.setLength(0);
     getKeyBuffer.append("" + node.splitchar);
     TSTNode currentNode;
---------------
-------------
@@ -68,7 +68,7 @@
      */
     private synchronized void loadEndPoints(TokenMetadata metadata) throws IOException
     {
-        endPointSnitch = (DatacenterEndPointSnitch) StorageService.instance().getEndPointSnitch();
+endPointSnitch = (DatacenterEndPointSnitch) StorageService.instance.getEndPointSnitch();
         this.tokens = new ArrayList<Token>(tokens);
         String localDC = endPointSnitch.getLocation(InetAddress.getLocalHost());
         dcMap = new HashMap<String, List<Token>>();
---------------
-------------
@@ -90,7 +90,7 @@
       tvd.close();
       tvx = null;
       assert state.docStoreSegmentName != null;
-      if (4+state.numDocsInStore*16 != state.directory.fileLength(state.docStoreSegmentName + "." + IndexFileNames.VECTORS_INDEX_EXTENSION))
+if (4+((long) state.numDocsInStore)*16 != state.directory.fileLength(state.docStoreSegmentName + "." + IndexFileNames.VECTORS_INDEX_EXTENSION))
         throw new RuntimeException("after flush: tvx size mismatch: " + state.numDocsInStore + " docs vs " + state.directory.fileLength(state.docStoreSegmentName + "." + IndexFileNames.VECTORS_INDEX_EXTENSION) + " length in bytes of " + state.docStoreSegmentName + "." + IndexFileNames.VECTORS_INDEX_EXTENSION);
 
       state.flushedFiles.add(state.docStoreSegmentName + "." + IndexFileNames.VECTORS_INDEX_EXTENSION);
---------------
-------------
@@ -53,7 +53,7 @@
 
   private class CellQueue extends PriorityQueue<SpansCell> {
     public CellQueue(int size) {
-      initialize(size);
+super(size);
     }
     
     @Override
---------------
-------------
@@ -120,7 +120,7 @@
 
     start = new Date();
 
-    Directory store = new FSDirectory("test.store", true);
+Directory store = FSDirectory.getDirectory("test.store", true);
     FieldInfos fis = new FieldInfos();
 
     TermInfosWriter writer = new TermInfosWriter(store, "words", fis);
---------------
-------------
@@ -23,7 +23,7 @@
 
 /**
 
- An interface for accessing Cloudscape UUIDs, unique identifiers.
+An interface for accessing Derby UUIDs, unique identifiers.
 		
 	<p>The values in the
 	system catalog held in ID columns with a type of CHAR(36) are the
---------------
-------------
@@ -68,7 +68,7 @@
    * Lots of randomly generated data is being indexed, and later on a "90% docs" faceted search
    * is performed. The results are compared to non-sampled ones.
    */
-  public void testCountUsingSamping() throws Exception {
+public void testCountUsingSampling() throws Exception {
     boolean useRandomSampler = random().nextBoolean();
     for (int partitionSize : partitionSizes) {
       try {
---------------
-------------
@@ -76,7 +76,7 @@
   }
   
   @AfterClass
-  public static void afterClass() throws Exception {
+public static void afterClass() {
     queryConverter = null;
   }
 
---------------
-------------
@@ -61,7 +61,7 @@
  *  </requestHandler>
  * 
  */
-public class StaxUpdateRequestHandler extends RequestHandlerBase
+public class StaxUpdateRequestHandler extends XmlUpdateRequestHandler
 {
   public static Logger log = Logger.getLogger(StaxUpdateRequestHandler.class.getName());
 
---------------
-------------
@@ -24,7 +24,7 @@
 /**
 
 The Diagnostic framework is meant to provide a way to include as much
-diagnostic capability within the distributed release of the cloudscape
+diagnostic capability within the distributed release of the Derby
 product without adversely affecting the runtime speed or foot print of
 a running configuration that needs not use this information.
 
---------------
-------------
@@ -28,7 +28,7 @@
 import org.apache.cassandra.db.filter.QueryPath;
 import org.apache.cassandra.dht.IPartitioner;
 import org.apache.cassandra.io.util.DataOutputBuffer;
-import org.apache.cassandra.io.SSTableWriter;
+import org.apache.cassandra.io.sstable.SSTableWriter;
 import static org.apache.cassandra.utils.FBUtilities.hexToBytes;
 import org.apache.commons.cli.*;
 import org.json.simple.JSONArray;
---------------
-------------
@@ -411,7 +411,7 @@
 			 * the check constraints and generation clauses.
 			 */
 			if  (numGenerationClauses > 0)
-            { tableElementList.bindAndValidateGenerationClauses( schemaDescriptor, fromList, generatedColumns ); }
+{ tableElementList.bindAndValidateGenerationClauses( schemaDescriptor, fromList, generatedColumns, baseTable ); }
 			if  (numCheckConstraints > 0) { tableElementList.bindAndValidateCheckConstraints(fromList); }
             if ( numReferenceConstraints > 0) { tableElementList.validateForeignKeysOnGenerationClauses( fromList, generatedColumns ); }
 		}
---------------
-------------
@@ -180,7 +180,7 @@
         SystemTable.setBootstrapped(true);
         setToken(getLocalToken());
         Gossiper.instance().addApplicationState(StorageService.STATE_NORMAL, new ApplicationState(partitioner_.getTokenFactory().toString(getLocalToken())));
-        logger_.info("Bootstrap completed! Now serving reads.");
+logger_.info("Bootstrap/move completed! Now serving reads.");
     }
 
     /** This method updates the local token on disk  */
---------------
-------------
@@ -54,7 +54,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new ItalianAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new ItalianAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
   
   /** test that the elisionfilter is working */
---------------
-------------
@@ -79,7 +79,7 @@
   }
 
   public long ramBytesUsed() {
-    return RamUsageEstimator.NUM_BYTES_ARRAY_HEADER + values.length;
+return RamUsageEstimator.sizeOf(values);
   }
 
   public void clear() {
---------------
-------------
@@ -91,7 +91,7 @@
      *  (DATABASE=<dbname>), (DRDAID = <drdaid>)"
      */
     private static final String CONNSTRING_FORMAT = 
-        "\\S+@[0-9]+.* \\(XID = .*\\), \\(SESSIONID = [0-9]+\\), " +
+"\\S+@\\-?[0-9]+.* \\(XID = .*\\), \\(SESSIONID = [0-9]+\\), " +
         "\\(DATABASE = [A-Za-z]+\\), \\(DRDAID = .*\\) "; 
     
     
---------------
-------------
@@ -284,7 +284,7 @@
       TermsEnum te = terms.iterator(null);
       DocsEnum de = null;
       while (te.next() != null) {
-        de = _TestUtil.docs(random(), te, liveDocs, de, false);
+de = _TestUtil.docs(random(), te, liveDocs, de, 0);
         int cnt = 0;
         while (de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
           cnt++;
---------------
-------------
@@ -198,7 +198,7 @@
       DirectoryReader reader = DirectoryReader.open(dir);
       IndexSearcher searcher = new IndexSearcher(reader);
       // System.out.println("segments="+searcher.getIndexReader().getSequentialSubReaders().length);
-      assertTrue(reader.getSequentialSubReaders().size() > 1);
+assertTrue(reader.leaves().size() > 1);
 
       for (int i=0; i<qiter; i++) {
         Filter filt = new Filter() {
---------------
-------------
@@ -304,7 +304,7 @@
             req.setAttribute("org.apache.solr.SolrCore", core);
               // Modify the request so each core gets its own /admin
             if( singlecore == null && path.startsWith( "/admin" ) ) {
-              req.getRequestDispatcher( path ).forward( request, response );
+req.getRequestDispatcher( pathPrefix == null ? path : pathPrefix + path ).forward( request, response );
               return; 
             }
           }
---------------
-------------
@@ -53,7 +53,7 @@
  
  <p>IndexReader instances for indexes on disk are usually constructed
  with a call to one of the static <code>DirectoryReader.open()</code> methods,
- e.g. {@link DirectoryReader#open(Directory)}. {@link DirectoryReader} implements
+e.g. {@link DirectoryReader#open(org.apache.lucene.store.Directory)}. {@link DirectoryReader} implements
  the {@link CompositeReader} interface, it is not possible to directly get postings.
 
  <p> For efficiency, in this API documents are often referred to via
---------------
-------------
@@ -26,7 +26,7 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.*;
+import static org.apache.lucene.analysis.VocabularyAssert.*;
 
 /**
  * Simple tests for {@link FrenchLightStemFilter}
---------------
-------------
@@ -417,7 +417,7 @@
   static ScoreDocComparator comparatorAuto (final IndexReader reader, final String fieldname)
   throws IOException {
     final String field = fieldname.intern();
-    Object lookupArray = FieldCache.DEFAULT.getAuto (reader, field);
+Object lookupArray = ExtendedFieldCache.EXT_DEFAULT.getAuto (reader, field);
     if (lookupArray instanceof FieldCache.StringIndex) {
       return comparatorString (reader, field);
     } else if (lookupArray instanceof int[]) {
---------------
-------------
@@ -360,7 +360,7 @@
               "finishDocument".equals(trace[i].getMethodName())) {
             sawAbortOrFlushDoc = true;
           }
-          if ("merge".equals(trace[i])) {
+if ("merge".equals(trace[i].getMethodName())) {
             sawMerge = true;
           }
           if ("close".equals(trace[i].getMethodName())) {
---------------
-------------
@@ -176,7 +176,7 @@
             List<Pair<Long,Long>> sections = sstable.getPositionsForRanges(ranges);
             if (sections.isEmpty())
                 continue;
-            pending.add(new PendingFile(sstable, desc, SSTable.COMPONENT_DATA, sections, type));
+pending.add(new PendingFile(sstable, desc, SSTable.COMPONENT_DATA, sections, type, sstable.estimatedKeys()));
         }
         logger.info("Stream context metadata {}, {} sstables.", pending, sstables.size());
         return pending;
---------------
-------------
@@ -111,7 +111,7 @@
         }
         output.writeVInt(suffix);
         output.writeBytes(term.bytes.bytes, term.bytes.offset + prefix, suffix);
-        lastTerm.bytes.copy(term.bytes);
+lastTerm.bytes.copyBytes(term.bytes);
         lastTerm.field = term.field;
       } catch (IOException e) {
         throw new RuntimeException(e);
---------------
-------------
@@ -69,7 +69,7 @@
 
         this.cfs = cfs;
         creationTime = System.currentTimeMillis();
-        THRESHOLD = cfs.getMemtableThroughputInMB() * 1024 * 1024;
+THRESHOLD = cfs.getMemtableThroughputInMB() * 1024L * 1024L;
         THRESHOLD_COUNT = (long) (cfs.getMemtableOperationsInMillions() * 1024 * 1024);
     }
 
---------------
-------------
@@ -353,7 +353,7 @@
   /** Used for testing */
   private void addMyself() {
     synchronized(allInstances) {
-      final int size=0;
+final int size = allInstances.size();
       int upto = 0;
       for(int i=0;i<size;i++) {
         final ConcurrentMergeScheduler other = (ConcurrentMergeScheduler) allInstances.get(i);
---------------
-------------
@@ -65,7 +65,7 @@
      */
     public Pair<AbstractBounds,AbstractBounds> split(Token token)
     {
-        assert contains(token);
+assert left.equals(token) || contains(token);
         AbstractBounds lb = createFrom(token);
         // we contain this token, so only one of the left or right can be empty
         AbstractBounds rb = lb != null && token.equals(right) ? null : new Range(token, right);
---------------
-------------
@@ -447,7 +447,7 @@
       if (last != null) {
         assertTrue(last.compareTo(cur) < 0);
       }
-      last = new BytesRef(cur);
+last = BytesRef.deepCopyOf(cur);
     } 
     // LUCENE-3314: the results after next() already returned null are undefined,
     // assertNull(termEnum.next());
---------------
-------------
@@ -143,7 +143,7 @@
     filtOptThreshold = getFloat("query/boolTofilterOptimizer/@threshold",.05f);
     
     useFilterForSortedQuery = getBool("query/useFilterForSortedQuery", false);
-    queryResultWindowSize = getInt("query/queryResultWindowSize", 1);
+queryResultWindowSize = Math.max(1, getInt("query/queryResultWindowSize", 1));
     queryResultMaxDocsCached = getInt("query/queryResultMaxDocsCached", Integer.MAX_VALUE);
     enableLazyFieldLoading = getBool("query/enableLazyFieldLoading", false);
 
---------------
-------------
@@ -52,7 +52,7 @@
     Path outpath = new Path(fs.getWorkingDirectory(), "output");
 
     if (fs.exists(outpath)) {
-      FileUtil.fullyDelete(fs, outpath);
+fs.delete(outpath, true);
     }
 
     return outpath;
---------------
-------------
@@ -96,6 +96,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new ArabicAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new ArabicAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -86,7 +86,7 @@
     // NOTE: this does only test the chunked reads and NOT if the Bug is triggered.
     //final int tmpFileSize = 1024 * 1024 * 5;
     final int inputBufferSize = 128;
-    File tmpInputFile = File.createTempFile("IndexInput", "tmpFile");
+File tmpInputFile = _TestUtil.createTempFile("IndexInput", "tmpFile", TEMP_DIR);
     tmpInputFile.deleteOnExit();
     writeBytes(tmpInputFile, TEST_FILE_LENGTH);
 
---------------
-------------
@@ -92,7 +92,7 @@
          writer.addDocument(d);
             
       }
-      writer.close();
+writer.shutdown();
       return directory;
    }
 }
---------------
-------------
@@ -157,7 +157,7 @@
    * modify it.
    */
 
-  public Set<String> files() throws IOException {
+public Set<String> files() {
     if (setFiles == null) {
       throw new IllegalStateException("files were not computed yet");
     }
---------------
-------------
@@ -662,7 +662,7 @@
         td = dd.getTableDescriptor(tableId);
 
         if (updateStatisticsAll) {
-            cds = td.getConglomerateDescriptors();
+cds = null;
         } else {
             cds = new ConglomerateDescriptor[1];
             cds[0] = dd.getConglomerateDescriptor(
---------------
-------------
@@ -222,7 +222,7 @@
                   try {
                     provider = new ContextProvider(context, ref, factory.getInitialContext(environment));
                   } finally {
-                    context.ungetService(ref); // we didn't get something back, so this was no good.
+if (provider == null) context.ungetService(ref); // we didn't get something back, so this was no good.
                   }
                   break;
                 } else {
---------------
-------------
@@ -100,7 +100,7 @@
           endOffset = off;
           cp = readCodePoint();
         } while (cp >= 0 && isTokenChar(cp));
-        offsetAtt.setOffset(startOffset, endOffset);
+offsetAtt.setOffset(correctOffset(startOffset), correctOffset(endOffset));
         streamState = State.INCREMENT;
         return true;
       }
---------------
-------------
@@ -324,7 +324,7 @@
   private boolean setPrevious(CharsRef current) {
     // don't need to copy, once we fix https://issues.apache.org/jira/browse/LUCENE-3277
     // still, called only from assert
-    previous = new CharsRef(current);
+previous = CharsRef.deepCopyOf(current);
     return true;
   }
   
---------------
-------------
@@ -97,7 +97,7 @@
     assertEquals(1, termPositions.freq());
     assertEquals(2, termPositions.nextPosition());
     reader.close();
-    writer.close();
+writer.shutdown();
     // 3) reset stream and consume tokens again
     stream.reset();
     checkTokens(stream);
---------------
-------------
@@ -224,7 +224,7 @@
             else
             {
                 Column column = cosc.column;
-                css_.out.printf("=> (column=%s, value=%s; timestamp=%d)\n", new String(column.name, "UTF-8"),
+css_.out.printf("=> (column=%s, value=%s, timestamp=%d)\n", new String(column.name, "UTF-8"),
                                 new String(column.value, "UTF-8"), column.timestamp);
             }
         }
---------------
-------------
@@ -20,7 +20,7 @@
 import java.io.IOException;
 
 import org.apache.lucene.index.values.DocValues;
-import org.apache.lucene.index.values.ValuesEnum;
+import org.apache.lucene.index.values.DocValuesEnum;
 import org.apache.lucene.util.AttributeSource;
 
 /** Enumerates indexed fields.  You must first call {@link
---------------
-------------
@@ -543,7 +543,7 @@
         for (String id : elevations.ids) {
           term.copyChars(id);
           if (seen.contains(id) == false  && termsEnum.seekExact(term, false)) {
-            docsEnum = termsEnum.docs(liveDocs, docsEnum, false);
+docsEnum = termsEnum.docs(liveDocs, docsEnum, 0);
             if (docsEnum != null) {
               int docId = docsEnum.nextDoc();
               if (docId == DocIdSetIterator.NO_MORE_DOCS ) continue;  // must have been deleted
---------------
-------------
@@ -43,7 +43,7 @@
   @Test
   public void testVectorMatrixMultiplicationMapper() throws Exception {
     VectorMatrixMultiplicationMapper mapper = new VectorMatrixMultiplicationMapper();
-    Configuration conf = new Configuration();
+Configuration conf = getConfiguration();
     
     // set up all the parameters for the job
     Vector toSave = new DenseVector(VECTOR);
---------------
-------------
@@ -238,7 +238,7 @@
         Class<?> cls = wc.getDefinedClass();
         Method method = cls.getMethod("test", new Class [] {String.class});
         Object result = method.invoke(cls.newInstance(), "hello");
-        Assert.assertEquals("Only the services from bundle impl2 should be selected", "ollehHELLO5", result);        
+Assert.assertEquals("All providers should be selected for this one", "ollehHELLO5", result);
 
         // Weave the AltTestClient class.
         URL cls2Url = getClass().getResource("AltTestClient.class");
---------------
-------------
@@ -47,6 +47,6 @@
 
   @Override
   public NGramTokenFilter create(TokenStream input) {
-    return new NGramTokenFilter(input, minGramSize, maxGramSize);
+return new NGramTokenFilter(luceneMatchVersion, input, minGramSize, maxGramSize);
   }
 }
---------------
-------------
@@ -53,7 +53,7 @@
     Field foo = newField("foo", "", TextField.TYPE_UNSTORED);
     doc.add(foo);
     for (int i = 0; i < 100; i++) {
-      foo.setValue(addValue());
+foo.setStringValue(addValue());
       writer.addDocument(doc);
     }
     reader = writer.getReader();
---------------
-------------
@@ -179,7 +179,7 @@
       double idealizedGain = 0.0;
       for (int i = 0; i < recommendedItems.size(); i++) {
         RecommendedItem item = recommendedItems.get(i);
-        double discount = i == 0 ? 1.0 : 1.0 / log2(i + 1);
+double discount = 1.0 / log2(i + 2.0); // Classical formulation says log(i+1), but i is 0-based here
         if (relevantItemIDs.contains(item.getItemID())) {
           cumulativeGain += discount;
         }
---------------
-------------
@@ -52,7 +52,7 @@
       writer.addDocument(doc);
     }
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
   }
   
   public void test() throws Exception {
---------------
-------------
@@ -68,7 +68,7 @@
     // delete the output directory
     JobConf conf = new JobConf(KMeansJob.class);
     Path outPath = new Path(output);
-    FileSystem fs = FileSystem.get(conf);
+FileSystem fs = FileSystem.get(outPath.toUri(), conf);
     if (fs.exists(outPath)) {
       fs.delete(outPath, true);
     }
---------------
-------------
@@ -352,7 +352,7 @@
     // Save off prefs for the life of this loop iteration
     PreferenceArray userPreferences = dataModel.getPreferencesFromUser(userID);
     int length = userPreferences.length();
-    for (int i = 0; i < length - 1; i++) {
+for (int i = 0; i < length; i++) { // Loop to length-1, not length-2, not for diffs but average item pref
       float prefAValue = userPreferences.getValue(i);
       long itemIDA = userPreferences.getItemID(i);
       FastByIDMap<RunningAverage> aMap = averageDiffs.get(itemIDA);
---------------
-------------
@@ -215,7 +215,7 @@
     try {
       SolrCore core = SolrCore.getSolrCore();
       InputStream input = core.getResourceLoader().openResource(path);
-      return IOUtils.toString( input );
+return IOUtils.toString( input, "UTF-8" );
     }
     catch( Exception ex ) {} // ignore it
     return "";
---------------
-------------
@@ -296,6 +296,6 @@
 
     public boolean isExpired()
     {
-        return System.currentTimeMillis() > creationTime + cfs.getMemtableFlushAfterMins() * 60 * 1000;
+return System.currentTimeMillis() > creationTime + cfs.getMemtableFlushAfterMins() * 60 * 1000L;
     }
 }
---------------
-------------
@@ -168,7 +168,7 @@
   private void addGroupField(Document doc, String groupField, String value, boolean canUseIDV) {
     doc.add(new Field(groupField, value, TextField.TYPE_STORED));
     if (canUseIDV) {
-      doc.add(new DocValuesField(groupField, new BytesRef(value), DocValues.Type.BYTES_VAR_SORTED));
+doc.add(new SortedBytesDocValuesField(groupField, new BytesRef(value)));
     }
   }
 
---------------
-------------
@@ -193,7 +193,7 @@
             c2.close();
             fail();
         } catch (SQLException e) {
-            assertSQLState("8006", e);
+assertSQLState("08006", e);
         }
     }
 }
---------------
-------------
@@ -62,7 +62,7 @@
        super.setUp();
        File file = new File(System.getProperty("tempDir"), "testIndex");
        _TestUtil.rmDir(file);
-       dir = FSDirectory.getDirectory(file);
+dir = FSDirectory.getDirectory(file, null, false);
     }
 
 
---------------
-------------
@@ -48,7 +48,7 @@
 	{
 		List<Message> responses_ = new ArrayList<Message>();
 		
-		public void response(Message msg)
+public synchronized void response(Message msg)
 		{
 			responses_.add(msg);
 			if ( responses_.size() == ConsistencyManager.this.replicas_.size() )
---------------
-------------
@@ -72,7 +72,7 @@
       FixedBitSet visited = new FixedBitSet(ir.maxDoc());
       TermsEnum te = terms.iterator(null);
       while (te.next() != null) {
-        DocsEnum de = _TestUtil.docs(random(), te, null, null, false);
+DocsEnum de = _TestUtil.docs(random(), te, null, null, 0);
         while (de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
           visited.set(de.docID());
         }
---------------
-------------
@@ -69,7 +69,7 @@
 
   public LeaderElector(SolrZkClient zkClient) {
     this.zkClient = zkClient;
-    zkCmdExecutor = new ZkCmdExecutor((int) (zkClient.getZkClientTimeout()/1000.0 + 3000));
+zkCmdExecutor = new ZkCmdExecutor(zkClient.getZkClientTimeout());
   }
   
   // for tests
---------------
-------------
@@ -235,7 +235,7 @@
 
     for (int i = offset; i < offset + num; i++) {
       Document doc = new Document();
-      doc.add(new Field("id", i + "", TextField.TYPE_STORED));
+doc.add(new TextField("id", i + "", Field.Store.YES));
       sourceTypes[i] = valueType;
       switch (valueType) {
       case VAR_INTS:
---------------
-------------
@@ -413,7 +413,7 @@
     attr.setFuzzyMinSimilarity(fuzzyMinSim);
   }
   
-  public void setFieldsBoost(Map<CharSequence, Float> boosts) {
+public void setFieldsBoost(Map<String, Float> boosts) {
     FieldBoostMapAttribute attr = getQueryConfigHandler().addAttribute(FieldBoostMapAttribute.class);
     attr.setFieldBoostMap(boosts);
   }
---------------
-------------
@@ -162,7 +162,7 @@
 
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new BrazilianAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new BrazilianAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -45,7 +45,7 @@
 {
     private static boolean debug_system_procedures_created = false;
 
-    abstract void testList(Connection conn) throws SQLException;
+abstract public void testList(Connection conn) throws SQLException;
 
     void runTests(String[] argv)
         throws Throwable
---------------
-------------
@@ -106,7 +106,7 @@
       do {
         // System.out.println("  iter termCount=" + termCount + " term=" +
         // enumerator.term().toBytesString());
-        docsEnum = termsEnum.docs(acceptDocs, docsEnum, false);
+docsEnum = termsEnum.docs(acceptDocs, docsEnum, 0);
         int docid;
         while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
           bitSet.set(docid);
---------------
-------------
@@ -150,7 +150,7 @@
         assertEquals("Expected " + describeCompare(u1, u2, expC) + ", got "
                 + describeCompare(u1, u2, c), expC, c);
 
-        if (u1.version() == 1)
+if (u1.version() == 1 && u2.version() == 1)
             assertEquals(c, sign(TimeUUIDType.instance.compare(bytebuffer(u1), bytebuffer(u2))));
 
         logJdkUUIDCompareToVariance(u1, u2, c);
---------------
-------------
@@ -37,7 +37,7 @@
 
   public static final String FIELD = "field";
   public static final QueryParser qp =
-    new QueryParser(Version.LUCENE_CURRENT, FIELD, new WhitespaceAnalyzer());
+new QueryParser(TEST_VERSION_CURRENT, FIELD, new WhitespaceAnalyzer());
 
   @Override
   public void tearDown() throws Exception {
---------------
-------------
@@ -33,7 +33,7 @@
 /**
  * This class represents a REVOKE statement.
  */
-public class RevokeNode extends MiscellaneousStatementNode
+public class RevokeNode extends DDLStatementNode
 {
     private PrivilegeNode privileges;
     private List grantees;
---------------
-------------
@@ -124,7 +124,7 @@
                 rm.add(cf);
         }
         Message message = rm.makeRowMutationMessage();
-        WriteResponseHandler responseHandler = new WriteResponseHandler(1);
+WriteResponseHandler responseHandler = new WriteResponseHandler(1, tableName);
         MessagingService.instance.sendRR(message, new InetAddress[] { endPoint }, responseHandler);
 
         try
---------------
-------------
@@ -146,7 +146,7 @@
     private SSTableReader writeSortedContents() throws IOException
     {
         logger.info("Writing " + this);
-        SSTableWriter writer = new SSTableWriter(cfs.getFlushPath(), columnFamilies.size(), partitioner);
+SSTableWriter writer = new SSTableWriter(cfs.getFlushPath(), columnFamilies.size(), cfs.metadata, partitioner);
 
         for (Map.Entry<DecoratedKey, ColumnFamily> entry : columnFamilies.entrySet())
             writer.append(entry.getKey(), entry.getValue());
---------------
-------------
@@ -46,7 +46,7 @@
     {
         try
         {
-            RowMutation rm = RowMutation.fromBytes(message.getMessageBody());
+RowMutation rm = RowMutation.fromBytes(message.getMessageBody(), message.getVersion());
             if (logger_.isDebugEnabled())
               logger_.debug("Applying " + rm);
 
---------------
-------------
@@ -306,7 +306,7 @@
         s = new String(chars, 0, 2);
       }
       allTerms.add(s);
-      f.setValue(s);
+f.setStringValue(s);
 
       writer.addDocument(d);
 
---------------
-------------
@@ -84,7 +84,7 @@
   public ICUTokenizerFactory(Map<String,String> args) {
     super(args);
     tailored = new HashMap<Integer,String>();
-    String rulefilesArg = args.remove(RULEFILES);
+String rulefilesArg = get(args, RULEFILES);
     if (rulefilesArg != null) {
       List<String> scriptAndResourcePaths = splitFileNames(rulefilesArg);
       for (String scriptAndResourcePath : scriptAndResourcePaths) {
---------------
-------------
@@ -246,7 +246,7 @@
 		// current plans using "this" node as the key.  If needed, we'll
 		// then make the call to revert the plans in OptimizerImpl's
 		// getNextDecoratedPermutation() method.
-		addOrLoadBestPlanMapping(true, this);
+updateBestPlanMap(ADD_PLAN, this);
 
 		leftResultSet = optimizeSource(
 							optimizer,
---------------
-------------
@@ -72,7 +72,7 @@
 		serviceASAP, although no harm is done if this still maintains that this
 		should be serviced ASAP ...
 
-	    @exception StandardException  Standard cloudscape exception policy
+@exception StandardException  Standard Derby exception policy
 
 		<P>MT - depends on the work.  Be wary of multiple DaemonService thread
 		calling at the same time if you subscribe or enqueue multiple times.
---------------
-------------
@@ -100,7 +100,7 @@
     doc.add(new NumericDocValuesField("dv1", 5));
     doc.add(new BinaryDocValuesField("dv2", new BytesRef("hello world")));
     iwriter.addDocument(doc);
-    iwriter.close();
+iwriter.shutdown();
     
     // Now search the index:
     IndexReader ireader = DirectoryReader.open(directory); // read-only=true
---------------
-------------
@@ -132,7 +132,7 @@
     }
    @Override
    public String toString(){
-      StringBuffer sb = new StringBuffer();
+StringBuilder sb = new StringBuilder();
       sb.append("{");
       boolean first=true;
       for(Map.Entry<N, V> entry : this.entrySet()){
---------------
-------------
@@ -436,7 +436,7 @@
    */
   public String getNormFileName(int number) {
     if (hasSeparateNorms(number)) {
-      return IndexFileNames.fileNameFromGeneration(name, "s" + number, normGen.get(number));
+return IndexFileNames.fileNameFromGeneration(name, IndexFileNames.SEPARATE_NORMS_EXTENSION + number, normGen.get(number));
     } else {
       // single file for all norms
       return IndexFileNames.fileNameFromGeneration(name, IndexFileNames.NORMS_EXTENSION, WITHOUT_GEN);
---------------
-------------
@@ -1251,7 +1251,7 @@
         byte[] data = zkClient.getData(zkPath + "/" + file, null, null, true);
         dir.mkdirs(); 
         log.info("Write file " + new File(dir, file));
-        FileUtils.writeStringToFile(new File(dir, file), new String(data, "UTF-8"), "UTF-8");
+FileUtils.writeByteArrayToFile(new File(dir, file), data);
       } else {
         downloadFromZK(zkClient, zkPath + "/" + file, new File(dir, file));
       }
---------------
-------------
@@ -1688,7 +1688,7 @@
 			ResultColumn resultColumn = (ResultColumn) elementAt(index);
 
 			/* Skip over generated columns */
-			if (resultColumn.isGenerated() || resultColumn.isGeneratedForUnmatchedColumnInInsert())
+if (resultColumn.isGenerated())
 			{
 				continue;
 			}
---------------
-------------
@@ -242,7 +242,7 @@
   @Override
   public String toString() {
     StringBuilder result = new StringBuilder(200);
-    result.append("GenericDataModel[users:");
+result.append("GenericBooleanPrefDataModel[users:");
     for (int i = 0; i < Math.min(3, userIDs.length); i++) {
       if (i > 0) {
         result.append(',');
---------------
-------------
@@ -1053,7 +1053,7 @@
 
 		if (variableLength != -1 && variableLength > declaredLength)
 				throw StandardException.newException(SQLState.LANG_STRING_TRUNCATION, getTypeName(), 
-							"XX-RESOLVE-XX",
+"(Binary data value not displayed)",
 							String.valueOf(declaredLength));
 	}
 
---------------
-------------
@@ -537,7 +537,7 @@
       }
       searchers.close();
       mgr.close();
-      writer.close();
+writer.shutdown();
       dir.close();
     }
   }
---------------
-------------
@@ -124,7 +124,7 @@
         
         // Validate: symbolic names must match
         String appSymbolicName = applicationMetadata.getApplicationSymbolicName();
-        String depSymbolicName = applicationMetadata.getApplicationSymbolicName();
+String depSymbolicName = deploymentMetadata.getApplicationSymbolicName();
         if (!appSymbolicName.equals(depSymbolicName)) {
           throw new ManagementException (MessageUtil.getMessage("APPMANAGEMENT0002E", ebaFile.getName(), appSymbolicName, depSymbolicName));
         }
---------------
-------------
@@ -49,7 +49,7 @@
 
 class ConsistencyChecker implements Runnable
 {
-    private static Logger logger_ = LoggerFactory.getLogger(ConsistencyManager.class);
+private static Logger logger_ = LoggerFactory.getLogger(ConsistencyChecker.class);
     private static ExpiringMap<String, String> readRepairTable_ = new ExpiringMap<String, String>(DatabaseDescriptor.getRpcTimeout());
 
     private final String table_;
---------------
-------------
@@ -97,7 +97,7 @@
   
   @Test
   public void testCachedThread_FullImport() throws Exception {
-    int numThreads = random.nextInt(9) + 1; // between one and 10
+int numThreads = random.nextInt(8) + 2; // between 2 and 10
     String config = getCachedConfig(random.nextBoolean(), random.nextBoolean(), numThreads);
     runFullImport(config);
   }
---------------
-------------
@@ -269,7 +269,7 @@
 	  //check if the property is set to not show select count and set the static variable
         //accordingly. 
         boolean showNoCountForSelect = Boolean.valueOf(util.getSystemProperty("ij.showNoCountForSelect")).booleanValue();
-        JDBCDisplayUtil.showSelectCount = !showNoCountForSelect;
+JDBCDisplayUtil.setShowSelectCount( !showNoCountForSelect );
 
         //check if the property is set to not show initial connections and accordingly set the
         //static variable.
---------------
-------------
@@ -711,7 +711,7 @@
         td = dd.getTableDescriptor(tableId);
 
         if (updateStatisticsAll) {
-            cds = td.getConglomerateDescriptors();
+cds = null;
         } else {
             cds = new ConglomerateDescriptor[1];
             cds[0] = dd.getConglomerateDescriptor(
---------------
-------------
@@ -1294,7 +1294,7 @@
         FBUtilities.sortSampledKeys(keys, range);
 
         if (keys.size() < 3)
-            return partitioner_.getRandomToken();
+return partitioner_.midpoint(range.left, range.right);
         else
             return keys.get(keys.size() / 2).token;
     }
---------------
-------------
@@ -28,7 +28,7 @@
  *
  **/
 public class BlockJoinParentQParserPlugin extends QParserPlugin {
-  public static String NAME = "parent";
+public static final String NAME = "parent";
 
   @Override
   public QParser createParser(String qstr, SolrParams localParams, SolrParams params, SolrQueryRequest req) {
---------------
-------------
@@ -54,7 +54,7 @@
                 final UUID version = UUIDGen.getUUID(col.name());
                 if (version.timestamp() > DatabaseDescriptor.getDefsVersion().timestamp())
                 {
-                    final Migration m = Migration.deserialize(col.value());
+final Migration m = Migration.deserialize(col.value(), message.getVersion());
                     assert m.getVersion().equals(version);
                     StageManager.getStage(Stage.MIGRATION).submit(new WrappedRunnable()
                     {
---------------
-------------
@@ -33,7 +33,7 @@
 /**
  * This class represents a GRANT statement.
  */
-public class GrantNode extends MiscellaneousStatementNode
+public class GrantNode extends DDLStatementNode
 {
 	private PrivilegeNode privileges;
 	private List grantees;
---------------
-------------
@@ -125,7 +125,7 @@
                 columnSize = Integer.parseInt(cmd.getOptionValue("S"));
 
             if (cmd.hasOption("C"))
-                cardinality = Integer.parseInt(cmd.getOptionValue("t"));
+cardinality = Integer.parseInt(cmd.getOptionValue("C"));
 
             if (cmd.hasOption("d"))
                 nodes = cmd.getOptionValue("d").split(",");
---------------
-------------
@@ -241,7 +241,7 @@
              {"XSTB6","Cannot substitute a transaction table with another while one is already in use.","50000"},
              {"XXXXX","Normal database session close.","40000"}};
 
-        JDBC.assertFullResultSet(rs, expectedRows);
+JDBC.assertUnorderedResultSet(rs, expectedRows);
         conn.rollback();
         s.close();
     }
---------------
-------------
@@ -146,7 +146,7 @@
   public void testConnectionRefused() throws MalformedURLException {
     int unusedPort = findUnusedPort(); // XXX even if fwe found an unused port
                                        // it might not be unused anymore
-    HttpSolrServer server = new HttpSolrServer("http://127.0.0.1:" + unusedPort
+HttpSolrServer server = new HttpSolrServer("http" + (isSSLMode() ? "s" : "") + "://127.0.0.1:" + unusedPort
         + "/solr");
     server.setConnectionTimeout(500);
     SolrQuery q = new SolrQuery("*:*");
---------------
-------------
@@ -96,7 +96,7 @@
   }
 
   /** @see #closeWhileHandlingException(Exception, Closeable...) */
-  public static <E extends Exception> void closeWhileHandlingException(E priorException, Iterable<Closeable> objects) throws E, IOException {
+public static <E extends Exception> void closeWhileHandlingException(E priorException, Iterable<? extends Closeable> objects) throws E, IOException {
     Throwable th = null;
 
     for (Closeable object : objects) {
---------------
-------------
@@ -101,7 +101,7 @@
     if (queries != null) {
       in = new BufferedReader(new FileReader(queries));
     } else {
-      in = new BufferedReader(new InputStreamReader(System.int, "UTF-8"));
+in = new BufferedReader(new InputStreamReader(System.in, "UTF-8"));
     }
 
     while (true) {
---------------
-------------
@@ -229,7 +229,7 @@
 
   // firstDocID is ignored since nextDoc() initializes 'current'
   @Override
-  protected boolean score(Collector collector, int max, int firstDocID) throws IOException {
+public boolean score(Collector collector, int max, int firstDocID) throws IOException {
     boolean more;
     Bucket tmp;
     BucketScorer bs = new BucketScorer();
---------------
-------------
@@ -67,7 +67,7 @@
 
     // Don't use _TestUtil.getTempDir so that we own the
     // randomness (ie same seed will point to same dir):
-    Directory dir = newFSDirectory(new File(LuceneTestCase.TEMP_DIR, "longpostings" + "." + random.nextLong()));
+Directory dir = newFSDirectory(_TestUtil.getTempDir("longpostings" + "." + random.nextLong()));
 
     final int NUM_DOCS = (int) ((TEST_NIGHTLY ? 4e6 : (RANDOM_MULTIPLIER*2e4)) * (1+random.nextDouble()));
 
---------------
-------------
@@ -116,7 +116,7 @@
  */
 public final class ShingleMatrixFilter extends TokenStream {
 
-  public static Character defaultSpacerCharacter = new Character('_');
+public static Character defaultSpacerCharacter = Character.valueOf('_');
   public static TokenSettingsCodec defaultSettingsCodec = new OneDimensionalNonWeightedTokenSettingsCodec();
   public static boolean ignoringSinglePrefixOrSuffixShingleByDefault = false;
 
---------------
-------------
@@ -61,7 +61,7 @@
     {
         // Response is been managed by the map so make it 1 for the superclass.
         super(writeEndpoints, hintedEndpoints, consistencyLevel);
-        assert consistencyLevel == ConsistencyLevel.LOCAL_QUORUM;
+assert consistencyLevel == ConsistencyLevel.EACH_QUORUM;
 
         strategy = (NetworkTopologyStrategy) Table.open(table).getReplicationStrategy();
 
---------------
-------------
@@ -164,7 +164,7 @@
 
     static class ExternalClient extends SSTableLoader.Client
     {
-        private final Map<String, Map<String, CFMetaData>> knownCfs = new HashMap<String, Map<String, CFMetaData>>();
+private final Map<String, Set<String>> knownCfs = new HashMap<String, Set<String>>();
         private final SSTableLoader.OutputHandler outputHandler;
 
         public ExternalClient(SSTableLoader.OutputHandler outputHandler)
---------------
-------------
@@ -147,7 +147,7 @@
 
   public URL toURL() throws MalformedURLException
   {
-    String entryURL = "jar:" + url + "!/" + getParent().getName() + getName();
+String entryURL = "jar:" + url + "!/" + getName();
     URL result = new URL(entryURL);
     return result;
   }
---------------
-------------
@@ -567,7 +567,7 @@
 	//clear the parent resultset hash table;
 	public void clearParentResultSets();
 
-	public Hashtable getParentResultSets();
+public Enumeration getParentResultSetKeys();
 
 	/**
 	 * beetle 3865: updateable cursor using index.  A way of communication
---------------
-------------
@@ -132,7 +132,7 @@
     FieldQuery fq = new FieldQuery( tq( "d" ), true, true );
     FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
     assertEquals( 1, stack.termList.size() );
-    assertEquals( "d(6,7,3)", stack.pop().toString() );
+assertEquals( "d(9,10,3)", stack.pop().toString() );
   }
   
   public void test1PhraseLongMV() throws Exception {
---------------
-------------
@@ -35,7 +35,7 @@
   @Test
   public void testSet() throws Exception {
     FileSystem fs = FileSystem.get(new Configuration());
-    Path inpath = fs.makeQualified(new Path(Resources.getResource("wdbc").toString()));
+Path inpath = fs.makeQualified(new Path(Resources.getResource("wdbc").toURI()));
     
     DataSet dataset = FileInfoParser.parseFile(fs, inpath);
     DataSet.initialize(dataset);
---------------
-------------
@@ -325,7 +325,7 @@
 		specified on the JDBC url at the database create time. This collation 
 		is what Derby 10.2 and prior have supported)
 		2)TERRITORY_BASED (the collation will be based on language 
-		region specified by the exisiting Derby attribute called territory. 
+region specified by the existing Derby attribute called territory.
 		If the territory attribute is not specified at the database create 
 		time, Derby will use java.util.Locale.getDefault to determine the 
 		territory for the newly created database. 
---------------
-------------
@@ -86,7 +86,7 @@
   public PHPSerializedWriter(Writer writer, SolrQueryRequest req, SolrQueryResponse rsp, boolean CESU8) {
     super(writer, req, rsp);
     this.CESU8 = CESU8;
-    this.utf8 = CESU8 ? null : new BytesRef(10);
+this.utf8 = CESU8 ? null : new BytesRef();
     // never indent serialized PHP data
     doIndent = false;
   }
---------------
-------------
@@ -51,6 +51,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new NorwegianAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new NorwegianAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -4045,7 +4045,7 @@
       final int termsIndexDivisor;
       final boolean loadDocStores;
 
-      if (poolReaders && mergedSegmentWarmer != null) {
+if (mergedSegmentWarmer != null) {
         // Load terms index & doc stores so the segment
         // warmer can run searches, load documents/term
         // vectors
---------------
-------------
@@ -3134,7 +3134,7 @@
       return false;
     }
 
-    final ReadersAndLiveDocs mergedDeletes = commitMergedDeletes(merge);
+final ReadersAndLiveDocs mergedDeletes =  merge.info.docCount == 0 ? null : commitMergedDeletes(merge);
 
     assert mergedDeletes == null || mergedDeletes.pendingDeleteCount != 0;
 
---------------
-------------
@@ -73,7 +73,7 @@
   }
 
   public int numFeatures() {
-    return userFeatures[0].length;
+return userFeatures.length > 0 ? userFeatures[0].length : 0;
   }
 
   public int numUsers() {
---------------
-------------
@@ -438,7 +438,7 @@
     long beforeCount = results.getResults().getNumFound();
     int cnt = TEST_NIGHTLY ? 2933 : 313;
     try {
-      suss.setConnectionTimeout(15000);
+suss.setConnectionTimeout(30000);
       for (int i = 0; i < cnt; i++) {
         index_specific(suss, id, docId++, "text_t", "some text so that it not's negligent work to parse this doc, even though it's still a pretty short doc");
       }
---------------
-------------
@@ -219,7 +219,7 @@
           iw.commit();
         }
       }
-      iw.close();
+iw.shutdown();
 
 
       DirectoryReader reader = DirectoryReader.open(dir);
---------------
-------------
@@ -36,7 +36,7 @@
 
     /** Message, modeled after CloseFilterInputStream in the client. */
     private static final String MESSAGE =
-            MessageService.getTextMessage(MessageId.CONN_ALREADY_CLOSED); 
+MessageService.getTextMessage(MessageId.OBJECT_CLOSED);
     
     /** Tells if this stream has been closed. */
     private boolean closed;
---------------
-------------
@@ -553,7 +553,7 @@
       expected = new String[] {"_0.cfs",
                                "_0_1.del",
                                "_0_1.s" + contentFieldIndex,
-                               "segments_2",
+"segments_3",
                                "segments.gen"};
 
       String[] actual = dir.listAll();
---------------
-------------
@@ -284,7 +284,7 @@
       TermsEnum te = terms.iterator(null);
       DocsEnum de = null;
       while (te.next() != null) {
-        de = _TestUtil.docs(random(), te, liveDocs, de, 0);
+de = _TestUtil.docs(random(), te, liveDocs, de, false);
         int cnt = 0;
         while (de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
           cnt++;
---------------
-------------
@@ -408,7 +408,7 @@
         ResultSet trigARS = trigASt.executeQuery(
                 "select a, length(a), b from testClobTriggerA order by b");
         ResultSet trigBRS = trigBSt.executeQuery(
-                "select a, length(a), b from testClobTriggerA order by b");
+"select a, length(a), b from testClobTriggerB order by b");
 
         int count = 0;
         while (origRS.next()) {
---------------
-------------
@@ -68,7 +68,7 @@
   @Test
   public void testBasic() throws IOException {
     URI uri = dfsCluster.getURI();
-    Path lockPath = new Path(uri.toString(), "/lock");
+Path lockPath = new Path(uri.toString(), "/basedir/lock");
     HdfsLockFactory lockFactory = new HdfsLockFactory(lockPath, new Configuration());
     Lock lock = lockFactory.makeLock("testlock");
     boolean success = lock.obtain();
---------------
-------------
@@ -219,7 +219,7 @@
             ByteArrayOutputStream bos = new ByteArrayOutputStream();
             DataOutputStream dos = new DataOutputStream( bos );
             StreamStatusMessage.serializer().serialize(streamStatusMessage, dos);
-            return new Message(StorageService.getLocalStorageEndPoint(), "", StorageService.bootStrapTerminateVerbHandler_, new Object[]{bos.toByteArray()});
+return new Message(StorageService.getLocalStorageEndPoint(), "", StorageService.bootStrapTerminateVerbHandler_, bos.toByteArray());
         }
         
         protected StreamContextManager.StreamStatus streamStatus_;
---------------
-------------
@@ -116,7 +116,7 @@
         return seekBytesRef;
       }
     } else {
-      seekBytesRef.copy(term);
+seekBytesRef.copyBytes(term);
     }
 
     // seek to the next possible string;
---------------
-------------
@@ -754,7 +754,7 @@
    * is active and {@link #RANDOM_MULTIPLIER}, but also with some random fudge.
    */
   public static int atLeast(Random random, int i) {
-    int min = (TEST_NIGHTLY ? 5*i : i) * RANDOM_MULTIPLIER;
+int min = (TEST_NIGHTLY ? 3*i : i) * RANDOM_MULTIPLIER;
     int max = min+(min/2);
     return _TestUtil.nextInt(random, min, max);
   }
---------------
-------------
@@ -749,7 +749,7 @@
             {"C"}
         };
         
-        JDBC.assertFullResultSet(rs, expRS, true);
+JDBC.assertUnorderedResultSet(rs, expRS, true);
         
         assertStatementError("42Y55", st,
             " drop table t2");
---------------
-------------
@@ -224,7 +224,7 @@
         continue;
       }
 
-      if (ir != null) { // use the user index
+if (ir != null && field != null) { // use the user index
         sugWord.freq = ir.docFreq(new Term(field, sugWord.string)); // freq in the index
         // don't suggest a word that is not present in the field
         if ((morePopular && goalFreq > sugWord.freq) || sugWord.freq < 1) {
---------------
-------------
@@ -37,7 +37,7 @@
 import org.apache.derby.iapi.services.monitor.Monitor;
 import org.apache.derby.iapi.store.raw.data.DataFactory;
 import org.apache.derby.io.StorageFile;
-import org.apache.derby.shared.common.error.ExceptionUtil;
+import org.apache.derby.iapi.error.ExceptionUtil;
 import org.apache.derby.shared.common.reference.MessageId;
 
 /**
---------------
-------------
@@ -18,7 +18,7 @@
 import java.util.Collections;
 import java.util.Map;
 
-import org.apache.aries.subsystem.core.internal.ResourceHelper;
+import org.apache.aries.subsystem.core.ResourceHelper;
 import org.apache.felix.bundlerepository.Capability;
 import org.apache.felix.bundlerepository.Requirement;
 import org.apache.felix.bundlerepository.Resource;
---------------
-------------
@@ -53,7 +53,7 @@
     public static Version getBundleVersion(Bundle bundle) {
         Dictionary headers = bundle.getHeaders();
         String version = (String)headers.get(Constants.BUNDLE_VERSION);
-        return (version != null) ? Version.parseVersion(version) : null;
+return (version != null) ? Version.parseVersion(version) : Version.emptyVersion;
     }
     
 }
---------------
-------------
@@ -606,7 +606,7 @@
 						+ " SQLSTATE: " + m);
 			}
 		}
-		if (e.getMessage().equals(null)) {
+if (e.getMessage() == null) {
 			System.out.println("NULL error message detected");
 			System.out.println("Here is the NULL exection - " + e.toString());
 			System.out.println("Stack trace of the NULL exception - ");
---------------
-------------
@@ -20,7 +20,7 @@
 import java.io.Serializable;
 import java.util.Arrays;
 
-public class Pattern implements Serializable, Cloneable {
+public class Pattern implements Serializable {
 
   private static final long serialVersionUID = 8698199782842762173L;
 
---------------
-------------
@@ -27,7 +27,7 @@
 
   @Override
   protected double doComputeResult(int rowA, int rowB, Iterable<Cooccurrence> cooccurrences, double weightOfVectorA,
-      double weightOfVectorB, int numberOfColumns) {
+double weightOfVectorB, long numberOfColumns) {
     double cooccurrenceCount = countElements(cooccurrences);
     if (cooccurrenceCount == 0) {
       return Double.NaN;
---------------
-------------
@@ -1,6 +1,6 @@
 /*
 
-   Derby - Class com.ihost.cs.Operator
+Derby - Class org.apache.derby.iapi.util.Operator
 
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -342,7 +342,7 @@
     }
 
     // test setDocumentNumber()
-    IndexReader ir = IndexReader.open(dir);
+IndexReader ir = IndexReader.open(dir, true);
     DocNumAwareMapper docNumAwareMapper = new DocNumAwareMapper();
     assertEquals(-1, docNumAwareMapper.getDocumentNumber());
 
---------------
-------------
@@ -246,7 +246,7 @@
   
     SweetSpotSimilarity ss = new SweetSpotSimilarity() {
         @Override
-        public float tf(int freq) {
+public float tf(float freq) {
           return hyperbolicTf(freq);
         }
       };
---------------
-------------
@@ -57,7 +57,7 @@
     writer.addDocument(doc("solr", "solr is a very popular search server and is using lucene"));
     writer.addDocument(doc("nutch", "nutch is an internet search engine with web crawler and is using lucene and hadoop"));
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     searcher = newSearcher(reader);
     searcher.setSimilarity(new TFSimilarity());
   }
---------------
-------------
@@ -689,7 +689,7 @@
     Directory dir = newDirectory();
     IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
     cfg.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
+RandomIndexWriter iw = new RandomIndexWriter(random(), dir, cfg);
     Document doc = new Document();
     LongField field = new LongField("f", 0L, Store.YES);
     doc.add(field);
---------------
-------------
@@ -147,7 +147,7 @@
 
       final IResource foundResource = new IResource() {
         @Override
-        public InputStream open() throws IOException {
+public InputStream open() {
           return new ByteArrayInputStream(asBytes);
         }
 
---------------
-------------
@@ -26,7 +26,7 @@
 import java.security.NoSuchAlgorithmException;
 import java.util.Properties;
 
-import org.apache.cassandra.config.DatabaseDescriptor.ConfigurationException;
+import org.apache.cassandra.config.ConfigurationException;
 import org.apache.cassandra.thrift.AccessLevel;
 import org.apache.cassandra.thrift.AuthenticationException;
 import org.apache.cassandra.thrift.AuthenticationRequest;
---------------
-------------
@@ -434,7 +434,7 @@
                                 slop,
                                 ordered);
   
-        spanScorer = snq.weight(searcher).scorer(leaves[i], ScorerContext.def());
+spanScorer = searcher.createNormalizedWeight(snq).scorer(leaves[i], ScorerContext.def());
       } finally {
         searcher.setSimilarityProvider(oldSim);
       }
---------------
-------------
@@ -216,7 +216,7 @@
     
     public static Test suite() {
         TestSuite ts = new TestSuite ("ClobUpdateableReaderTest");
-        ts.addTest(TestConfiguration.embeddedSuite(
+ts.addTest(TestConfiguration.defaultSuite(
                     ClobUpdateableReaderTest.class));
         TestSuite encSuite = new TestSuite ("ClobUpdateableReaderTest:encrypted");
         encSuite.addTestSuite (ClobUpdateableReaderTest.class);
---------------
-------------
@@ -624,7 +624,7 @@
               break;
             }
           }
-          TokenStream ts = analyzer.tokenStream("ignore", new StringReader(term));
+TokenStream ts = analyzer.tokenStream("ignore", term);
           CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);
           ts.reset();
           while(ts.incrementToken()) {
---------------
-------------
@@ -87,7 +87,7 @@
     {
         RowMutation first = rms.get(0);
         String tablename = first.getTable();
-        String cfname = first.columnFamilyNames().iterator().next();
+String cfname = first.getColumnFamilies().iterator().next().metadata().cfName;
 
         Table table = Table.open(tablename);
         ColumnFamilyStore store = table.getColumnFamilyStore(cfname);
---------------
-------------
@@ -208,7 +208,7 @@
             }
           }
 
-          reader = new MultiReader(directory, infos, closeDirectory, readers);
+reader = new MultiSegmentReader(directory, infos, closeDirectory, readers);
         }
         reader.deletionPolicy = deletionPolicy;
         return reader;
---------------
-------------
@@ -347,7 +347,7 @@
       
       //make sure leaders are in cloud state
       for (int i = 0; i < sliceCount; i++) {
-        assertNotNull(reader.getLeaderUrl("collection1", "shard" + (i + 1)), 15000);
+assertNotNull(reader.getLeaderUrl("collection1", "shard" + (i + 1), 15000));
       }
 
     } finally {
---------------
-------------
@@ -165,7 +165,7 @@
 
     try {
       JobClient.runJob(conf);
-      FileSystem fs = FileSystem.get(conf);
+FileSystem fs = FileSystem.get(outPath.toUri(), conf);
       return isConverged(clustersOut, conf, fs);
     } catch (IOException e) {
       log.warn(e.toString(), e);
---------------
-------------
@@ -117,7 +117,7 @@
     double result;
     Map<String,Integer> classes = nominalMap.get(label);
     if (classes != null) {
-      Integer ord = classes.get(data);
+Integer ord = classes.get(ARFFType.removeQuotes(data));
       if (ord != null) {
         result = ord;
       } else {
---------------
-------------
@@ -54,7 +54,7 @@
      * java org.apache.derbyTesting.functionTests.tests.i18n.LocalizedAttributeScriptTest
      * </code>
      */
-    public static void main()
+public static void main(String[] args)
     {
         junit.textui.TestRunner.run(getSuite());
     }
---------------
-------------
@@ -42,6 +42,6 @@
 
 public class Constants {
 
-  public static final String VERSION = "8.3.0";
+public static final String VERSION = "8.4.0";
 
 }
---------------
-------------
@@ -75,7 +75,7 @@
    * @param reader
    */
   final void add(IndexReader reader) {
-    for (final AtomicReaderContext ctx : reader.getTopReaderContext().leaves()) {
+for (final AtomicReaderContext ctx : reader.leaves()) {
       final AtomicReader r = ctx.reader();
       mergeState.readers.add(r);
     }
---------------
-------------
@@ -130,7 +130,7 @@
         indexerNamespace.put(LAST_INDEX_TIME, EPOCH);
       }
       indexerNamespace.put(INDEX_START_TIME, dataImporter.getIndexStartTime());
-      indexerNamespace.put("request", reqParams.getRawParams());
+indexerNamespace.put("request", new HashMap<String,Object>(reqParams.getRawParams()));
       for (Entity entity : dataImporter.getConfig().getEntities()) {
         String key = entity.getName() + "." + SolrWriter.LAST_INDEX_KEY;
         Object lastIndex = persistedProperties.get(key);
---------------
-------------
@@ -76,7 +76,7 @@
                                       RAMDirectory directory,
                                       boolean createNew,
                                       int startingId) throws IOException {
-    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(Version.LUCENE_41, new WhitespaceAnalyzer(Version.LUCENE_41)));
+IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(Version.LUCENE_42, new WhitespaceAnalyzer(Version.LUCENE_42)));
 
     try {
       for (int i = 0; i < DOCS.length; i++) {
---------------
-------------
@@ -169,7 +169,7 @@
         infoStream.message("IndexUpgrader", "All segments upgraded to version " + Constants.LUCENE_MAIN_VERSION);
       }
     } finally {
-      w.close();
+w.shutdown();
     }
   }
   
---------------
-------------
@@ -70,6 +70,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new CzechAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new CzechAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -127,7 +127,7 @@
     
     reader = iw.getReader();
     searcher = newSearcher(reader);
-    iw.close();
+iw.shutdown();
   }
   
   @AfterClass
---------------
-------------
@@ -229,7 +229,7 @@
         wait(500);
       } catch (InterruptedException e) {
         Thread.currentThread().interrupt();
-        throw new RuntimeException(e);
+break;
       }
       left = expire - System.currentTimeMillis();
     }
---------------
-------------
@@ -143,7 +143,7 @@
 
   private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
   // nocommit
-  private final SimpleDocValuesFormat defaultDVFormat = SimpleDocValuesFormat.forName("Memory");
+private final SimpleDocValuesFormat defaultDVFormat = SimpleDocValuesFormat.forName("Lucene41");
 
   // nocommit need simpleNormsFormat
 }
---------------
-------------
@@ -165,7 +165,7 @@
         }
         catch( ConnectException cex ) {
           srsp.setException(cex); //????
-        } catch (Throwable th) {
+} catch (Exception th) {
           srsp.setException(th);
           if (th instanceof SolrException) {
             srsp.setResponseCode(((SolrException)th).code());
---------------
-------------
@@ -40,7 +40,7 @@
     double oldTotalWeight = totalWeight;
     totalWeight += weight;
     if (oldTotalWeight <= 0.0) {
-      average = datum * weight;
+average = datum;
     } else {
       average = average * oldTotalWeight / totalWeight + datum * weight / totalWeight;
     }
---------------
-------------
@@ -474,7 +474,7 @@
   }
 
   long getIndexSize() {
-    return FileUtils.sizeOfDirectory(new File(core.getIndexDir()));
+return FileUtils.sizeOfDirectory(new File(core.getNewIndexDir()));
   }
 
   /**
---------------
-------------
@@ -37,7 +37,7 @@
 
 public abstract class AbstractCompactionIterable implements Iterable<AbstractCompactedRow>, CompactionInfo.Holder
 {
-    private static Logger logger = LoggerFactory.getLogger(CompactionIterable.class);
+private static Logger logger = LoggerFactory.getLogger(AbstractCompactionIterable.class);
 
     protected final OperationType type;
     protected final CompactionController controller;
---------------
-------------
@@ -333,7 +333,7 @@
 			)
 			{
 				if (server != null)
-					server.consoleExceptionPrintTrace(e);
+server.consoleExceptionPrint(e);
 				else
 					e.printStackTrace();  // default output stream is System.out
 			}
---------------
-------------
@@ -33,7 +33,7 @@
     conf.setInt(HASH_OFFSET_PROPERTY_NAME, left);
   }
   
-  int offset;
+private int offset;
 
   @Override
   public int getPartition(GramKey key, Gram value, int numPartitions) {
---------------
-------------
@@ -30,7 +30,7 @@
   private MockDataSet mock;
 
   @Override
-  protected void setUp() throws Exception {
+protected void setUp() {
     rng = new MersenneTwisterRNG();
     mock = new MockDataSet(rng, 100);
   }
---------------
-------------
@@ -2918,7 +2918,7 @@
 
   /** Returns true if there are changes that have not been committed */
   public final boolean hasUncommittedChanges() {
-    return changeCount != lastCommitChangeCount;
+return changeCount != lastCommitChangeCount || docWriter.anyChanges() || bufferedDeletesStream.any();
   }
 
   private final void commitInternal() throws IOException {
---------------
-------------
@@ -291,7 +291,7 @@
     @Test
     public void testInteger() throws SQLException
     {
-÷        PreparedStatement stmt = con.prepareStatement("update JdbcInteger set ?=?, ?=? where key = ?");
+PreparedStatement stmt = con.prepareStatement("update JdbcInteger set ?=?, ?=? where key = ?");
         for (int i = 0; i < 5; i++)
         {
             byte[] key = Integer.toString(i).getBytes();
---------------
-------------
@@ -664,7 +664,7 @@
 	protected byte readByte () throws DRDAProtocolException
 	{
 		ensureBLayerDataInBuffer (1, ADJUST_LENGTHS);
-		return (byte) (buffer[pos++] & 0xff);
+return buffer[pos++];
 	}
 
 	/**
---------------
-------------
@@ -1,4 +1,4 @@
-package org.apache.lucene.util.cache;
+package org.apache.lucene.util;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
---------------
-------------
@@ -219,7 +219,7 @@
         filter = resultSet.getTopFilter();
       }
 
-      DocIdSet readerSet = filter.getDocIdSet(context);
+DocIdSet readerSet = filter.getDocIdSet(context, null);  // this set only includes live docs
       if (readerSet == null) readerSet=DocIdSet.EMPTY_DOCIDSET;
       return new JoinScorer(this, readerSet.iterator(), getBoost());
     }
---------------
-------------
@@ -172,7 +172,7 @@
   }
   
   private void doDelete(DeleteUpdateCommand cmd, List<Node> nodes,
-      ModifiableSolrParams params) throws IOException {
+ModifiableSolrParams params) {
     
     flushAdds(1);
     
---------------
-------------
@@ -236,7 +236,7 @@
     //writer.setUseCompoundFile(false);
     writer.addDocument(doc);
     writer.flush();
-    SegmentInfo info = writer.segmentInfos.info(writer.segmentInfos.size()-1);
+SegmentInfo info = writer.newestSegment();
     writer.close();
     return info;
   }
---------------
-------------
@@ -108,7 +108,7 @@
         List<String> files = new ArrayList<String>();
         for (PendingFile pf : StreamInManager.getIncomingFiles(InetAddress.getByName(host)))
         {
-            files.add(String.format("%s: %s", pf.getDescriptor().ksname, pf.toString()));
+files.add(String.format("%s: %s", pf.desc.ksname, pf.toString()));
         }
         return files;
     }
---------------
-------------
@@ -65,7 +65,7 @@
     
     reader = iw.getReader();
     searcher = new IndexSearcher(reader);
-    iw.close();
+iw.shutdown();
   }
   
   @Override
---------------
-------------
@@ -64,7 +64,7 @@
       while (term != null) {
         T shape = readShape(term);
         if( shape != null ) {
-          docs = te.docs(null, docs, false);
+docs = te.docs(null, docs, 0);
           Integer docid = docs.nextDoc();
           while (docid != DocIdSetIterator.NO_MORE_DOCS) {
             idx.add( docid, shape );
---------------
-------------
@@ -80,7 +80,7 @@
       }
     }
     w.forceMerge(1);
-    w.close();
+w.shutdown();
     dir.close();
   }
   
---------------
-------------
@@ -99,7 +99,7 @@
 
     public String toString()
     {
-        return getFilename() + "/" + StringUtils.join(sections, ",") + "\n\t progress=" + progress + "/" + size + " - " + progress*100/size + "%";
+return getFilename() + " sections=" + sections.size() + " progress=" + progress + "/" + size + " - " + progress*100/size + "%";
     }
 
     public static class PendingFileSerializer implements ICompactSerializer<PendingFile>
---------------
-------------
@@ -161,7 +161,7 @@
 
   @Override
   public OrderedIntDoubleMapping clone() {
-    return new OrderedIntDoubleMapping(indices, values, numMappings);
+return new OrderedIntDoubleMapping(indices.clone(), values.clone(), numMappings);
   }
 
 }
---------------
-------------
@@ -162,7 +162,7 @@
   // temporary working objects...
   // be careful not to use these recursively...
   private final ArrayList tlst = new ArrayList();
-  private final Calendar cal = Calendar.getInstance(TimeZone.getTimeZone("GMT"));
+private final Calendar cal = Calendar.getInstance(TimeZone.getTimeZone("GMT"), Locale.US);
   private final StringBuilder sb = new StringBuilder();
 
   public XMLWriter(Writer writer, IndexSchema schema, SolrQueryRequest req, String version) {
---------------
-------------
@@ -554,7 +554,7 @@
         }
     }
 
-    private class CompactionExecutor extends DebuggableThreadPoolExecutor
+private static class CompactionExecutor extends DebuggableThreadPoolExecutor
     {
         private volatile ColumnFamilyStore cfs;
         private volatile CompactionIterator ci;
---------------
-------------
@@ -83,7 +83,7 @@
           instance = loadDoubleField(field);
           break;
         default: 
-          throw new AssertionError(); // nocommit, implement the other types
+throw new AssertionError();
       }
       numericInstances.put(field.number, instance);
     }
---------------
-------------
@@ -43,7 +43,7 @@
     }
     w.commit();
     w.deleteDocuments(new Term("id", "" + (NUM_DOCS-1)));
-    w.close();
+w.shutdown();
     input = DirectoryReader.open(dir);
   }
   
---------------
-------------
@@ -26,7 +26,7 @@
 
 import org.apache.cassandra.utils.UUIDGen;
 
-public class JdbcTimeUUID extends JdbcLong
+public class JdbcTimeUUID extends AbstractJdbcUUID
 {
     public static final JdbcTimeUUID instance = new JdbcTimeUUID();
     
---------------
-------------
@@ -134,7 +134,7 @@
 	 */
 	public float	getFloat() throws StandardException
 	{
-		if (Math.abs(value) > Float.MAX_VALUE)
+if (Float.isInfinite((float)value))
 			throw StandardException.newException(SQLState.LANG_OUTSIDE_RANGE_FOR_DATATYPE, TypeId.REAL_NAME);
 		return (float) value;
 	}
---------------
-------------
@@ -136,7 +136,7 @@
 						+ " SQLSTATE: " + m);
 			}
 		}
-		if (e.getMessage().equals(null)) {
+if (e.getMessage() == null) {
 			e.printStackTrace(System.out);
 		}
 		System.out.println("During - " + where
---------------
-------------
@@ -164,7 +164,7 @@
                 columnFamilyType = ColumnFamilyType.valueOf(cmd.getOptionValue("y"));
 
             if (cmd.hasOption("k"))
-                ignoreErrors = Boolean.parseBoolean(cmd.getOptionValue("k"));
+ignoreErrors = true;
 
             if (cmd.hasOption("i"))
                 progressInterval = Integer.parseInt(cmd.getOptionValue("i"));
---------------
-------------
@@ -83,7 +83,7 @@
 
         try
         {
-            rows = StorageProxy.readProtocol(commands, select.getConsistencyLevel());
+rows = StorageProxy.read(commands, select.getConsistencyLevel());
         }
         catch (TimeoutException e)
         {
---------------
-------------
@@ -715,7 +715,7 @@
 	}
 	
 	protected Subsystem getRootSubsystem() {
-		return getOsgiService(Subsystem.class);
+return getOsgiService(Subsystem.class, "(&(objectClass=org.osgi.service.subsystem.Subsystem)(subsystem.id=0))", DEFAULT_TIMEOUT);
 	}
 	
 	protected Subsystem getRootSubsystemInState(Subsystem.State state, long timeout) throws InterruptedException {
---------------
-------------
@@ -111,7 +111,7 @@
     }
 
     // Execute GET statement
-    private void executeGet(CommonTree ast) throws TException, NotFoundException, InvalidRequestException
+private void executeGet(CommonTree ast) throws TException, NotFoundException, InvalidRequestException, UnavailableException
     {
         if (!CliMain.isConnected())
             return;
---------------
-------------
@@ -1887,7 +1887,7 @@
     public void listenToUnitOfWork() {
         if (!listenToUnitOfWork_) {
             listenToUnitOfWork_ = true;
-            connection_.CommitAndRollbackListeners_.add(this);
+connection_.CommitAndRollbackListeners_.put(this,null);
         }
     }
 
---------------
-------------
@@ -953,7 +953,7 @@
             CallableStatement cs = conn.prepareCall(
                 "CALL SYSCS_UTIL.SYSCS_COMPRESS_TABLE(?, ?, ?)");
             cs.setString(1, "APP");
-            cs.setString(2, "testLongVarChar");
+cs.setString(2, "TESTLONGVARCHAR");
             cs.setInt(3, 0);
             cs.execute();
 
---------------
-------------
@@ -28,7 +28,7 @@
     {
         super(referent, q);
         this.tracker = tracker;
-        this.path = referent.path;
+this.path = referent.getFilename();
         this.size = referent.bytesOnDisk();
     }
 
---------------
-------------
@@ -65,7 +65,7 @@
     }
 
     @Override
-    public void files(Directory dir, SegmentInfo info, int formatId, Set<String> files) throws IOException {}
+public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {}
   };
   
   @Override
---------------
-------------
@@ -164,7 +164,7 @@
 		TransactionController nestedTc = null;
 
 		try {
-			nestedTc = tc.startNestedUserTransaction(false);
+nestedTc = tc.startNestedUserTransaction(false, true);
 			useTc = nestedTc;
 		} catch (StandardException e) {
 			if (SanityManager.DEBUG) {
---------------
-------------
@@ -2253,7 +2253,7 @@
 
     public boolean locatorsUpdateCopy() throws SQLException {
         checkForClosedConnection();
-        return false;
+return true;
     }
 
     public boolean supportsStatementPooling() throws SQLException {
---------------
-------------
@@ -114,7 +114,7 @@
     indexSearcher = newSearcher(directoryReader);
     result = indexSearcher.search(new MatchAllDocsQuery(), 2);
     assertEquals(2, result.totalHits);
-    writer.close();
+writer.shutdown();
     indexSearcher.getIndexReader().close();
     dir.close();
   }
---------------
-------------
@@ -95,7 +95,7 @@
         CompactionManager.instance.checkAllColumnFamilies();
 
         // start server internals
-        StorageService.instance().initServer();
+StorageService.instance.initServer();
         
         // now we start listening for clients
         CassandraServer cassandraServer = new CassandraServer();
---------------
-------------
@@ -158,7 +158,7 @@
   // in the extremes.
 
   public VariableGapTermsIndexWriter(SegmentWriteState state, IndexTermSelector policy) throws IOException {
-    final String indexFileName = IndexFileNames.segmentFileName(state.segmentName, state.formatId, TERMS_INDEX_EXTENSION);
+final String indexFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_INDEX_EXTENSION);
     out = state.directory.createOutput(indexFileName, state.context);
     boolean success = false;
     try {
---------------
-------------
@@ -989,7 +989,7 @@
         nextRow call.  In this case the client should severe all references to 
         the row after returning it from getNextRowFromRowSource().
 
-		@exception StandardException Cloudscape Standard Error Policy
+@exception StandardException Standard Derby Error Policy
 	 */
 	public DataValueDescriptor[] getNextRowFromRowSource() throws StandardException
 	{ 
---------------
-------------
@@ -49,7 +49,7 @@
   }
 
   @Override
-  protected void setUp() throws Exception {
+protected void setUp() {
     rng = new MersenneTwisterRNG();
     mock = new MockDataSet(rng, 50);
   }
---------------
-------------
@@ -51,7 +51,7 @@
                 logger.debug(srm.toString());
 
             StreamOutSession session = StreamOutSession.create(srm.table, message.getFrom(), srm.sessionId);
-            StreamOut.transferRanges(session, srm.columnFamilies, srm.ranges, srm.type);
+StreamOut.transferRangesForRequest(session, srm.ranges, srm.type);
         }
         catch (IOException ex)
         {
---------------
-------------
@@ -613,7 +613,7 @@
 	{
 		NodeFactory		nodeFactory = getNodeFactory();
 
-        QueryTreeNode trueNode = nodeFactory.getNode(
+QueryTreeNode trueNode = (QueryTreeNode) nodeFactory.getNode(
 										C_NodeTypes.BOOLEAN_CONSTANT_NODE,
 										Boolean.TRUE,
 										getContextManager());
---------------
-------------
@@ -53,6 +53,6 @@
 
         ColumnFamily retrieved = store.getColumnFamily(QueryFilter.getIdentityFilter(dk, new QueryPath("Super1", "SC1".getBytes())));
         assert retrieved.getColumn("SC1".getBytes()).getSubColumn(getBytes(1)).isMarkedForDelete();
-        assertNull(ColumnFamilyStore.removeDeleted(retrieved, Integer.MAX_VALUE));
+assertNull(Util.cloneAndRemoveDeleted(retrieved, Integer.MAX_VALUE));
     }
 }
---------------
-------------
@@ -32,7 +32,7 @@
     this.text = text;
   }
 
-  public <T> T newInstance(String cname, Class<T> expectedType, String... subpackages) {
+public <T> T newInstance(String cname, Class<T> expectedType) {
     return null;
   }
 
---------------
-------------
@@ -30,7 +30,7 @@
 
 import org.junit.Test;
 
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.CleanupHelper;
 import org.apache.cassandra.db.filter.QueryPath;
 import org.apache.cassandra.utils.FBUtilities;
---------------
-------------
@@ -871,7 +871,7 @@
    */
   public static XmlDoc doc(String... fieldsAndValues) {
     XmlDoc d = new XmlDoc();
-    d.xml = TestHarness.makeSimpleDoc(fieldsAndValues).toString();
+d.xml = TestHarness.makeSimpleDoc(fieldsAndValues);
     return d;
   }
 
---------------
-------------
@@ -44,7 +44,7 @@
       if (!reverse) return new SortField(fieldName, nullStringLastComparatorSource);
       else return new SortField(fieldName, SortField.STRING, true);
     } else if (nullFirst) {
-      if (reverse) return new SortField(fieldName, nullStringLastComparatorSource);
+if (reverse) return new SortField(fieldName, nullStringLastComparatorSource, true);
       else return new SortField(fieldName, SortField.STRING, false);
     } else {
       return new SortField(fieldName, SortField.STRING, reverse);
---------------
-------------
@@ -274,7 +274,7 @@
       if (obj instanceof IndexReader) {
         IndexReader[] subs = ((IndexReader)obj).getSequentialSubReaders();
         for (int j = 0; (null != subs) && (j < subs.length); j++) {
-          all.add(subs[j].getFieldCacheKey());
+all.add(subs[j].getCoreCacheKey());
         }
       }
       
---------------
-------------
@@ -148,7 +148,7 @@
       for (Map.Entry<String,Slice> entry : slices.entrySet()) {
         Map<String,Replica> shards = entry.getValue().getReplicasMap();
         for (Map.Entry<String,Replica> shard : shards.entrySet()) {
-          if (verbose) System.out.println("rstate:"
+if (verbose) System.out.println("replica:" + shard.getValue().getName() + " rstate:"
               + shard.getValue().getStr(ZkStateReader.STATE_PROP)
               + " live:"
               + clusterState.liveNodesContain(shard.getValue().getNodeName()));
---------------
-------------
@@ -50,7 +50,7 @@
 
   @Test
   public void testProcessOutput() throws Exception {
-    Configuration conf = new Configuration();
+Configuration conf = getConfiguration();
     conf.setInt("mapred.map.tasks", NUM_MAPS);
 
     Random rng = RandomUtils.getRandom();
---------------
-------------
@@ -952,7 +952,7 @@
    * is active and {@link #RANDOM_MULTIPLIER}, but also with some random fudge.
    */
   public static int atLeast(Random random, int i) {
-    int min = (TEST_NIGHTLY ? 3*i : i) * RANDOM_MULTIPLIER;
+int min = (TEST_NIGHTLY ? 2*i : i) * RANDOM_MULTIPLIER;
     int max = min+(min/2);
     return _TestUtil.nextInt(random, min, max);
   }
---------------
-------------
@@ -49,7 +49,7 @@
 
     public ByteBuffer fromString(String source)
     {
-        return ByteBuffer.wrap(source.getBytes(Charsets.US_ASCII));
+return ByteBufferUtil.bytes(source, Charsets.US_ASCII);
     }
 
     public void validate(ByteBuffer bytes) throws MarshalException
---------------
-------------
@@ -872,7 +872,7 @@
       }
 
       @Override
-      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) {
+public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) {
         if (reuse == null || !(reuse instanceof MemoryDocsEnum)) {
           reuse = new MemoryDocsEnum();
         }
---------------
-------------
@@ -155,7 +155,7 @@
     private SSTableReader writeSortedContents() throws IOException
     {
         logger.info("Writing " + this);
-        SSTableWriter writer = cfs.createFlushWriter(columnFamilies.size());
+SSTableWriter writer = new SSTableWriter(cfs.getFlushPath(), columnFamilies.size(), cfs.metadata, cfs.partitioner);
 
         for (Map.Entry<DecoratedKey, ColumnFamily> entry : columnFamilies.entrySet())
             writer.append(entry.getKey(), entry.getValue());
---------------
-------------
@@ -300,7 +300,7 @@
           throw new CorruptIndexException("doc counts differ for segment " + segment + ": fieldsReader shows " + fieldsReaderOrig.size() + " but segmentInfo shows " + si.docCount);
         }
 
-        if (fieldInfos.hasVectors()) { // open term vector files only as needed
+if (si.getHasVectors()) { // open term vector files only as needed
           termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);
         }
       }
---------------
-------------
@@ -198,7 +198,7 @@
     return null;
   }
 
-  public final Appendable append(char c) throws IOException {
+public final Appendable append(char c) {
     write(c);
     return this;
   }
---------------
-------------
@@ -3020,7 +3020,7 @@
         this.cKey = other.cKey;
         this.stream = other.stream;
         this._clobValue = other._clobValue;
-        this.localeFinder = localeFinder;
+this.localeFinder = other.localeFinder;
     }
 
     /**
---------------
-------------
@@ -139,7 +139,7 @@
         //remember in setup a locator is already created
         //hence expected value is 2
         assertEquals("The locator values returned by " +
-            "SYSIBM.CLOBCREATELOCATOR() are incorrect", 2, locator);
+"SYSIBM.CLOBCREATELOCATOR() are incorrect", 4, locator);
         cs.close();
     }
 
---------------
-------------
@@ -71,7 +71,7 @@
     String STORE_SHUTDOWN_MSG               = "D002";
     String STORE_BACKUP_STARTED             = "D004";
     String STORE_MOVED_BACKUP               = "D005";
-    String STORE_COPIED_DB_DIR              = "D006";
+String STORE_DATA_SEG_BACKUP_COMPLETED  = "D006";
     String STORE_EDITED_SERVICEPROPS        = "D007";
     String STORE_ERROR_EDIT_SERVICEPROPS    = "D008";
     String STORE_COPIED_LOG                 = "D009";
---------------
-------------
@@ -1764,7 +1764,7 @@
     String REPLICATION_CONNECTION_EXCEPTION                        = "XRE04";
     String REPLICATION_LOG_OUT_OF_SYNCH                            = "XRE05";
     String REPLICATION_MASTER_TIMED_OUT                            = "XRE06";
-    String REPLICATION_UNABLE_TO_STOP_MASTER                       = "XRE07";
+String REPLICATION_NOT_IN_MASTER_MODE                          = "XRE07";
     String REPLICATION_SLAVE_STARTED_OK                            = "XRE08";
     String CANNOT_START_SLAVE_ALREADY_BOOTED                       = "XRE09";
     String REPLICATION_CONFLICTING_ATTRIBUTES                      = "XRE10";
---------------
-------------
@@ -63,7 +63,7 @@
 
 			if (firstTime) {
 				Logs.reportString("----------------------------------------------");
-				Logs.reportMessage("CSLOOK_Header", "schemas");
+Logs.reportMessage("CSLOOK_SchemasHeader");
 				Logs.reportString("----------------------------------------------\n");
 			}
 
---------------
-------------
@@ -158,7 +158,7 @@
    * @see #baselineTf
    */
   @Override
-  public float tf(int freq) {
+public float tf(float freq) {
     return baselineTf(freq);
   }
   
---------------
-------------
@@ -48,7 +48,7 @@
         GossipDigestAck2Message gDigestAck2Message;
         try
         {
-            gDigestAck2Message = GossipDigestAck2Message.serializer().deserialize(dis);
+gDigestAck2Message = GossipDigestAck2Message.serializer().deserialize(dis, message.getVersion());
         }
         catch (IOException e)
         {
---------------
-------------
@@ -72,7 +72,7 @@
         "blueberry pizza",
     };
     directory = newDirectory();
-    RandomIndexWriter iw = new RandomIndexWriter(random, directory);
+RandomIndexWriter iw = new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));
     
     for (int i=0; i<N_DOCS; i++) {
       add(docText[i%docText.length], iw);
---------------
-------------
@@ -75,7 +75,7 @@
     assertTrue("We could not get the lock when it should be available", success);
     success = lock.obtain();
     assertFalse("We got the lock but it should be unavailble", success);
-    lock.release();
+lock.close();
     success = lock.obtain();
     assertTrue("We could not get the lock when it should be available", success);
     success = lock.obtain();
---------------
-------------
@@ -155,7 +155,7 @@
       throws Exception {
     
     JettySolrRunner jetty = new JettySolrRunner(solrHome.getAbsolutePath(),
-        context, 0, solrConfigOverride, schemaOverride);
+context, 0, solrConfigOverride, schemaOverride, true, null, sslConfig);
 
     jetty.setShards(shardList);
     
---------------
-------------
@@ -45,7 +45,7 @@
 
 
     public PlainTextDictionary (InputStream dictFile) {
-        in=new BufferedReader(new InputStreamReader(System.in));
+in=new BufferedReader(new InputStreamReader(dictFile));
     }
 
 
---------------
-------------
@@ -526,7 +526,7 @@
 			DB_Schema.doSchemas(this.conn,
 				(tableList != null) && (targetSchema == null));
 
-            DB_Sequence.doSequences( conn );
+if ( at10_6 ) { DB_Sequence.doSequences( conn ); }
 
 			if (tableList == null) {
 			// Don't do these if user just wants table-related objects.
---------------
-------------
@@ -34,6 +34,6 @@
         String diskUtilization = String.valueOf(StorageService.instance.getLoad());
         if (logger_.isDebugEnabled())
           logger_.debug("Disseminating load info ...");
-        Gossiper.instance.addApplicationState(LoadDisseminator.loadInfo_, new ApplicationState(diskUtilization));
+Gossiper.instance.addLocalApplicationState(LoadDisseminator.loadInfo_, new ApplicationState(diskUtilization));
     }
 }
---------------
-------------
@@ -88,7 +88,7 @@
     reader = iw.getReader();
     s1 = newSearcher(reader);
     s2 = newSearcher(reader);
-    iw.close();
+iw.shutdown();
   }
   
   @AfterClass
---------------
-------------
@@ -122,7 +122,7 @@
   /**
    * This is the internal Lucene version, recorded into each segment.
    */
-  public static final String LUCENE_MAIN_VERSION = ident("4.3");
+public static final String LUCENE_MAIN_VERSION = ident("4.3.1");
 
   /**
    * This is the Lucene version for display purposes.
---------------
-------------
@@ -107,7 +107,7 @@
 	 */
 	@Deprecated // TODO remove in 3.2
 	public void setExclusionTable( Map<?,?> exclusiontable ) {
-		exclusions = new HashSet(exclusiontable.keySet());
+exclusions = exclusiontable.keySet();
 	}
 }
 
---------------
-------------
@@ -185,7 +185,7 @@
         assert MessagingService.getBits(msheader, 3, 1) == 0 : "Stream received before stream reply";
         int version = MessagingService.getBits(msheader, 15, 8);
 
-        int totalSize = input.readInt();
+input.readInt(); // Read total size
         String id = input.readUTF();
         Header header = Header.serializer().deserialize(input, version);
 
---------------
-------------
@@ -595,7 +595,7 @@
      * Triggers repairs with all neighbors for the given table, cfs and range.
      * Typical lifecycle is: start() then join(). Executed in client threads.
      */
-    class RepairSession extends WrappedRunnable implements IEndpointStateChangeSubscriber, IFailureDetectionEventListener
+static class RepairSession extends WrappedRunnable implements IEndpointStateChangeSubscriber, IFailureDetectionEventListener
     {
         private final String sessionName;
         private final String tablename;
---------------
-------------
@@ -53,7 +53,7 @@
   /** Acts like LetterTokenizer. */
   // the ugly regex below is incomplete Unicode 5.2 [:Letter:]
   public static final CharacterRunAutomaton SIMPLE =
-    new CharacterRunAutomaton(new RegExp("[A-Za-zªµºÀ-ÖØ-öø-Ｚ]+").toAutomaton());
+new CharacterRunAutomaton(new RegExp("[A-Za-zªµºÀ-ÖØ-öø-ˁ]+").toAutomaton());
 
   private final CharacterRunAutomaton runAutomaton;
   private final boolean lowerCase;
---------------
-------------
@@ -302,7 +302,7 @@
           if (next != null) {
             next.close();
           }
-        } catch (Exception ioe) {
+} catch (IOException ioe) {
           // keep first IOException we hit but keep
           // closing the rest
           if (err == null) {
---------------
-------------
@@ -57,7 +57,7 @@
     private AdminService adminService = null;
     private ServiceFactoryStub stub;
     private String serviceName = StorageStub.SERVICE_TYPE_RETURN;
-    private static File incomingFeed = new File("src/test/org/apache/lucene/gdata/server/registry/TestEntityBuilderIncomingFeed.xml");
+private static File incomingFeed = new File("src/core/src/test/org/apache/lucene/gdata/server/registry/TestEntityBuilderIncomingFeed.xml");
     BufferedReader reader;
     static{
         
---------------
-------------
@@ -99,7 +99,7 @@
   }
 
   /** Releases exclusive access. */
-  public abstract void release();
+public abstract void release() throws IOException;
 
   /** Returns true if the resource is currently locked.  Note that one must
    * still call {@link #obtain()} before using the resource. */
---------------
-------------
@@ -64,7 +64,7 @@
       query.add(booleanQuery1, BooleanClause.Occur.MUST);
       query.add(new TermQuery(new Term(FIELD, "9")), BooleanClause.Occur.MUST_NOT);
 
-      IndexSearcher indexSearcher = new IndexSearcher(directory);
+IndexSearcher indexSearcher = new IndexSearcher(directory, true);
       ScoreDoc[] hits = indexSearcher.search(query, null, 1000).scoreDocs;
       assertEquals("Number of matched documents", 2, hits.length);
 
---------------
-------------
@@ -122,7 +122,7 @@
     this.realPrefixLength = prefixLength > termLength ? termLength : prefixLength;
     // if minSimilarity >= 1, we treat it as number of edits
     if (minSimilarity >= 1f) {
-      this.minSimilarity = 1 - (minSimilarity+1) / this.termLength;
+this.minSimilarity = 0; // just driven by number of edits
       maxEdits = (int) minSimilarity;
       raw = true;
     } else {
---------------
-------------
@@ -63,7 +63,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), a, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -73,7 +73,7 @@
 
 			if (firstTime) {
 				Logs.reportString("----------------------------------------------");
-				Logs.reportMessage("CSLOOK_IndexesHeader");
+Logs.reportMessage("DBLOOK_IndexesHeader");
 				Logs.reportString("----------------------------------------------\n");
 			}
 
---------------
-------------
@@ -823,7 +823,7 @@
      * @param connAttrs connection attributes
      * @return
      */
-    public static Properties getDataSourcePropertiesForDatabase
+private static Properties getDataSourcePropertiesForDatabase
     	(String databaseName, String connAttrs) 
     {
         Properties attrs = new Properties();
---------------
-------------
@@ -226,7 +226,7 @@
         		{"XSLA1","Log Record has been sent to the stream, but it cannot be applied to the store (Object {0}).  This may cause recovery problems also.","45000"},
         		{"XSLA2","System will shutdown, got I/O Exception while accessing log file.","45000"},
         		{"XSLA3","Log Corrupted, has invalid data in the log stream.","45000"},
-        		{"XSLA4","Cannot write to the log, most likely the log is full.  Please delete unnecessary files.  It is also possible that the file system is read only, or the disk has failed, or some other problems with the media.  ","45000"},
+{"XSLA4","Error encountered when attempting to write the transaction recovery log. Most likely the disk holding the recovery log is full. If the disk is full, the only way to proceed is to free up space on the disk by either expanding it or deleting files not related to Derby. It is also possible that the file system and/or disk where the Derby transaction log resides is read-only. The error can also be encountered if the disk or file system has failed.","45000"},
         		{"XSLA5","Cannot read log stream for some reason to rollback transaction {0}.","45000"},
         		{"XSLA6","Cannot recover the database.","45000"},
         		{"XSLA7","Cannot redo operation {0} in the log.","45000"},
---------------
-------------
@@ -263,7 +263,7 @@
     CategoryDocumentBuilder builder = new CategoryDocumentBuilder(tw, iParams);
     builder.setCategoryPaths(categories);
     builder.build(d);
-    d.add(new Field("content", content, TextField.TYPE_STORED));
+d.add(new TextField("content", content, Field.Store.YES));
     iw.addDocument(d);
   }
   
---------------
-------------
@@ -110,7 +110,7 @@
 
   @Override
   public boolean equals(Object o) {
-    return o.getClass() == OrdFieldSource.class && this.field.equals(((OrdFieldSource)o).field);
+return o != null && o.getClass() == OrdFieldSource.class && this.field.equals(((OrdFieldSource)o).field);
   }
 
   private static final int hcode = OrdFieldSource.class.hashCode();
---------------
-------------
@@ -77,7 +77,7 @@
     conf.setCombinerClass(BayesWeightSummerReducer.class);
     conf.setReducerClass(BayesWeightSummerReducer.class);    
     conf.setOutputFormat(BayesWeightSummerOutputFormat.class);
-    FileSystem dfs = FileSystem.get(conf);
+FileSystem dfs = FileSystem.get(outPath.toUri(), conf);
     if (dfs.exists(outPath))
       dfs.delete(outPath, true);
     client.setConf(conf);
---------------
-------------
@@ -90,7 +90,7 @@
 
   /** single Judgement of a trec quality query */
   private static class QRelJudgement {
-    private String queryID;
+String queryID;
     private HashMap<String,String> relevantDocs;
     
     QRelJudgement(String queryID) {
---------------
-------------
@@ -32,7 +32,7 @@
     <li> {@link WildcardQuery}
     <li> {@link PhraseQuery}
     <li> {@link PrefixQuery}
-    <li> {@link PhrasePrefixQuery}
+<li> {@link MultiPhraseQuery}
     <li> {@link FuzzyQuery}
     <li> {@link RangeQuery}
     <li> {@link org.apache.lucene.search.spans.SpanQuery}
---------------
-------------
@@ -186,7 +186,7 @@
     @Override
     public void reflectWith(AttributeReflector reflector) {
       fillBytesRef();
-      reflector.reflect(TermToBytesRefAttribute.class, "bytes", bytes);
+reflector.reflect(TermToBytesRefAttribute.class, "bytes", new BytesRef(bytes));
       reflector.reflect(NumericTermAttribute.class, "shift", shift);
       reflector.reflect(NumericTermAttribute.class, "rawValue", getRawValue());
       reflector.reflect(NumericTermAttribute.class, "valueSize", valueSize);
---------------
-------------
@@ -197,7 +197,7 @@
     
     addCommit(uReq, cmd);
     
-    log.debug("Distrib commit to:" + nodes + " params:" + params);
+log.debug("Distrib commit to: {} params: {}", nodes, params);
     
     for (Node node : nodes) {
       submit(new Req(cmd.toString(), node, uReq, false));
---------------
-------------
@@ -61,7 +61,7 @@
     writer.forceMerge(1);
 
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     searcher = newSearcher(reader);
 
   }
---------------
-------------
@@ -226,7 +226,7 @@
       Field content = newField("content", "", TextField.TYPE_UNSTORED);
       doc.add(content);
       docNoGroup.add(content);
-      NumericField id = new NumericField("id");
+NumericField id = new NumericField("id", NumericField.DataType.INT);
       doc.add(id);
       docNoGroup.add(id);
       final GroupDoc[] groupDocs = new GroupDoc[numDocs];
---------------
-------------
@@ -116,7 +116,7 @@
 
   private final int readerIndex(int n) {	  // find reader for doc n:
     int lo = 0;					  // search starts array
-    int hi = readers.length - 1                   // for first element less
+int hi = readers.length - 1;                  // for first element less
 
     while (hi >= lo) {
       int mid = (lo + hi) >> 1;
---------------
-------------
@@ -376,7 +376,7 @@
   public void testContextSensitiveCollate() throws Exception {
     //                     DirectSolrSpellChecker   IndexBasedSpellChecker
     String[] dictionary = {"direct",                "default_teststop" };
-    for(int i=0 ; i<1 ; i++) {
+for(int i=0 ; i<=1 ; i++) {
       assertQ(
         req(
           "q", "teststop:(flew AND form AND heathrow)",
---------------
-------------
@@ -59,7 +59,7 @@
     CBayesThetaNormalizerDriver normalizer = new CBayesThetaNormalizerDriver();
     normalizer.runJob(input, output, params);
     
-    if (Boolean.parseBoolean(params.get("skipCleanup"))) {
+if (params.isSkipCleanup()) {
       return;
     }
     
---------------
-------------
@@ -57,7 +57,7 @@
      * returns an iterator that returns columns from the given SSTable
      * matching the Filter criteria in sorted order.
      */
-    public abstract IColumnIterator getSSTableColumnIterator(SSTableReader sstable, String key);
+public abstract IColumnIterator getSSTableColumnIterator(SSTableReader sstable, DecoratedKey key);
 
     /**
      * collects columns from reducedColumns into returnCF.  Termination is determined
---------------
-------------
@@ -84,7 +84,7 @@
   private static File convertGLFile(File originalFile, boolean ratings) throws IOException {
     // Now translate the file; remove commas, then convert "::" delimiter to comma
     File resultFile = new File(new File(System.getProperty("java.io.tmpdir")),
-                                     "taste." + (ratings ? "ratings" : "movies") + ".txt");
+(ratings ? "ratings" : "movies") + ".txt");
     if (!resultFile.exists()) {
       PrintWriter writer = null;
       try {
---------------
-------------
@@ -348,7 +348,7 @@
           doc = docMap[doc];                      // map around deletions
         doc += base;                              // convert to merged space
 
-        if (lastDoc != 0 && doc <= lastDoc)
+if (doc < 0 || (df > 0 && doc <= lastDoc))
           throw new IllegalStateException("docs out of order (" + doc +
               " <= " + lastDoc + " )");
 
---------------
-------------
@@ -682,7 +682,7 @@
         }
 
         for (RandomDoc otherSideDoc : otherMatchingDocs) {
-          DocsEnum docsEnum = MultiFields.getTermDocsEnum(topLevelReader, MultiFields.getLiveDocs(topLevelReader), "id", new BytesRef(otherSideDoc.id), false);
+DocsEnum docsEnum = MultiFields.getTermDocsEnum(topLevelReader, MultiFields.getLiveDocs(topLevelReader), "id", new BytesRef(otherSideDoc.id), 0);
           assert docsEnum != null;
           int doc = docsEnum.nextDoc();
           expectedResult.set(doc);
---------------
-------------
@@ -61,7 +61,7 @@
 	present in the page data array.  It is accessed directly by the
 	FileContainer.  Any change made to the borrowed space is not managed or
 	seen by the allocation page.
-	<P
+<P>
 	The reason for having this borrowed space is so that the container header
 	does not need to have a page of its own.
 
---------------
-------------
@@ -39,7 +39,7 @@
 
   @Override
   public InputStream getResourceStream(String template_name) throws ResourceNotFoundException {
-    return loader.openResource(template_name);
+return loader.openResource("velocity/" + template_name);
   }
 
   @Override
---------------
-------------
@@ -251,7 +251,7 @@
       writer.flush();
     } finally {
       if (shouldClose) {
-        Closeables.closeQuietly(writer);
+Closeables.close(writer, true);
       }
     }
 
---------------
-------------
@@ -28,7 +28,7 @@
 public final class FuzzyQuery extends MultiTermQuery {
   
   public final static float defaultMinSimilarity = 0.5f;
-  public final static int defaultPrefixLength = 2;
+public final static int defaultPrefixLength = 0;
   
   private float minimumSimilarity;
   private int prefixLength;
---------------
-------------
@@ -386,7 +386,7 @@
     }
 
     @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) {
+public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) {
       return new RAMDocsEnum(ramField.termToDocs.get(current), liveDocs);
     }
 
---------------
-------------
@@ -77,7 +77,7 @@
     
     try {
       bytes = WovenProxyGenerator.getWovenProxy(wovenClass.getBytes(),
-          wovenClass.getClassName(), wovenClass.getBundleWiring().getClassLoader());
+wovenClass.getBundleWiring().getClassLoader());
       
     } catch (Exception e) {
       if(e instanceof RuntimeException && 
---------------
-------------
@@ -570,7 +570,7 @@
   /** Returns true if <code>o</code> is equal to this. */
   @Override
   public boolean equals(Object o) {
-    if (SpatialDistanceQuery.class != o.getClass()) return false;
+if (!super.equals(o)) return false;
     SpatialDistanceQuery other = (SpatialDistanceQuery)o;
     return     this.latCenter == other.latCenter
             && this.lonCenter == other.lonCenter
---------------
-------------
@@ -32,7 +32,7 @@
 public class RangeSliceVerbHandler implements IVerbHandler
 {
 
-    private static final Logger logger = LoggerFactory.getLogger(IndexScanVerbHandler.class);
+private static final Logger logger = LoggerFactory.getLogger(RangeSliceVerbHandler.class);
 
     public void doVerb(Message message)
     {
---------------
-------------
@@ -115,7 +115,7 @@
         // perform compatibility init
         cores = new CoreContainer(new SolrResourceLoader(instanceDir));
         SolrConfig cfg = solrConfigFilename == null ? new SolrConfig() : new SolrConfig(solrConfigFilename);
-        CoreDescriptor dcore = new CoreDescriptor(cores, "", cfg.getResourceLoader().getInstanceDir());
+CoreDescriptor dcore = new CoreDescriptor(cores, "", ".");
         SolrCore singlecore = new SolrCore(null, null, cfg, null, dcore);
         abortOnConfigurationError = cfg.getBool(
                 "abortOnConfigurationError", abortOnConfigurationError);
---------------
-------------
@@ -110,7 +110,7 @@
 			majority_ = (responseCount >> 1) + 1;  
 		}
 		
-		public void response(Message message)
+public synchronized void response(Message message)
 		{
 			if (logger_.isDebugEnabled())
 			  logger_.debug("Received responses in DataRepairHandler : " + message.toString());
---------------
-------------
@@ -164,7 +164,7 @@
       SolrException.log(log, e);
     }
     try {
-      ExecutorUtil.shutdownAndAwaitTermination(commExecutor);
+ExecutorUtil.shutdownNowAndAwaitTermination(commExecutor);
     } catch (Throwable e) {
       SolrException.log(log, e);
     }
---------------
-------------
@@ -57,7 +57,7 @@
     {
 		super(
             xactFactory, logFactory, dataFactory, dataValueFactory, 
-            false, null);
+false, null, false);
 
 		// always want to hold latches & containers open past the commit/abort
 		setPostComplete();
---------------
-------------
@@ -103,7 +103,7 @@
       // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date
       final List<? extends FakeDeleteAtomicIndexReader> sr = input.getSequentialSubReaders();
       w.addIndexes(sr.toArray(new IndexReader[sr.size()])); // TODO: maybe take List<IR> here?
-      w.close();
+w.shutdown();
     }
     System.err.println("Done.");
   }
---------------
-------------
@@ -303,7 +303,7 @@
 		// current plans using "this" node as the key.  If needed, we'll
 		// then make the call to revert the plans in OptimizerImpl's
 		// getNextDecoratedPermutation() method.
-		addOrLoadBestPlanMapping(true, this);
+updateBestPlanMap(ADD_PLAN, this);
 
 		/* If the childResult is instanceof Optimizable, then we optimizeIt.
 		 * Otherwise, we are going into a new query block.  If the new query
---------------
-------------
@@ -152,7 +152,7 @@
       slf4jImpl = StaticLoggerBinder.getSingleton().getLoggerFactoryClassStr();
       log.info("SLF4J impl is " + slf4jImpl);
       if (fname == null) {
-        if (slf4jImpl.indexOf("Log4j") > 0) {
+if ("org.slf4j.impl.Log4jLoggerFactory".equals(slf4jImpl)) {
           fname = "Log4j";
         } else if (slf4jImpl.indexOf("JDK") > 0) {
           fname = "JUL";
---------------
-------------
@@ -95,7 +95,7 @@
   }
 
   @Override
-  public long size() throws IOException {
+public long size() {
     return -1;
   }
 
---------------
-------------
@@ -115,7 +115,7 @@
 
                 if (!Arrays.equals(ColumnFamily.digest(row_.cf), digest))
                 {
-                    IResponseResolver<Row> readResponseResolver = new ReadResponseResolver(table_, replicas_.size());
+IResponseResolver<Row> readResponseResolver = new ReadResponseResolver(table_);
                     IAsyncCallback responseHandler;
                     if (replicas_.contains(FBUtilities.getLocalAddress()))
                         responseHandler = new DataRepairHandler(row_, replicas_.size(), readResponseResolver);
---------------
-------------
@@ -49,7 +49,7 @@
       @Override
       public void evaluate() throws Throwable {
         if (NestedTestSuite.class.isAssignableFrom(d.getTestClass())) {
-          LuceneTestCase.assumeTrue("Nested suite class ignored (started as stand-along).",
+LuceneTestCase.assumeTrue("Nested suite class ignored (started as stand-alone).",
               isRunningNested());
         }
         s.evaluate();
---------------
-------------
@@ -409,7 +409,7 @@
 		if (bestCD.isIndex())
 		{
 			columnPosition = bestCD.getIndexDescriptor().
-			  getKeyColumnPosition(new Integer(columnPosition)).intValue();
+getKeyColumnPosition(columnPosition);
 
 			if (SanityManager.DEBUG)
 			{
---------------
-------------
@@ -92,7 +92,7 @@
   /** If the total number of positions (summed across all docs
    *  for this term) is <= maxPositions, then the postings are
    *  inlined into terms dict */
-  public PulsingPostingsWriter(int maxPositions, PostingsWriterBase wrappedPostingsWriter) throws IOException {
+public PulsingPostingsWriter(int maxPositions, PostingsWriterBase wrappedPostingsWriter) {
     pending = new Position[maxPositions];
     for(int i=0;i<maxPositions;i++) {
       pending[i] = new Position();
---------------
-------------
@@ -32,7 +32,7 @@
     Directory dir = newDirectory();
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
     writer.commit();
-    writer.close();
+writer.shutdown();
     IndexReader ir = DirectoryReader.open(dir);
     Dictionary dictionary = new HighFrequencyDictionary(ir, "bogus", 0.1f);
     BytesRefIterator tf = dictionary.getEntryIterator();
---------------
-------------
@@ -135,7 +135,7 @@
                 if (cached != null)
                 {
                     QueryFilter keyFilter = new QueryFilter(key, filter.path, filter.filter);
-                    returnCF = cfs.filterColumnFamily(cached, keyFilter, cfs.metadata.gcGraceSeconds);
+returnCF = cfs.filterColumnFamily(cached, keyFilter, gcBefore);
                 }
                 else
                 {
---------------
-------------
@@ -3204,7 +3204,7 @@
                 "where c.referenceid = t.tableid and t.tablename='D3175'");
         JDBC.assertUnorderedResultSet(rs, new String[][]{
                     {"X", "1", "VARCHAR(12)", null, null, null, null, "D3175", "T", "R"},
-                    {"ID", "2", "INTEGER NOT NULL", "GENERATED_BY_DEFAULT", "22", "1", "1", "D3175", "T", "R"}
+{"ID", "2", "INTEGER NOT NULL", "GENERATED_BY_DEFAULT", "3", "1", "1", "D3175", "T", "R"}
                 });
     }
 
---------------
-------------
@@ -287,6 +287,6 @@
     Vector pdf = classifier.classify(new DenseVector(2));
     assertEquals("[0,0]", "[0.333, 0.333, 0.333]", AbstractCluster.formatVector(pdf, null));
     pdf = classifier.classify(new DenseVector(2).assign(2));
-    assertEquals("[2,2]", "[0.545, 0.273, 0.182]", AbstractCluster.formatVector(pdf, null));
+assertEquals("[2,2]", "[0.429, 0.429, 0.143]", AbstractCluster.formatVector(pdf, null));
   }
 }
---------------
-------------
@@ -66,7 +66,7 @@
   public void testLongPostings() throws Exception {
     // Don't use _TestUtil.getTempDir so that we own the
     // randomness (ie same seed will point to same dir):
-    Directory dir = newFSDirectory(new File(LuceneTestCase.TEMP_DIR, "longpostings" + "." + random.nextLong()));
+Directory dir = newFSDirectory(_TestUtil.getTempDir("longpostings" + "." + random.nextLong()));
 
     final int NUM_DOCS = (int) ((TEST_NIGHTLY ? 4e6 : (RANDOM_MULTIPLIER*2e4)) * (1+random.nextDouble()));
 
---------------
-------------
@@ -62,7 +62,7 @@
      * The maximum length in bytes for strings sent by {@code writeLDString()},
      * which is the maximum unsigned integer value that fits in two bytes.
      */
-    private final static int MAX_VARCHAR_BYTE_LENGTH = 0xFFFF;
+final static int MAX_VARCHAR_BYTE_LENGTH = 0xFFFF;
 
     /**
      * Output buffer.
---------------
-------------
@@ -175,7 +175,7 @@
     public final boolean skipTo(int target)
 	throws IOException
     {
-	while (target > _termPositionsQueue.peek().doc())
+while (_termPositionsQueue.peek() != null && target > _termPositionsQueue.peek().doc())
 	{
 	    TermPositions tp = (TermPositions)_termPositionsQueue.pop();
 
---------------
-------------
@@ -166,7 +166,7 @@
         //GSI: I suppose we could toString the payload, but I don't think that would be a good idea 
         payloadBoost.setDescription("scorePayload(...)");
         result.setValue(nonPayloadExpl.getValue() * avgPayloadScore);
-        result.setDescription("btq");
+result.setDescription("btq, product of:");
         return result;
       }
     }
---------------
-------------
@@ -17,7 +17,7 @@
 package org.apache.solr.schema;
 
 import org.apache.lucene.search.SortField;
-import org.apache.lucene.search.function.ValueSource;
+import org.apache.solr.search.function.ValueSource;
 import org.apache.lucene.document.Field;
 import org.apache.solr.util.BCDUtils;
 import org.apache.solr.request.XMLWriter;
---------------
-------------
@@ -147,7 +147,7 @@
      * explicitly setting it to {@code false}.
      */
     public static Test suite() {
-        String property = "derby.storage.indexStats.debug.forceOldBehavior";
+String property = "derby.storage.indexStats.debug.keepDisposableStats";
         TestSuite suite = new TestSuite("KeepDisposableStatsPropertyTestSuite");
         // Test the default (expected to be false).
         suite.addTest(
---------------
-------------
@@ -85,7 +85,7 @@
   }
 
   public void testSubclassConcurrentMergeScheduler() throws IOException {
-    MockDirectoryWrapper dir = newDirectory();
+MockDirectoryWrapper dir = newMockDirectory();
     dir.failOn(new FailOnlyOnMerge());
 
     Document doc = new Document();
---------------
-------------
@@ -296,7 +296,7 @@
                                 }
                             }
                         };
-                        timeoutFuture = executors.schedule(r, 10, TimeUnit.SECONDS);
+timeoutFuture = executors.schedule(r, timeout, TimeUnit.MILLISECONDS);
                         state = State.WaitForInitialReferences;
                         break;
                     case WaitForInitialReferences:
---------------
-------------
@@ -39,7 +39,7 @@
    *
    */
   public void init(NamedList args) {
-    Integer v = (Integer)args.get("termInfosIndexDivisor");
+Integer v = (Integer)args.get("setTermIndexInterval");
     if (v != null) {
       termInfosIndexDivisor = v.intValue();
     }
---------------
-------------
@@ -54,7 +54,7 @@
   }
   
   @AfterClass
-  public static void afterClass() throws Exception {
+public static void afterClass() {
     queryConverter = null;
   }
 
---------------
-------------
@@ -542,7 +542,7 @@
 						+ " " + se.getErrorCode());
 			}
 		}
-		if (e.getMessage().equals(null)) {
+if (e.getMessage() == null) {
 			System.out.println(getThreadName()
 					+ " dbUtil --> NULL error message detected");
 			System.out
---------------
-------------
@@ -326,7 +326,7 @@
     }
 
     @Override
-    public boolean incrementToken() throws IOException {
+public boolean incrementToken() {
       if (tokenIterator.hasNext()) {
         clearAttributes();
         AttributeSource next = tokenIterator.next();
---------------
-------------
@@ -97,7 +97,7 @@
         // set the ldap properties
         setDatabaseProperty("derby.connection.requireAuthentication", "true", conn);
         setDatabaseProperty("derby.authentication.provider", "LDAP", conn);
-        setDatabaseProperty("derby.authentication.server", "noSuchServer", conn);
+setDatabaseProperty("derby.authentication.server", "noSuchServer.invalid", conn);
         setDatabaseProperty("derby.authentication.ldap.searchBase", "o=dnString", conn);
         setDatabaseProperty("derby.authentication.ldap.searchFilter","(&(objectClass=inetOrgPerson)(uid=%USERNAME%))", conn);
         commit();
---------------
-------------
@@ -52,7 +52,7 @@
     public static final String TABLE1 = "Keyspace1";
     public static final String CF1 = "Indexed1";
     public static final String CF2 = "Standard1";
-    public static final ByteBuffer COLUMN = ByteBuffer.wrap("birthdate".getBytes());
+public static final ByteBuffer COLUMN = ByteBufferUtil.bytes("birthdate");
     public static final ByteBuffer VALUE = ByteBuffer.allocate(8);
     static
     {
---------------
-------------
@@ -384,7 +384,7 @@
 		}
 		else // StandardException or run time exception, log it first
 		{
-			String info = LocalizedResource.getMessage("IJ_01SeeClouLog", t.toString(), t.getMessage());
+String info = LocalizedResource.getMessage("IJ_01SeeLog", t.toString(), t.getMessage());
 			//		t.printStackTrace(System.out);
 			throw new ijException(info);
 		}
---------------
-------------
@@ -340,7 +340,7 @@
         cause = stacktraces.next();
       // RuntimeException instead of IOException because
       // super() does not throw IOException currently:
-      throw new RuntimeException("MockRAMDirectory: cannot close: there are still open files: " + openFiles, cause);
+throw new RuntimeException("MockDirectoryWrapper: cannot close: there are still open files: " + openFiles, cause);
     }
     open = false;
     delegate.close();
---------------
-------------
@@ -55,7 +55,7 @@
  */
 
 /** A clause in a BooleanQuery. */
-public final class BooleanClause {
+public class BooleanClause {
   /** The query whose matching documents are combined by the boolean query. */
   public Query query;
   /** If true, documents documents which <i>do not</i>
---------------
-------------
@@ -794,7 +794,7 @@
       if (!core.isReloaded() && ulog != null) {
         // disable recovery in case shard is in construction state (for shard splits)
         Slice slice = getClusterState().getSlice(collection, shardId);
-        if (!Slice.CONSTRUCTION.equals(slice.getState()) && !isLeader) {
+if (!Slice.CONSTRUCTION.equals(slice.getState()) || !isLeader) {
           Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()
               .getUpdateLog().recoverFromLog();
           if (recoveryFuture != null) {
---------------
-------------
@@ -249,7 +249,7 @@
     Set<Node> nodes = deletes.keySet();
     for (Node node : nodes) {
       List<DeleteRequest> dlist = deletes.get(node);
-      if (dlist == null || dlist.size() < limit) return false;
+if (dlist == null || dlist.size() < limit) continue;
       UpdateRequestExt ureq = new UpdateRequestExt();
       
       ModifiableSolrParams combinedParams = new ModifiableSolrParams();
---------------
-------------
@@ -302,7 +302,7 @@
      * and compacted sstables. Files that cannot be recognized will be ignored.
      * @return A list of Descriptors that were removed.
      */
-    static void scrubDataDirectories(String table, String columnFamily)
+public static void scrubDataDirectories(String table, String columnFamily)
     {
         for (Map.Entry<Descriptor,Set<Component>> sstableFiles : files(table, columnFamily, true).entrySet())
         {
---------------
-------------
@@ -142,7 +142,7 @@
     Path outPath = new Path(outputTmpBasePath.getParent(), "productWith");
     JobConf conf = MatrixMultiplicationJob.createMatrixMultiplyJobConf(rowPath, other.rowPath, outPath, other.numCols);
     JobClient.runJob(conf);
-    DistributedRowMatrix out = new DistributedRowMatrix(outPath, outputTmpPath, numRows, other.numCols());
+DistributedRowMatrix out = new DistributedRowMatrix(outPath, outputTmpPath, numCols, other.numCols());
     out.configure(conf);
     return out;
   }
---------------
-------------
@@ -78,7 +78,7 @@
         continue;
       }
       long df = dictionary.get(e.index());
-      if (df / vectorCount > maxDfPercent) {
+if (df * 100.0 / vectorCount > maxDfPercent) {
         continue;
       }
       if (df < minDf) {
---------------
-------------
@@ -94,7 +94,7 @@
           
           synchronized (this) {
             if (detachedManager == null) {
-              detachedManager = temp;
+detachedManager = new SynchronizedEntityManagerWrapper(temp);
               temp = null;
             }
           }
---------------
-------------
@@ -54,7 +54,7 @@
   @Override
   protected void setup(Context context) throws IOException, InterruptedException {
     super.setup(context);
-    Parameters params = Parameters.fromString(context.getConfiguration().get(PFPGrowth.PFP_PARAMETERS, ""));
+Parameters params = new Parameters(context.getConfiguration().get(PFPGrowth.PFP_PARAMETERS, ""));
     splitter = Pattern.compile(params.get(PFPGrowth.SPLIT_PATTERN, PFPGrowth.SPLITTER.toString()));
   }
 }
---------------
-------------
@@ -28,7 +28,7 @@
 import org.apache.lucene.search.spell.TermFreqIterator;
 import org.apache.lucene.search.suggest.Lookup;
 import org.apache.lucene.search.suggest.SortedTermFreqIteratorWrapper;
-import org.apache.lucene.search.suggest.fst.Sort.ByteSequencesWriter;
+import org.apache.lucene.search.suggest.Sort.ByteSequencesWriter;
 import org.apache.lucene.store.ByteArrayDataInput;
 import org.apache.lucene.store.ByteArrayDataOutput;
 import org.apache.lucene.store.InputStreamDataInput;
---------------
-------------
@@ -48,7 +48,7 @@
    * When the useWhiteList parameter is set to true then accept the token if its type is contained in the stopTypes
    */
   @Override
-  protected boolean accept() throws IOException {
+protected boolean accept() {
     return useWhiteList == stopTypes.contains(typeAttribute.type());
   }
 }
---------------
-------------
@@ -593,7 +593,7 @@
                 // look for first character
                 byte[] b;
                 try
-                { // pattern is not necessarily a cloudscape Blob
+{ // pattern is not necessarily a Derby Blob
                     b = pattern.getBytes(1,1);
                 }
                 catch (SQLException e)
---------------
-------------
@@ -51,6 +51,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new DanishAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new DanishAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -70,7 +70,7 @@
     
     reader = writer.getReader();
     searcher = newSearcher(reader);
-    writer.close();
+writer.shutdown();
   }
   
   @AfterClass
---------------
-------------
@@ -65,7 +65,7 @@
     BufferedReader d = new BufferedReader(new InputStreamReader(
         TestParser.class.getResourceAsStream("reuters21578.txt"), "US-ASCII"));
     dir = newDirectory();
-    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(Version.LUCENE_40, analyzer));
+IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
     String line = d.readLine();
     while (line != null) {
       int endOfDate = line.indexOf('\t');
---------------
-------------
@@ -37,7 +37,7 @@
     final Directory directory = new RAMDirectory();
     RAMDirectoryFactory factory = new RAMDirectoryFactory()  {
       @Override
-      protected Directory create(String path) throws IOException {
+protected Directory create(String path) {
         return directory;
       }
     };
---------------
-------------
@@ -80,7 +80,7 @@
         Calendar cal = Calendar.getInstance();
         // Make sure that we are not so close to midnight that TODAY might be yesterday before
         // we are finished using it.
-        while( cal.get( Calendar.HOUR) == 23 && cal.get( Calendar.MINUTE) == 58)
+while( cal.get( Calendar.HOUR) == 23 && cal.get( Calendar.MINUTE) >= 58)
         {
             try
             {
---------------
-------------
@@ -572,7 +572,7 @@
         // noon->NULL by inner function
         // NULL->NULL by outer due to RETURN NULL ON NULL INPUT
         ps.setTime(1, noon); // noon->NULL->NULL
-        JDBC.assertSingleValueResultSet(ps.executeQuery(), "11:00:00");        
+JDBC.assertSingleValueResultSet(ps.executeQuery(), null);
         ps.setTime(1, null); // NULL->11:00:00->11:30:00
         JDBC.assertSingleValueResultSet(ps.executeQuery(), "11:30:00");
 
---------------
-------------
@@ -49,7 +49,7 @@
   public Object transformRow(Map<String, Object> aRow, Context context) {
 
     for (Map<String, String> map : context.getAllEntityFields()) {
-      Locale locale = Locale.getDefault();
+Locale locale = Locale.ROOT;
       String customLocale = map.get("locale");
       if(customLocale != null){
         locale = new Locale(customLocale);
---------------
-------------
@@ -54,7 +54,7 @@
 	<LI> ID CHAR(36) - not nullable.  Internal identifier of the compiled statement.
 	<LI> SCHEMANAME VARCHAR(128) - nullable.  Schema the statement was compiled in.
 	<LI> SQL_TEXT VARCHAR(32672) - not nullable.  Text of the statement
-	<LI> UNICODE BIT/BOOLEAN - not nullable.  True if the statement is compiled as a pure unicode string, false if it handled unicode escapes.
+<LI> UNICODE BIT/BOOLEAN - not nullable.  Always true.
 	<LI> VALID BIT/BOOLEAN - not nullable.  True if the statement is currently valid, false otherwise
 	<LI> COMPILED_AT TIMESTAMP nullable - time statement was compiled, requires STATISTICS TIMING to be enabled.
 
---------------
-------------
@@ -47,7 +47,7 @@
  */
 public class UpdateRequest extends AbstractUpdateRequest {
   
-  private static final String VER = "ver";
+public static final String VER = "ver";
   public static final String OVERWRITE = "ow";
   public static final String COMMIT_WITHIN = "cw";
   private Map<SolrInputDocument,Map<String,Object>> documents = null;
---------------
-------------
@@ -74,7 +74,7 @@
 		Vector	aggregateVector)
 			throws StandardException
 	{
-		super.bindExpression(fromList, subqueryList, 
+bindOperand(fromList, subqueryList,
 							 aggregateVector);
 
 		/* Set type info for this node */
---------------
-------------
@@ -55,7 +55,7 @@
     q = mfqp.parse("+one +two");
     assertEquals("+(b:one t:one) +(b:two t:two)", q.toString());
 
-    q = mfqp.parse("+one -two -three)");
+q = mfqp.parse("+one -two -three");
     assertEquals("+(b:one t:one) -(b:two t:two) -(b:three t:three)", q.toString());
     
     q = mfqp.parse("one^2 two");
---------------
-------------
@@ -95,7 +95,7 @@
     public static void announce(final UUID version, Set<InetAddress> hosts)
     {
         MessageProducer prod = new CachingMessageProducer(new MessageProducer() {
-            public Message getMessage(int protocolVersion) throws IOException
+public Message getMessage(Integer protocolVersion) throws IOException
             {
                 return makeVersionMessage(version, protocolVersion);
             }
---------------
-------------
@@ -34,7 +34,7 @@
  * <br>Example: <code>{!raw f=myfield}Foo Bar</code> creates <code>TermQuery(Term("myfield","Foo Bar"))</code>
  */
 public class RawQParserPlugin extends QParserPlugin {
-  public static String NAME = "raw";
+public static final String NAME = "raw";
 
   @Override
   public void init(NamedList args) {
---------------
-------------
@@ -119,7 +119,7 @@
         }
 
         @Override
-        public long getUniqueTermCount() throws IOException {
+public long size() throws IOException {
           return -1;
         }
       });
---------------
-------------
@@ -199,7 +199,7 @@
         if (skipDoc != 0 && skipDoc >= doc)
           numSkipped += skipInterval;
 
-        if ((count + numSkipped + skipInterval) > df)
+if ((count + numSkipped + skipInterval) >= df)
           break;                                  // no more skips
 
         skipDoc += skipStream.readVInt();
---------------
-------------
@@ -997,7 +997,7 @@
 
 	public static void main(String[] args) throws Throwable
 	{
-		int port = 9160;		
+int port = DatabaseDescriptor.getThriftPort();
 		try
 		{
 			CassandraServer peerStorageServer = new CassandraServer();
---------------
-------------
@@ -97,7 +97,7 @@
         }
 
         TopGroups<BytesRef>[] topGroupsArr = new TopGroups[topGroups.size()];
-        rb.mergedTopGroups.put(groupField, TopGroups.merge(topGroups.toArray(topGroupsArr), groupSort, sortWithinGroup, groupOffsetDefault, docsPerGroupDefault));
+rb.mergedTopGroups.put(groupField, TopGroups.merge(topGroups.toArray(topGroupsArr), groupSort, sortWithinGroup, groupOffsetDefault, docsPerGroupDefault, TopGroups.ScoreMergeMode.None));
       }
 
       for (String query : commandTopDocs.keySet()) {
---------------
-------------
@@ -948,7 +948,7 @@
         if (reconciler == null)
         {
             if (clockType == ClockType.Timestamp)    
-                reconciler = new TimestampReconciler(); // default
+reconciler = TimestampReconciler.instance; // default
             else
                 throw new ConfigurationException("No reconciler specified for column family " + cf_def.name);
 
---------------
-------------
@@ -33,7 +33,7 @@
  * a row trigger.  It is instantiated at execution time.
  * There is one per row trigger.
  */
-class RowTriggerExecutor extends GenericTriggerExecutor
+public class RowTriggerExecutor extends GenericTriggerExecutor
 {
 	/**
 	 * Constructor
---------------
-------------
@@ -64,7 +64,7 @@
     //-----------------------event callback methods-------------------------------
 
     public void listenToUnitOfWork() {
-        agent_.connection_.CommitAndRollbackListeners_.put(this,null);
+agent_.connection_.CommitAndRollbackListeners_.add(this);
     }
 
     public void completeLocalCommit(java.util.Iterator listenerIterator) {
---------------
-------------
@@ -45,7 +45,7 @@
     @Test
     public void testFragmentCreation() throws Exception {
         Bundle exportBundle = makeBundleWithExports("export.bundle", "1.2.3",
-                "export.package;version=\"1.0.0\";singleton:=true");
+"export.package;version=\"1.0.0\";uses:=\"foo.jar,bar.jar\";singleton:=true");
 
         Dictionary fragmentHeaders = makeFragmentFromExportBundle(exportBundle)
                 .getHeaders();
---------------
-------------
@@ -756,7 +756,7 @@
             // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?
             // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)
             // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?
-            docsEnum = termsEnum.docs(null, docsEnum, 0);
+docsEnum = termsEnum.docs(null, docsEnum, false);
             c=0;
 
             if (docsEnum instanceof MultiDocsEnum) {
---------------
-------------
@@ -70,7 +70,7 @@
         requestedDefault = null;
     }
     
-    private void setDefault(final TimeZone tz) throws SecurityException{
+public static void setDefault(final TimeZone tz) {
         if (tz== null) {
             throw new IllegalArgumentException("tz cannot be <null>");
         }
---------------
-------------
@@ -203,7 +203,7 @@
           bottomSameReader = true;
           readerGen[bottomSlot] = currentReaderGen;
         } else {
-          final int index = termsIndex.lookupTerm(bottomValue, tempBR);
+final int index = termsIndex.lookupTerm(bottomValue);
           if (index < 0) {
             bottomOrd = -index - 2;
             bottomSameReader = false;
---------------
-------------
@@ -69,7 +69,7 @@
     if (input.incrementToken()) {
       payloadAttr.setPayload(new BytesRef(("pos: " + pos).getBytes()));
       int posIncr;
-      if (i % 2 == 1) {
+if (pos == 0 || i % 2 == 1) {
         posIncr = 1;
       } else {
         posIncr = 0;
---------------
-------------
@@ -240,7 +240,7 @@
     }
 
     public void testSetBufferSize() throws IOException {
-      File indexDir = new File(TEMP_DIR, "testSetBufferSize");
+File indexDir = _TestUtil.getTempDir("testSetBufferSize");
       MockFSDirectory dir = new MockFSDirectory(indexDir, random);
       try {
         IndexWriter writer = new IndexWriter(
---------------
-------------
@@ -32,7 +32,7 @@
   /** Creates a new MockCharFilterFactory */
   public MockCharFilterFactory(Map<String,String> args) {
     super(args);
-    remainder = getInt(args, "remainder", 0, false);
+remainder = requireInt(args, "remainder");
     if (!args.isEmpty()) {
       throw new IllegalArgumentException("Unknown parameters: " + args);
     }
---------------
-------------
@@ -86,7 +86,7 @@
         List<Token> keyTokens = new ArrayList<Token>();
         for (int i = 0; i < 5; i++) {
             endPointTokens.add(new StringToken(String.valueOf((char)('a' + i * 2))));
-            keyTokens.add(partitioner.getToken(String.valueOf((char)('a' + i * 2 + 1))));
+keyTokens.add(partitioner.getToken(String.valueOf((char)('a' + i * 2 + 1)).getBytes()));
         }
         for (String table : DatabaseDescriptor.getNonSystemTables())
             testGetEndpoints(tmd, strategy, endPointTokens.toArray(new Token[0]), keyTokens.toArray(new Token[0]), table);
---------------
-------------
@@ -31,7 +31,7 @@
 import org.apache.aries.application.management.BundleInfo;
 import org.apache.aries.application.management.ResolverException;
 import org.apache.aries.util.filesystem.IFile;
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 import org.apache.aries.util.manifest.ManifestProcessor;
 
 public class DeploymentMetadataFactoryImpl implements DeploymentMetadataFactory
---------------
-------------
@@ -183,7 +183,7 @@
         }
 
         StreamOutSession session = StreamOutSession.create(keyspace, LOCAL, null);
-        StreamOut.transferSSTables(session, ssTableReaders, ranges);
+StreamOut.transferSSTables(session, ssTableReaders, ranges, OperationType.BOOTSTRAP);
 
         session.await();
 
---------------
-------------
@@ -61,7 +61,7 @@
 
 public final class MessagingService implements MessagingServiceMBean
 {
-    private static final int version_ = 1;
+public static final int version_ = 1;
     //TODO: make this parameter dynamic somehow.  Not sure if config is appropriate.
     private SerializerType serializerType_ = SerializerType.BINARY;
 
---------------
-------------
@@ -92,7 +92,7 @@
         }
 
         public ByteBuffer next() {
-            return ByteBuffer.wrap(Integer.toString(i++).getBytes());
+return ByteBufferUtil.bytes(Integer.toString(i++));
         }
 
         public void remove() {
---------------
-------------
@@ -262,7 +262,7 @@
         int portNumber = Integer.parseInt(ast.getChild(1).getText());
         Tree idList = ast.getChild(0);
         
-        StringBuffer hostName = new StringBuffer();
+StringBuilder hostName = new StringBuilder();
         int idCount = idList.getChildCount(); 
         for (int idx = 0; idx < idCount; idx++)
         {
---------------
-------------
@@ -67,7 +67,7 @@
   // creates 8 fields with different options and does "duels" of fields against each other
   public void test() throws Exception {
     Directory dir = newDirectory();
-    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {
+Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {
       @Override
       protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
         Tokenizer tokenizer = new MockTokenizer(reader);
---------------
-------------
@@ -2043,7 +2043,7 @@
       Bits liveDocs = ar.getLiveDocs();
       int maxDoc = ar.maxDoc();
       for (int i = 0; i < maxDoc; i++) {
-        if (liveDocs.get(i)) {
+if (liveDocs == null || liveDocs.get(i)) {
           assertTrue(liveIds.remove(ar.document(i).get("id")));
         }
       }
---------------
-------------
@@ -75,7 +75,7 @@
   @Override
   public String getResolvedEntityAttribute(String name) {
     checkLimited();
-    return super.getResolvedEntityAttribute(name);
+return entity == null ? null : getVariableResolver().replaceTokens(entity.allAttributes.get(name));
   }
 
   @Override
---------------
-------------
@@ -82,7 +82,7 @@
       iw.addDocument(doc);
     }
     iw.forceMerge(1);
-    iw.close();
+iw.shutdown();
     r = DirectoryReader.open(dir);
     reader = getOnlySegmentReader(r);
     searcher = new IndexSearcher(reader);
---------------
-------------
@@ -88,7 +88,7 @@
   
   public void doTest(int[] docs) throws Exception {
     Directory dir = makeIndex();
-    IndexReader reader = IndexReader.open(dir);
+IndexReader reader = IndexReader.open(dir, true);
     for (int i = 0; i < docs.length; i++) {
       Document d = reader.document(docs[i], SELECTOR);
       d.get(MAGIC_FIELD);
---------------
-------------
@@ -68,7 +68,7 @@
             throw new DataImportHandlerException(DataImportHandlerException.SEVERE, "Invalid Locale specified for field: " + fld);
           }
         } else {
-          locale = Locale.getDefault();
+locale = Locale.ROOT;
         }
 
         Object val = row.get(srcCol);
---------------
-------------
@@ -267,7 +267,7 @@
       }
 
       @Override
-      protected Reader initReader(Reader reader) {
+protected Reader initReader(String fieldName, Reader reader) {
         return new LegacyHTMLStripCharFilter(CharReader.get(new BufferedReader(reader)));
       }
     };
---------------
-------------
@@ -124,7 +124,7 @@
     final Random random = new Random(random().nextLong());
     for (int i=0; i<iter; i++) {
       tenum.seekCeil(new BytesRef("val"));
-      tdocs = _TestUtil.docs(random, tenum, MultiFields.getLiveDocs(reader), tdocs, 0);
+tdocs = _TestUtil.docs(random, tenum, MultiFields.getLiveDocs(reader), tdocs, false);
       while (tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
         ret += tdocs.docID();
       }
---------------
-------------
@@ -153,7 +153,7 @@
     // lob length
     static final short LOBLENGTH = 4;
 
-    static final String UTF8ENCODING = "UTF8";
+public static final String UTF8ENCODING = "UTF8";
 
     private static final int OVERRIDE_TABLE_SIZE = 0xff;
 
---------------
-------------
@@ -25,7 +25,7 @@
   SingleFieldTestDb db1 = new SingleFieldTestDb(docs1, fieldName);
 
   public void normalTest1(String query, int[] expdnrs) throws Exception {
-    BooleanQueryTest bqt = new BooleanQueryTest( query, expdnrs, db1, fieldName, this,
+BooleanQueryTst bqt = new BooleanQueryTst( query, expdnrs, db1, fieldName, this,
                                                 new BasicQueryFactory(maxBasicQueries));
     bqt.setVerbose(verbose);
     bqt.doTest();
---------------
-------------
@@ -723,7 +723,7 @@
 											null,
 											(TableDescriptor) null,
 											null,
-											0, 0, false);
+0, 0);
 			rc.setColumnDescriptor(null, cd);
 			rc.setVirtualColumnId(index + 1);
 		}
---------------
-------------
@@ -57,7 +57,7 @@
  * This class will also maintain histograms of the load information
  * of other nodes in the cluster.
  */
-public final class StorageService implements IEndPointStateChangeSubscriber, StorageServiceMBean
+public class StorageService implements IEndPointStateChangeSubscriber, StorageServiceMBean
 {
     private static Logger logger_ = Logger.getLogger(StorageService.class);     
 
---------------
-------------
@@ -70,7 +70,7 @@
     doc.add(bar);
     
     for (int i = 0; i < 100; i++) {
-      bar.setValue("singleton");
+bar.setStringValue("singleton");
       writer.addDocument(doc);
     }
     
---------------
-------------
@@ -162,7 +162,7 @@
     coreProperties.putAll(defaultProperties);
     coreProperties.put(CORE_NAME, name);
     coreProperties.put(CORE_INSTDIR, instanceDir);
-    coreProperties.put(CORE_ABS_INSTDIR, convertToAbsolute(instanceDir, container.getSolrHome()));
+coreProperties.put(CORE_ABS_INSTDIR, convertToAbsolute(instanceDir, container.getCoreRootDirectory()));
 
     for (String propname : coreProps.stringPropertyNames()) {
 
---------------
-------------
@@ -631,7 +631,7 @@
       randomIOExceptionRateOnOpen = 0.0;
       if (DirectoryReader.indexExists(this)) {
         if (LuceneTestCase.VERBOSE) {
-          System.out.println("\nNOTE: MockDirectoryWrapper: now crash");
+System.out.println("\nNOTE: MockDirectoryWrapper: now crush");
         }
         crash(); // corrupt any unsynced-files
         if (LuceneTestCase.VERBOSE) {
---------------
-------------
@@ -89,7 +89,7 @@
     
     private static Logger logger_ = Logger.getLogger(MessagingService.class);
     
-    private static MessagingService messagingService_ = new MessagingService();
+private static volatile MessagingService messagingService_ = new MessagingService();
 
     private static final int MESSAGE_DESERIALIZE_THREADS = 4;
 
---------------
-------------
@@ -27,7 +27,7 @@
 
 
 public class ZkCmdExecutor {
-  private long retryDelay = 1300L; // 300 ms over for padding
+private long retryDelay = 1500L; // 500 ms over for padding
   private int retryCount;
   private List<ACL> acl = ZooDefs.Ids.OPEN_ACL_UNSAFE;
   
---------------
-------------
@@ -129,7 +129,7 @@
             // JSR169 support was only added with 10.1, so don't
             // run 10.0 to later upgrade if that's what our jvm is supporting.
             if (!(JDBC.vmSupportsJSR169() && 
-                (OLD_VERSIONS[i][0]==10) && (OLD_VERSIONS[i][1]==0))); 
+(OLD_VERSIONS[i][0]==10) && (OLD_VERSIONS[i][1]==0)))
             suite.addTest(UpgradeRun.suite(OLD_VERSIONS[i]));
         }
         
---------------
-------------
@@ -637,7 +637,7 @@
                     {
                         if (!subColumn.isMarkedForDelete() || subColumn.getLocalDeletionTime() > gcBefore)
                         {
-                            sc.addColumn(subColumn.name(), subColumn);
+sc.addColumn(subColumn);
                         }
                     }
                 }
---------------
-------------
@@ -775,7 +775,7 @@
   }
 
   public boolean anyDocValuesFields() {
-    for (FieldInfo fi : fieldInfos) {
+for (FieldInfo fi : this) {
       if (fi.hasDocValues()) { 
         return true;
       }
---------------
-------------
@@ -1013,7 +1013,7 @@
     osw.flush();
   }
   
-  private void readObject(java.io.ObjectInputStream in) throws IOException, ClassNotFoundException {
+private void readObject(java.io.ObjectInputStream in) throws IOException {
     InputStreamReader isr = new InputStreamReader(in, "UTF-8");
     this.deserializeFromStreamReader(isr);
   }
---------------
-------------
@@ -259,7 +259,7 @@
     }
 
     public String getUrl() {
-      return "http" + (isSSLMode() ? "s" : "") + "://127.0.0.1:" + port + "/solr";
+return buildUrl(port, "/solr");
     }
 
     public String getSchemaFile() {
---------------
-------------
@@ -78,7 +78,7 @@
 
         if (!StorageService.instance.isClientMode())
         {
-            cfs.snapshot(Table.getTimestampedSnapshotName(null));
+cfs.snapshot(Table.getTimestampedSnapshotName(cfs.columnFamily));
 
             CompactionManager.instance.getCompactionLock().lock();
             cfs.flushLock.lock();
---------------
-------------
@@ -1389,7 +1389,7 @@
 
         // getColumnFamily will check if CF exists for us
         String columnFamily = CliCompiler.getColumnFamily(statement, keyspacesMap.get(keySpace).cf_defs);
-        String rawColumName = statement.getChild(1).getText();
+String rawColumName = CliUtils.unescapeSQLString(statement.getChild(1).getText());
 
         CfDef cfDef = getCfDef(columnFamily);
 
---------------
-------------
@@ -154,7 +154,7 @@
 				&& operand.getTypeServices() == null)
 				return this;
 
-		super.bindExpression(fromList, subqueryList,
+bindOperand(fromList, subqueryList,
 				aggregateVector);
 
 		if (operatorType == SQRT || operatorType == ABSOLUTE)
---------------
-------------
@@ -437,7 +437,7 @@
 
   // Inherit javadoc
   public IndexInput openInput(String name) throws IOException {
-    return new FSIndexInput(new File(directory, name));
+return openInput(name, BufferedIndexInput.BUFFER_SIZE);
   }
 
   // Inherit javadoc
---------------
-------------
@@ -89,7 +89,7 @@
     {
         Cassandra.Client client = getClient();
 
-        String key_user_id = "1";
+byte[] key_user_id = "1".getBytes();
 
         long timestamp = System.currentTimeMillis();
         ColumnPath cp = new ColumnPath("Standard1");
---------------
-------------
@@ -145,7 +145,7 @@
     // A positioned update section must come from the same package as its query section.
     Section getPositionedUpdateSection(Section querySection) throws SqlException {
         Connection connection = agent_.connection_;
-        return getDynamicSection(connection.resultSetHoldability_);
+return getDynamicSection(connection.holdability());
     }
 
     // Get a section for a jdbc 1 positioned update/delete for the corresponding query.
---------------
-------------
@@ -59,7 +59,7 @@
     BayesThetaNormalizerDriver normalizer = new BayesThetaNormalizerDriver();
     normalizer.runJob(input, output, params);
     
-    if (Boolean.parseBoolean(params.get("skipCleanup"))) {
+if (params.isSkipCleanup()) {
       return;
     }
     
---------------
-------------
@@ -35,7 +35,7 @@
     writer.append(new Text(documentId2), new Text(text2));
     writer.close();
 
-    DocumentProcessor.tokenizeDocuments(input, DefaultAnalyzer.class, output);
+DocumentProcessor.tokenizeDocuments(input, DefaultAnalyzer.class, output, configuration);
 
     FileStatus[] statuses = fs.listStatus(output);
     assertEquals(1, statuses.length);
---------------
-------------
@@ -86,7 +86,7 @@
     addOption("vectorSize", "vs", "Truncate vectors to <vs> length when dumping (most useful when in"
             + " conjunction with -sort", false);
     addOption(buildOption("filter", "fi", "Only dump out those vectors whose name matches the filter." 
-            + "  Multiple items may be specified by repeating the argument.", true, 1, 100, false, null));
++ "  Multiple items may be specified by repeating the argument.", true, 1, Integer.MAX_VALUE, false, null));
 
     if (parseArguments(args, false, true) == null) {
       return -1;
---------------
-------------
@@ -185,7 +185,7 @@
     }
     
     private static FakeDeleteAtomicIndexReader[] initSubReaders(IndexReader reader) {
-      final List<AtomicReaderContext> leaves = reader.getTopReaderContext().leaves();
+final List<AtomicReaderContext> leaves = reader.leaves();
       final FakeDeleteAtomicIndexReader[] subs = new FakeDeleteAtomicIndexReader[leaves.size()];
       int i = 0;
       for (final AtomicReaderContext ctx : leaves) {
---------------
-------------
@@ -1,6 +1,6 @@
 /*
  
-Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun
+Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun_Local_StateTest_part1_1
  
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -89,7 +89,7 @@
     document = new String[]{"ff"};
     result = classifier.classify(model, document, "unknown");
     assertTrue("category is null and it shouldn't be", result != null);
-    assertTrue(result + " is not equal to " + "unknown", result.getLabel().equals("unknown"));
+assertTrue(result + " is not equal to " + "d", result.getLabel().equals("d"));
 
     document = new String[]{"cc"};
     result = classifier.classify(model, document, "unknown");
---------------
-------------
@@ -336,7 +336,7 @@
 				"Gesamtspeicher: #####	Freier Speicher: #####");
 		germanOutputs.put("sedMemorySearch", "Gesamtspeicher: [0-9]*	Freier Speicher: [0-9]*");
 		germanOutputs.put("sedMemoryReplace", "Gesamtspeicher: #####	Freier Speicher: #####");
-		germanOutputs.put("RuntimeInfoLocaleString", "\tAnwsg-ID\t\tSQL-Text\n\t--------------\t------------\n\n\n\nSitzungsnummer");
+germanOutputs.put("RuntimeInfoLocaleString", "\tAnwsg-ID\t\tSQL-Text\n\t-------------\t-----------\n\n\n\nSitzungsnummer");
 		
 		outputs = new HashMap();
 		outputs.put(englishLocale, englishOutputs);
---------------
-------------
@@ -83,7 +83,7 @@
             columnCount += column.getObjectCount();
         }
 
-        BloomFilter bf = new BloomFilter(columnCount, 4);
+BloomFilter bf = BloomFilter.getFilter(columnCount, 4);
         for (IColumn column : columns)
         {
             bf.add(column.name());
---------------
-------------
@@ -719,7 +719,7 @@
         SolrCore newCore = core.reload(solrLoader, core);
         // keep core to orig name link
         solrCores.removeCoreToOrigName(newCore, core);
-        registerCore(false, name, newCore, false);
+registerCore(false, name, newCore, false, false);
       } finally {
         solrCores.removeFromPendingOps(name);
       }
---------------
-------------
@@ -29,7 +29,7 @@
 public class D_ActiveLock extends D_Lock  {
 
 	/**
-		@exception StandardException Standard cloudscape policy
+@exception StandardException Standard Derby policy
 	*/
     public String diag()
         throws StandardException
---------------
-------------
@@ -1219,7 +1219,7 @@
 
     for (int i = 0; i < segmentInfos.size(); i++) {
       final SegmentInfo info = segmentInfos.info(i);
-      count += info.docCount - info.getDelCount();
+count += info.docCount - numDeletedDocs(info);
     }
     return count;
   }
---------------
-------------
@@ -92,7 +92,7 @@
         return executorService_.isShutdown();
     }    
     
-    public long getTaskCount(){
+public long getPendingTasks(){
         return (executorService_.getTaskCount() - executorService_.getCompletedTaskCount());
     }
     /* Finished implementing the IStage interface methods */
---------------
-------------
@@ -625,7 +625,7 @@
 	Bulk Load a B-tree secondary index.
 
 	@see Conglomerate#load
-	@exception StandardException Standard Cloudscape Error policy.
+@exception StandardException Standard Derby Error policy.
 	raise SQLState.STORE_CONGLOMERATE_DUPLICATE_KEY_EXCEPTION if a duplicate 
     key is detected in the load.
 	**/
---------------
-------------
@@ -178,7 +178,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -406,7 +406,7 @@
           if (VERBOSE) {
             System.out.println("    " + docCount + ": docID=" + posting.docID + " freq=" + posting.positions.size());
           }
-          postingsConsumer.startDoc(posting.docID, posting.positions.size());
+postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);
           seenDocs.set(posting.docID);
           if (doPos) {
             totalTF += posting.positions.size();
---------------
-------------
@@ -532,7 +532,7 @@
 			if (tableList == null) {
 			// Don't do these if user just wants table-related objects.
                 DB_Jar.doJars(sourceDBName, this.conn, at10_9);
-				DB_Alias.doProceduresFunctionsAndUDTs(this.conn, at10_6 );
+DB_Alias.doPFAU(this.conn, at10_6 );
 			}
 
 			DB_Table.doTables(this.conn, tableIdToNameMap);
---------------
-------------
@@ -41,7 +41,7 @@
 
   @Override
   public PostingsReaderBase postingsReaderBase(SegmentReadState state) throws IOException {
-    return new Lucene41PostingsReader(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+return new Lucene41PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
   }
 
   @Override
---------------
-------------
@@ -435,7 +435,7 @@
       Sort sort = searcher.weightSort(rb.getSortSpec().getSort());
       SortField[] sortFields = sort==null ? new SortField[]{SortField.FIELD_SCORE} : sort.getSort();
       NamedList<Object[]> sortVals = new NamedList<Object[]>(); // order is important for the sort fields
-      Field field = new StringField("dummy", ""); // a dummy Field
+Field field = new StringField("dummy", "", Field.Store.NO); // a dummy Field
       IndexReaderContext topReaderContext = searcher.getTopReaderContext();
       AtomicReaderContext[] leaves = topReaderContext.leaves();
       AtomicReaderContext currentLeaf = null;
---------------
-------------
@@ -180,7 +180,7 @@
             throw new RuntimeException("Corrupted hint name " + joined.toString());
         String[] parts = new String[2];
         parts[0] = new String(ArrayUtils.subarray(joined.array(), joined.position()+joined.arrayOffset(), index));
-        parts[1] = new String(ArrayUtils.subarray(joined.array(), index+1, joined.limit()));
+parts[1] = new String(ArrayUtils.subarray(joined.array(), index+1, joined.limit()+joined.arrayOffset()));
         return parts;
 
     }
---------------
-------------
@@ -224,7 +224,7 @@
                       totalVariantDocFreqs+=fe.docFreq();
                       float score=boostAtt.getBoost();
                       if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){
-                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    
+ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)),score,startTerm);
                         variantsQ.insertWithOverflow(st);
                         minScore = variantsQ.top().score; // maintain minScore
                       }
---------------
-------------
@@ -763,7 +763,7 @@
       long h = 0x98761234;  // something non-zero for length==0
       for (int i = bits.length; --i>=0;) {
       h ^= bits[i];
-      h = (h << 1) | (h >>> 31); // rotate left
+h = (h << 1) | (h >>> 63); // rotate left
     }
     return (int)((h>>32) ^ h);  // fold leftmost bits into right
   }
---------------
-------------
@@ -23,5 +23,5 @@
 
 public interface IMessageSink
 {
-    public Message handleMessage(Message message);    
+public Message handleMessage(Message message, InetAddress to);
 }
---------------
-------------
@@ -452,7 +452,7 @@
   // LUCENE-1404
   public void testNPESpanQuery() throws Throwable {
     final Directory dir = new MockRAMDirectory();
-    final IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT, Collections.emptySet()), IndexWriter.MaxFieldLength.LIMITED);
+final IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet()), IndexWriter.MaxFieldLength.LIMITED);
 
     // Add documents
     addDoc(writer, "1", "the big dogs went running to the market");
---------------
-------------
@@ -66,7 +66,7 @@
 			logger_.debug("Handle Digest reponses");
 			for( Message response : responses_ )
 			{
-				byte[] body = (byte[])response.getMessageBody()[0];            
+byte[] body = response.getMessageBody();
 	            bufIn.reset(body, body.length);
 	            try
 	            {	               
---------------
-------------
@@ -95,7 +95,7 @@
         else
         {
             if (column_path.super_column == null)
-                throw new InvalidRequestException("column parameter is not optional for super CF " + column_path.column_family);
+throw new InvalidRequestException("supercolumn parameter is not optional for super CF " + column_path.column_family);
         }
         if (column_path.column != null)
         {
---------------
-------------
@@ -1086,7 +1086,7 @@
   @Override
   public void setUp() throws Exception {
     super.setUp();
-    indexDir = new File(TEMP_DIR, "IndexReaderReopen");
+indexDir = _TestUtil.getTempDir("IndexReaderReopen");
   }
   
   public void testCloseOrig() throws Throwable {
---------------
-------------
@@ -79,7 +79,7 @@
     return new IntDocValues(this) {
      @Override
       public int intVal(int doc) {
-        return (end - sindex.getOrd(doc+off));
+return (end - sindex.getOrd(doc+off) - 1);
       }
     };
   }
---------------
-------------
@@ -28,7 +28,7 @@
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.*;
+import static org.apache.lucene.analysis.VocabularyAssert.*;
 
 /**
  * Simple tests for {@link PortugueseMinimalStemFilter}
---------------
-------------
@@ -691,7 +691,7 @@
         
         try
         {
-            oldCfm.apply(cf_def);
+CFMetaData.applyImplicitDefaults(cf_def);
             UpdateColumnFamily update = new UpdateColumnFamily(cf_def);
             applyMigrationOnStage(update);
             return DatabaseDescriptor.getDefsVersion().toString();
---------------
-------------
@@ -670,7 +670,7 @@
           // that there are multiple docs in the add... so make sure that
           // objects can handle that.
 
-          cmd.id = null;  // reset the id for this add     
+cmd.indexedId = null;  // reset the id for this add
 
           if (eventType !=0) {
             eventType=xpp.getEventType();
---------------
-------------
@@ -217,7 +217,7 @@
         cfStore.storeLocation(ssTable);
         buffer.close();
         isFlushed_ = true;
-        logger_.info("Completed flushing " + this);
+logger_.info("Completed flushing " + ssTable.getFilename());
     }
 
     public String toString()
---------------
-------------
@@ -194,7 +194,7 @@
         
         st.executeUpdate(
             " insert into bt2 values (28, 82, null, '15:47:28', "
-            + "'"+Timestamp.valueOf("0000-00-00 15:47:28.0")+"', null)");
++ "'"+Timestamp.valueOf("2007-02-23 15:47:27.544")+"', null)");
         
         //create indexes
         
---------------
-------------
@@ -323,7 +323,7 @@
 
     PerDocConsumer perDocConsumer = perDocConsumers.get(0);
     if (perDocConsumer == null) {
-      PerDocWriteState perDocWriteState = docState.docWriter.newPerDocWriteState(0);
+PerDocWriteState perDocWriteState = docState.docWriter.newPerDocWriteState("");
       DocValuesFormat dvFormat = docState.docWriter.codec.docValuesFormat();
       perDocConsumer = dvFormat.docsConsumer(perDocWriteState);
       perDocConsumers.put(0, perDocConsumer);
---------------
-------------
@@ -127,7 +127,7 @@
 
     public FieldCacheTermsFilterDocIdSet(FieldCache.DocTermsIndex fcsi) {
       this.fcsi = fcsi;
-      openBitSet = new OpenBitSet(this.fcsi.size());
+openBitSet = new OpenBitSet(this.fcsi.numOrd());
       final BytesRef spare = new BytesRef();
       for (int i=0;i<terms.length;i++) {
         int termNumber = this.fcsi.binarySearchLookup(terms[i], spare);
---------------
-------------
@@ -145,7 +145,7 @@
     int num = 100 * RANDOM_MULTIPLIER;
     for (int i = 0; i < num; i++) {
       ir1 = IndexReader.open(dir1, false);
-      doTest(10,100);
+doTest(10,10);
       ir1.close();
     }
     dir1.close();
---------------
-------------
@@ -66,7 +66,7 @@
         }
         while (true)
         {
-            Future<Integer> ft = CompactionManager.instance.submitMinor(store);
+Future<Integer> ft = CompactionManager.instance.submitMinorIfNeeded(store);
             if (ft.get() == 0)
                 break;
         }
---------------
-------------
@@ -35,7 +35,7 @@
  * limitations under the License.
  */
 
-public class FacetTestCase extends LuceneTestCase {
+public abstract class FacetTestCase extends LuceneTestCase {
   
   private static final IntEncoder[] ENCODERS = new IntEncoder[] {
     new SortingIntEncoder(new UniqueValuesIntEncoder(new VInt8IntEncoder())),
---------------
-------------
@@ -59,7 +59,7 @@
       ts.reset();
       while (ts.incrementToken()) {
         termAtt.fillBytesRef();
-        SpanTermQuery stq = new SpanTermQuery(new Term(fieldName, new BytesRef(bytes)));
+SpanTermQuery stq = new SpanTermQuery(new Term(fieldName, BytesRef.deepCopyOf(bytes)));
         clausesList.add(stq);
       }
       ts.end();
---------------
-------------
@@ -67,7 +67,7 @@
     public void init(NamedList args) {
         super.init(args);
         SolrParams p = SolrParams.toSolrParams(args);
-        restrictToField = p.get("termSourceField");
+termSourceField = p.get("termSourceField");
         spellcheckerIndexDir = p.get("spellcheckerIndexDir");
         try {
             spellChecker = new SpellChecker(FSDirectory.getDirectory(spellcheckerIndexDir));
---------------
-------------
@@ -72,7 +72,7 @@
 
 			if (firstTime) {
 				Logs.reportString("----------------------------------------------");
-				Logs.reportMessage("CSLOOK_Header", "indexes");
+Logs.reportMessage("CSLOOK_IndexesHeader");
 				Logs.reportString("----------------------------------------------\n");
 			}
 
---------------
-------------
@@ -47,7 +47,7 @@
 	messages can be localized.
 
 	REMIND: May want to investigate putting some of this in the protocol
-	side, for the errors that any Cloudscape JDBC driver might return.
+side, for the errors that any Derby JDBC driver might return.
 
 	The ASSERT mechanism is a wrapper of the basic services,
 	to ensure that failed asserts at this level will behave
---------------
-------------
@@ -351,7 +351,7 @@
           return new TokenStreamComponents(tokenizer, new WordDelimiterFilter(tokenizer, flags, protectedWords));
         }
       };
-      checkRandomData(random(), a, 10000*RANDOM_MULTIPLIER, 20, false, false);
+checkRandomData(random(), a, 200, 20, false, false);
     }
   }
   
---------------
-------------
@@ -194,7 +194,7 @@
   }
 
   private void addDoc(RAMDirectory ramDir1, String s, boolean create) throws IOException {
-    IndexWriter iw = new IndexWriter(ramDir1, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), create, IndexWriter.MaxFieldLength.LIMITED);
+IndexWriter iw = new IndexWriter(ramDir1, new StandardAnalyzer(TEST_VERSION_CURRENT), create, IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     doc.add(new Field("body", s, Field.Store.YES, Field.Index.ANALYZED));
     iw.addDocument(doc);
---------------
-------------
@@ -1239,7 +1239,7 @@
 						if (firstColumn != null && leftOpnd instanceof LikeEscapeOperatorNode)
 						{
 							LikeEscapeOperatorNode likeNode = (LikeEscapeOperatorNode) leftOpnd;
-							if (likeNode.getLeftOperand().isParameterNode())
+if (likeNode.getLeftOperand().requiresTypeFromContext())
 							{
 								ValueNode receiver = ((TernaryOperatorNode) likeNode).getReceiver();
 								if (receiver instanceof ColumnReference)
---------------
-------------
@@ -478,7 +478,7 @@
     ResultSet rs = null;
     try {
       conn = dataSource.getConnection();
-      stmt = conn.prepareStatement(getPreferenceSQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
+stmt = conn.prepareStatement(getPreferenceTimeSQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
       stmt.setFetchDirection(ResultSet.FETCH_FORWARD);
       stmt.setFetchSize(1);
       setLongParameter(stmt, 1, userID);
---------------
-------------
@@ -49,7 +49,7 @@
     writer.close();
 
     Searcher searcher = new IndexSearcher(store, true);
-      QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, "field", new SimpleAnalyzer());
+QueryParser parser = new QueryParser(TEST_VERSION_CURRENT, "field", new SimpleAnalyzer());
     Query query = parser.parse("a NOT b");
     //System.out.println(query);
     ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
---------------
-------------
@@ -2453,7 +2453,7 @@
             // DERBY-4564. Make replication tests use derby.tests.networkServerTimeout proeprty
             String userStartTimeout = getSystemProperty("derby.tests.networkServerStartTimeout");
             long startTimeout = (userStartTimeout != null )? 
-            		Long.parseLong(userStartTimeout): DEFAULT_SERVER_START_TIMEOUT;
+(Long.parseLong(userStartTimeout) * 1000): DEFAULT_SERVER_START_TIMEOUT;
             long iterations = startTimeout / PINGSERVER_SLEEP_TIME_MILLIS;		
             util.DEBUG(debugId+"************** Do .start().");
             serverThread.start();
---------------
-------------
@@ -58,7 +58,7 @@
       ((i % 2 == 0) ? swriter1 : swriter2).addDocument(doc);
     }
     writer.forceMerge(1); swriter1.forceMerge(1); swriter2.forceMerge(1);
-    writer.close(); swriter1.close(); swriter2.close();
+writer.shutdown(); swriter1.shutdown(); swriter2.shutdown();
     
     reader = DirectoryReader.open(dir);
     searcher = newSearcher(reader);
---------------
-------------
@@ -101,7 +101,7 @@
     assertTrue("Invalid version: "+version,
                version.equals(Constants.LUCENE_MAIN_VERSION+"-SNAPSHOT") ||
                version.equals(Constants.LUCENE_MAIN_VERSION));
-    assertTrue(version + " should start with: "+Constants.LUCENE_VERSION,
+assertTrue(Constants.LUCENE_VERSION + " should start with: "+version,
                Constants.LUCENE_VERSION.startsWith(version));
   }
 }
---------------
-------------
@@ -60,7 +60,7 @@
                     InetAddress hint = InetAddress.getByAddress(addressBytes);
                     if (logger_.isDebugEnabled())
                         logger_.debug("Adding hint for " + hint);
-                    RowMutation hintedMutation = new RowMutation(Table.SYSTEM_TABLE, rm.getTable());
+RowMutation hintedMutation = new RowMutation(Table.SYSTEM_TABLE, rm.getTable().getBytes(FBUtilities.UTF8));
                     hintedMutation.addHints(rm.key(), addressBytes);
                     hintedMutation.apply();
                 }
---------------
-------------
@@ -1993,7 +1993,7 @@
 
     for (int i = 0; i < segmentInfos.size(); i++) {
       final SegmentInfo info = segmentInfos.info(i);
-      count += info.docCount - info.getDelCount();
+count += info.docCount - numDeletedDocs(info);
     }
     return count;
   }
---------------
-------------
@@ -71,7 +71,7 @@
   private boolean verifyIndex(Directory directory, int startAt) throws IOException
   {
     boolean fail = false;
-    IndexReader reader = IndexReader.open(directory);
+IndexReader reader = IndexReader.open(directory, true);
 
     int max = reader.maxDoc();
     for (int i = 0; i < max; i++)
---------------
-------------
@@ -125,7 +125,7 @@
    *  term.  This will return null if the field or term does
    *  not exist. */
   public static DocsEnum getTermDocsEnum(IndexReader r, Bits liveDocs, String field, BytesRef term) throws IOException {
-    return getTermDocsEnum(r, liveDocs, field, term);
+return getTermDocsEnum(r, liveDocs, field, term, DocsEnum.FLAG_FREQS);
   }
   
   /** Returns {@link DocsEnum} for the specified field &
---------------
-------------
@@ -27,7 +27,7 @@
 /**
  * @author yonik
  */
-public class TestDocSet {
+public class DocSetPerf {
 
   // use test instead of assert since asserts may be turned off
   public static void test(boolean condition) {
---------------
-------------
@@ -405,7 +405,7 @@
   // input can be null, host, or url_prefix://host
   private String getHostAddress(String host) throws IOException {
 
-    if (host == null) {
+if (host == null || host.length() == 0) {
       String hostaddress;
       try {
         hostaddress = InetAddress.getLocalHost().getHostAddress();
---------------
-------------
@@ -846,7 +846,7 @@
     TermQuery key = null;
 
     if (useCache) {
-      key = new TermQuery(new Term(deState.fieldName, new BytesRef(deState.termsEnum.term())));
+key = new TermQuery(new Term(deState.fieldName, BytesRef.deepCopyOf(deState.termsEnum.term())));
       DocSet result = filterCache.get(key);
       if (result != null) return result;
     }
---------------
-------------
@@ -177,7 +177,7 @@
         /* We need this so that we do not suspect a convict. */
         boolean isConvicted = false;
         double phi = hbWnd.phi(now);
-        logger_.trace("PHI for " + ep + " : " + phi);
+logger_.info("PHI for " + ep + " : " + phi);
         
         /*
         if ( phi > phiConvictThreshold_ )
---------------
-------------
@@ -57,7 +57,7 @@
       SortField sortField0;
       if (field.equals("doc")) {
         sortField0 = SortField.FIELD_DOC;
-      } if (field.equals("score")) {
+} else if (field.equals("score")) {
         sortField0 = SortField.FIELD_SCORE;
       } else if (field.equals("noscore")) {
         doScore = false;
---------------
-------------
@@ -61,7 +61,7 @@
     for(i=0;i<35;i++) {
       addDoc(writer, i);
     }
-    mergePolicy.setUseCompoundFile(false);
+((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);
     for(;i<45;i++) {
       addDoc(writer, i);
     }
---------------
-------------
@@ -41,7 +41,7 @@
 
         try
         {
-            StreamRequestMessage srm = StreamRequestMessage.serializer().deserialize(new DataInputStream(bufIn));
+StreamRequestMessage srm = StreamRequestMessage.serializer().deserialize(new DataInputStream(bufIn), MessagingService.version_);
             StreamInSession session = StreamInSession.get(to, srm.sessionId);
             session.closeIfFinished();
         }
---------------
-------------
@@ -112,7 +112,7 @@
 
                 if (!Arrays.equals(ColumnFamily.digest(row_.cf), digest))
                 {
-                    IResponseResolver<Row> readResponseResolver = new ReadResponseResolver(table_);
+IResponseResolver<Row> readResponseResolver = new ReadResponseResolver(table_, replicas_.size());
                     IAsyncCallback responseHandler;
                     if (replicas_.contains(FBUtilities.getLocalAddress()))
                         responseHandler = new DataRepairHandler(row_, replicas_.size(), readResponseResolver);
---------------
-------------
@@ -184,7 +184,7 @@
     }
   }
   
-  private MultiPhraseQuery randomPhraseQuery(long seed) throws Exception {
+private MultiPhraseQuery randomPhraseQuery(long seed) {
     Random random = new Random(seed);
     int length = _TestUtil.nextInt(random, 2, 5);
     MultiPhraseQuery pq = new MultiPhraseQuery();
---------------
-------------
@@ -443,7 +443,7 @@
         {
             validated++;
             // MerkleTree uses XOR internally, so we want lots of output bits here
-            byte[] rowhash = FBUtilities.hash("SHA-256", row.key.key.getBytes(), row.buffer.getData());
+byte[] rowhash = FBUtilities.hash("SHA-256", row.key.key, row.buffer.getData());
             return new MerkleTree.RowHash(row.key.token, rowhash);
         }
 
---------------
-------------
@@ -48,7 +48,7 @@
     // test merge factor picked up
     SolrCore core = h.getCore();
 
-    IndexWriter writer = ((DirectUpdateHandler2)core.getUpdateHandler()).getIndexWriterProvider().getIndexWriter(core);
+IndexWriter writer = ((DirectUpdateHandler2)core.getUpdateHandler()).getSolrCoreState().getIndexWriter(core);
 
     assertEquals("Mergefactor was not picked up", ((LogMergePolicy)writer.getConfig().getMergePolicy()).getMergeFactor(), 8);
     
---------------
-------------
@@ -290,7 +290,7 @@
 
     // constructor
     private CustomScorer(CustomScoreProvider provider, CustomWeight w, float qWeight,
-        Scorer subQueryScorer, Scorer[] valSrcScorers) throws IOException {
+Scorer subQueryScorer, Scorer[] valSrcScorers) {
       super(w);
       this.qWeight = qWeight;
       this.subQueryScorer = subQueryScorer;
---------------
-------------
@@ -138,7 +138,7 @@
   @Deprecated
   public ImportedServiceImpl (String ifaceName, Map<String, String> attributes) throws InvalidAttributeException {
     
-    _optional = false;
+_optional = ("optional".equals(attributes.get("availability:")));
     _iface = ifaceName;
     _isMultiple = false;
     _componentName = null;
---------------
-------------
@@ -453,7 +453,7 @@
 
     // these queries should be exactly ordered and scores should exactly match
     query("q","*:*", "sort",i1+" desc");
-    //query("q","*:*", "sort",i1+" desc", "fl","*,score");
+query("q","*:*", "sort",i1+" desc", "fl","*,score");
     handle.put("maxScore", SKIPVAL);
     query("q","{!func}"+i1);// does not expect maxScore. So if it comes ,ignore it. NamedListCodec.writeSolrDocumentList()
     //is agnostic of request params.
---------------
-------------
@@ -136,7 +136,7 @@
 						+ " SQLSTATE: " + m);
 			}
 		}
-		if (e.getMessage().equals(null)) {
+if (e.getMessage() == null) {
 			e.printStackTrace(System.out);
 		}
 		System.out.println("During - " + where
---------------
-------------
@@ -321,7 +321,7 @@
       
       // TODO: benchmark doing this backwards
       for (int i = 1; i < matchers.length; i++)
-        if (matchers[i].run(term.bytes, 0, term.length)) {
+if (matchers[i].run(term.bytes, term.offset, term.length)) {
           // this sucks, we convert just to score based on length.
           if (codePointCount == -1) {
             codePointCount = UnicodeUtil.codePointCount(term);
---------------
-------------
@@ -109,7 +109,7 @@
             case ANY:
                 return 1;
             case QUORUM:
-                return (DatabaseDescriptor.getQuorum(table)/ 2) + 1;
+return (DatabaseDescriptor.getReplicationFactor(table) / 2) + 1;
             case ALL:
                 return DatabaseDescriptor.getReplicationFactor(table);
             default:
---------------
-------------
@@ -579,7 +579,7 @@
         System.getProperty("line.separator") + "transient=true" +
         System.getProperty("line.separator") + "loadOnStartup=true", Charsets.UTF_8.toString());
 
-    FileUtils.writeStringToFile(new File(subHome, "solrconfig.snippet.randomindexconfig.xml"), rand_snip);
+FileUtils.writeStringToFile(new File(subHome, "solrconfig.snippet.randomindexconfig.xml"), rand_snip, Charsets.UTF_8.toString());
 
     FileUtils.writeStringToFile(new File(subHome, "solrconfig.xml"), config, Charsets.UTF_8.toString());
 
---------------
-------------
@@ -1,5 +1,5 @@
 /**
-f * Licensed to the Apache Software Foundation (ASF) under one
+* Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
  * regarding copyright ownership.  The ASF licenses this file
---------------
-------------
@@ -230,7 +230,7 @@
     assert !omitTF;
 
     final int delta = position - lastPosition;
-    assert delta > 0 || position == 0: "position=" + position + " lastPosition=" + lastPosition;            // not quite right (if pos=0 is repeated twice we don't catch it)
+assert delta >= 0: "position=" + position + " lastPosition=" + lastPosition;            // not quite right (if pos=0 is repeated twice we don't catch it)
     lastPosition = position;
 
     if (storePayloads) {
---------------
-------------
@@ -195,7 +195,7 @@
     {
         DataOutputStream out = new DataOutputStream(new FileOutputStream(desc.filenameFor(SSTable.COMPONENT_STATS)));
         EstimatedHistogram.serializer.serialize(rowSizes, out);
-        EstimatedHistogram.serializer.serialize(rowSizes, out);
+EstimatedHistogram.serializer.serialize(columnnCounts, out);
         out.close();
     }
 
---------------
-------------
@@ -49,7 +49,7 @@
 
     public ReadResponseResolver(String table, int responseCount)
     {
-        assert 1 <= responseCount && responseCount <= DatabaseDescriptor.getReplicationFactor()
+assert 1 <= responseCount && responseCount <= DatabaseDescriptor.getReplicationFactor(table)
             : "invalid response count " + responseCount;
 
         this.responseCount = responseCount;
---------------
-------------
@@ -224,7 +224,7 @@
     Directory directory = newDirectory();
     Analyzer stopAnalyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET);
     RandomIndexWriter writer = new RandomIndexWriter(random(), directory, 
-        newIndexWriterConfig( Version.LUCENE_40, stopAnalyzer));
+newIndexWriterConfig(TEST_VERSION_CURRENT, stopAnalyzer));
     Document doc = new Document();
     doc.add(newTextField("field", "the stop words are here", Field.Store.YES));
     writer.addDocument(doc);
---------------
-------------
@@ -77,7 +77,7 @@
 
   // firstDocID is ignored since nextDoc() sets 'doc'
   @Override
-  protected boolean score(Collector c, int end, int firstDocID) throws IOException {
+public boolean score(Collector c, int end, int firstDocID) throws IOException {
     c.setScorer(this);
     while (doc < end) {                           // for docs in window
       c.collect(doc);                      // collect score
---------------
-------------
@@ -50,7 +50,7 @@
   
   class CountingRAMDirectory extends MockDirectoryWrapper {
     public CountingRAMDirectory(Directory delegate) {
-      super(delegate);
+super(random, delegate);
     }
 
     public IndexInput openInput(String fileName) throws IOException {
---------------
-------------
@@ -130,7 +130,7 @@
     throws InvalidRequestException
     {
         if (cosc.column != null)
-            AvroValidation.validateColumnPath(keyspace, newColumnPath(cfName, cosc.super_column.name, cosc.column.name));
+AvroValidation.validateColumnPath(keyspace, newColumnPath(cfName, null, cosc.column.name));
 
         if (cosc.super_column != null)
             for (Column c : cosc.super_column.columns)
---------------
-------------
@@ -157,7 +157,7 @@
         addCmdHelp(header, "cleanup [keyspace] [cfnames]", "Run cleanup on one or more column family");
         addCmdHelp(header, "compact [keyspace] [cfnames]", "Force a (major) compaction on one or more column family");
         addCmdHelp(header, "scrub [keyspace] [cfnames]", "Scrub (rebuild sstables for) one or more column family");
-        addCmdHelp(header, "upgradesstables [keyspace] [cfnames]", "Scrub (rebuild sstables for) one or more column family");
+addCmdHelp(header, "upgradesstables [keyspace] [cfnames]", "Upgrade sstables for one or more column family");
         addCmdHelp(header, "invalidatekeycache [keyspace] [cfnames]", "Invalidate the key cache of one or more column family");
         addCmdHelp(header, "invalidaterowcache [keyspace] [cfnames]", "Invalidate the key cache of one or more column family");
         addCmdHelp(header, "getcompactionthreshold <keyspace> <cfname>", "Print min and max compaction thresholds for a given column family");
---------------
-------------
@@ -132,7 +132,7 @@
       provider = c.newInstance();
       provider.init(args);
     } catch (Exception e) {
-      throw new SolrException(ErrorCode.BAD_REQUEST, "Error instansiating exhange rate provider "+exchangeRateProviderClass+". Please check your FieldType configuration", e);
+throw new SolrException(ErrorCode.BAD_REQUEST, "Error instantiating exhange rate provider "+exchangeRateProviderClass+": " + e.getMessage(), e);
     }
   }
 
---------------
-------------
@@ -26,7 +26,7 @@
 import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 import org.apache.lucene.util.LuceneTestCase;
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.*;
+import static org.apache.lucene.analysis.VocabularyAssert.*;
 
 /**
  * Test the snowball filters against the snowball data tests
---------------
-------------
@@ -44,7 +44,7 @@
       dir.setAssertNoUnrefencedFilesOnClose(true);
       IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, null));
       iw.addDocument(new Document());
-      iw.close();
+iw.shutdown();
       IndexOutput output = dir.createOutput("_hello.world", IOContext.DEFAULT);
       output.writeString("i am unreferenced!");
       output.close();
---------------
-------------
@@ -89,7 +89,7 @@
 
   private static final int DEFAULT_MAX_SIMILAR_ITEMS_PER_ITEM = 100;
   private static final int DEFAULT_MAX_PREFS_PER_USER = 1000;
-  private static final int DEFAULT_MIN_PREFS_PER_USER = 2;
+private static final int DEFAULT_MIN_PREFS_PER_USER = 1;
 
   public static void main(String[] args) throws Exception {
     ToolRunner.run(new ItemSimilarityJob(), args);
---------------
-------------
@@ -285,7 +285,7 @@
           String segName = input.readString();
           Codec codec = Codec.forName(input.readString());
           //System.out.println("SIS.read seg=" + seg + " codec=" + codec);
-          SegmentInfo info = codec.segmentInfoFormat().getSegmentInfosReader().read(directory, segName, IOContext.READ);
+SegmentInfo info = codec.segmentInfoFormat().getSegmentInfoReader().read(directory, segName, IOContext.READ);
           info.setCodec(codec);
           long delGen = input.readLong();
           int delCount = input.readInt();
---------------
-------------
@@ -79,7 +79,7 @@
 
   @Override
   public boolean equals(Object o) {
-    if (o.getClass() !=  ReverseOrdFieldSource.class) return false;
+if (o == null || (o.getClass() !=  ReverseOrdFieldSource.class)) return false;
     ReverseOrdFieldSource other = (ReverseOrdFieldSource)o;
     return this.field.equals(other.field);
   }
---------------
-------------
@@ -214,7 +214,7 @@
     boolean optimize = true;
 
     Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));
+IndexWriter writer = new IndexWriter(dir1, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setReaderTermsIndexDivisor(2));
     writer.setInfoStream(infoStream);
     // create the index
     createIndexNoClose(!optimize, "index1", writer);
---------------
-------------
@@ -113,7 +113,7 @@
 		needToObjectifyStream = (this.constantAction.getTriggerInfo() != null);
 	}
 
-	public final long	modifiedRowCount() { return rowCount + RowUtil.rowCountBase; }
+public final long	modifiedRowCount() { return rowCount + RowUtil.getRowCountBase(); }
 
 
 	/**
---------------
-------------
@@ -249,7 +249,7 @@
         
         // Make a new dir that will enforce disk usage:
         MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));
-        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));
+writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy(false)));
         IOException err = null;
 
         MergeScheduler ms = writer.getConfig().getMergeScheduler();
---------------
-------------
@@ -30,7 +30,7 @@
   }
   
   public static class Nested1 extends WithNestedTests.AbstractNestedTest {
-    public void testDummy() {
+public void testDummy() throws Exception {
       Directory dir = newDirectory();
       System.out.println(dir.toString());
     }
---------------
-------------
@@ -793,7 +793,7 @@
     try {
       core = coreContainer.getCore(cname);
       if (core != null) {
-        syncStrategy = new SyncStrategy();
+syncStrategy = new SyncStrategy(core.getCoreDescriptor().getCoreContainer().getUpdateShardHandler());
         
         Map<String,Object> props = new HashMap<String,Object>();
         props.put(ZkStateReader.BASE_URL_PROP, zkController.getBaseUrl());
---------------
-------------
@@ -54,7 +54,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new ItalianAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new ItalianAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
   
   /** test that the elisionfilter is working */
---------------
-------------
@@ -62,7 +62,7 @@
 
 			if (firstTime) {
 				Logs.reportString("----------------------------------------------");
-				Logs.reportMessage("CSLOOK_Header", "checks");
+Logs.reportMessage("CSLOOK_ChecksHeader");
 				Logs.reportString("----------------------------------------------\n");
 			}
 
---------------
-------------
@@ -134,7 +134,7 @@
     if (!termsEnum.seekExact(termBytes, false)) {
       return -1;
     }
-    DocsEnum docs = termsEnum.docs(MultiFields.getLiveDocs(r), null, false);
+DocsEnum docs = termsEnum.docs(MultiFields.getLiveDocs(r), null, 0);
     int id = docs.nextDoc();
     if (id != DocIdSetIterator.NO_MORE_DOCS) {
       int next = docs.nextDoc();
---------------
-------------
@@ -183,7 +183,7 @@
               public Class<? extends ListResourceBundle> run()
               {
                 try {
-                  return b.loadClass(bundleName);
+return (Class<? extends ListResourceBundle>) b.loadClass(bundleName);
                 } catch (ClassNotFoundException e) {
                   return null;
                 }
---------------
-------------
@@ -38,7 +38,7 @@
  * This class manages the streaming of multiple files 
  * one after the other. 
 */
-public final class StreamManager
+public class StreamManager
 {   
     private static Logger logger_ = Logger.getLogger( StreamManager.class );
         
---------------
-------------
@@ -60,7 +60,7 @@
 
         InetAddress myEndpoint = InetAddress.getByName("127.0.0.1");
         Range range3 = ss.getPrimaryRangeForEndPoint(three);
-        Token fakeToken = ((IPartitioner)StorageService.getPartitioner()).midpoint(range3.left(), range3.right());
+Token fakeToken = ((IPartitioner)StorageService.getPartitioner()).midpoint(range3.left, range3.right);
         assert range3.contains(fakeToken);
         ss.onChange(myEndpoint, StorageService.MOVE_STATE, new ApplicationState(StorageService.STATE_BOOTSTRAPPING + StorageService.Delimiter + ss.getPartitioner().getTokenFactory().toString(fakeToken)));
         tmd = ss.getTokenMetadata();
---------------
-------------
@@ -54,7 +54,7 @@
       if ((clusterPath != null) && (clusterPath.length() > 0)) {
         KMeansUtil.configureWithClusterInfo(new Path(clusterPath), clusters);
         if (clusters.isEmpty()) {
-          throw new IllegalStateException("Cluster is empty!");
+throw new IllegalStateException("No clusters found. Check your -c path.");
         }
       }  
       this.clusterer = new KMeansClusterer(measure);
---------------
-------------
@@ -224,6 +224,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new PersianAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new PersianAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -89,7 +89,7 @@
       throws IOException {
     
     Directory baseFromDir = getBaseDir(fromDir);
-    Directory baseToDir = getBaseDir(fromDir);
+Directory baseToDir = getBaseDir(toDir);
     
     if (baseFromDir instanceof FSDirectory && baseToDir instanceof FSDirectory) {
       File dir1 = ((FSDirectory) baseFromDir).getDirectory();
---------------
-------------
@@ -102,7 +102,7 @@
 
   static {
     // does not yet work with ssl
-    sslConfig = null;
+ALLOW_SSL = false;
   }
   
   @BeforeClass
---------------
-------------
@@ -845,7 +845,7 @@
   }
   
   private void testDebugQueries() throws Exception {
-    handle.put("explain", UNORDERED);
+handle.put("explain", SKIPVAL);
     handle.put("debug", UNORDERED);
     handle.put("time", SKIPVAL);
     query("q", "now their fox sat had put", "fl", "*,score",
---------------
-------------
@@ -607,7 +607,7 @@
     public List<TokenRange> describe_ring(String keyspace)
     {
         List<TokenRange> ranges = new ArrayList<TokenRange>();
-        for (Map.Entry<Range, List<String>> entry : StorageService.instance.getRangeToEndPointMap(keyspace).entrySet())
+for (Map.Entry<Range, List<String>> entry : StorageService.instance.getRangeToEndpointMap(keyspace).entrySet())
         {
             Range range = entry.getKey();
             List<String> endpoints = entry.getValue();
---------------
-------------
@@ -395,7 +395,7 @@
         FileDataInput input;
         if (indexBuffers == null)
         {
-            input = new BufferedRandomAccessFile(path, "r");
+input = new BufferedRandomAccessFile(indexFilename(), "r");
             ((BufferedRandomAccessFile)input).seek(p);
         }
         else
---------------
-------------
@@ -61,6 +61,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new GermanAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new GermanAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -54,7 +54,7 @@
 {
     public String asHex(String str)
     {
-        return bytesToHex(ByteBuffer.wrap(str.getBytes()));
+return bytesToHex(ByteBufferUtil.bytes(str));
     }
 
     @Test
---------------
-------------
@@ -1554,7 +1554,7 @@
         } catch (Throwable e) {
           // do not allow decref() operations to fail since they are typically called in finally blocks
           // and throwing another exception would be very unexpected.
-          SolrException.log(log, "Error closing searcher:", e);
+SolrException.log(log, "Error closing searcher:" + this, e);
         }
       }
     };
---------------
-------------
@@ -93,7 +93,7 @@
     TokenStream tokenStream = analyzer.tokenStream("field", new StringReader("abcd   "));
     TeeSinkTokenFilter tee = new TeeSinkTokenFilter(tokenStream);
     TokenStream sink = tee.newSinkTokenStream();
-    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);
+FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);
     ft.setStoreTermVectors(true);
     ft.setStoreTermVectorOffsets(true);
     ft.setStoreTermVectorPositions(true);
---------------
-------------
@@ -256,7 +256,7 @@
       log.info("Input: {} Clusters In: {} Out: {} Distance: {}", input, clustersIn, output, measure);
     }
     ClusterClassifier.writePolicy(new KMeansClusteringPolicy(), clustersIn);
-    ClusterClassificationDriver.run(input, output, new Path(output, PathDirectory.CLUSTERED_POINTS_DIRECTORY),
+ClusterClassificationDriver.run(conf, input, output, new Path(output, PathDirectory.CLUSTERED_POINTS_DIRECTORY),
         clusterClassificationThreshold, true, runSequential);
   }
   
---------------
-------------
@@ -80,7 +80,7 @@
         {
             public void uncaughtException(Thread t, Throwable e)
             {
-                logger.error("Fatal exception in thread " + t, e);
+logger.error("Uncaught exception in thread " + t, e);
                 if (e instanceof OutOfMemoryError)
                 {
                     System.exit(100);
---------------
-------------
@@ -120,7 +120,7 @@
         return new ByteArrayInputStream(bout.toByteArray());
     }
     
-    //@Test
+@Test
     public void testDeadlock() throws Exception {
       bundleContext.registerService("java.util.Set",new HashSet<Object>(), null);
       
---------------
-------------
@@ -425,7 +425,7 @@
 		int xaRetVal = xaResource.XA_OK;
 
 		try {
-			xaResource.prepare(xid);
+xaRetVal = xaResource.prepare(xid);
 			if (SanityManager.DEBUG)
 			{
 				connThread.trace("prepared xa transaction: xaRetVal=" +
---------------
-------------
@@ -37,7 +37,7 @@
       Set<Integer> deleted = new HashSet<Integer>();
       List<BytesRef> terms = new ArrayList<BytesRef>();
 
-      int numDocs = r.nextInt(100*_TestUtil.getRandomMultiplier());
+int numDocs = _TestUtil.nextInt(r, 1, 100*_TestUtil.getRandomMultiplier());
       Document doc = new Document();
       Field f = new Field("field", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
       doc.add(f);
---------------
-------------
@@ -30,7 +30,7 @@
 import org.apache.cassandra.service.StorageService;
 import org.apache.cassandra.utils.ByteBufferUtil;
 
-public abstract class Token<T> implements RingPosition<Token<T>>, Serializable
+public abstract class Token<T> implements RingPosition<Token<T>>
 {
     private static final long serialVersionUID = 1L;
 
---------------
-------------
@@ -46,7 +46,7 @@
     public Timestamp timeInstant;
 
     // methods to be registered as functions
-    public static Price makePrice( ) { return new Price( "USD", new BigDecimal( 1 ), DEFAULT_TIMESTAMP ); }
+public static Price makePrice( ) { return new Price( "USD", BigDecimal.valueOf(1L), DEFAULT_TIMESTAMP ); }
     public static Price makePrice( String currencyCode, BigDecimal amount, Timestamp timeInstant ) { return new Price( currencyCode, amount, timeInstant ); }
     public static String getCurrencyCode( Price price ) { return price.currencyCode; }
     public static BigDecimal getAmount( Price price ) { return price.amount; }
---------------
-------------
@@ -144,7 +144,7 @@
   public static class DefaultCollectionModel implements CollectionModel {
     @Override
     public float computeProbability(BasicStats stats) {
-      return (float)stats.getTotalTermFreq() / (stats.getNumberOfFieldTokens() +1);
+return (stats.getTotalTermFreq()+1F) / (stats.getNumberOfFieldTokens()+1F);
     }
     
     @Override
---------------
-------------
@@ -455,7 +455,7 @@
       coreProps.setProperty(propName, propValue);
     }
 
-    return new CoreDescriptor(container, name, instancedir, coreProps);
+return new CoreDescriptor(container, name, instancedir, coreProps, params);
   }
 
   private static String checkNotEmpty(String value, String message) {
---------------
-------------
@@ -2634,7 +2634,7 @@
             assertTrue("FAIL - only embedded should be this exception",
                     usingEmbedded());
             assertEquals("FAIL - wrong exception", "ERROR 40XD0: Container " +
-                    "has been closed", ioe.getMessage());
+"has been closed.", ioe.getMessage());
         }
 
         rollback();
---------------
-------------
@@ -55,7 +55,7 @@
     {
         try
         {
-            long timeout = System.currentTimeMillis() - startTime + DatabaseDescriptor.getRpcTimeout();
+long timeout = DatabaseDescriptor.getRpcTimeout() - (System.currentTimeMillis() - startTime);
             boolean success;
             try
             {
---------------
-------------
@@ -81,7 +81,7 @@
               //NumericDocValues ndv = ar.getNumericDocValues("number");
               FieldCache.Longs ndv = FieldCache.DEFAULT.getLongs(ar, "number", false);
               //BinaryDocValues bdv = ar.getBinaryDocValues("bytes");
-              BinaryDocValues bdv = FieldCache.DEFAULT.getTerms(ar, "bytes");
+BinaryDocValues bdv = FieldCache.DEFAULT.getTerms(ar, "bytes", false);
               SortedDocValues sdv = FieldCache.DEFAULT.getTermsIndex(ar, "sorted");
               startingGun.await();
               int iters = atLeast(1000);
---------------
-------------
@@ -46,7 +46,7 @@
   }
   
   @AfterClass
-  public static void afterClass() throws Exception {
+public static void afterClass() {
     if (savedFactory == null) {
       System.clearProperty("solr.directoryFactory");
     } else {
---------------
-------------
@@ -95,7 +95,7 @@
         Thread.sleep(1000);
 
         long start = System.currentTimeMillis();
-        CompactionManager.instance.doCompaction(store, sstables, (int) (System.currentTimeMillis() / 1000) - DatabaseDescriptor.getCFMetaData(TABLE1, "Standard1").gcGraceSeconds);
+CompactionManager.instance.doCompaction(store, sstables, (int) (System.currentTimeMillis() / 1000) - DatabaseDescriptor.getCFMetaData(TABLE1, "Standard1").getGcGraceSeconds());
         System.out.println(String.format("%s: sstables=%d rowsper=%d colsper=%d: %d ms",
                                          this.getClass().getName(),
                                          sstableCount,
---------------
-------------
@@ -36,7 +36,7 @@
  *  with something like PCollections -- http://code.google.com
  */
 
-public final class Column implements IColumn
+public class Column implements IColumn
 {
     private static Logger logger_ = Logger.getLogger(Column.class);
 
---------------
-------------
@@ -194,7 +194,7 @@
    * @return facet sort or default of true
    */
   public boolean getFacetSort() {
-    return this.getBool(FacetParams.FACET_SORT, false);
+return this.getBool(FacetParams.FACET_SORT, true);
   }
 
   /** set facet sort
---------------
-------------
@@ -74,7 +74,7 @@
       FixedBitSet visited = new FixedBitSet(ir.maxDoc());
       TermsEnum te = terms.iterator(null);
       while (te.next() != null) {
-        DocsEnum de = _TestUtil.docs(random(), te, null, null, false);
+DocsEnum de = _TestUtil.docs(random(), te, null, null, 0);
         while (de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
           visited.set(de.docID());
         }
---------------
-------------
@@ -3816,7 +3816,7 @@
                 if( i > 20 )
                 {
                     throw StandardException.newException(
-                                SQLState.LOG_FULL, ioe, null );
+SQLState.LOG_FULL, ioe);
                 }
             }
         }
---------------
-------------
@@ -49,7 +49,7 @@
     int subIndex = ReaderUtil.subIndex(number, leaves); // find the reader with this document in it
     SpanTermQuery query = new SpanTermQuery(new Term("field", English.intToEnglish(number).trim()));
     SpanQueryFilter filter = new SpanQueryFilter(query);
-    SpanFilterResult result = filter.bitSpans(leaves[subIndex]);
+SpanFilterResult result = filter.bitSpans(leaves[subIndex], leaves[subIndex].reader.getLiveDocs());
     DocIdSet docIdSet = result.getDocIdSet();
     assertTrue("docIdSet is null and it shouldn't be", docIdSet != null);
     assertContainsDocId("docIdSet doesn't contain docId 10", docIdSet, number - leaves[subIndex].docBase);
---------------
-------------
@@ -30,7 +30,7 @@
 public class TestCartesianShapeFilter extends TestCase {
 
   public void testSerializable() throws IOException {
-    CartesianShapeFilter filter = new CartesianShapeFilter(new Shape("1"),
+CartesianShapeFilter filter = new CartesianShapeFilter(new Shape(1),
         "test");
     try {
       ByteArrayOutputStream bos = new ByteArrayOutputStream();
---------------
-------------
@@ -646,7 +646,7 @@
 
             assert getMemtableThreadSafe() == oldMemtable;
             oldMemtable.freeze();
-            final ReplayPosition ctx = writeCommitLog ? CommitLog.instance.getContext() : null;
+final ReplayPosition ctx = writeCommitLog ? CommitLog.instance.getContext() : ReplayPosition.NONE;
 
             // submit the memtable for any indexed sub-cfses, and our own.
             List<ColumnFamilyStore> icc = new ArrayList<ColumnFamilyStore>(indexedColumns.size());
---------------
-------------
@@ -48,7 +48,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -81,7 +81,7 @@
                             OrdinalMap map, IndexWriter destIndexWriter,
                             DirectoryTaxonomyWriter destTaxWriter) throws IOException {
     // merge the taxonomies
-    destTaxWriter.addTaxonomies(new Directory[] { srcTaxDir }, new OrdinalMap[] { map });
+destTaxWriter.addTaxonomy(srcTaxDir, map);
 
     PayloadProcessorProvider payloadProcessor = new FacetsPayloadProcessorProvider(
         srcIndexDir, map.getMap(), new DefaultFacetIndexingParams());
---------------
-------------
@@ -104,7 +104,7 @@
         if (fractOrAbs < 0)
             throw new UnsupportedOperationException("unexpected negative value " + fractOrAbs);
 
-        if (0 < fractOrAbs && fractOrAbs < 1)
+if (0 < fractOrAbs && fractOrAbs <= 1)
         {
             // fraction
             return Math.max(1, (long)(fractOrAbs * total));
---------------
-------------
@@ -47,7 +47,7 @@
     private String tableName;
     private String oldName;
     private String newName;
-    private int cfId;
+private Integer cfId;
     
     RenameColumnFamily(DataInputStream din) throws IOException
     {
---------------
-------------
@@ -8,7 +8,7 @@
 
 import org.apache.aries.util.filesystem.IDirectory;
 import org.apache.aries.util.filesystem.IFile;
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
---------------
-------------
@@ -585,7 +585,7 @@
 
       // Finally tell anyone who wants to know
       resourceLoader.inform( resourceLoader );
-      resourceLoader.inform( this );
+resourceLoader.inform( this );  // last call before the latch is released.
       instance = this;   // set singleton for backwards compatibility
     } catch (IOException e) {
       throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);
---------------
-------------
@@ -193,7 +193,7 @@
 
   public LogWatcherConfig getLogWatcherConfig() {
     return new LogWatcherConfig(
-        getBool(CfgProp.SOLR_LOGGING_ENABLED, false),
+getBool(CfgProp.SOLR_LOGGING_ENABLED, true),
         get(CfgProp.SOLR_LOGGING_CLASS, null),
         get(CfgProp.SOLR_LOGGING_WATCHER_THRESHOLD, null),
         getInt(CfgProp.SOLR_LOGGING_WATCHER_SIZE, 50)
---------------
-------------
@@ -227,7 +227,7 @@
             }
 
             // Create a BufferedReader to read the list of tests to run
-            runlistFile = new BufferedReader(new InputStreamReader(is));
+runlistFile = new BufferedReader(new InputStreamReader(is, "UTF-8"));
             if (runlistFile == null)
             {
                 System.out.println("The suite runall file could not be read.");
---------------
-------------
@@ -297,7 +297,7 @@
 
         SimpleOrderedMap<Object> fileInfo = new SimpleOrderedMap<Object>();
         files.add(f, fileInfo);
-        List<String> fchildren = zkClient.getChildren(adminFile, null, true);
+List<String> fchildren = zkClient.getChildren(adminFile + "/" + f, null, true);
         if (fchildren.size() > 0) {
           fileInfo.add("directory", true);
         } else {
---------------
-------------
@@ -117,7 +117,7 @@
 
     public boolean isEmpty()
     {
-        boolean cfIrrelevant = ColumnFamilyStore.removeDeleted(emptyColumnFamily, gcBefore) == null;
+boolean cfIrrelevant = ColumnFamilyStore.removeDeletedCF(emptyColumnFamily, gcBefore) == null;
         return cfIrrelevant && columnCount == 0;
     }
 
---------------
-------------
@@ -139,7 +139,7 @@
         int chunkSize = (int) (metadataWriter.chunkOffsetBy(realMark.nextChunkIndex) - chunkOffset - 4);
 
         out.seek(chunkOffset);
-        out.read(compressed, 0, chunkSize);
+out.readFully(compressed, 0, chunkSize);
 
         // decompress data chunk and store its length
         int validBytes = Snappy.rawUncompress(compressed, 0, chunkSize, buffer, 0);
---------------
-------------
@@ -25,7 +25,7 @@
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.CloseableThreadLocal;
-import org.apache.lucene.util.cache.DoubleBarrelLRUCache;
+import org.apache.lucene.util.DoubleBarrelLRUCache;
 
 /** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
  * Directory.  Pairs are accessed either by Term or by ordinal position the
---------------
-------------
@@ -172,7 +172,7 @@
 
         for (File file : clogs)
         {
-            int bufferSize = (int)Math.min(file.length(), 32 * 1024 * 1024);
+int bufferSize = (int) Math.min(Math.max(file.length(), 1), 32 * 1024 * 1024);
             BufferedRandomAccessFile reader = new BufferedRandomAccessFile(new File(file.getAbsolutePath()), "r", bufferSize, true);
 
             try
---------------
-------------
@@ -76,7 +76,7 @@
             byte[] bytes = new byte[readCtx.bufOut_.getLength()];
             System.arraycopy(readCtx.bufOut_.getData(), 0, bytes, 0, bytes.length);
 
-            Message response = message.getReply(FBUtilities.getLocalAddress(), bytes);
+Message response = message.getReply(FBUtilities.getLocalAddress(), bytes, message.getVersion());
             if (logger_.isDebugEnabled())
               logger_.debug(String.format("Read key %s; sending response to %s@%s",
                                           ByteBufferUtil.bytesToHex(command.key), message.getMessageId(), message.getFrom()));
---------------
-------------
@@ -76,7 +76,7 @@
         Long serviceId = (Long) reference.getProperty(Constants.SERVICE_ID);
         //API stipulates versions for compendium services with static ObjectName
         //This shouldn't happen but added as a consistency check
-        if (getTrackingCount() > 0) {
+if (trackedId != null) {
             String serviceDescription = (String) ((reference.getProperty(Constants.SERVICE_DESCRIPTION) != null) ? 
                     reference.getProperty(Constants.SERVICE_DESCRIPTION) : reference.getProperty(Constants.OBJECTCLASS));
             logger.log(LogService.LOG_WARNING, "Detected secondary ServiceReference for [" + serviceDescription
---------------
-------------
@@ -38,7 +38,7 @@
     }
 
     IndexReader r = w.getReader();
-    w.close();
+w.shutdown();
 
     for(String fileName : d.listAll()) {
       try {
---------------
-------------
@@ -324,7 +324,7 @@
     Document doc = new Document();
     doc.add(newTextField("body", "blah the footest blah", Field.Store.NO));
     iw.addDocument(doc);
-    iw.close();
+iw.shutdown();
 
     StandardQueryParser mfqp = new StandardQueryParser();
 
---------------
-------------
@@ -179,7 +179,7 @@
      * @param conChild connection object used to obtain synchronization object
      */
     TemporaryClob (String data, ConnectionChild conChild)
-                          throws IOException, SQLException, StandardException {
+throws IOException, StandardException {
         if (conChild == null) {
             throw new NullPointerException("conChild cannot be <null>");
         }
---------------
-------------
@@ -947,7 +947,7 @@
   public static String getXP(Node n, String xpath, boolean concatAll)
       throws XPathExpressionException {
     NodeList nodes = getNodesFromXP(n, xpath);
-    StringBuffer sb = new StringBuffer();
+StringBuilder sb = new StringBuilder();
     if (nodes.getLength() > 0) {
       for(int i = 0; i < nodes.getLength() ; i++) {
         sb.append(nodes.item(i).getNodeValue() + " ");
---------------
-------------
@@ -69,7 +69,7 @@
   private ComplexPhraseQuery currentPhraseQuery = null;
 
   /** @deprecated Use {@link
-  #ComplexPhraseQueryParser{Version, String, Analyzer)}
+#ComplexPhraseQueryParser(Version, String, Analyzer)}
   instead.*/
   public ComplexPhraseQueryParser(String f, Analyzer a) {
     this(Version.LUCENE_24, f, a);
---------------
-------------
@@ -214,7 +214,7 @@
         }
         SSTableReader ssTable = writer.closeAndOpenReader();
         cfStore.onMemtableFlush(cLogCtx);
-        cfStore.storeLocation(ssTable);
+cfStore.addSSTable(ssTable);
         buffer.close();
         isFlushed_ = true;
         logger_.info("Completed flushing " + this);
---------------
-------------
@@ -462,7 +462,7 @@
     }
 
     // save last input
-    lastInput.copy(input);
+lastInput.copyInts(input);
 
     //System.out.println("  count[0]=" + frontier[0].inputCount);
   }
---------------
-------------
@@ -121,7 +121,7 @@
 
   @Override
   public void write(TextResponseWriter writer, String name, IndexableField f) throws IOException {
-    writer.writeStr(name, f.stringValue(), false);
+writer.writeStr(name, f.stringValue(), true);
   }
 
   @Override
---------------
-------------
@@ -64,7 +64,7 @@
       while (term != null) {
         T shape = readShape(term);
         if( shape != null ) {
-          docs = te.docs(null, docs, false);
+docs = te.docs(null, docs, 0);
           Integer docid = docs.nextDoc();
           while (docid != DocIdSetIterator.NO_MORE_DOCS) {
             idx.add( docid, shape );
---------------
-------------
@@ -983,7 +983,7 @@
         }
         
         @Override
-        public void close() throws IOException {}
+public void close() {}
       }));
 
     try {
---------------
-------------
@@ -722,7 +722,7 @@
                         break;
 
                     case DRDAConstants.DRDA_TYPE_NBOOLEAN:
-                        write1Byte(((Short) inputs[i]).shortValue());
+writeBoolean(((Boolean) inputs[i]).booleanValue());
                         break;
                     case DRDAConstants.DRDA_TYPE_NINTEGER:
                         writeIntFdocaData(((Integer) inputs[i]).intValue());
---------------
-------------
@@ -240,7 +240,7 @@
   public void testParsingAndSearching() throws Exception {
     String field = "content";
     boolean dbg = false;
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, field, new WhitespaceAnalyzer());
+QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, field, new WhitespaceAnalyzer());
     qp.setAllowLeadingWildcard(true);
     String docs[] = {
         "\\ abcdefg1",
---------------
-------------
@@ -410,7 +410,7 @@
 			 */
 			if (! columnTypesAndLengthsMatch())
 			{
-				treeTop = genNormalizeResultSetNode(this, false);	
+treeTop = genNormalizeResultSetNode(false);
 			}
 
 			treeTop = (ResultSetNode) getNodeFactory().getNode(
---------------
-------------
@@ -39,7 +39,7 @@
     String[] similar = spellChecker.suggestSimilar("treeword", 2);
     assertEquals(2, similar.length);
     assertEquals(similar[0], "threeword");
-    assertEquals(similar[1], "twoword");
+assertEquals(similar[1], "oneword");
     spellChecker.close();
     ramDir.close();
   }
---------------
-------------
@@ -34,7 +34,7 @@
  * @since solr 1.3
  */
 public class ContextImpl extends Context {
-  private DataConfig.Entity entity;
+protected DataConfig.Entity entity;
 
   private ContextImpl parent;
 
---------------
-------------
@@ -40,7 +40,7 @@
       return false;
     }
     for(int arcUpto=0;arcUpto<node.numArcs;arcUpto++) {
-      final Builder.Arc arc = node.arcs[arcUpto];
+final Builder.Arc<T> arc = node.arcs[arcUpto];
       if (arc.label != scratchArc.label ||
           !arc.output.equals(scratchArc.output) ||
           ((Builder.CompiledNode) arc.target).address != scratchArc.target ||
---------------
-------------
@@ -74,7 +74,7 @@
 		try {
 			assertSymbolicName("org.apache.aries.subsystem.feature1", subsystem);
 			assertVersion("1.0.0", subsystem);
-			assertConstituents(2, subsystem);
+assertConstituents(3, subsystem);
 			// TODO Test internal events for installation.
 			startSubsystem(subsystem);
 			// TODO Test internal events for starting.
---------------
-------------
@@ -148,7 +148,7 @@
       // from a docsAndPositionsEnum.
     }
     ir.close();
-    iw.close();
+iw.shutdown();
     dir.close();
   }
 }
---------------
-------------
@@ -111,7 +111,7 @@
           minValue = maxValue = 0;
         }
         // if we exceed the range of positive longs we must switch to fixed ints
-        if ((maxValue - minValue) < (((long)1) << 63) && (maxValue - minValue) > 0) {
+if ((maxValue - minValue) < (((long)1) << 63) && (maxValue - minValue) >= 0) {
           writePackedInts(docCount);
         } else {
           writeFixedInts(docCount);
---------------
-------------
@@ -368,7 +368,7 @@
     ***/
 
     long startmask = -1L << startIndex;
-    long endmask = (endIndex&0x3c)==0 ? 0 : -1L >>> (64-endIndex);
+long endmask = (endIndex&0x3f)==0 ? 0 : -1L >>> (64-endIndex);
 
     if (this.wlen <= endWord) {
       this.wlen = endWord;
---------------
-------------
@@ -90,7 +90,7 @@
    *          this method.
    * @return true if more matching documents may remain.
    */
-  protected boolean score(Collector collector, int max, int firstDocID) throws IOException {
+public boolean score(Collector collector, int max, int firstDocID) throws IOException {
     collector.setScorer(this);
     int doc = firstDocID;
     while (doc < max) {
---------------
-------------
@@ -119,7 +119,7 @@
 		@return the length of the data written into data, or -1 if the end of the
 		scan has been reached.
 
-		@exception StandardException Standard Cloudscape error policy
+@exception StandardException Standard Derby error policy
 	*/
 	public LogRecord getNextRecord(ArrayInputStream input, 
 								   TransactionId tranId, 
---------------
-------------
@@ -73,7 +73,7 @@
     Parameters params = new Parameters(context.getConfiguration().get(PFPGrowth.PFP_PARAMETERS, ""));
     
     int i = 0;
-    for (Pair<String,Long> e : PFPGrowth.deserializeList(params, PFPGrowth.F_LIST, context.getConfiguration())) {
+for (Pair<String,Long> e : PFPGrowth.readFList(context.getConfiguration())) {
       fMap.put(e.getFirst(), i++);
     }
     
---------------
-------------
@@ -150,7 +150,7 @@
     return new IRStatisticsImpl(precision.getAverage(), recall.getAverage(), fallOut.getAverage());
   }
 
-  private void processOtherUser(Object id,
+private static void processOtherUser(Object id,
                                 Collection<Item> relevantItems,
                                 Collection<User> trainingUsers,
                                 User user2) {
---------------
-------------
@@ -1084,7 +1084,7 @@
   @Override
   public void setUp() throws Exception {
     super.setUp();
-    indexDir = new File(TEMP_DIR, "IndexReaderReopen");
+indexDir = _TestUtil.getTempDir("IndexReaderReopen");
   }
   
   public void testCloseOrig() throws Throwable {
---------------
-------------
@@ -38,7 +38,7 @@
  * @lucene.internal
  * @lucene.experimental
  */
-final class BytesRefArray {
+public final class BytesRefArray {
   private final ByteBlockPool pool;
   private int[] offsets = new int[1];
   private int lastElement = 0;
---------------
-------------
@@ -251,7 +251,7 @@
       for (int i = 0; i < len; ++i) {
         terms[i] = RandomPicks.randomFrom(random(), sampleTerms);
         if (weird) {
-          positionsIncrements[i] = random().nextInt(1 << 18);
+positionsIncrements[i] = _TestUtil.nextInt(random(), 1, 1 << 18);
           startOffsets[i] = random().nextInt();
           endOffsets[i] = random().nextInt();
         } else if (i == 0) {
---------------
-------------
@@ -46,7 +46,7 @@
   public static class Writer extends DerefBytesWriterBase {
     public Writer(Directory dir, String id, Counter bytesUsed, IOContext context)
         throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context, Type.BYTES_FIXED_DEREF);
     }
 
     @Override
---------------
-------------
@@ -316,7 +316,7 @@
       System.out.println("TEST: done join [" + (System.currentTimeMillis()-t0) + " ms]; addCount=" + addCount + " delCount=" + delCount);
     }
     writer.commit();
-    assertEquals(addCount.get() - delCount.get(), writer.numDocs());
+assertEquals("index=" + writer.segString(), addCount.get() - delCount.get(), writer.numDocs());
       
     writer.close(false);
     dir.close();
---------------
-------------
@@ -338,7 +338,7 @@
 		// and re-use it. This way, the column alteration only changes the
 		// aspects of the autoincrement settings that it intends to change,
 		// and does not lose the other aspecs.
-		if (defaultNode == null)
+if (keepCurrentDefault)
 			defaultInfo = (DefaultInfoImpl)cd.getDefaultInfo();
 		if (autoinc_create_or_modify_Start_Increment ==
 				ColumnDefinitionNode.MODIFY_AUTOINCREMENT_RESTART_VALUE)
---------------
-------------
@@ -142,7 +142,7 @@
             return new BatchInstallResult("Failed to install bundles arguments can't be null").toCompositeData(); 
         }
         
-        if(locations != null && locations != null && locations.length != urls.length){
+if(locations != null && locations.length != urls.length){
             return new BatchInstallResult("Failed to install bundles size of arguments should be same").toCompositeData(); 
         }
         long[] ids = new long[locations.length];
---------------
-------------
@@ -286,7 +286,7 @@
         String segName = input.readString();
         Codec codec = Codec.forName(input.readString());
         //System.out.println("SIS.read seg=" + seg + " codec=" + codec);
-        SegmentInfo info = codec.segmentInfoFormat().getSegmentInfosReader().read(directory, segName, IOContext.READ);
+SegmentInfo info = codec.segmentInfoFormat().getSegmentInfoReader().read(directory, segName, IOContext.READ);
         info.setCodec(codec);
         long delGen = input.readLong();
         int delCount = input.readInt();
---------------
-------------
@@ -34,7 +34,7 @@
 public class TestTopKResultsHandlerRandom extends BaseTestTopK {
   
   private List<FacetResult> countFacets(int partitionSize, int numResults, final boolean doComplement)
-      throws IOException, IllegalAccessException, InstantiationException {
+throws IOException {
     Query q = new MatchAllDocsQuery();
     FacetSearchParams facetSearchParams = searchParamsWithRequests(numResults, partitionSize);
     FacetsCollector fc = new FacetsCollector(facetSearchParams, indexReader, taxoReader) {
---------------
-------------
@@ -56,7 +56,7 @@
 
   A b-tree controller corresponds to an instance of an open b-tree conglomerate.
   <P>
-  <B>Concurrency Notes<\B>
+<B>Concurrency Notes</B>
   <P>
   The concurrency rules are derived from OpenBTree.
   <P>
---------------
-------------
@@ -72,7 +72,7 @@
 
     // plan to add a set of useful stopwords, consider changing some of the
     // interior filters.
-    StandardAnalyzer analyzer = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT, Collections.emptySet());
+StandardAnalyzer analyzer = new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet());
     // TODO: something about lock timeouts and leftover locks.
     IndexWriter writer = new IndexWriter(storeDirectory, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
     writer.commit();
---------------
-------------
@@ -2044,7 +2044,7 @@
         
         cSt = prepareCall(
             "call SYSCS_UTIL.SYSCS_EXPORT_TABLE ('IEP', 'T1' , "
-            + "'extinout/t1.dat' , '\\a', '\\', null) ");
++ "'extinout/t1.dat' , '\\', '\\', null) ");
         assertStatementError("XIE0J", cSt);
                 
         //DO A VALID EXPORT AND  IMPORT
---------------
-------------
@@ -245,7 +245,7 @@
 		// current plans using "this" node as the key.  If needed, we'll
 		// then make the call to revert the plans in OptimizerImpl's
 		// getNextDecoratedPermutation() method.
-		addOrLoadBestPlanMapping(true, this);
+updateBestPlanMap(ADD_PLAN, this);
 
 		leftResultSet = optimizeSource(
 							optimizer,
---------------
-------------
@@ -34,7 +34,7 @@
 	messages can be localized.
 
 	REMIND: May want to investigate putting some of this in the protocol
-	side, for the errors that any Cloudscape JDBC driver might return.
+side, for the errors that any Derby JDBC driver might return.
 
 	The ASSERT mechanism is a wrapper of the basic services,
 	to ensure that failed asserts at this level will behave
---------------
-------------
@@ -246,7 +246,7 @@
           SolrResourceLoader loader = core.getResourceLoader();
           SolrSpellChecker checker = (SolrSpellChecker) loader.newInstance(className);
           if (checker != null) {
-            String dictionary = checker.init(spellchecker, loader);
+String dictionary = checker.init(spellchecker, core);
             if (dictionary != null) {
               boolean isDefault = dictionary.equals(SolrSpellChecker.DEFAULT_DICTIONARY_NAME);
               if (isDefault == true && hasDefault == false){
---------------
-------------
@@ -104,7 +104,7 @@
     addParser("literal", new ValueSourceParser() {
       @Override
       public ValueSource parse(FunctionQParser fp) throws ParseException {
-        return new LiteralValueSource(fp.getString());
+return new LiteralValueSource(fp.parseArg());
       }
     });
     addParser("rord", new ValueSourceParser() {
---------------
-------------
@@ -318,7 +318,7 @@
         DecoratedKey dkey = StorageService.getPartitioner().decorateKey(MIGRATIONS_KEY);
         Table defs = Table.open(Table.SYSTEM_TABLE);
         ColumnFamilyStore cfStore = defs.getColumnFamilyStore(Migration.MIGRATIONS_CF);
-        QueryFilter filter = QueryFilter.getSliceFilter(dkey, new QueryPath(MIGRATIONS_CF), UUIDGen.decompose(start), UUIDGen.decompose(end), null, false, 1000);
+QueryFilter filter = QueryFilter.getSliceFilter(dkey, new QueryPath(MIGRATIONS_CF), UUIDGen.decompose(start), UUIDGen.decompose(end), false, 1000);
         ColumnFamily cf = cfStore.getColumnFamily(filter);
         return cf.getSortedColumns();
     }
---------------
-------------
@@ -84,7 +84,7 @@
 	}
 
 	/**
-		@exception StandardException Standard Cloudscape error policy
+@exception StandardException Standard Derby error policy
 	*/
 	protected void doIt(BaseContainerHandle handle)
 		throws StandardException {
---------------
-------------
@@ -69,7 +69,7 @@
   public CharsRef indexedToReadable(BytesRef input, CharsRef charsRef) {
     // TODO: this could be more efficient, but the sortable types should be deprecated instead
     final char[] indexedToReadable = indexedToReadable(input.utf8ToChars(charsRef).toString()).toCharArray();
-    charsRef.copy(indexedToReadable, 0, indexedToReadable.length);
+charsRef.copyChars(indexedToReadable, 0, indexedToReadable.length);
     return charsRef;
   }
 
---------------
-------------
@@ -125,7 +125,7 @@
   protected Reader openStream(File file) throws FileNotFoundException,
           UnsupportedEncodingException {
     if (encoding == null) {
-      return new InputStreamReader(new FileInputStream(file));
+return new InputStreamReader(new FileInputStream(file), "UTF-8");
     } else {
       return new InputStreamReader(new FileInputStream(file), encoding);
     }
---------------
-------------
@@ -498,7 +498,7 @@
 
       final IndexReader reader = searcher.getIndexReader();
       if (reader.maxDoc() > 0) {
-        for (final AtomicReaderContext ctx : reader.getTopReaderContext().leaves()) {
+for (final AtomicReaderContext ctx : reader.leaves()) {
           Terms terms = ctx.reader().terms(F_WORD);
           if (terms != null)
             termsEnums.add(terms.iterator(null));
---------------
-------------
@@ -137,7 +137,7 @@
   // Test that FieldScoreQuery returns docs with expected score.
   private void doTestCustomScore (String field, FieldScoreQuery.Type tp, double dboost) throws CorruptIndexException, Exception {
     float boost = (float) dboost;
-    IndexSearcher s = new IndexSearcher(dir);
+IndexSearcher s = new IndexSearcher(dir, true);
     FieldScoreQuery qValSrc = new FieldScoreQuery(field,tp); // a query that would score by the field
     QueryParser qp = new QueryParser(TEXT_FIELD,anlzr); 
     String qtxt = "first aid text"; // from the doc texts in FunctionQuerySetup.
---------------
-------------
@@ -60,7 +60,7 @@
     }
 
     Document doc = new Document();
-    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);
+FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);
     ft.setOmitNorms(true);
     ft.setIndexOptions(IndexOptions.DOCS_ONLY);
     Field field = new Field("field", new MyTokenStream(), ft);
---------------
-------------
@@ -446,7 +446,7 @@
   }
 
   /** for testing DateTools support */
-  private String getDate(Date d, DateTools.Resolution resolution) throws Exception {
+private String getDate(Date d, DateTools.Resolution resolution) {
     return DateTools.dateToString(d, resolution);
   }
 
---------------
-------------
@@ -32,7 +32,7 @@
  */
 public interface MultiTermRewriteMethodAttribute extends Attribute {
   
-  public static final CharSequence TAG_ID = "MultiTermRewriteMethodAttribute";
+public static final String TAG_ID = "MultiTermRewriteMethodAttribute";
   
   public void setMultiTermRewriteMethod(MultiTermQuery.RewriteMethod method);
 
---------------
-------------
@@ -89,7 +89,7 @@
 
     // test that leader and replica have same doc count
     
-    checkShardConsistency("shard1", false); 
+checkShardConsistency("shard1", false, false);
     SolrQuery query = new SolrQuery("*:*");
     query.setParam("distrib", "false");
     long client1Docs = shardToJetty.get("shard1").get(0).client.solrClient.query(query).getResults().getNumFound();
---------------
-------------
@@ -137,7 +137,7 @@
         {
             Column c = new Column();
             c.setName(Arrays.copyOf(word.getBytes(), word.getLength()));
-            c.setValue(ByteBufferUtil.bytes(String.valueOf(sum));
+c.setValue(ByteBufferUtil.bytes(String.valueOf(sum)));
             c.setTimestamp(System.currentTimeMillis());
 
             Mutation m = new Mutation();
---------------
-------------
@@ -44,7 +44,7 @@
 import org.apache.derby.iapi.store.access.xa.XAXactId;
 import org.apache.derby.impl.jdbc.EmbedConnection;
 import org.apache.derby.impl.jdbc.TransactionResourceImpl;
-import org.apache.derby.shared.common.sanity.SanityManager;
+import org.apache.derby.iapi.services.sanity.SanityManager;
 import org.apache.derby.iapi.services.property.PropertyUtil;
 import org.apache.derby.iapi.reference.Property;
 
---------------
-------------
@@ -1,6 +1,6 @@
 /*
 
-   Derby - Class org.apache.derbyTesting.functionTests.tests.jdbcapi.parameterMapping
+Derby - Class org.apache.derbyTesting.functionTests.tests.junitTests.derbyNet.CompatibilityTest
 
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -777,7 +777,7 @@
 													(RequiredRowOrdering) null,
 													getCompilerContext().getNumTables(),
 													  lcc);
-			((OptimizerImpl)optimizer).prepForNextRound();
+optimizer.prepForNextRound();
 
 			if (sourceResultSet == leftResultSet)
 			{
---------------
-------------
@@ -75,7 +75,7 @@
       disconnectedTimer = null;
     }
     if (!isClosed) {
-      disconnectedTimer = new Timer();
+disconnectedTimer = new Timer(true);
       disconnectedTimer.schedule(new TimerTask() {
         
         @Override
---------------
-------------
@@ -63,7 +63,7 @@
     writer.addDocument(createDocument("Document 5", 1192209943000L));
 
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
   }
 
   @Override
---------------
-------------
@@ -196,7 +196,7 @@
       indexReaderLock.readLock().lock();
       // TODO (Facet): avoid Multi*?
       Bits liveDocs = MultiFields.getLiveDocs(indexReader);
-      DocsEnum docs = MultiFields.getTermDocsEnum(indexReader, liveDocs, Consts.FULL, new BytesRef(path), false);
+DocsEnum docs = MultiFields.getTermDocsEnum(indexReader, liveDocs, Consts.FULL, new BytesRef(path), 0);
       if (docs != null && docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
         ret = docs.docID();
       }
---------------
-------------
@@ -27,7 +27,7 @@
   /** An array containing some common English words that are not usually useful
     for searching. */
   public static final String[] ENGLISH_STOP_WORDS = {
-    "a", "and", "are", "as", "at", "be", "but", "by",
+"a", "an", "and", "are", "as", "at", "be", "but", "by",
     "for", "if", "in", "into", "is", "it",
     "no", "not", "of", "on", "or", "s", "such",
     "t", "that", "the", "their", "then", "there", "these",
---------------
-------------
@@ -97,7 +97,7 @@
         KSMetaData ksm = makeNewKeyspaceDefinition(existing);
         CFMetaData.purge(cfm);
         DatabaseDescriptor.setTableDefinition(ksm, newVersion);
-        Table.open(ksm.name).dropCf(cfm.cfName);
+Table.open(ksm.name).dropCf(cfm.cfId);
         
         // indicate that some files need to be deleted (eventually)
         SystemTable.markForRemoval(cfm);
---------------
-------------
@@ -54,7 +54,7 @@
         this.sessionId = sessionId;
     }
 
-    public Message getMessage(int version) throws IOException
+public Message getMessage(Integer version) throws IOException
     {
         ByteArrayOutputStream bos = new ByteArrayOutputStream();
         DataOutputStream dos = new DataOutputStream( bos );
---------------
-------------
@@ -77,7 +77,7 @@
         // reinitialize the table.
         KSMetaData ksm = DatabaseDescriptor.getTableDefinition(cfm.tableName);
         ksm = makeNewKeyspaceDefinition(ksm);
-        Table.open(ksm.name).addCf(cfm.cfName);
+Table.open(ksm.name).initCf(cfm.cfId, cfm.cfName);
         DatabaseDescriptor.setTableDefinition(ksm, newVersion);
         
         // force creation of a new commit log segment.
---------------
-------------
@@ -29,7 +29,7 @@
 
 public class Job {
 
-  public static void main(String[] args) throws Exception {
+public static void main(String[] args) throws IOException {
     if (args.length == 6) {
       String input = args[0];
       String output = args[1];
---------------
-------------
@@ -67,7 +67,7 @@
         }
         boolean shouldPurge = major || !cfStore.isKeyInRemainingSSTables(key, sstables);
 
-        if (rows.size() > 1 || shouldPurge)
+if (rows.size() > 1 || shouldPurge || !rows.get(0).sstable.descriptor.isLatestVersion)
         {
             ColumnFamily cf = null;
             for (SSTableIdentityIterator row : rows)
---------------
-------------
@@ -20,7 +20,7 @@
   private CDFitness expected;
 
   @Override
-  protected void setUp() throws Exception {
+protected void setUp() {
     // generate random evaluatons and calculate expectations
     evaluations = new ArrayList<CDFitness>();
     Random rng = new Random();
---------------
-------------
@@ -177,7 +177,7 @@
       while(it.hasNext()) {
         EntityManagerFactoryManager mgr = it.next();
         ServiceReference reference = getProviderServiceReference(mgr.getParsedPersistenceUnits());
-        if(ref != null) {
+if(reference != null) {
           managersToManage.put(mgr, reference);
           it.remove();
         }
---------------
-------------
@@ -217,7 +217,7 @@
     Directory directory = newDirectory();
     Analyzer stopAnalyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET);
     RandomIndexWriter writer = new RandomIndexWriter(random(), directory, 
-        newIndexWriterConfig( Version.LUCENE_40, stopAnalyzer));
+newIndexWriterConfig(TEST_VERSION_CURRENT, stopAnalyzer));
     Document doc = new Document();
     doc.add(newTextField("field", "the stop words are here", Field.Store.YES));
     writer.addDocument(doc);
---------------
-------------
@@ -53,7 +53,7 @@
     String text = "This is the text to be indexed. " + longTerm;
     doc.add(newTextField("fieldname", text, Field.Store.YES));
     iwriter.addDocument(doc);
-    iwriter.close();
+iwriter.shutdown();
     
     // Now search the index:
     IndexReader ireader = DirectoryReader.open(directory); // read-only=true
---------------
-------------
@@ -64,7 +64,7 @@
         LOGGER.debug("Starting blueprint extender...");
 
         this.context = context;
-        eventDispatcher = new BlueprintEventDispatcher(context);
+eventDispatcher = new BlueprintEventDispatcher(context, executors);
         handlers = new NamespaceHandlerRegistryImpl(context);
         executors = Executors.newScheduledThreadPool(3);
         containers = new HashMap<Bundle, BlueprintContainerImpl>();
---------------
-------------
@@ -203,7 +203,7 @@
       for (int i=0; i<qiter; i++) {
         Filter filt = new Filter() {
           @Override
-          public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
             return BitsFilteredDocIdSet.wrap(randSet(context.reader().maxDoc()), acceptDocs);
           }
         };
---------------
-------------
@@ -92,7 +92,7 @@
     long end = System.currentTimeMillis();
     System.out.println("milliseconds for creation of " + ndocs + " docs = " + (end-start));
 
-    IndexReader reader = IndexReader.open(dir);
+IndexReader reader = IndexReader.open(dir, true);
     TermEnum tenum = reader.terms(new Term("foo","val"));
     TermDocs tdocs = reader.termDocs();
 
---------------
-------------
@@ -477,7 +477,7 @@
     }
 
     @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags /* ignored */) throws IOException {
+public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs /* ignored */) throws IOException {
       TVDocsEnum docsEnum;
       if (reuse != null && reuse instanceof TVDocsEnum) {
         docsEnum = (TVDocsEnum) reuse;
---------------
-------------
@@ -113,7 +113,7 @@
                                          field("first",  "bubba"),
                                          field("last",   "jones")     }));
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     searcher = newSearcher(reader);
   }
 
---------------
-------------
@@ -2075,7 +2075,7 @@
             return Types.REAL;
 
         if ("DOUBLE".equals(type) || "DOUBLE PRECISION".equals(type))
-            return Types.INTEGER;
+return Types.DOUBLE;
         
         if ("DATE".equals(type))
             return Types.DATE;
---------------
-------------
@@ -57,7 +57,7 @@
     String xml = 
       "<random>" +
       " <document>" +
-      "  <node name=\"id\" enhance=\"2.2\" value=\"12345\"/>" +
+"  <node name=\"id\" value=\"12345\"/>" +
       "  <node name=\"name\" value=\"kitten\"/>" +
       "  <node name=\"text\" enhance=\"3\" value=\"some other day\"/>" +
       "  <node name=\"title\" enhance=\"4\" value=\"A story\"/>" +
---------------
-------------
@@ -43,7 +43,7 @@
 
         try
         {
-            StreamReply reply = StreamReply.serializer.deserialize(new DataInputStream(bufIn));
+StreamReply reply = StreamReply.serializer.deserialize(new DataInputStream(bufIn), message.getVersion());
             logger.debug("Received StreamReply {}", reply);
             StreamOutSession session = StreamOutSession.get(message.getFrom(), reply.sessionId);
 
---------------
-------------
@@ -28,7 +28,7 @@
   by wrapping composite readers with {@link SlowCompositeReaderWrapper}.
  
  <p>IndexReader instances for indexes on disk are usually constructed
- with a call to one of the static <code>DirectoryReader,open()</code> methods,
+with a call to one of the static <code>DirectoryReader.open()</code> methods,
  e.g. {@link DirectoryReader#open(Directory)}. {@link DirectoryReader} implements
  the {@code CompositeReader} interface, it is not possible to directly get postings.
  <p> Concrete subclasses of IndexReader are usually constructed with a call to
---------------
-------------
@@ -47,6 +47,6 @@
   @Override
   protected SolrServer createNewSolrServer()
   {
-    return new EmbeddedSolrServer( h.getCore() );
+return new EmbeddedSolrServer( h.getCoreContainer(), "" );
   }
 }
---------------
-------------
@@ -632,7 +632,7 @@
       randomIOExceptionRateOnOpen = 0.0;
       if (DirectoryReader.indexExists(this)) {
         if (LuceneTestCase.VERBOSE) {
-          System.out.println("\nNOTE: MockDirectoryWrapper: now crash");
+System.out.println("\nNOTE: MockDirectoryWrapper: now crush");
         }
         crash(); // corrupt any unsynced-files
         if (LuceneTestCase.VERBOSE) {
---------------
-------------
@@ -177,7 +177,7 @@
     this.numIndexEntries = (nIndexEntries >= 0) ? nIndexEntries : 0;
     long maxIndexEntry = maxHighValue + numValues - 1; // clear upper bits, set upper bits, start at zero
     this.nIndexEntryBits = (maxIndexEntry <= 0) ? 0
-                          : (64 - Long.numberOfLeadingZeros(maxIndexEntry - 1));
+: (64 - Long.numberOfLeadingZeros(maxIndexEntry));
     long numLongsForIndexBits = numLongsForBits(numIndexEntries * nIndexEntryBits);
     if (numLongsForIndexBits > Integer.MAX_VALUE) {
       throw new IllegalArgumentException("numLongsForIndexBits too large to index a long array: " + numLongsForIndexBits);
---------------
-------------
@@ -32,7 +32,7 @@
 import org.apache.aries.unittest.fixture.ArchiveFixture;
 import org.apache.aries.unittest.fixture.ArchiveFixture.ZipFixture;
 import org.apache.aries.util.filesystem.FileUtils;
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 import org.junit.Test;
 import org.osgi.framework.Constants;
 
---------------
-------------
@@ -63,7 +63,7 @@
   @Override
   protected BytesRef getDocGroupValue(int doc) {
     final int ord = index.getOrd(doc);
-    return ord == 0 ? null : index.lookup(ord, scratchBytesRef);
+return ord == -1 ? null : index.lookup(ord, scratchBytesRef);
   }
 
   @Override
---------------
-------------
@@ -79,7 +79,7 @@
                 "%nAvailable commands: ring, info, cleanup, compact, cfstats, snapshot [snapshotname], clearsnapshot, " +
                 "tpstats, flush, drain, repair, decommission, move, loadbalance, removetoken, " +
                 "setcachecapacity <keyspace> <cfname> <keycachecapacity> <rowcachecapacity>, " +
-                "getcompactionthreshold, setcompactionthreshold [minthreshold] ([maxthreshold])" +
+"getcompactionthreshold, setcompactionthreshold [minthreshold] ([maxthreshold]), " +
                 "streams [host]");
         String usage = String.format("java %s --host <arg> <command>%n", NodeCmd.class.getName());
         hf.printHelp(usage, "", options, header);
---------------
-------------
@@ -868,7 +868,7 @@
     if (r.nextBoolean()) {
       if (rarely(r)) {
         // crazy value
-        c.setTermIndexInterval(random.nextBoolean() ? _TestUtil.nextInt(r, 1, 31) : _TestUtil.nextInt(r, 129, 1000));
+c.setTermIndexInterval(r.nextBoolean() ? _TestUtil.nextInt(r, 1, 31) : _TestUtil.nextInt(r, 129, 1000));
       } else {
         // reasonable value
         c.setTermIndexInterval(_TestUtil.nextInt(r, 32, 128));
---------------
-------------
@@ -44,7 +44,7 @@
   /** Creates a new LimitTokenPositionFilterFactory */
   public LimitTokenPositionFilterFactory(Map<String,String> args) {
     super(args);
-    maxTokenPosition = getInt(args, MAX_TOKEN_POSITION_KEY);
+maxTokenPosition = requireInt(args, MAX_TOKEN_POSITION_KEY);
     consumeAllTokens = getBoolean(args, CONSUME_ALL_TOKENS_KEY, false);
     if (!args.isEmpty()) {
       throw new IllegalArgumentException("Unknown parameters: " + args);
---------------
-------------
@@ -453,7 +453,7 @@
   public Query rewrite(IndexReader reader) throws IOException {
     final Query childRewrite = childQuery.rewrite(reader);
     if (childRewrite != childQuery) {
-      Query rewritten = new ToParentBlockJoinQuery(childQuery,
+Query rewritten = new ToParentBlockJoinQuery(origChildQuery,
                                 childRewrite,
                                 parentsFilter,
                                 scoreMode);
---------------
-------------
@@ -224,7 +224,7 @@
     lastWordId = wordId;
   }
 
-  protected final String getBaseFileName(String baseDir) throws IOException {
+protected final String getBaseFileName(String baseDir) {
     return baseDir + File.separator + implClazz.getName().replace('.', File.separatorChar);
   }
   
---------------
-------------
@@ -100,7 +100,7 @@
     hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals(0, hits.length);
     
-    writer.close();
+writer.shutdown();
     reader.close();
     directory.close();
   }
---------------
-------------
@@ -33,7 +33,7 @@
 <LI> org.apache.derby.iapi.store.access.xa.XAXactId : 
 this class is a specific implementation of the JTA Xid interface
 <LI> org.apache.derby.impl.store.access.GlobalXactId : 
-this class represents internal cloudscape transaction ids
+this class represents internal Derby transaction ids
 </UL>
 <P>
 The main reason for this class is to ensure that equality etc. works in a
---------------
-------------
@@ -89,7 +89,7 @@
 			// of the first allocation page. And it is because we
 			// just opened the stream and the first allocation page
 			// is located at the beginning of the file.
-			readHeader(dis);
+readHeader(getEmbryonicPage(dis));
 
 			return true;
 
---------------
-------------
@@ -241,7 +241,7 @@
 
   @Override
   public void write(TextResponseWriter writer, String name, IndexableField f) throws IOException {
-    writer.writeStr(name, f.stringValue(), false);
+writer.writeStr(name, f.stringValue(), true);
   }
 
   @Override
---------------
-------------
@@ -205,7 +205,7 @@
   /** Whether the relevance score is needed to sort documents. */
   boolean needsScores() {
     for (SortField sortField : fields) {
-      if (sortField.getType() == SortField.Type.SCORE) {
+if (sortField.needsScores()) {
         return true;
       }
     }
---------------
-------------
@@ -80,7 +80,7 @@
   private void fill() throws IOException {
     StringBuilder buffered = new StringBuilder();
     char [] temp = new char [1024];
-    for (int cnt = in.read(temp); cnt > 0; cnt = in.read(temp)) {
+for (int cnt = input.read(temp); cnt > 0; cnt = input.read(temp)) {
       buffered.append(temp, 0, cnt);
     }
     transformedInput = new StringReader(processPattern(buffered).toString());
---------------
-------------
@@ -43,7 +43,7 @@
       writer.addDocument(doc);
     }
     writer.close();
-    IndexReader reader = IndexReader.open(dir);
+IndexReader reader = IndexReader.open(dir, false);
     reader.deleteDocument(5);
     reader.close();
 
---------------
-------------
@@ -255,7 +255,7 @@
  **/
 final class TermStatsQueue extends PriorityQueue<TermStats> {
   TermStatsQueue(int size) {
-    initialize(size);
+super(size);
   }
   
   @Override
---------------
-------------
@@ -75,7 +75,7 @@
         MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
         try
         {
-            mbs.registerMBean(this, new ObjectName("org.apache.cassandra.concurrent:type=COMMITLOG"));
+mbs.registerMBean(this, new ObjectName("org.apache.cassandra.db:type=Commitlog"));
         }
         catch (Exception e)
         {
---------------
-------------
@@ -887,7 +887,7 @@
           }
 
           // reboot the writer on the new index
-          core.getUpdateHandler().newIndexWriter();
+core.getUpdateHandler().newIndexWriter(true);
 
         } catch (IOException e) {
           LOG.warn("Unable to get IndexCommit on startup", e);
---------------
-------------
@@ -125,7 +125,7 @@
           final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(
               "OutOfMemoryError likely caused by the Sun VM Bug described in "
               + "https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize "
-              + "with a value smaller than the current chunks size (" + chunkSize + ")");
++ "with a value smaller than the current chunk size (" + chunkSize + ")");
           outOfMemoryError.initCause(e);
           throw outOfMemoryError;
         }
---------------
-------------
@@ -1010,7 +1010,7 @@
       final Class<? extends Directory> clazz = Class.forName(clazzName).asSubclass(Directory.class);
       // If it is a FSDirectory type, try its ctor(File)
       if (FSDirectory.class.isAssignableFrom(clazz)) {
-        final File tmpFile = File.createTempFile("test", "tmp", TEMP_DIR);
+final File tmpFile = _TestUtil.createTempFile("test", "tmp", TEMP_DIR);
         tmpFile.delete();
         tmpFile.mkdir();
         registerTempFile(tmpFile);
---------------
-------------
@@ -350,7 +350,7 @@
     if (index >= 0 && index < size()) {
       setQuick(index, value);
     } else {
-      throw new IndexException();
+throw new IndexException(index, size());
     }
   }
 
---------------
-------------
@@ -60,7 +60,7 @@
         return gDigestList_;
     }
     
-    Map<InetAddress, EndPointState> getEndPointStateMap()
+Map<InetAddress, EndPointState> getEndpointStateMap()
     {
         return epStateMap_;
     }
---------------
-------------
@@ -60,7 +60,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -41,7 +41,7 @@
   @Override
   public void setUp() throws Exception {
     super.setUp();
-    tempDir = File.createTempFile("jrecrash", "tmp", TEMP_DIR);
+tempDir = _TestUtil.getTempDir("jrecrash");
     tempDir.delete();
     tempDir.mkdir();
   }
---------------
-------------
@@ -530,7 +530,7 @@
       
       throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,
                               "Error CREATEing SolrCore '" + dcore.getName() + "': " +
-                              ex.getMessage(), ex);
+ex.getMessage() + rootMsg, ex);
     }
   }
 
---------------
-------------
@@ -257,7 +257,7 @@
         // count collection array only needs to be as big as the number of terms we are
         // going to collect counts for.
         final int[] counts = this.counts = new int[nTerms];
-        DocIdSet idSet = baseSet.getDocIdSet(context);
+DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs
         DocIdSetIterator iter = idSet.iterator();
 
 
---------------
-------------
@@ -173,6 +173,6 @@
     FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
     SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
     sfb.setMultiValuedSeparator( '/' );
-    assertEquals( " b c//<b>d</b> e", sfb.createFragment( reader, 0, F, ffl ) );
+assertEquals( "//a b c//<b>d</b> e", sfb.createFragment( reader, 0, F, ffl ) );
   }
 }
---------------
-------------
@@ -25,7 +25,7 @@
 import org.apache.mahout.common.RandomUtils;
 
 /** A {@link WritableComparable} encapsulating a user ID. */
-public final class UserWritable implements WritableComparable<UserWritable> {
+public class UserWritable implements WritableComparable<UserWritable> {
 
   private long userID;
 
---------------
-------------
@@ -27,7 +27,7 @@
 import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.*;
+import static org.apache.lucene.analysis.VocabularyAssert.*;
 
 /**
  * Test the German stemmer. The stemming algorithm is known to work less 
---------------
-------------
@@ -61,7 +61,7 @@
    * Returns the current Term in the enumeration.
    */
   public Term term() {
-    return /*term == null ? null :*/ term.getTerm();
+return term == null ? null : term.getTerm();
   }
 
   /**
---------------
-------------
@@ -160,7 +160,7 @@
 
         public VersionedValue rpcaddress(InetAddress endpoint)
         {
-            return new VersionedValue(endpoint.toString());
+return new VersionedValue(endpoint.getHostAddress());
         }
 
         public VersionedValue releaseVersion()
---------------
-------------
@@ -30,7 +30,7 @@
 
 public class TimestampReconcilerTest
 {   
-    private static final TimestampReconciler reconciler = new TimestampReconciler();
+private static final TimestampReconciler reconciler = TimestampReconciler.instance;
 
     @Test
     public void testReconcileNormal()
---------------
-------------
@@ -40,7 +40,7 @@
       doc.add(f);
       final Field idField = newField("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS);
       doc.add(idField);
-      int num = atLeast(5000);
+int num = atLeast(4097);
       for(int id=0;id<num;id++) {
         if (random.nextInt(4) == 3) {
           f.setValue("a");
---------------
-------------
@@ -55,7 +55,7 @@
 		Obtain a Container shared or exclusive lock	until
 		the end of the nested transaction.
 
-		@exception StandardException Standard Cloudscape error policy
+@exception StandardException Standard Derby error policy
 	*/
 	public boolean lockContainer(
     Transaction     t, 
---------------
-------------
@@ -487,7 +487,7 @@
                 {                        
                     throw new ConfigurationException("read_repair_chance must be between 0.0 and 1.0");
                 }
-                cfDefs[j++] = new CFMetaData(keyspace.name, cf.name, columnType, comparator, subcolumnComparator, cf.comment, cf.rows_cached, cf.preloadRowCache, cf.keys_cached, cf.read_repair_chance);
+cfDefs[j++] = new CFMetaData(keyspace.name, cf.name, columnType, comparator, subcolumnComparator, cf.comment, cf.rows_cached, cf.preload_row_cache, cf.keys_cached, cf.read_repair_chance);
             }
             defs.add(new KSMetaData(keyspace.name, strategyClass, keyspace.replication_factor, cfDefs));
             
---------------
-------------
@@ -156,7 +156,7 @@
 
   @Override
   public String toString() {
-    return super.toString() + " lockFactory=" + getLockFactory();
+return getClass().getSimpleName() + '@' + Integer.toHexString(hashCode()) + " lockFactory=" + getLockFactory();
   }
 
   /**
---------------
-------------
@@ -405,7 +405,7 @@
 		if (shutdownDatabase != null)
 			info.put(Attribute.SHUTDOWN_ATTR, "true");
 
-		Connection conn = findDriver().connect(jdbcurl, info);
+Connection conn = findDriver().connect( jdbcurl, info, loginTimeout );
 
 		// JDBC driver's getConnection method returns null if
 		// the driver does not handle the request's URL.
---------------
-------------
@@ -78,7 +78,7 @@
 
   @Override
   public void testSize() {
-    assertEquals("size", 3, getTestVector().getNumNondefaultElements());
+assertEquals("size", 3, getTestVector().getNumNonZeroElements());
   }
 
   @Override
---------------
-------------
@@ -160,7 +160,7 @@
 
   /** verify if a node contains a tag */
   public boolean containsTag(String tagName) {
-    return this.tags.containsKey(tagName);
+return this.tags.containsKey(tagName.toLowerCase());
   }
 
   public Object getTag(String tagName) {
---------------
-------------
@@ -70,7 +70,7 @@
       writer.addDocument(doc);
     }
     writer.close();
-    searcher = new IndexSearcher(directory);
+searcher = new IndexSearcher(directory, true);
   }
 
   protected String[] docFields = {
---------------
-------------
@@ -57,7 +57,7 @@
     int numFullGroups = aLen / 3;
     int numBytesInPartialGroup = aLen - 3 * numFullGroups;
     int resultLen = 4 * ((aLen + 2) / 3);
-    StringBuffer result = new StringBuffer(resultLen);
+StringBuilder result = new StringBuilder(resultLen);
     char[] intToAlpha = intToBase64;
 
     // Translate all full groups from byte array elements to Base64
---------------
-------------
@@ -58,7 +58,7 @@
       writer.addDocument(doc);
     }
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
   }
   
   @Override
---------------
-------------
@@ -45,7 +45,7 @@
       Element e = elements.next();
       result += e.get() * getScoreForLabelFeature(label, e.index());
     }
-    return -result;
+return result;
   }
   
   @Override
---------------
-------------
@@ -90,7 +90,7 @@
    *  are enabled, to write the vectors to
    *  RAMOutputStream, which is then quickly flushed to
    *  the real term vectors files in the Directory. */  @Override
-  void finish() throws IOException {
+void finish() {
     if (!doVectors || termsHashPerField.bytesHash.size() == 0) {
       return;
     }
---------------
-------------
@@ -82,7 +82,7 @@
 
 
 	/**
-		Committed Drop state of the container.  If a post comit action
+Committed Drop state of the container.  If a post commit action
 		determined that the drop container operation is committed, the whole
 		container may be removed and space reclaimed.
 
---------------
-------------
@@ -94,7 +94,7 @@
      */
 	public QueryTreeNode bind() throws StandardException
 	{
-        privileges = (PrivilegeNode) privileges.bind( new HashMap());
+privileges = (PrivilegeNode) privileges.bind( new HashMap(), grantees);
         return this;
     } // end of bind
 
---------------
-------------
@@ -2489,7 +2489,7 @@
 				if (pos < args.length) {
 					setSSLMode(getSSLModeValue(args[pos]));
 				} else {
-					setSSLMode(SSL_OFF);
+consolePropertyMessage("DRDA_MissingValue.U", "DRDA_SslMode.I");
 				}
 				break;
 
---------------
-------------
@@ -57,7 +57,7 @@
       writer.addDocument(doc);
     }
     writer.close();
-    reader = IndexReader.open(directory);
+reader = IndexReader.open(directory, true);
   }
 
   public void testInfoStream() throws Exception {
---------------
-------------
@@ -52,7 +52,7 @@
 public class OversampleWithDepthTest extends LuceneTestCase {
   
   @Test
-  public void testCountWithdepthUsingSamping() throws Exception, IOException {
+public void testCountWithdepthUsingSampling() throws Exception, IOException {
     Directory indexDir = newDirectory();
     Directory taxoDir = newDirectory();
     
---------------
-------------
@@ -86,7 +86,7 @@
              "org.apache.hadoop.io.serializer.JavaSerialization,org.apache.hadoop.io.serializer.WritableSerialization"); 
     // Dont ever forget this. People should keep track of how hadoop conf parameters and make or break a piece of code
 
-    FileSystem dfs = FileSystem.get(conf);
+FileSystem dfs = FileSystem.get(outPath.toUri(), conf);
     if (dfs.exists(outPath))
       dfs.delete(outPath, true);
 
---------------
-------------
@@ -544,7 +544,7 @@
 				else
 				{
 					throw StandardException.newException(SQLState.LANG_AMBIGUOUS_COLUMN_NAME, 
-							 columnReference.getFullColumnName());
+columnReference.getSQLColumnName());
 				}
 			}
 
---------------
-------------
@@ -80,7 +80,7 @@
       }
     }
     w.forceMerge(1);
-    w.close();
+w.shutdown();
     dir.close();
   }
   
---------------
-------------
@@ -96,7 +96,7 @@
 		@see org.apache.derby.iapi.store.raw.xact.RawTransaction#recoveryRollbackFirst
 	*/
 	
-	 public void checkLogicalOperationOK() 
+public void checkLogicalOperationOk()
          throws StandardException 
      {
 		throw StandardException.newException(
---------------
-------------
@@ -32,7 +32,7 @@
   /** Creates a new MockCharFilterFactory */
   public MockCharFilterFactory(Map<String,String> args) {
     super(args);
-    remainder = getInt(args, "remainder", 0, false);
+remainder = requireInt(args, "remainder");
     if (!args.isEmpty()) {
       throw new IllegalArgumentException("Unknown parameters: " + args);
     }
---------------
-------------
@@ -59,6 +59,6 @@
   protected void map(Text key, VectorWritable value, Context context) throws IOException, InterruptedException {
     Vector result = classifier.classifyFull(value.get());
     //the key is the expected value
-    context.write(key, new VectorWritable(result));
+context.write(new Text(key.toString().split("/")[1]), new VectorWritable(result));
   }
 }
---------------
-------------
@@ -1045,7 +1045,7 @@
                                     setRAMBufferSizeMB(0.5).setMaxBufferedDocs(-1).setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setReaderPooling(false)) {
         @Override
         public void doAfterFlush() {
-          assertTrue("only " + docsInSegment.get() + " in segment", closing.get() || docsInSegment.get() >= 10);
+assertTrue("only " + docsInSegment.get() + " in segment", closing.get() || docsInSegment.get() >= 7);
           docsInSegment.set(0);
           sawAfterFlush.set(true);
         }
---------------
-------------
@@ -67,7 +67,7 @@
   }
   
   public void test() throws Exception {
-    NumericDocValues fooNorms = MultiSimpleDocValues.simpleNormValues(reader, "foo");
+NumericDocValues fooNorms = MultiDocValues.getNormValues(reader, "foo");
     for (int i = 0; i < reader.maxDoc(); i++) {
       assertEquals(expected.get(i).intValue(), fooNorms.get(i) & 0xff);
     }
---------------
-------------
@@ -698,7 +698,7 @@
     return newArr;
   }
 
-  static SimilarityFactory readSimilarity(ResourceLoader loader, Node node) {
+static SimilarityFactory readSimilarity(SolrResourceLoader loader, Node node) {
     if (node==null) {
       return null;
     } else {
---------------
-------------
@@ -42,6 +42,6 @@
 
 public class Constants {
 
-  public static final String VERSION = "8.2.0";
+public static final String VERSION = "8.3.0";
 
 }
---------------
-------------
@@ -282,7 +282,7 @@
     assertTrue("hits Size: " + hits.totalHits + " is not: " + 1, hits.totalHits == 1);
     int[] results = new int[1];
     results[0] = 0;//hits.scoreDocs[0].doc;
-    CheckHits.checkHitCollector(query, PayloadHelper.NO_PAYLOAD_FIELD, searcher, results);
+CheckHits.checkHitCollector(random, query, PayloadHelper.NO_PAYLOAD_FIELD, searcher, results);
   }
 
   // must be static for weight serialization tests 
---------------
-------------
@@ -156,7 +156,7 @@
 
   private void putVal(String name, Object val, Map map) {
     if(val == null) map.remove(name);
-    else entitySession.put(name, val);
+else map.put(name, val);
   }
 
   @Override
---------------
-------------
@@ -459,7 +459,7 @@
             }
             catch (StandardException se)
             {
-                if ( !se.getMessageId().equals( SQLState.LOCK_TIMEOUT ) ) { throw se; }
+if ( !se.isLockTimeout() ) { throw se; }
             }
             finally
             {
---------------
-------------
@@ -492,7 +492,7 @@
       fail("fake disk full IOExceptions not hit");
     } catch (IOException ioe) {
       // expected
-      assertTrue(ftdm.didFail1);
+assertTrue(ftdm.didFail1 || ftdm.didFail2);
     }
     _TestUtil.checkIndex(dir);
     ftdm.clearDoFail();
---------------
-------------
@@ -1852,7 +1852,7 @@
     public DocsEnum reset(int[] docIDs, int[] freqs) {
       this.docIDs = docIDs;
       this.freqs = freqs;
-      upto = -1;
+docID = upto = -1;
       return this;
     }
 
---------------
-------------
@@ -63,7 +63,7 @@
                                        int bufferSizeInMB) throws IOException
     {
         super(directory, new CFMetaData(keyspace, columnFamily, subComparator == null ? ColumnFamilyType.Standard : ColumnFamilyType.Super, comparator, subComparator));
-        this.bufferSize = bufferSizeInMB * 1024 * 1024;
+this.bufferSize = bufferSizeInMB * 1024L * 1024L;
     }
 
     protected void writeRow(DecoratedKey key, ColumnFamily columnFamily) throws IOException
---------------
-------------
@@ -89,7 +89,7 @@
       }
       writer.close();
       
-      reader = IndexReader.open(directory);
+reader = IndexReader.open(directory, true);
       PrefixQuery query = new PrefixQuery(new Term("category", "foo"));
       rw1 = query.rewrite(reader);
       
---------------
-------------
@@ -666,7 +666,7 @@
 
     public List<TokenRange> describe_ring(String keyspace)throws InvalidRequestException
     {
-        if (!DatabaseDescriptor.getNonSystemTables().contains(keyspace))
+if (keyspace == null || !DatabaseDescriptor.getNonSystemTables().contains(keyspace))
             throw new InvalidRequestException("There is no ring for the keyspace: " + keyspace);
         List<TokenRange> ranges = new ArrayList<TokenRange>();
         Token.TokenFactory tf = StorageService.getPartitioner().getTokenFactory();
---------------
-------------
@@ -101,7 +101,7 @@
                     "Invalid ObjectName? Please report this as a bug.", e);
         }
 
-        Map<Range, List<String>> rangeMap = ssProxy.getRangeToEndPointMap();
+Map<Range, List<String>> rangeMap = ssProxy.getRangeToEndPointMap(null);
         List<Range> ranges = new ArrayList<Range>(rangeMap.keySet());
         Collections.sort(ranges);
         
---------------
-------------
@@ -444,7 +444,7 @@
       byte[] emptyOutputBytes = new byte[(int) ros.getFilePointer()];
       ros.writeTo(emptyOutputBytes, 0);
 
-      if (true || !packed) {
+if (!packed) {
         // reverse
         final int stopAt = emptyOutputBytes.length/2;
         int upto = 0;
---------------
-------------
@@ -187,7 +187,7 @@
     boolean qStrict;
 
     public CustomWeight(IndexSearcher searcher) throws IOException {
-      this.subQueryWeight = subQuery.weight(searcher);
+this.subQueryWeight = subQuery.createWeight(searcher);
       this.valSrcWeights = new Weight[valSrcQueries.length];
       for(int i = 0; i < valSrcQueries.length; i++) {
         this.valSrcWeights[i] = valSrcQueries[i].createWeight(searcher);
---------------
-------------
@@ -51,7 +51,7 @@
     doc.add(newTextField(FN, "the quick brown fox jumps over the lazy ??? dog 493432 49344", Field.Store.NO));
     writer.addDocument(doc);
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     searcher = newSearcher(reader);
   }
   
---------------
-------------
@@ -74,7 +74,7 @@
     private BytesRef seekTerm;
     private int upto = 0;
 
-    SeekingTermSetTermsEnum(TermsEnum tenum, BytesRefHash terms) throws IOException {
+SeekingTermSetTermsEnum(TermsEnum tenum, BytesRefHash terms) {
       super(tenum);
       this.terms = terms;
 
---------------
-------------
@@ -439,7 +439,7 @@
 		}
 		rs.close();
 
-		if (rows >= 0)
+if (expectedRows >= 0)
 			Assert.assertEquals("Unexpected row count:", expectedRows, rows); 
 	}
 	
---------------
-------------
@@ -288,7 +288,7 @@
      * @param onDiskType The object read that represents the type.
      * @return A type descriptor.
      */
-    private static TypeDescriptor getStoredType(Object onDiskType)
+public static TypeDescriptor getStoredType(Object onDiskType)
     {
         if (onDiskType instanceof OldRoutineType)
             return ((OldRoutineType) onDiskType).getCatalogType();
---------------
-------------
@@ -592,7 +592,7 @@
 
 			try	{
 				try	{
-					theResults.finish(); // release the result set, don't just close it
+theResults.close();
 				    
 				    if (this.singleUseActivation != null)
 				    {
---------------
-------------
@@ -57,7 +57,7 @@
         suite.addTest(StreamingColumnTest.suite());
         suite.addTest(Derby3625Test.suite());
         suite.addTest(Derby4577Test.suite());
-        suite.addTest(Derby151Test.suite());
+suite.addTest(InterruptResilienceTest.suite());
         suite.addTest(Derby4676Test.suite());
         suite.addTest(BootLockTest.suite());
         suite.addTest(PositionedStoreStreamTest.suite());
---------------
-------------
@@ -82,7 +82,7 @@
     }
     batchSize = getIntFromContext("batchSize", 20);
     customFilter = getStringFromContext("customFilter", "");
-    String s = getStringFromContext("fetchMailsSince", "");
+String s = getStringFromContext("fetchMailsSince", null);
     if (s != null)
       try {
         fetchMailsSince = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").parse(s);
---------------
-------------
@@ -44,7 +44,7 @@
   }
 
   @Override
-  protected Reader initReader(Reader reader) {
+protected Reader initReader(String fieldName, Reader reader) {
     return charStream(reader);
   }
 }
---------------
-------------
@@ -498,7 +498,7 @@
                 commandSpecifics);
         
         // Ensure it completes without failures.
-        assertEquals(0, spawned.complete(false));
+assertEquals(0, spawned.complete());
         
         return spawned.getFullServerOutput();
     }
---------------
-------------
@@ -120,7 +120,7 @@
     }
 
     if (data.isEmpty()) {
-      return new Leaf(-1);
+return new Leaf(Double.NaN);
     }
 
     double sum = 0.0;
---------------
-------------
@@ -524,7 +524,7 @@
 {
 	public FragmentQueue(int size)
 	{
-		initialize(size);
+super(size);
 	}
 
 	@Override
---------------
-------------
@@ -34,7 +34,7 @@
         buffer_ = ByteBuffer.allocate(16);
     }
 
-    public byte[] read() throws IOException, ReadNotCompleteException
+public byte[] read() throws IOException
     {        
         return doRead(buffer_);
     }
---------------
-------------
@@ -2043,7 +2043,7 @@
       Bits liveDocs = ar.getLiveDocs();
       int maxDoc = ar.maxDoc();
       for (int i = 0; i < maxDoc; i++) {
-        if (liveDocs.get(i)) {
+if (liveDocs == null || liveDocs.get(i)) {
           assertTrue(liveIds.remove(ar.document(i).get("id")));
         }
       }
---------------
-------------
@@ -69,7 +69,7 @@
       // expected exception
     }
     ir.close();
-    iw.close();
+iw.shutdown();
     dir.close();
   }
 }
---------------
-------------
@@ -74,7 +74,7 @@
 	private AccountDataBeanImpl account;
 
 	@ManyToOne(fetch = FetchType.EAGER)
-	@JoinColumn(name = "QUOTE_SYMBOL", columnDefinition="VARCHAR(250)")
+@JoinColumn(name = "QUOTE_SYMBOL", columnDefinition="VARCHAR(255)")
 	private QuoteDataBeanImpl quote;
 
 	public HoldingDataBeanImpl() {
---------------
-------------
@@ -130,7 +130,7 @@
       GroupHead groupHead = groups.get(groupValue);
       if (groupHead == null) {
         groupHead = new GroupHead(groupValue, sortWithinGroup, doc);
-        groups.put(groupValue == null ? null : new BytesRef(groupValue), groupHead);
+groups.put(groupValue == null ? null : BytesRef.deepCopyOf(groupValue), groupHead);
         temporalResult.stop = true;
       } else {
         temporalResult.stop = false;
---------------
-------------
@@ -62,7 +62,7 @@
     IndexReader reader = null;
     
     try {
-      reader = IndexReader.open(directory);
+reader = IndexReader.open(directory, true);
       for(int i = 1; i <= numThreads; i++)
         testTermPositionVectors(reader, i);
       
---------------
-------------
@@ -619,7 +619,7 @@
         while (ringIter.hasNext())
         {
             Token token = ringIter.next();
-            if (remainder == null || !remainder.contains(token))
+if (remainder == null || !(remainder.left.equals(token) || remainder.contains(token)))
                 // no more splits
                 break;
             Pair<AbstractBounds,AbstractBounds> splits = remainder.split(token);
---------------
-------------
@@ -1923,7 +1923,7 @@
         return new TokenStreamComponents(tokenizer, new ASCIIFoldingFilter(tokenizer));
       } 
     };
-    checkRandomData(random(), a, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -122,7 +122,7 @@
     writer.addDocument(doc);
     writer.close();
 
-    IndexSearcher searcher = new IndexSearcher(ramDir);
+IndexSearcher searcher = new IndexSearcher(ramDir, true);
     TopDocs td = searcher.search(query,null,10);
     //System.out.println("slop: "+slop+"  query: "+query+"  doc: "+doc+"  Expecting number of hits: "+expectedNumResults+" maxScore="+td.getMaxScore());
     assertEquals("slop: "+slop+"  query: "+query+"  doc: "+doc+"  Wrong number of hits", expectedNumResults, td.totalHits);
---------------
-------------
@@ -59,7 +59,7 @@
         return instance_;
     }
     
-    class BinaryMemtableFlusher implements Runnable
+static class BinaryMemtableFlusher implements Runnable
     {
         private BinaryMemtable memtable_;
         
---------------
-------------
@@ -566,7 +566,7 @@
      * @return true if so
      */
 	public boolean supportsConvert() {
-		return true;
+return false;
 	}
 
     /**
---------------
-------------
@@ -44,7 +44,7 @@
     blockShift = Integer.numberOfTrailingZeros(blockSize);
     blockMask = blockSize - 1;
     final int numBlocks = (int) (valueCount / blockSize) + (valueCount % blockSize == 0 ? 0 : 1);
-    if (numBlocks * blockSize < valueCount) {
+if ((long) numBlocks * blockSize < valueCount) {
       throw new IllegalArgumentException("valueCount is too large for this block size");
     }
     minValues = new long[numBlocks];
---------------
-------------
@@ -1,6 +1,6 @@
 /*
 
-   Derby - Class org.apache.derbyTesting.functionTests.tests.lang.OptimizerOverridingTest
+Derby - Class org.apache.derbyTesting.functionTests.tests.lang.OptimizerOverridesTest
 
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -137,6 +137,6 @@
 
 
   public SolrCore getSolrCore() {
-    return dataImporter.getCore();
+return dataImporter == null ? null : dataImporter.getCore();
   }
 }
---------------
-------------
@@ -29,7 +29,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-public class CleanupHelper
+public class CleanupHelper extends SchemaLoader
 {
     private static Logger logger = LoggerFactory.getLogger(CleanupHelper.class);
 
---------------
-------------
@@ -598,7 +598,7 @@
     private static boolean randomlyReadRepair(ReadCommand command)
     {
         CFMetaData cfmd = DatabaseDescriptor.getTableMetaData(command.table).get(command.getColumnFamilyName());
-        return cfmd.readRepairChance > random.nextDouble();
+return cfmd.getReadRepairChance() > random.nextDouble();
     }
 
     public long getReadOperations()
---------------
-------------
@@ -22,6 +22,6 @@
 
 public interface IFailureNotification
 {   
-    public void suspect(InetAddress ep);
+public void convict(InetAddress ep);
     public void revive(InetAddress ep);
 }
---------------
-------------
@@ -77,7 +77,7 @@
       assertFalse(fileExtensions.contains(ext));
     }
     reader.close();
-    writer.close();
+writer.shutdown();
 
     files = fsd.listAll();
     for(int i=0;i<files.length;i++) {
---------------
-------------
@@ -96,7 +96,7 @@
   }
 
   @Override
-  public long getUniqueTermCount() throws IOException {
+public long size() throws IOException {
     return -1;
   }
 
---------------
-------------
@@ -140,7 +140,7 @@
       // from a docsAndPositionsEnum.
     }
     ir.close();
-    iw.close();
+iw.shutdown();
     dir.close();
   }
 }
---------------
-------------
@@ -42,7 +42,7 @@
         mbeanServerTracker.open();
 
         mbeanTracker = new MBeanTracker(context);
-        mbeanTracker.open();
+mbeanTracker.open(true);
     }
 
     public void stop(BundleContext context) throws Exception {
---------------
-------------
@@ -38,7 +38,7 @@
     private final int maxDoc;
     private final Bits liveDocs;
 
-    MatchAllScorer(IndexReader reader, Bits liveDocs, Weight w, float score) throws IOException {
+MatchAllScorer(IndexReader reader, Bits liveDocs, Weight w, float score) {
       super(w);
       this.liveDocs = liveDocs;
       this.score = score;
---------------
-------------
@@ -224,7 +224,7 @@
   }
   
   @AfterClass
-  public static void afterClass() throws Exception {
+public static void afterClass() {
     tokenizers = null;
     tokenfilters = null;
     charfilters = null;
---------------
-------------
@@ -53,7 +53,7 @@
    * <p><em>This method is expensive and should only be called for discovery
    * of new service providers on the given classpath/classloader!</em>
    */
-  public void reload(ClassLoader classloader) {
+public synchronized void reload(ClassLoader classloader) {
     final LinkedHashMap<String,S> services = new LinkedHashMap<String,S>(this.services);
     final SPIClassIterator<S> loader = SPIClassIterator.get(clazz, classloader);
     while (loader.hasNext()) {
---------------
-------------
@@ -171,7 +171,7 @@
 						+ " SQLSTATE: " + m);
 			}
 		}
-		if (e.getMessage().equals(null)) {
+if (e.getMessage() == null) {
 			System.out.println("NULL error message detected");
 			System.out.println("Here is the NULL exection - " + e.toString());
 			System.out.println("Stack trace of the NULL exception - ");
---------------
-------------
@@ -4206,7 +4206,7 @@
 		ResultColumn	rc = (ResultColumn) nodeFactory.getNode
 			(
 				C_NodeTypes.RESULT_COLUMN,
-				null,
+columnName,
 				nodeFactory.getNode
 				(
 					C_NodeTypes.COLUMN_REFERENCE,
---------------
-------------
@@ -362,7 +362,7 @@
               "finishDocument".equals(trace[i].getMethodName())) {
             sawAbortOrFlushDoc = true;
           }
-          if ("merge".equals(trace[i])) {
+if ("merge".equals(trace[i].getMethodName())) {
             sawMerge = true;
           }
           if ("close".equals(trace[i].getMethodName())) {
---------------
-------------
@@ -177,7 +177,7 @@
     // Second in an FSDirectory:
     String tempDir = System.getProperty("java.io.tmpdir");
     File dirPath = new File(tempDir, "lucene.test.atomic");
-    directory = FSDirectory.getDirectory(dirPath);
+directory = FSDirectory.getDirectory(dirPath, null, false);
     runTest(directory);
     directory.close();
     _TestUtil.rmDir(dirPath);
---------------
-------------
@@ -48,7 +48,7 @@
         Map<String, CFMetaData> cfmap = new HashMap<String, CFMetaData>();
         for (CFMetaData cfm : cfDefs)
             cfmap.put(cfm.cfName, cfm);
-        this.cfMetaData = Collections.<String, CFMetaData>unmodifiableMap(cfmap);
+this.cfMetaData = Collections.unmodifiableMap(cfmap);
     }
     
     public boolean equals(Object obj)
---------------
-------------
@@ -61,7 +61,7 @@
 import org.apache.lucene.index.IndexReader;
 
 /** A Query that matches documents containing terms with a specified prefix. */
-final public class PrefixQuery extends Query {
+public class PrefixQuery extends Query {
   private Term prefix;
   private IndexReader reader;
   private BooleanQuery query;
---------------
-------------
@@ -194,7 +194,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new DutchAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new DutchAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
   
 }
---------------
-------------
@@ -123,7 +123,7 @@
    * @return true if more matching documents may remain.
    */
   @Override
-  protected boolean score(Collector collector, int max, int firstDocID) throws IOException {
+public boolean score(Collector collector, int max, int firstDocID) throws IOException {
     // firstDocID is ignored since nextDoc() sets 'currentDoc'
     collector.setScorer(this);
     while (currentDoc < max) {
---------------
-------------
@@ -64,7 +64,7 @@
     names1.add( FacetComponent.COMPONENT_NAME );
     
     args = new NamedList();
-    args.add( SearchHandler.INIT_FISRT_COMPONENTS, names0 );
+args.add( SearchHandler.INIT_FIRST_COMPONENTS, names0 );
     args.add( SearchHandler.INIT_LAST_COMPONENTS, names1 );
     handler = new SearchHandler();
     handler.init( args );
---------------
-------------
@@ -54,7 +54,7 @@
         super.bytesPerChar = BYTES_PER_CHAR;
         EmbedStatement embStmt = (EmbedStatement)createStatement();
         EmbedConnection embCon =(EmbedConnection)getConnection();
-        iClob = new TemporaryClob(embCon.getDBName(), embStmt);
+iClob = new TemporaryClob(embStmt);
         transferData(
             new LoopingAlphabetReader(CLOBLENGTH, CharAlphabet.tamil()),
             iClob.getWriter(1L),
---------------
-------------
@@ -178,7 +178,7 @@
     return simpleTag("optimize", args);
   }
 
-  private static String simpleTag(String tag, String... args) {
+public static String simpleTag(String tag, String... args) {
     try {
       StringWriter r = new StringWriter();
 
---------------
-------------
@@ -2326,7 +2326,7 @@
                     public void run()
                     {
                         // TODO each call to transferRanges re-flushes, this is potentially a lot of waste
-                        StreamOut.transferRanges(newEndpoint, Table.open(table), Arrays.asList(range), callback, OperationType.UNBOOTSTRAP);
+StreamOut.transferRanges(newEndpoint, table, Arrays.asList(range), callback, OperationType.UNBOOTSTRAP);
                     }
                 });
             }
---------------
-------------
@@ -214,7 +214,7 @@
     boolean optimize = true;
 
     Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));
+IndexWriter writer = new IndexWriter(dir1, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setReaderTermsIndexDivisor(2));
     writer.setInfoStream(infoStream);
     // create the index
     createIndexNoClose(!optimize, "index1", writer);
---------------
-------------
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.assertVocabulary;
+import static org.apache.lucene.analysis.VocabularyAssert.assertVocabulary;
 
 import java.io.IOException;
 import java.io.Reader;
---------------
-------------
@@ -57,7 +57,7 @@
     if (elements.length != 3) {
       throw new IOException("Expected input of length 3, received "
           + elements.length + ". Please make sure you adhere to "
-          + "the structure of (i,j,value) for representing a graph in text.");
++ "the structure of (i,j,value) for representing a graph in text. Input line was: '"+value+"'.");
     } else if (elements[0].length() == 0 || elements[1].length() == 0 || elements[2].length() == 0) {
       throw new IOException("Found an element of 0 length. Please be sure you adhere to the structure of "
           + "(i,j,value) for  representing a graph in text.");
---------------
-------------
@@ -86,7 +86,7 @@
    *  compressionLevel (constants are defined in
    *  java.util.zip.Deflater). */
   public static byte[] compressString(String value, int compressionLevel) {
-    BytesRef result = new BytesRef(10);
+BytesRef result = new BytesRef();
     UnicodeUtil.UTF16toUTF8(value, 0, value.length(), result);
     return compress(result.bytes, 0, result.length, compressionLevel);
   }
---------------
-------------
@@ -176,7 +176,7 @@
         // exceedingly rare (Yonik calculates 1 in ~429,000)
         // times) that this loop requires more than one try:
         IndexReader ir = writer.getReader();
-        writer.close();
+writer.shutdown();
         return ir;
       }
 
---------------
-------------
@@ -69,7 +69,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -60,7 +60,7 @@
     private final Comparator<BytesRef> comp;
 
     public Writer(Directory dir, String id, Comparator<BytesRef> comp,
-        Counter bytesUsed, IOContext context, float acceptableOverheadRatio) throws IOException {
+Counter bytesUsed, IOContext context, float acceptableOverheadRatio) {
       super(dir, id, CODEC_NAME_IDX, CODEC_NAME_DAT, VERSION_CURRENT, bytesUsed, context, acceptableOverheadRatio, Type.BYTES_VAR_SORTED);
       this.comp = comp;
       size = 0;
---------------
-------------
@@ -73,7 +73,7 @@
  *   EACH_QUORUM  Returns the record with the most recent timestamp once a majority of replicas within each datacenter have replied.
  *   ALL          Returns the record with the most recent timestamp once all replicas have replied (implies no replica may be down)..
  */
-public enum ConsistencyLevel implements TEnum {
+public enum ConsistencyLevel implements org.apache.thrift.TEnum {
   ONE(1),
   QUORUM(2),
   LOCAL_QUORUM(3),
---------------
-------------
@@ -38,7 +38,7 @@
  *
  */
 public interface PreparedStatement
-	extends Dependent, Provider
+extends Dependent
 {
 
 	/**
---------------
-------------
@@ -209,7 +209,7 @@
       super(orig.categoryPath, num);
       this.orig = orig;
       setDepth(orig.getDepth());
-      setNumLabel(orig.getNumLabel());
+setNumLabel(0); // don't label anything as we're over-sampling
       setResultMode(orig.getResultMode());
       setSortOrder(orig.getSortOrder());
     }
---------------
-------------
@@ -213,7 +213,7 @@
       throw new IOException("file " + name + " already exists");
     else {
       if (existing!=null) {
-        sizeInBytes -= existing.sizeInBytes;
+sizeInBytes.getAndAdd(-existing.sizeInBytes);
         existing.directory = null;
       }
 
---------------
-------------
@@ -95,7 +95,7 @@
         Schema writer = Schema.parse(dec.readString(new Utf8()).toString());
         SpecificDatumReader<T> reader = new SpecificDatumReader<T>(writer);
         reader.setExpected(ob.getSchema());
-        return new SpecificDatumReader<T>(writer).read(ob, dec);
+return reader.read(ob, dec);
     }
 
 	/**
---------------
-------------
@@ -92,7 +92,7 @@
       assert pos < nextPos;
 
       // Cannot read from already freed past:
-      assert nextPos - pos <= count;
+assert nextPos - pos <= count: "nextPos=" + nextPos + " pos=" + pos + " count=" + count;
 
       final int index = getIndex(pos);
       return buffer[index];
---------------
-------------
@@ -27,7 +27,7 @@
 
   @Test
   public void testBasicUsage() throws Exception {
-     checkCorrectClassification(new KNearestNeighborClassifier(1), new BytesRef("technology"), new MockAnalyzer(random()), categoryFieldName);
+checkCorrectClassification(new KNearestNeighborClassifier(1), TECHNOLOGY_INPUT, TECHNOLOGY_RESULT, new MockAnalyzer(random()), categoryFieldName);
   }
 
 }
---------------
-------------
@@ -31,7 +31,7 @@
         bytes_ = bytes;
     }
 
-    public byte[] read() throws IOException, ReadNotCompleteException
+public byte[] read() throws IOException
     {        
         morphState();
         return bytes_;
---------------
-------------
@@ -30,7 +30,7 @@
 import org.apache.commons.collections.IteratorUtils;
 
 import com.google.common.collect.Collections2;
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.db.*;
 import org.apache.cassandra.db.marshal.AbstractType;
 
---------------
-------------
@@ -52,7 +52,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), a, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -1422,7 +1422,7 @@
 		 */
 		if (eliminateSort)
 		{
-			prnRSN.adjustForSortElimination();
+prnRSN.adjustForSortElimination(orderByList);
 		}
 
 		/* Set the cost of this node in the generated node */
---------------
-------------
@@ -3055,7 +3055,7 @@
     public void listenToUnitOfWork() {
         if (!listenToUnitOfWork_) {
             listenToUnitOfWork_ = true;
-            connection_.CommitAndRollbackListeners_.add(this);
+connection_.CommitAndRollbackListeners_.put(this,null);
         }
     }
 
---------------
-------------
@@ -71,7 +71,7 @@
   @Override
   protected void setup(Context context) throws IOException, InterruptedException {
     super.setup(context);
-    Parameters params = Parameters.fromString(context.getConfiguration().get(PFPGrowth.PFP_PARAMETERS, ""));
+Parameters params = new Parameters(context.getConfiguration().get(PFPGrowth.PFP_PARAMETERS, ""));
     
     OpenObjectIntHashMap<String> fMap = new OpenObjectIntHashMap<String>();
     int i = 0;
---------------
-------------
@@ -262,7 +262,7 @@
     ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION,
         OverseerCollectionProcessor.DELETEALIAS, "name", name);
     
-    handleResponse(OverseerCollectionProcessor.CREATEALIAS, m, rsp);
+handleResponse(OverseerCollectionProcessor.DELETEALIAS, m, rsp);
   }
 
   private void handleDeleteAction(SolrQueryRequest req, SolrQueryResponse rsp) throws KeeperException, InterruptedException {
---------------
-------------
@@ -45,7 +45,7 @@
     ARFFModel model = new MapBackedARFFModel();
     model.addLabel("b1shkt70694difsmmmdv0ikmoh", 77);
     model.addType(77, ARFFType.REAL);
-    assertTrue(0.0 == model.getValue("b1shkt70694difsmmmdv0ikmoh", 77));
+assertTrue(Double.isNaN(model.getValue("b1shkt70694difsmmmdv0ikmoh", 77)));
   }
 
   @Test
---------------
-------------
@@ -71,7 +71,7 @@
 
     // in this first phase, request only the unique key field
     // and any fields needed for merging.
-    sreq.params.set("group.distibuted.first","true");
+sreq.params.set("group.distributed.first","true");
 
     if ( (rb.getFieldFlags() & SolrIndexSearcher.GET_SCORES)!=0 || rb.getSortSpec().includesScore()) {
       sreq.params.set(CommonParams.FL, rb.req.getSchema().getUniqueKeyField().getName() + ",score");
---------------
-------------
@@ -76,7 +76,7 @@
     doc.add(new FacetField("A", "C"));
     indexWriter.addDocument(config.build(taxoWriter, doc));
     
-    indexWriter.close();
+indexWriter.shutdown();
     taxoWriter.close();
   }
 
---------------
-------------
@@ -50,7 +50,7 @@
   @Test
   public void testConcatenateVectorsReducer() throws Exception {
     
-    Configuration configuration = new Configuration();
+Configuration configuration = getConfiguration();
     configuration.set(ConcatenateVectorsJob.MATRIXA_DIMS, "5");
     configuration.set(ConcatenateVectorsJob.MATRIXB_DIMS, "3");
     
---------------
-------------
@@ -433,7 +433,7 @@
       coreProps.setProperty(propName, propValue);
     }
 
-    return new CoreDescriptor(container, name, instancedir, coreProps);
+return new CoreDescriptor(container, name, instancedir, coreProps, params);
   }
 
   private static String checkNotEmpty(String value, String message) {
---------------
-------------
@@ -497,7 +497,7 @@
 
         RowMutation rm = new RowMutation("Keyspace1", ROW.key);
         ColumnFamily cf = ColumnFamily.create("Keyspace1", "Super1");
-        SuperColumn sc = new SuperColumn("sc1".getBytes(), LongType.instance, ClockType.Timestamp);
+SuperColumn sc = new SuperColumn("sc1".getBytes(), LongType.instance, ClockType.Timestamp, null);
         sc.addColumn(new Column(getBytes(1), "val1".getBytes(), new TimestampClock(1L)));
         cf.addColumn(sc);
         rm.add(cf);
---------------
-------------
@@ -29,7 +29,7 @@
 
 public final class CFMetaData
 {
-    public final static double DEFAULT_KEY_CACHE_SIZE = 0.1;
+public final static double DEFAULT_KEY_CACHE_SIZE = 200000;
     public final static double DEFAULT_ROW_CACHE_SIZE = 0.0;
 
     public final String tableName;            // name of table which has this column family
---------------
-------------
@@ -44,7 +44,7 @@
 
         try
         {
-            CompletedFileStatus streamStatus = CompletedFileStatus.serializer().deserialize(new DataInputStream(bufIn));
+FileStatus streamStatus = FileStatus.serializer().deserialize(new DataInputStream(bufIn));
 
             switch (streamStatus.getAction())
             {
---------------
-------------
@@ -430,7 +430,7 @@
     if(zkController != null) {
       try {
         synchronized (zkController.getZkStateReader().getUpdateLock()) {
-          zkController.addShardZkNodeWatches();
+zkController.getZkStateReader().addShardZkNodeWatches();
           zkController.getZkStateReader().updateCloudState(true);
         }
       } catch (InterruptedException e) {
---------------
-------------
@@ -161,7 +161,7 @@
                   "language java parameter style java");
 
                 s.executeUpdate(
-                  "create function EMC.GETARTICLE(path VARCHAR(40)) " +
+"create function EMC.GETARTICLE(path VARCHAR(60)) " +
                   "RETURNS VARCHAR(256) " +
                   "NO SQL " +
                   "external name 'org.apache.derbyTesting.databaseclassloader.emc.getArticle' " +
---------------
-------------
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.assertVocabulary;
+import static org.apache.lucene.analysis.VocabularyAssert.assertVocabulary;
 
 import java.io.IOException;
 import java.io.Reader;
---------------
-------------
@@ -20,7 +20,7 @@
 /**
  * Non-specialized {@link BulkOperation} for {@link PackedInts.Format#PACKED_SINGLE_BLOCK}.
  */
-class BulkOperationPackedSingleBlock extends BulkOperation {
+final class BulkOperationPackedSingleBlock extends BulkOperation {
 
   private static final int BLOCK_COUNT = 1;
 
---------------
-------------
@@ -49,7 +49,7 @@
  * used here just writes each block as a series of vInt.
  */
 
-public class MockFixedIntBlockPostingsFormat extends PostingsFormat {
+public final class MockFixedIntBlockPostingsFormat extends PostingsFormat {
 
   private final int blockSize;
 
---------------
-------------
@@ -379,7 +379,7 @@
     Iterator<String> iter =  params.getParameterNamesIterator();
     while (iter.hasNext()) {
       String param = iter.next();
-      if (param.indexOf(OverseerCollectionProcessor.COLL_PROP_PREFIX) != -1) {
+if (param.startsWith(OverseerCollectionProcessor.COLL_PROP_PREFIX)) {
         props.put(param, params.get(param));
       }
     }
---------------
-------------
@@ -118,7 +118,7 @@
 
     if (te.docFreq() > maxTermDocFreq) {
       TopTerm topTerm = new TopTerm();
-      topTerm.term = new BytesRef(term);
+topTerm.term = BytesRef.deepCopyOf(term);
       topTerm.termNum = termNum;
       bigTerms.put(topTerm.termNum, topTerm);
 
---------------
-------------
@@ -41,7 +41,7 @@
   @Before
   public void before() throws IOException {
     lucene2seq = new SequenceFilesFromLuceneStorageMRJob();
-    Configuration configuration = new Configuration();
+Configuration configuration = getConfiguration();
     Path seqOutputPath = new Path(getTestTempDirPath(), "seqOutputPath");//don't make the output directory
     lucene2SeqConf = new LuceneStorageConfiguration(configuration, asList(getIndexPath1(), getIndexPath2()),
             seqOutputPath, SingleFieldDocument.ID_FIELD, asList(SingleFieldDocument.FIELD));
---------------
-------------
@@ -274,7 +274,7 @@
     }
     
     if (clusters.isEmpty()) {
-      throw new IllegalStateException("No input clusters found. Check your -c argument.");
+throw new IllegalStateException("No input clusters found in " + clustersIn + ". Check your -c argument.");
     }
     
     Path priorClustersPath = new Path(output, Cluster.INITIAL_CLUSTERS_DIR);   
---------------
-------------
@@ -65,7 +65,7 @@
       sb.delete(0, sb.length());
     }
     final IndexReader r = w.getReader();
-    w.close();
+w.shutdown();
 
     final long endTime = System.currentTimeMillis();
     if (VERBOSE) System.out.println("BUILD took " + (endTime-startTime));
---------------
-------------
@@ -49,7 +49,7 @@
     private static final int phiConvictThreshold_ = 8;
     /* The Failure Detector has to have been up for at least 1 min. */
     private static final long uptimeThreshold_ = 60000;
-    private static IFailureDetector failureDetector_;
+private static volatile IFailureDetector failureDetector_;
     /* Used to lock the factory for creation of FailureDetector instance */
     private static Lock createLock_ = new ReentrantLock();
     /* The time when the module was instantiated. */
---------------
-------------
@@ -50,7 +50,7 @@
 
     ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals(0, hits.length);
-    writer.close();
+writer.shutdown();
     reader.close();
     store.close();
   }
---------------
-------------
@@ -58,7 +58,7 @@
 
 			if (firstTime) {
 				Logs.reportString("----------------------------------------------");
-				Logs.reportMessage("CSLOOK_Header", "views");
+Logs.reportMessage("CSLOOK_ViewsHeader");
 				Logs.reportString("----------------------------------------------\n");
 			}
 
---------------
-------------
@@ -155,7 +155,7 @@
     return sb;
   }
   
-  protected DocData getNextDocData() throws NoMoreDataException, Exception {
+protected synchronized DocData getNextDocData() throws NoMoreDataException, Exception {
     if (reader==null) {
       openNextFile();
     }
---------------
-------------
@@ -151,7 +151,7 @@
         {
         case Types.BIGINT:
             return 8;
-		case org.apache.derby.iapi.reference.JDBC30Translation.SQL_TYPES_BOOLEAN:
+case Types.BOOLEAN:
         case Types.BIT:
             return 1;
         case Types.BINARY:
---------------
-------------
@@ -549,7 +549,7 @@
     Document d = new Document();
     d.add(parentStreamField);
 
-    fullPathField.setValue(categoryPath.toString(delimiter, length));
+fullPathField.setStringValue(categoryPath.toString(delimiter, length));
     d.add(fullPathField);
 
     // Note that we do no pass an Analyzer here because the fields that are
---------------
-------------
@@ -175,7 +175,7 @@
                 try {
                     JDBC.dropSchema(dmd, schema);
                 } catch (SQLException e) {
-                    sqle = null;
+sqle = e;
                 }
             }
             // No errors means all the schemas we wanted to
---------------
-------------
@@ -41,7 +41,7 @@
  */
 
 /** partially deprecated until unit tests are in place.  Until this time, this class/interface is unsupported. */
-public class OldQRDecomposition {
+public class OldQRDecomposition implements QR {
 
   /** Array for internal storage of decomposition. */
   private final Matrix qr;
---------------
-------------
@@ -148,7 +148,7 @@
     {
         public void doVerb(Message message)
         {
-            Message reply = message.getInternalReply(new byte[] {(byte)(isMoveable_.get() ? 1 : 0)});
+Message reply = message.getInternalReply(new byte[] {(byte)(isMoveable_.get() ? 1 : 0)}, message.getVersion());
             MessagingService.instance().sendOneWay(reply, message.getFrom());
             if ( isMoveable_.get() )
             {
---------------
-------------
@@ -187,7 +187,7 @@
     String newClassName = "$" + aClass.getSimpleName() + aClass.hashCode();
     String packageName = aClass.getPackage().getName();
     if (packageName.startsWith("java.") || packageName.startsWith("javax.")) {
-      packageName = "com.ibm.osgi.blueprint.proxy." + packageName;
+packageName = "org.apache.aries.blueprint.proxy." + packageName;
     }
     String fullNewClassName = (packageName + "." + newClassName).replaceAll("\\.", "/");
 
---------------
-------------
@@ -265,7 +265,7 @@
 
     _TestUtil.checkIndex(dir);
 
-    IndexReader reader = IndexReader.open(dir);
+IndexReader reader = IndexReader.open(dir, true);
     // f1
     TermFreqVector tfv1 = reader.getTermFreqVector(0, "f1");
     assertNotNull(tfv1);
---------------
-------------
@@ -254,7 +254,7 @@
     return dvs;
   }
 
-  NumericDocValues getSimpleNormValues(String field) throws IOException {
+NumericDocValues getNormValues(String field) throws IOException {
     FieldInfo fi = fieldInfos.fieldInfo(field);
     if (fi == null) {
       // Field does not exist
---------------
-------------
@@ -52,7 +52,7 @@
             (
              agent.logWriter_,
              new ClientMessageId(SQLState.DATA_TYPE_NOT_SUPPORTED),
-             sqlType.toString()
+sqlType
              ).getSQLException();
     }
 
---------------
-------------
@@ -69,7 +69,7 @@
       Document doc = new Document();
       doc.add(idField);
       final String id = ""+i;
-      idField.setValue(id);
+idField.setStringValue(id);
       docs.put(id, doc);
       if (VERBOSE) {
         System.out.println("TEST: add doc id=" + id);
---------------
-------------
@@ -183,7 +183,7 @@
                 }
                 if (newRatio > MAX_SANE_LIVE_RATIO)
                 {
-                    logger.warn("setting live ratio to maximum of 64 instead of {}, newRatio");
+logger.warn("setting live ratio to maximum of 64 instead of {}", newRatio);
                     newRatio = MAX_SANE_LIVE_RATIO;
                 }
                 cfs.liveRatio = Math.max(cfs.liveRatio, newRatio);
---------------
-------------
@@ -310,7 +310,7 @@
 
           if (comparator == null) {
             comparator = sortField.getComparator(1,0);
-            comparator.setNextReader(subReader, offset);
+comparator = comparator.setNextReader(subReader, offset);
             if (comparators != null)
               comparators[idx] = comparator;
           }
---------------
-------------
@@ -1303,7 +1303,7 @@
             view = data.getView();
             // startAt == minimum is ok, but stopAt == minimum is confusing because all IntervalTree deals with
             // is Comparable, so it won't know to special-case that.
-            Comparable stopInTree = stopAt.isEmpty() ? view.intervalTree.max : stopAt;
+Comparable stopInTree = stopAt.isEmpty() ? view.intervalTree.max() : stopAt;
             sstables = view.intervalTree.search(new Interval(startWith, stopInTree));
             if (SSTableReader.acquireReferences(sstables))
                 break;
---------------
-------------
@@ -134,7 +134,7 @@
         DirectoryReader open = null;
         for (int i = 0; i < num; i++) {
           Document doc = new Document();// docs.nextDoc();
-          doc.add(newField("id", "test", StringField.TYPE_UNSTORED));
+doc.add(newStringField("id", "test", Field.Store.NO));
           writer.updateDocument(new Term("id", "test"), doc);
           if (random().nextInt(3) == 0) {
             if (open == null) {
---------------
-------------
@@ -71,7 +71,7 @@
           writer.updateDocument(idTerm, doc);
         } catch (RuntimeException re) {
           if (VERBOSE) {
-            System.out.println("EXC: ");
+System.out.println(Thread.currentThread().getName() + ": EXC: ");
             re.printStackTrace(System.out);
           }
           try {
---------------
-------------
@@ -108,7 +108,7 @@
     Bits liveDocs = MultiFields.getLiveDocs(indexReader);
     int updatedCount = countIntersection(MultiFields.getTermDocsEnum(indexReader, liveDocs,
                                                                      drillDownTerm.field(), drillDownTerm.bytes(),
-                                                                     false),
+0),
                                          docIds.iterator());
 
     fresNode.setValue(updatedCount);
---------------
-------------
@@ -2813,7 +2813,7 @@
     // Tests that if FSDir is opened w/ a NoLockFactory (or SingleInstanceLF),
     // then IndexWriter ctor succeeds. Previously (LUCENE-2386) it failed 
     // when listAll() was called in IndexFileDeleter.
-    Directory dir = newFSDirectory(new File(TEMP_DIR, "emptyFSDirNoLock"), NoLockFactory.getNoLockFactory());
+Directory dir = newFSDirectory(_TestUtil.getTempDir("emptyFSDirNoLock"), NoLockFactory.getNoLockFactory());
     new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random))).close();
     dir.close();
   }
---------------
-------------
@@ -858,7 +858,7 @@
     protected void finish() throws IOException {
       TopDocsCollector topDocsCollector = (TopDocsCollector) collector.getDelegate();
       TopDocs topDocs = topDocsCollector.topDocs();
-      GroupDocs<String> groupDocs = new GroupDocs<String>(topDocs.getMaxScore(), topDocs.totalHits, topDocs.scoreDocs, query.toString(), null);
+GroupDocs<String> groupDocs = new GroupDocs<String>(Float.NaN, topDocs.getMaxScore(), topDocs.totalHits, topDocs.scoreDocs, query.toString(), null);
       if (main) {
         mainResult = getDocList(groupDocs);
       } else {
---------------
-------------
@@ -165,7 +165,7 @@
         this.tokens = tokens;
         termAtt = addAttribute(CharTermAttribute.class);
         offsetAtt = addAttribute(OffsetAttribute.class);
-        posincAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);
+posincAtt = addAttribute(PositionIncrementAttribute.class);
       }
 
       @Override
---------------
-------------
@@ -66,7 +66,7 @@
  * Randomly combines terms index impl w/ postings impls.
  */
 
-public class MockRandomPostingsFormat extends PostingsFormat {
+public final class MockRandomPostingsFormat extends PostingsFormat {
   private final Random seedRandom;
   private final String SEED_EXT = "sd";
   
---------------
-------------
@@ -72,7 +72,7 @@
         }
 
         Object val = row.get(srcCol);
-        String styleSmall = style.toLowerCase();
+String styleSmall = style.toLowerCase(Locale.ENGLISH);
 
         if (val instanceof List) {
           List<String> inputs = (List) val;
---------------
-------------
@@ -26,7 +26,7 @@
 import java.util.Iterator;
 import java.util.Arrays;
 
-import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.utils.ReducingIterator;
 import org.apache.cassandra.db.*;
 import org.apache.cassandra.db.marshal.AbstractType;
---------------
-------------
@@ -18,7 +18,7 @@
 
 package org.apache.cassandra.net;
 
-public final class ProtocolHeader
+public class ProtocolHeader
 {
     public static final String SERIALIZER = "SERIALIZER";
     public static final String COMPRESSION = "COMPRESSION";
---------------
-------------
@@ -114,7 +114,7 @@
       for (ShardRequest sreq : rb.finished) {
         if ((sreq.purpose & ShardRequest.PURPOSE_GET_HIGHLIGHTS) == 0) continue;
         for (ShardResponse srsp : sreq.responses) {
-          NamedList hl = (NamedList)srsp.rsp.getResponse().get("highlighting");
+NamedList hl = (NamedList)srsp.getSolrResponse().getResponse().get("highlighting");
           for (int i=0; i<hl.size(); i++) {
             String id = hl.getName(i);
             ShardDoc sdoc = rb.resultIds.get(id);
---------------
-------------
@@ -41,7 +41,7 @@
  * and it does not support starting it from a remote 
  * machine.
  */
-final public class NetworkServerTestSetup extends TestSetup {
+final public class NetworkServerTestSetup extends BaseTestSetup {
 
     /** Setting maximum wait time to 300 seconds.   For some systems it looks
      *  like restarting a server to listen on the same port is blocked waiting
---------------
-------------
@@ -473,7 +473,7 @@
             }
             catch (StandardException se)
             {
-                if ( !se.getMessageId().equals( SQLState.LOCK_TIMEOUT ) ) { throw se; }
+if ( !se.isLockTimeout() ) { throw se; }
             }
             finally
             {
---------------
-------------
@@ -812,7 +812,7 @@
     public TopFieldDocs call() throws IOException {
       assert slice.leaves.length == 1;
       final TopFieldDocs docs = searcher.search(Arrays.asList(slice.leaves),
-          weight, after, nDocs, sort, true, doDocScores, doMaxScore);
+weight, after, nDocs, sort, true, doDocScores || sort.needsScores(), doMaxScore);
       lock.lock();
       try {
         final AtomicReaderContext ctx = slice.leaves[0];
---------------
-------------
@@ -232,7 +232,7 @@
     
     if(rsp.getResults().getNumFound() == 0) {
       // wait and try again for slower machines
-      Thread.sleep( 2000 ); // wait 1/2 seconds...
+Thread.sleep( 2000 ); // wait 2 seconds...
       
       rsp = server.query( new SolrQuery( "id:id3") );
     }
---------------
-------------
@@ -156,7 +156,7 @@
     //System.out.println("SPR.readTermsBlock termsIn.fp=" + termsIn.getFilePointer());
     if (termState.bytes == null) {
       termState.bytes = new byte[ArrayUtil.oversize(len, 1)];
-      termState.bytesReader = new ByteArrayDataInput(null);
+termState.bytesReader = new ByteArrayDataInput();
     } else if (termState.bytes.length < len) {
       termState.bytes = new byte[ArrayUtil.oversize(len, 1)];
     }
---------------
-------------
@@ -67,7 +67,7 @@
           e.getCause() instanceof UnableToProxyException){
         //This is a weaving failure that should be logged, but the class
         //can still be loaded
-        LOGGER.info(NLS.MESSAGES.getMessage("cannot.weave", wovenClass.getClassName()), e);
+LOGGER.trace(NLS.MESSAGES.getMessage("cannot.weave", wovenClass.getClassName()), e);
       } else {
         String failureMessage = NLS.MESSAGES.getMessage("fatal.weaving.failure", wovenClass.getClassName());
         //This is a failure that should stop the class loading!
---------------
-------------
@@ -50,7 +50,7 @@
    */
   @Override
   public void transform(Map<String, ?> result, ResponseBuilder rb, SolrDocumentSource solrDocumentSource) {
-    NamedList<Object> commands = new NamedList<Object>();
+NamedList<Object> commands = new SimpleOrderedMap<Object>();
     for (Map.Entry<String, ?> entry : result.entrySet()) {
       Object value = entry.getValue();
       if (TopGroups.class.isInstance(value)) {
---------------
-------------
@@ -71,7 +71,7 @@
  *
  * @lucene.experimental */
 
-public class DirectPostingsFormat extends PostingsFormat {
+public final class DirectPostingsFormat extends PostingsFormat {
 
   private final int minSkipCount;
   private final int lowFreqCutoff;
---------------
-------------
@@ -337,7 +337,7 @@
     protected int getTimestampLength()
     {
         return supportsTimestampNanoseconds() ?
-            DRDAConstants.JDBC_TIMESTAMP_LENGTH : DRDAConstants.DRDA_TIMESTAMP_LENGTH;
+DRDAConstants.JDBC_TIMESTAMP_LENGTH : DRDAConstants.DRDA_OLD_TIMESTAMP_LENGTH;
     }
 
 }
---------------
-------------
@@ -64,7 +64,7 @@
         for (int j = 0; j < session.getSuperColumns(); j++)
         {
             String superColumn = 'S' + Integer.toString(j);
-            ColumnParent parent = new ColumnParent("CounterSuper1").setSuper_column(superColumn.getBytes());
+ColumnParent parent = new ColumnParent("SuperCounter1").setSuper_column(superColumn.getBytes());
 
             long start = System.currentTimeMillis();
 
---------------
-------------
@@ -65,7 +65,7 @@
 
   public boolean equals(Object o) {
     if (this == o) return true;
-    if (!(o instanceof TermQuery)) return false;
+if (!(o instanceof SpanRegexQuery)) return false;
     final SpanRegexQuery that = (SpanRegexQuery) o;
     return term.equals(that.term) && getBoost() == that.getBoost();
   }
---------------
-------------
@@ -164,7 +164,7 @@
         ZkCoreNodeProps coreNodeProps = new ZkCoreNodeProps(nodeProps);
         String node = coreNodeProps.getNodeName();
         if (!liveNodes.contains(coreNodeProps.getNodeName())
-            && coreNodeProps.getState().equals(
+|| !coreNodeProps.getState().equals(
                 ZkStateReader.ACTIVE)) continue;
         if (nodes.put(node, nodeProps) == null) {
           String url = coreNodeProps.getCoreUrl();
---------------
-------------
@@ -652,7 +652,7 @@
             try
             {
                 List<Range> ranges = new ArrayList<Range>(differences);
-                List<SSTableReader> sstables = CompactionManager.instance.submitAnti(cfstore, ranges, remote).get();
+List<SSTableReader> sstables = CompactionManager.instance.submitAnticompaction(cfstore, ranges, remote).get();
                 Streaming.transferSSTables(remote, sstables, cf.left);
             }
             catch(Exception e)
---------------
-------------
@@ -41,7 +41,7 @@
   };
 
   public void test01Exceptions() throws Exception {
-    String m = ExceptionQueryTest.getFailQueries(exceptionQueries, verbose);
+String m = ExceptionQueryTst.getFailQueries(exceptionQueries, verbose);
     if (m.length() > 0) {
       fail("No ParseException for:\n" + m);
     }
---------------
-------------
@@ -51,7 +51,7 @@
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
     Random random = random();
-    checkRandomData(random, analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random, analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -164,7 +164,7 @@
         //remember in setup a locator is already created
         //hence expected value is 2
         assertEquals("The locator values returned by " +
-            "SYSIBM.BLOBCREATELOCATOR() are incorrect", 2, locator);
+"SYSIBM.BLOBCREATELOCATOR() are incorrect", 4, locator);
         cs.close();
     }
 
---------------
-------------
@@ -42,7 +42,7 @@
    * @param pattern
    *          the pattern to apply to the incoming term buffer
    **/
-  protected PatternKeywordMarkerFilter(TokenStream in, Pattern pattern) {
+public PatternKeywordMarkerFilter(TokenStream in, Pattern pattern) {
     super(in);
     this.matcher = pattern.matcher("");
   }
---------------
-------------
@@ -64,7 +64,7 @@
     BufferedReader d = new BufferedReader(new InputStreamReader(
         TestParser.class.getResourceAsStream("reuters21578.txt"), "US-ASCII"));
     dir = newDirectory();
-    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(Version.LUCENE_40, analyzer));
+IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
     String line = d.readLine();
     while (line != null) {
       int endOfDate = line.indexOf('\t');
---------------
-------------
@@ -1195,7 +1195,7 @@
 		// Everything worked so log connection to the database.
 		if (getLogConnections())
 	 		println2Log(database.dbName, session.drdaID,
-				"Cloudscape Network Server connected to database " +
+"Apache Derby Network Server connected to database " +
 						database.dbName);
 		return 0;
 	}
---------------
-------------
@@ -21,7 +21,7 @@
 
 package org.apache.derby.impl.io.vfmem;
 
-import org.apache.derby.shared.common.sanity.SanityManager;
+import org.apache.derby.iapi.services.sanity.SanityManager;
 
 /**
  * Stores data in blocks, and supports reading/writing data from/into these
---------------
-------------
@@ -116,5 +116,5 @@
      * pending on this stage to be executed.
      * @return task count.
      */
-    public long getTaskCount();
+public long getPendingTasks();
 }
---------------
-------------
@@ -42,6 +42,6 @@
 
 public class Constants {
 
-  public static final String VERSION = "11.0.0";
+public static final String VERSION = "11.1.0";
 
 }
---------------
-------------
@@ -586,7 +586,7 @@
           // try to stop the thread:
           t.setUncaughtExceptionHandler(null);
           Thread.setDefaultUncaughtExceptionHandler(null);
-          if (!t.getName().equals("main-EventThread")) t.interrupt();
+t.interrupt();
           try {
             t.join(THREAD_STOP_GRACE_MSEC);
           } catch (InterruptedException e) { e.printStackTrace(); }
---------------
-------------
@@ -89,7 +89,7 @@
                     return 1;
                 }
 
-                return -AbstractType.this.compare(o1, o2);
+return AbstractType.this.compare(o2, o1);
             }
         };
     }
---------------
-------------
@@ -160,7 +160,7 @@
     // not found in readahead cache, seek underlying stream
     int newDoc = docsEnum.advance(target);
     //System.out.println("ts.advance docsEnum=" + docsEnum);
-    if (newDoc != DocsEnum.NO_MORE_DOCS) {
+if (newDoc != NO_MORE_DOCS) {
       doc = newDoc;
       freq = docsEnum.freq();
     } else {
---------------
-------------
@@ -603,7 +603,7 @@
     //The query below will work for the same reason. 
     checkLangBasedQuery(s, "SELECT count(*) FROM SYS.SYSTABLES WHERE CASE " +
     		" WHEN 1=1 THEN TABLENAME ELSE TABLEID END = TABLENAME",
-    		new String[][] {{"22"} });   
+new String[][] {{"23"} });
 
     //Do some testing using CONCATENATION
     //following will fail because result string of concatenation has 
---------------
-------------
@@ -122,7 +122,7 @@
 		@param in data stored in the log stream that contains the record data
 				necessary to restore the row.
 
-		@exception StandardException Standard Cloudscape error policy
+@exception StandardException Standard Derby error policy
 		@exception IOException Method may read from InputStream		
 	*/
 	public Page findUndo(
---------------
-------------
@@ -91,7 +91,7 @@
           //int n = random.nextInt(2);
           if (type == 0) {
             int i = seq.addAndGet(1);
-            Document doc = TestIndexWriterReader.createDocument(i, "index1", 10);
+Document doc = DocHelper.createDocument(i, "index1", 10);
             writer.addDocument(doc);
             addCount++;
           } else if (type == 1) {
---------------
-------------
@@ -37,6 +37,6 @@
     }
 
     public static Test suite() {
-        return TestConfiguration.clientServerDecorator(LobLimitsTest.suite());
+return (TestConfiguration.clientServerDecorator(LobLimitsTest.suite()));
     }
 }
---------------
-------------
@@ -210,7 +210,7 @@
             SliceRange range = predicate.slice_range;
             if (range.count < 0)
                 throw new InvalidRequestException("get_slice requires non-negative count");
-            return getSlice(new SliceFromReadCommand(keyspace, key, column_parent, range.start, range.finish, range.is_ascending, range.count), consistency_level);
+return getSlice(new SliceFromReadCommand(keyspace, key, column_parent, range.start, range.finish, range.reversed, range.count), consistency_level);
         }
     }
 
---------------
-------------
@@ -186,7 +186,7 @@
     @Override
     public void reflectWith(AttributeReflector reflector) {
       fillBytesRef();
-      reflector.reflect(TermToBytesRefAttribute.class, "bytes", new BytesRef(bytes));
+reflector.reflect(TermToBytesRefAttribute.class, "bytes", BytesRef.deepCopyOf(bytes));
       reflector.reflect(NumericTermAttribute.class, "shift", shift);
       reflector.reflect(NumericTermAttribute.class, "rawValue", getRawValue());
       reflector.reflect(NumericTermAttribute.class, "valueSize", valueSize);
---------------
-------------
@@ -285,7 +285,7 @@
 			//Use the collation type and info of the schema in which this
 			//function is defined for the return value of the function
 			newTDWithCorrectCollation.setCollationType(
-		    	     getSchemaDescriptor(getObjectName().getSchemaName(), false).getCollationType());
+getSchemaDescriptor().getCollationType());
 			newTDWithCorrectCollation.setCollationDerivation(
 	        		StringDataValue.COLLATION_DERIVATION_IMPLICIT);
 			return newTDWithCorrectCollation;
---------------
-------------
@@ -65,7 +65,7 @@
         return cd;
     }
 
-    public static ColumnDefinition inflate(org.apache.cassandra.config.avro.ColumnDef cd) throws ConfigurationException
+public static ColumnDefinition inflate(org.apache.cassandra.config.avro.ColumnDef cd)
     {
         byte[] name = new byte[cd.name.remaining()];
         cd.name.get(name, 0, name.length);
---------------
-------------
@@ -127,7 +127,7 @@
         /* Establish a thrift connection to the cassandra instance */
         TSocket socket = new TSocket(DatabaseDescriptor.getListenAddress().getHostName(), DatabaseDescriptor.getRpcPort());
         TTransport transport = new TFramedTransport(socket);
-        TBinaryProtocol binaryProtocol = new TBinaryProtocol(transport, false, false);
+TBinaryProtocol binaryProtocol = new TBinaryProtocol(transport);
         Cassandra.Client cassandraClient = new Cassandra.Client(binaryProtocol);
         transport.open();
         thriftClient = cassandraClient;
---------------
-------------
@@ -73,7 +73,7 @@
     }
 
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
   }
 
   @AfterClass
---------------
-------------
@@ -91,7 +91,7 @@
     Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false);
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
     Document doc = new Document();
-    TokenStream tokenStream = analyzer.tokenStream("field", new StringReader("abcd   "));
+TokenStream tokenStream = analyzer.tokenStream("field", "abcd   ");
     TeeSinkTokenFilter tee = new TeeSinkTokenFilter(tokenStream);
     TokenStream sink = tee.newSinkTokenStream();
     FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);
---------------
-------------
@@ -35,7 +35,7 @@
   public static void beforeClass() throws Exception {
     directory = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), directory);
-    writer.close();
+writer.shutdown();
     reader = DirectoryReader.open(directory);
   }
 
---------------
-------------
@@ -54,7 +54,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -379,7 +379,7 @@
         
 		LanguageConnectionContext lcc = getLanguageConnectionContext();
 		if (lcc.getAutoincrementUpdate() == false)
-			resultSet.getResultColumns().checkAutoincrement(null);
+resultSet.getResultColumns().forbidOverrides(null);
 
 		/*
 		** Mark the columns in this UpdateNode's result column list as
---------------
-------------
@@ -185,7 +185,7 @@
         		{"XJ004","Database '{0}' not found.","40000"},
         		{"XJ015","Derby system shutdown.","50000"},
         		{"XJ028","The URL '{0}' is not properly formed.","40000"},
-        		{"XJ040","Failed to start database '{0}', see the next exception for details.","40000"},
+{"XJ040","Failed to start database '{0}' with class loader {1}, see the next exception for details.","40000"},
         		{"XJ041","Failed to create database '{0}', see the next exception for details.","40000"},
         		{"XJ049","Conflicting create attributes specified.","40000"},
         		{"XJ05B","JDBC attribute '{0}' has an invalid value '{1}', valid values are '{2}'.","40000"},
---------------
-------------
@@ -110,7 +110,7 @@
     
     taxoWriter.close();
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     
     taxo = new DirectoryTaxonomyReader(taxoDir);
   }
---------------
-------------
@@ -50,7 +50,7 @@
             "resultsetJdbc20",           
             
             // from old jdbcapi.runall
-            "derbyStress",
+// "derbyStress",       TODO: Need a way to control heap size from Junit tests
             // "prepStmtMetaData",  TODO: convert - different canon for client
             // "resultsetStream", TODO: investigate failure/convert needs ext files
             "maxfieldsize",
---------------
-------------
@@ -56,7 +56,7 @@
     // used by filter subclass tests
 
     static final double MAX_FAILURE_RATE = 0.1;
-    public static final BloomCalculations.BloomSpecification spec = BloomCalculations.computeBucketsAndK(MAX_FAILURE_RATE);
+public static final BloomCalculations.BloomSpecification spec = BloomCalculations.computeBloomSpec(15, MAX_FAILURE_RATE);
     static final int ELEMENTS = 10000;
 
     static final ResetableIterator<String> intKeys()
---------------
-------------
@@ -41,7 +41,7 @@
 
   A b-tree scan controller corresponds to an instance of an open b-tree scan.
   <P>
-  <B>Concurrency Notes<\B>
+<B>Concurrency Notes</B>
   <P>
   The concurrency rules are derived from OpenBTree.
   <P>
---------------
-------------
@@ -363,7 +363,7 @@
   }
 
   public Query rewrite(IndexReader reader) throws IOException {
-    if (clauses.size() == 1) {                    // optimize 1-clause queries
+if (minNrShouldMatch == 0 && clauses.size() == 1) {                    // optimize 1-clause queries
       BooleanClause c = (BooleanClause)clauses.get(0);
       if (!c.isProhibited()) {			  // just return clause
 
---------------
-------------
@@ -1002,7 +1002,7 @@
     if (XML.classpathMeetsXMLReqs()) {
         checkLangBasedQuery(s, "SELECT ID, XMLSERIALIZE(V AS CLOB) " +
         		" FROM DERBY_2961 ORDER BY 1",
-        		null);
+new String[][] {{"1",null}});
     }
     s.close();
  
---------------
-------------
@@ -69,7 +69,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -155,6 +155,6 @@
 		writer.close();
 
 		//
-		searcher = new IndexSearcher(rd);
+searcher = new IndexSearcher(rd, true);
 	}
 }
---------------
-------------
@@ -1120,7 +1120,7 @@
                     {
                         if (target != null)
                         {
-                            rangeFileLocation = rangeFileLocation + System.getProperty("file.separator") + "bootstrap";
+rangeFileLocation = rangeFileLocation + File.separator + "bootstrap";
                         }
                         FileUtils.createDirectory(rangeFileLocation);
                         String fname = new File(rangeFileLocation, mergedFileName).getAbsolutePath();
---------------
-------------
@@ -26,7 +26,7 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.*;
+import static org.apache.lucene.analysis.VocabularyAssert.*;
 
 /**
  * Simple tests for {@link SwedishLightStemFilter}
---------------
-------------
@@ -166,7 +166,7 @@
                 // This is a substatement; for now, we do not set any timeout
                 // for it. We might change this behaviour later, by linking
                 // timeout to its parent statement's timeout settings.
-				ResultSet rs = ps.execute(spsActivation, false, false, false, 0L);
+ResultSet rs = ps.execute(spsActivation, false, 0L);
                 if( rs.returnsRows())
                 {
                     // Fetch all the data to ensure that functions in the select list or values statement will
---------------
-------------
@@ -95,7 +95,7 @@
 		null if nothing to undo. 
 
 		@exception IOException Can be thrown by any of the methods of ObjectInput.
-		@exception StandardException Standard Cloudscape policy.
+@exception StandardException Standard Derby policy.
 
 		@see Loggable#releaseResource
 		@see Loggable#needsRedo
---------------
-------------
@@ -30,7 +30,7 @@
 
     public boolean isMarkedForDelete();
     public long getMarkedForDeleteAt();
-    public long mostRecentChangeAt();
+public long mostRecentLiveChangeAt();
     public byte[] name();
     public int size();
     public int serializedSize();
---------------
-------------
@@ -40,7 +40,7 @@
     @Override
     protected int getLevelForDistance(double degrees) {
       GeohashPrefixTree grid = new GeohashPrefixTree(ctx, GeohashPrefixTree.getMaxLevelsPossible());
-      return grid.getLevelForDistance(degrees) + 1;//returns 1 greater
+return grid.getLevelForDistance(degrees);
     }
 
     @Override
---------------
-------------
@@ -191,7 +191,7 @@
     final Directory dir = newDirectory();
     final RandomIndexWriter w = new RandomIndexWriter(random, dir);
     
-    final int numTerms = atLeast(1000);
+final int numTerms = atLeast(300);
 
     final Set<String> terms = new HashSet<String>();
     final Collection<String> pendingTerms = new ArrayList<String>();
---------------
-------------
@@ -291,7 +291,7 @@
 		try {
 			if (atLeastOneDebug)
 				dblook.writeVerboseOutput(
-					"CSLOOK_AtLeastOneDebug", null);
+"DBLOOK_AtLeastOneDebug", null);
 			logFile.close();
 			if (ddlFile != null)
 				ddlFile.close();
---------------
-------------
@@ -30,7 +30,7 @@
 		super(new OutputStreamWriter(o), true);
 		out = o;
 	}
-	public LocalizedOutput(OutputStream o, String enc) throws UnsupportedEncodingException {
+LocalizedOutput(OutputStream o, String enc) throws UnsupportedEncodingException {
 		super(new OutputStreamWriter(o, enc), true);
 		out = o;
 	}
---------------
-------------
@@ -63,7 +63,7 @@
         suite.addTest(CreateTableFromQueryTest.suite());
         suite.addTest(DatabaseClassLoadingTest.suite());
         suite.addTest(DynamicLikeOptimizationTest.suite());
-        suite.addTest(ExistsWithSetOpsTest.suite());
+suite.addTest(ExistsWithSubqueriesTest.suite());
         suite.addTest(GrantRevokeTest.suite());
         suite.addTest(GroupByExpressionTest.suite());
 		suite.addTest(LangScripts.suite());
---------------
-------------
@@ -230,7 +230,7 @@
 		throws StandardException
 	{
 		CollectNodesVisitor getCRs = new CollectNodesVisitor(ColumnReference.class);
-		super.accept(getCRs);
+accept(getCRs);
 		Vector colRefs = getCRs.getList();
 		for (Enumeration e = colRefs.elements(); e.hasMoreElements(); )
 		{
---------------
-------------
@@ -63,7 +63,7 @@
   }
 
   public DataSource getDataSource(String name) {
-    return dataImporter.getDataSourceInstance(entity);
+return dataImporter.getDataSourceInstance(entity, name, this);
   }
 
   public boolean isRootEntity() {
---------------
-------------
@@ -129,7 +129,7 @@
         cores.load(solrHome, fconf);
       } else {
         cores.defaultAbortOnConfigError = abortOnConfigurationError;
-        cores.load(solrHome, new ByteArrayInputStream(DEF_SOLR_XML.getBytes()));
+cores.load(solrHome, new ByteArrayInputStream(DEF_SOLR_XML.getBytes("UTF-8")));
         cores.configFile = fconf;
       }
       setAbortOnConfigurationError(0 < cores.numCoresAbortOnConfigError);
---------------
-------------
@@ -18,5 +18,5 @@
     public double keys_cached = CFMetaData.DEFAULT_KEY_CACHE_SIZE; 
     public double read_repair_chance = CFMetaData.DEFAULT_READ_REPAIR_CHANCE;
     public boolean preload_row_cache = CFMetaData.DEFAULT_PRELOAD_ROW_CACHE;
-    public Map<byte[], ColumnDefinition> column_metata = Collections.emptyMap();
+public RawColumnDefinition[] column_metadata = new RawColumnDefinition[0];
 }
---------------
-------------
@@ -77,7 +77,7 @@
             compositeManifest.put(Constants.BUNDLE_SYMBOLICNAME, "test-composite");
             compositeManifest.put(Constants.BUNDLE_VERSION, "1.0.0");
             // this import-package is used by the blueprint.sample
-            compositeManifest.put(Constants.IMPORT_PACKAGE, "org.osgi.service.blueprint.container");
+compositeManifest.put(Constants.IMPORT_PACKAGE, "org.osgi.service.blueprint;version=\"[1.0.0,2.0.0)\", org.osgi.service.blueprint.container;version=1.0");
             // this export-package is used by pax junit runner as it needs to see the blueprint sample package 
             // for the test after the blueprint sample is started.
             compositeManifest.put(Constants.EXPORT_PACKAGE, "org.apache.aries.blueprint.sample");
---------------
-------------
@@ -384,7 +384,7 @@
         synchronized (runners) {
           runner = runners.peek();
         }
-        if (runner == null)
+if (runner == null || scheduler.isTerminated())
           break;
         runner.runnerLock.lock();
         runner.runnerLock.unlock();
---------------
-------------
@@ -90,6 +90,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new GreekAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new GreekAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -2468,7 +2468,7 @@
             }
             catch (StandardException se)
             {
-                if (!se.getMessageId().equals(SQLState.LOCK_TIMEOUT))
+if (!se.isLockTimeout())
                 {
                     // if it is a timeout then escalate did not happen and
                     // just fall through.
---------------
-------------
@@ -71,7 +71,7 @@
   protected void setUp() throws Exception {
     super.setUp();
     similarityOne = new SimilarityOne();
-    anlzr = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+anlzr = new StandardAnalyzer(TEST_VERSION_CURRENT);
   }
   
   /**
---------------
-------------
@@ -393,7 +393,7 @@
 
                 String key = "Note for DERBY-" + issue.getKey();
                 //println("Release note: "+issue.getKey()+" - "+issue.getTitle());
-                Element paragraph = outputDoc.createElement(PARAGRAPH);
+Element paragraph = outputDoc.createElement(SPAN);
                 paragraph.appendChild(outputDoc.createTextNode(key + ": "));
                 cloneChildren(summaryText, paragraph);
                 insertLine(issuesSection);
---------------
-------------
@@ -464,7 +464,7 @@
             }
 
             header.turnOff(id);
-            if (header.isSafeToDelete())
+if (header.isSafeToDelete() && iter.hasNext())
             {
                 logger.info("Discarding obsolete commit log:" + segment);
                 segment.close();
---------------
-------------
@@ -91,7 +91,7 @@
 
   private TopDocsCollector doSearch(int numResults) throws IOException {
     Query q = new MatchAllDocsQuery();
-    IndexSearcher searcher = new IndexSearcher(dir);
+IndexSearcher searcher = new IndexSearcher(dir, true);
     TopDocsCollector tdc = new MyTopsDocCollector(numResults);
     searcher.search(q, tdc);
     searcher.close();
---------------
-------------
@@ -96,7 +96,7 @@
     String testString = "t";
     
     Analyzer analyzer = new MockAnalyzer(random());
-    TokenStream stream = analyzer.tokenStream("dummy", new StringReader(testString));
+TokenStream stream = analyzer.tokenStream("dummy", testString);
     stream.reset();
     while (stream.incrementToken()) {
       // consume
---------------
-------------
@@ -42,7 +42,7 @@
  */
 
 public class SurroundQParserPlugin extends QParserPlugin {
-  public static String NAME = "surround";
+public static final String NAME = "surround";
 
   @Override
   public void init(NamedList args) {
---------------
-------------
@@ -66,7 +66,7 @@
     //-----------------------event callback methods-------------------------------
 
     public void listenToUnitOfWork() {
-        agent_.connection_.CommitAndRollbackListeners_.add(this);
+agent_.connection_.CommitAndRollbackListeners_.put(this,null);
     }
 
     public void completeLocalCommit(java.util.Iterator listenerIterator) {
---------------
-------------
@@ -77,7 +77,7 @@
 
     // Open near-real-time searcher
     searcher = new IndexSearcher(DirectoryReader.open(indexWriter, true));
-    indexWriter.close();
+indexWriter.shutdown();
   }
 
   private FacetsConfig getConfig() {
---------------
-------------
@@ -67,7 +67,7 @@
     {
         // Since this is an OutputStream returned by Clob.setAsciiStream 
         // use Ascii encoding when creating the String from bytes
-        String str = new String(b, "US-ASCII");
+String str = new String(b, "ISO-8859-1");
         clob_.string_ = clob_.string_.substring(0, (int) offset_ - 1);
         clob_.string_ = clob_.string_.concat(str);
         clob_.asciiStream_ = new java.io.StringBufferInputStream(clob_.string_);
---------------
-------------
@@ -31,7 +31,7 @@
 import java.util.HashMap;
 import org.apache.thrift.TEnum;
 
-public enum IndexOperator implements TEnum {
+public enum IndexOperator implements org.apache.thrift.TEnum {
   EQ(0),
   GTE(1),
   GT(2),
---------------
-------------
@@ -1688,7 +1688,7 @@
 			ResultColumn resultColumn = (ResultColumn) elementAt(index);
 
 			/* Skip over generated columns */
-			if (resultColumn.isGenerated())
+if (resultColumn.isGenerated() || resultColumn.isGeneratedForUnmatchedColumnInInsert())
 			{
 				continue;
 			}
---------------
-------------
@@ -70,7 +70,7 @@
     typeTokenFilterFactory.create(input);
   }
 
-  @Test         x
+@Test
   public void testCreationWithWhiteList() throws Exception {
     TypeTokenFilterFactory typeTokenFilterFactory = new TypeTokenFilterFactory();
     Map<String, String> args = new HashMap<String, String>(DEFAULT_VERSION_PARAM);
---------------
-------------
@@ -96,7 +96,7 @@
         MBeanHandler packageStateHandler = new PackageStateMBeanHandler(bc, logger);
         packageStateHandler.open();
         mbeansHandlers.add(packageStateHandler);
-        MBeanHandler permissionAdminHandler = new PermissionAdminMBeanHandler(bc, logger);
+MBeanHandler permissionAdminHandler = new PermissionAdminMBeanHandler(agentContext);
         permissionAdminHandler.open();
         mbeansHandlers.add(permissionAdminHandler);
         MBeanHandler userAdminHandler = new UserAdminMBeanHandler(agentContext);
---------------
-------------
@@ -278,7 +278,7 @@
           continue;
         }
 
-        docsEnum = termsEnum.docs(null, docsEnum, false);
+docsEnum = termsEnum.docs(null, docsEnum, 0);
         int doc;
         while ((doc = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
           vals[doc] = fval;
---------------
-------------
@@ -745,7 +745,7 @@
     public static int getTimestampLength( boolean supportsTimestampNanoseconds )
     {
         return supportsTimestampNanoseconds ?
-            DRDAConstants.JDBC_TIMESTAMP_LENGTH : DRDAConstants.DRDA_TIMESTAMP_LENGTH;
+DRDAConstants.JDBC_TIMESTAMP_LENGTH : DRDAConstants.DRDA_OLD_TIMESTAMP_LENGTH;
     }
 
 }
---------------
-------------
@@ -219,7 +219,7 @@
         if (isStandard)
             startIColumn = new Column(filter.start);
         else
-            startIColumn = new SuperColumn(filter.start, null, cf.getClockType()); // ok to not have subcolumnComparator since we won't be adding columns to this object
+startIColumn = new SuperColumn(filter.start, null, cf.getClockType(), cf.getReconciler()); // ok to not have subcolumnComparator since we won't be adding columns to this object
 
         // can't use a ColumnComparatorFactory comparator since those compare on both name and time (and thus will fail to match
         // our dummy column, since the time there is arbitrary).
---------------
-------------
@@ -45,7 +45,7 @@
 import org.apache.lucene.index.values.Ints;
 import org.apache.lucene.index.values.DocValues;
 import org.apache.lucene.index.values.Floats;
-import org.apache.lucene.index.values.Values;
+import org.apache.lucene.index.values.Type;
 import org.apache.lucene.search.FieldCache; // not great (circular); used only to purge FieldCache entry on close
 import org.apache.lucene.util.BytesRef;
 
---------------
-------------
@@ -41,7 +41,7 @@
     public void testTokenGetsUpdated() throws IOException
     {
         SystemTable.StorageMetadata storageMetadata1 = SystemTable.initMetadata();
-        SystemTable.updateToken(StorageService.getPartitioner().getToken("503545744:0"));
+SystemTable.updateToken(StorageService.getPartitioner().getToken("503545744:0".getBytes()));
         SystemTable.StorageMetadata storageMetadata2 = SystemTable.initMetadata();
         Assert.assertTrue("smd should still be a singleton after updateToken", storageMetadata1 == storageMetadata2);
     }
---------------
-------------
@@ -63,7 +63,7 @@
   public void setContext( TransformContext context ) {
     try {
       IndexReader reader = qparser.getReq().getSearcher().getIndexReader();
-      readerContexts = reader.getTopReaderContext().leaves();
+readerContexts = reader.leaves();
       docValuesArr = new FunctionValues[readerContexts.size()];
 
       searcher = qparser.getReq().getSearcher();
---------------
-------------
@@ -659,7 +659,7 @@
     */
     public Row getSliceFrom(String key, String cf, boolean isAscending, int count) throws IOException
     {
-        Row row = new Row(key);
+Row row = new Row(table_, key);
         String[] values = RowMutation.getColumnAndColumnFamily(cf);
         String cfName = values[0];
         String startWith = values.length > 1 ? values[1] : "";
---------------
-------------
@@ -228,7 +228,7 @@
         		{"XSLA1","Log Record has been sent to the stream, but it cannot be applied to the store (Object {0}).  This may cause recovery problems also.","45000"},
         		{"XSLA2","System will shutdown, got I/O Exception while accessing log file.","45000"},
         		{"XSLA3","Log Corrupted, has invalid data in the log stream.","45000"},
-        		{"XSLA4","Cannot write to the log, most likely the log is full.  Please delete unnecessary files.  It is also possible that the file system is read only, or the disk has failed, or some other problems with the media.  ","45000"},
+{"XSLA4","Error encountered when attempting to write the transaction recovery log. Most likely the disk holding the recovery log is full. If the disk is full, the only way to proceed is to free up space on the disk by either expanding it or deleting files not related to Derby. It is also possible that the file system and/or disk where the Derby transaction log resides is read-only. The error can also be encountered if the disk or file system has failed.","45000"},
         		{"XSLA5","Cannot read log stream for some reason to rollback transaction {0}.","45000"},
         		{"XSLA6","Cannot recover the database.","45000"},
         		{"XSLA7","Cannot redo operation {0} in the log.","45000"},
---------------
-------------
@@ -240,7 +240,7 @@
       throw new RuntimeException("Unable to end & close TokenStream after analyzing range part: " + part, e);
     }
       
-    return new BytesRef(bytes);
+return BytesRef.deepCopyOf(bytes);
   }
   
   @Override
---------------
-------------
@@ -326,7 +326,7 @@
         final CFMetaData indexedCfMetadata = CFMetaData.newIndexMetadata(metadata, info, columnComparator);
         ColumnFamilyStore indexedCfs = ColumnFamilyStore.createColumnFamilyStore(table,
                                                                                  indexedCfMetadata.cfName,
-                                                                                 new LocalPartitioner(metadata.getColumn_metadata().get(info.name).validator),
+new LocalPartitioner(metadata.getColumn_metadata().get(info.name).getValidator()),
                                                                                  indexedCfMetadata);
 
         // link in indexedColumns.  this means that writes will add new data to the index immediately,
---------------
-------------
@@ -35,7 +35,7 @@
   private final PositionIncrementAttribute posIncAttribute =  addAttribute(PositionIncrementAttribute.class);
   
   // use a fixed version, as we don't care about case sensitivity.
-  private final CharArraySet previous = new CharArraySet(Version.LUCENE_50, 8, false);
+private final CharArraySet previous = new CharArraySet(Version.LUCENE_CURRENT, 8, false);
 
   /**
    * Creates a new RemoveDuplicatesTokenFilter
---------------
-------------
@@ -52,7 +52,7 @@
                 logger.debug(srm.toString());
 
             StreamOutSession session = StreamOutSession.create(srm.table, message.getFrom(), srm.sessionId);
-            StreamOut.transferRangesForRequest(session, srm.ranges);
+StreamOut.transferRangesForRequest(session, srm.ranges, srm.type);
         }
         catch (IOException ex)
         {
---------------
-------------
@@ -99,7 +99,7 @@
     DecimalFormat formatter = new DecimalFormat("###,###.###");
     for (int x = 0; x < infos.size(); x++) {
       SegmentInfo info = infos.info(x);
-      String sizeStr = formatter.format(info.sizeInBytes());
+String sizeStr = formatter.format(info.sizeInBytes(true));
       System.out.println(info.name + " " + sizeStr);
     }
   }
---------------
-------------
@@ -171,7 +171,7 @@
         obuilder.withLongName("input").withRequired(true).withArgument(
             abuilder.withName("input").withMinimum(1).withMaximum(1).create())
             .withDescription("The input dir containing the documents")
-            .withShortName("p").create();
+.withShortName("i").create();
     
     Option outputDirOpt =
         obuilder.withLongName("output").withRequired(true).withArgument(
---------------
-------------
@@ -68,7 +68,7 @@
         return memtable.getSliceIterator(cf, this, comparator);
     }
 
-    public ColumnIterator getSSTableColumnIterator(SSTableReader sstable) throws IOException
+public ColumnIterator getSSTableColumnIterator(SSTableReader sstable)
     {
         Predicate<IColumn> predicate = (bitmasks == null || bitmasks.isEmpty())
                                        ? Predicates.<IColumn>alwaysTrue()
---------------
-------------
@@ -25,7 +25,7 @@
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.SortField;
-import com.spatial4j.core.context.ParseUtils;
+import com.spatial4j.core.io.ParseUtils;
 import com.spatial4j.core.distance.DistanceUtils;
 import com.spatial4j.core.exception.InvalidShapeException;
 import org.apache.solr.common.SolrException;
---------------
-------------
@@ -183,7 +183,7 @@
      * in the data file. Binary search is performed on a list of these objects
      * to lookup keys within the SSTable data file.
      */
-    public class KeyPosition implements Comparable<KeyPosition>
+public static class KeyPosition implements Comparable<KeyPosition>
     {
         public final DecoratedKey key;
         public final long position;
---------------
-------------
@@ -86,7 +86,7 @@
                 }
                 rows.put(key, SSTableUtils.createCF(Long.MIN_VALUE, Integer.MIN_VALUE, cols));
             }
-            SSTableReader sstable = SSTableUtils.writeSSTable(rows);
+SSTableReader sstable = SSTableUtils.prepare().write(rows);
             sstables.add(sstable);
             store.addSSTable(sstable);
         }
---------------
-------------
@@ -37,7 +37,7 @@
 
   @Before
   public void before() throws IOException {
-    configuration = new Configuration();
+configuration = getConfiguration();
   }
 
   @After
---------------
-------------
@@ -886,7 +886,7 @@
         }
         
         // If switching, invalidate previous access level; force a new login.
-        if (keySpace.get() != null && !keySpace.get().equals(keyspace));
+if (keySpace.get() != null && !keySpace.get().equals(keyspace))
             loginDone.set(AccessLevel.NONE);
         
         keySpace.set(keyspace); 
---------------
-------------
@@ -104,7 +104,7 @@
 
   @Override
   public Object toObject(IndexableField f) {
-    if (f.numeric()) {
+if (f.numericDataType() != null) {
       final Number val = f.numericValue();
       if (val==null) return badFieldString(f);
       return (type == TrieTypes.DATE) ? new Date(val.longValue()) : val;
---------------
-------------
@@ -51,6 +51,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new PolishAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new PolishAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -73,7 +73,7 @@
 
 
     public void testSearch() throws Exception {
-        Query query = QueryParser.parse("test", "contents", analyzer);
+Query query = new QueryParser("contents",analyzer).parse("test");
 
         Hits hits = searcher.search(query);
 
---------------
-------------
@@ -51,6 +51,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new FinnishAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new FinnishAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -62,7 +62,7 @@
       int offset = random.nextInt(charArray.length);
       int length = charArray.length - offset;
       String str = new String(charArray, offset, length);
-      ref.copy(charArray, offset, length);
+ref.copyChars(charArray, offset, length);
       assertEquals(str, ref.toString());  
     }
     
---------------
-------------
@@ -276,7 +276,7 @@
         catch( IOException ioe)
         {
             if( create)
-                throw StandardException.newException( SQLState.SERVICE_DIRECTORY_CREATE_ERROR, dataDirectory, ioe);
+throw StandardException.newException( SQLState.SERVICE_DIRECTORY_CREATE_ERROR, ioe, dataDirectory);
             else
                 throw StandardException.newException( SQLState.DATABASE_NOT_FOUND, ioe, dataDirectory);
         }
---------------
-------------
@@ -290,7 +290,7 @@
         success = true;
       } catch (IOException ioe) {
         failure.clearDoFail();
-        writer.shutdown(false);
+writer.close();
       }
       if (VERBOSE) {
         System.out.println("TEST: success=" + success);
---------------
-------------
@@ -156,7 +156,7 @@
 		switch (typeId)
 		{
 			case Types.TIMESTAMP:
-				size = 26;
+size = 29;
 				break;
 			case Types.DATE:
 				size = 10;
---------------
-------------
@@ -95,7 +95,7 @@
             return SPACE;
         }
         remainingNonBlanks--;
-        return alphabet.nextByte();
+return (alphabet.nextByte() & 0xff);
     }
 
     public int read(byte[] buf, int off, int length) {
---------------
-------------
@@ -99,7 +99,7 @@
 
   @Override
   public int hashCode() {
-    return getDirectory().hashCode() + getSegmentsFileName().hashCode();
+return (int) (getDirectory().hashCode() + getVersion());
   }
 
   /** Returns the version for this IndexCommit.  This is the
---------------
-------------
@@ -62,7 +62,7 @@
       transactions.add(Arrays.asList("D", "A", "C", "E", "B"));
       transactions.add(Arrays.asList("C", "A", "B", "E"));
       transactions.add(Arrays.asList("B", "A", "D"));
-      transactions.add(Arrays.asList("D"));
+transactions.add(Arrays.asList("D", "D", "", "D", "D"));
       transactions.add(Arrays.asList("D", "B"));
       transactions.add(Arrays.asList("A", "D", "E"));
       transactions.add(Arrays.asList("B", "C"));
---------------
-------------
@@ -93,7 +93,7 @@
   */
   public static void main(String args[]) {
         // adjust the application in accordance with derby.ui.locale and derby.ui.codeset
-        LocalizedResource.getInstance();
+LocalizedResource.getInstance().init();
 
 		LocalizedOutput out;
 
---------------
-------------
@@ -40,7 +40,7 @@
  * <LI> JDBC 2.0 - Java 2 - JDK 1.2,1.3
  * </UL>
  */
-public class ClientConnectionPoolDataSource extends ClientBaseDataSource 
+public class ClientConnectionPoolDataSource extends ClientDataSource
                                            implements ConnectionPoolDataSource {
     private static final long serialVersionUID = -539234282156481377L;
     public static final String className__ = "org.apache.derby.jdbc.ClientConnectionPoolDataSource";
---------------
-------------
@@ -41,7 +41,7 @@
   @Override
   public void setUp() throws Exception {
       super.setUp();
-      workDir = new File(TEMP_DIR, "TestMultiMMap");
+workDir = _TestUtil.getTempDir("TestMultiMMap");
       workDir.mkdirs();
   }
   
---------------
-------------
@@ -2198,7 +2198,7 @@
    *  {@link MergeScheduler} that is able to run merges in
    *  background threads. */
   public void optimize(boolean doWait) throws CorruptIndexException, IOException {
-    optimize(1, true);
+optimize(1, doWait);
   }
 
   /** Just like {@link #optimize(int)}, except you can
---------------
-------------
@@ -323,7 +323,7 @@
 		}
 
 		if (cursor == null || cursor.isClosed()) {
-			throw StandardException.newException(SQLState.LANG_CURSOR_CLOSED, cursorName);	
+throw StandardException.newException(SQLState.LANG_CURSOR_NOT_FOUND, cursorName);
 		}
 	}
 
---------------
-------------
@@ -40,7 +40,7 @@
   }
 
   @Override
-  public void build(SolrCore core, SolrIndexSearcher searcher) {
+public void build(SolrCore core, SolrIndexSearcher searcher) throws IOException {
 
   }
 
---------------
-------------
@@ -62,7 +62,7 @@
     if (similarity == null) {
       throw new UnsupportedOperationException("requires a TFIDFSimilarity (such as DefaultSimilarity)");
     }
-    final NumericDocValues norms = readerContext.reader().simpleNormValues(field);
+final NumericDocValues norms = readerContext.reader().getNormValues(field);
 
     if (norms == null) {
       return new ConstDoubleDocValues(0.0, this);
---------------
-------------
@@ -81,7 +81,7 @@
       }
     }
     w.forceMerge(1);
-    w.close();
+w.shutdown();
     if (VERBOSE) {
       boolean found = false;
       for (String file : dir.listAll()) {
---------------
-------------
@@ -335,7 +335,7 @@
       
       private static class ScoreTermQueue extends PriorityQueue<ScoreTerm> {        
         public ScoreTermQueue(int size){
-          initialize(size);
+super(size);
         }
         
         /* (non-Javadoc)
---------------
-------------
@@ -164,7 +164,7 @@
         this.tokens = tokens;
         termAtt = addAttribute(CharTermAttribute.class);
         offsetAtt = addAttribute(OffsetAttribute.class);
-        posincAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);
+posincAtt = addAttribute(PositionIncrementAttribute.class);
       }
 
       @Override
---------------
-------------
@@ -154,7 +154,7 @@
    *          offset in input arrays where partition starts
    */
   protected boolean isSelfPartition (int ordinal, FacetArrays facetArrays, int offset) {
-    int partitionSize = facetArrays.getArraysLength();
+int partitionSize = facetArrays.arrayLength;
     return ordinal / partitionSize == offset / partitionSize;
   }
 
---------------
-------------
@@ -54,7 +54,7 @@
 
     public static Test suite() {
         
-        if (! isSunJVM()) {
+if (isIBMJVM()) {
             // DERBY-4463 test fails on IBM VM 1.5.
             // It's fixed in IBM VM 1.6 SR9 and above.
             // Remove this condition when that issue is solved in IBM VM 1.5 SR13.
---------------
-------------
@@ -675,7 +675,7 @@
     } catch(Exception e) {
       // unexpected exception...
       SolrConfig.severeErrors.add( e );
-      throw new SolrException( SolrException.ErrorCode.SERVER_ERROR,"Schema Parsing Failed",e,false);
+throw new SolrException( SolrException.ErrorCode.SERVER_ERROR,"Schema Parsing Failed: " + e.getMessage(), e,false);
     }
 
     // create the field analyzers
---------------
-------------
@@ -281,7 +281,7 @@
     }
 
     // test debugging
-    handle.put("explain", UNORDERED);
+handle.put("explain", SKIPVAL);
     handle.put("debug", UNORDERED);
     handle.put("time", SKIPVAL);
     query("q","now their fox sat had put","fl","*,score",CommonParams.DEBUG_QUERY, "true");
---------------
-------------
@@ -67,7 +67,7 @@
         Class.forName("org.tartarus.snowball.ext." + name + "Stemmer").asSubclass(SnowballProgram.class);
       stemmer = stemClass.newInstance();
     } catch (Exception e) {
-      throw new RuntimeException(e.toString());
+throw new IllegalArgumentException("Invalid stemmer class specified: " + name, e);
     }
   }
 
---------------
-------------
@@ -77,7 +77,7 @@
     }
     writer.commit();
     writer.forceMerge(1);
-    writer.close();
+writer.shutdown();
 
     AtomicReader reader = getOnlySegmentReader(DirectoryReader.open(dir));
     
---------------
-------------
@@ -968,7 +968,7 @@
             CallableStatement cs = conn.prepareCall(
                 "CALL SYSCS_UTIL.SYSCS_COMPRESS_TABLE(?, ?, ?)");
             cs.setString(1, "APP");
-            cs.setString(2, "testLongVarChar");
+cs.setString(2, "TESTLONGVARCHAR");
             cs.setInt(3, 0);
             cs.execute();
 
---------------
-------------
@@ -52,7 +52,7 @@
 	    {
             RowMutationMessage rmMsg = RowMutationMessage.serializer().deserialize(rowMutationCtx.buffer_);
             RowMutation rm = rmMsg.getRowMutation();            	                
-            rowMutationCtx.row_.key(rm.key());
+rowMutationCtx.row_.setKey(rm.key());
             rm.applyBinary(rowMutationCtx.row_);
 	
 	    }        
---------------
-------------
@@ -97,7 +97,7 @@
       connector.setReuseAddress(true);
       QueuedThreadPool threadPool = (QueuedThreadPool) connector.getThreadPool();
       if (threadPool != null) {
-        threadPool.setMaxStopTimeMs(100);
+threadPool.setMaxStopTimeMs(1000);
       }
       server.setConnectors(new Connector[] { connector });
       server.setSessionIdManager(new HashSessionIdManager(new Random()));
---------------
-------------
@@ -332,7 +332,7 @@
    * @param key the key
    * @param cmd the patch command
    */
-  public void add(CharSequence key, CharSequence cmd) {
+void add(CharSequence key, CharSequence cmd) {
     if (key == null || cmd == null) {
       return;
     }
---------------
-------------
@@ -320,7 +320,7 @@
     throws IOException, ClassNotFoundException, InterruptedException {
     
     ClusterClassifier.writePolicy(new FuzzyKMeansClusteringPolicy(m, convergenceDelta), clustersIn);
-    ClusterClassificationDriver.run(input, output, new Path(output, PathDirectory.CLUSTERED_POINTS_DIRECTORY), threshold, true,
+ClusterClassificationDriver.run(input, output, new Path(output, PathDirectory.CLUSTERED_POINTS_DIRECTORY), threshold, emitMostLikely,
         runSequential);
   }
 }
---------------
-------------
@@ -118,7 +118,7 @@
     Directory dir = newFSDirectory(newDir);
     IndexWriter iw = new IndexWriter(
         dir,
-        new IndexWriterConfig(Version.LUCENE_40, new StandardAnalyzer(Version.LUCENE_40))
+new IndexWriterConfig(TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT))
     );
     Document doc = new Document();
     doc.add(new TextField("id", "2", Field.Store.YES));
---------------
-------------
@@ -67,7 +67,7 @@
     @Override
     protected void finalize()
     {
-        assert references.get() == 0;
+assert references.get() <= 0;
         assert peer == 0;
     }
     
---------------
-------------
@@ -1,6 +1,6 @@
 /*
  
-Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun
+Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun_Local
  
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -72,7 +72,7 @@
   private void fill() throws IOException {
     StringBuilder buffered = new StringBuilder();
     char [] temp = new char [1024];
-    for (int cnt = in.read(temp); cnt > 0; cnt = in.read(temp)) {
+for (int cnt = input.read(temp); cnt > 0; cnt = input.read(temp)) {
       buffered.append(temp, 0, cnt);
     }
     transformedInput = new StringReader(processPattern(buffered).toString());
---------------
-------------
@@ -711,7 +711,7 @@
                     SQLState.BLOB_NONPOSITIVE_LENGTH,
                     new Long(length));
         }
-        if (length > (this.length() - pos)) {
+if (length > (this.length() - (pos -1))) {
             throw Util.generateCsSQLException(
                     SQLState.POS_AND_LENGTH_GREATER_THAN_LOB,
                     new Long(pos), new Long(length));
---------------
-------------
@@ -147,7 +147,7 @@
     if (tempDir == null)
       throw new IOException("java.io.tmpdir undefined, cannot run test");
     File indexDir = new File(tempDir, "lucenetestindex");
-    Directory rd = FSDirectory.getDirectory(indexDir, null, false);
+Directory rd = FSDirectory.getDirectory(indexDir);
     IndexThread.id = 0;
     IndexThread.idStack.clear();
     IndexModifier index = new IndexModifier(rd, new StandardAnalyzer(), create);
---------------
-------------
@@ -135,7 +135,7 @@
       }
       iw.forceMerge(1);
     }
-    iw.close();
+iw.shutdown();
     if (VERBOSE) {
       System.out.println("TEST: setUp done close");
     }
---------------
-------------
@@ -89,7 +89,7 @@
     {
         String testName = "InterruptResilienceTest";
 
-        if (! isSunJVM()) {
+if (isIBMJVM()) {
             // DERBY-4463 test fails on IBM VM 1.5.
             // It's fixed in IBM VM 1.6 SR9 and above.
             // Remove this condition when that issue is solved in IBM VM 1.5 SR13.
---------------
-------------
@@ -53,7 +53,7 @@
 	final XAXactId			xid;	
 	/**
 		When an XAResource suspends a transaction (end(TMSUSPEND)) it must be resumed
-		using the same XAConnection. This has been the traditional Cloudscape behaviour,
+using the same XAConnection. This has been the traditional Cloudscape/Derby behaviour,
 		though there does not seem to be a specific reference to this behaviour in
 		the JTA spec. Note that while the transaction is suspended by this XAResource,
 		another XAResource may join the transaction and suspend it after the join.
---------------
-------------
@@ -153,7 +153,7 @@
 	public BigDecimal	getBigDecimal()
 	{
 		if (isNull()) return null;
-		return new BigDecimal(value);
+return new BigDecimal(Double.toString(value));
 	}
 
     // for lack of a specification: getDouble()==0 gives true
---------------
-------------
@@ -107,7 +107,7 @@
       JettySolrRunner j = createJetty(jettyHome, null, "shard" + (i + 2));
       jettys.add(j);
       clients.add(createNewSolrServer(j.getLocalPort()));
-      sb.append("127.0.0.1:").append(j.getLocalPort()).append(context);
+sb.append(buildUrl(j.getLocalPort()));
     }
 
     shards = sb.toString();
---------------
-------------
@@ -151,7 +151,7 @@
     searcherThread1.join();
     searcherThread2.join();
 
-    writer.close();
+writer.shutdown();
 
     assertTrue("hit unexpected exception in indexer", !indexerThread.failed);
     assertTrue("hit unexpected exception in indexer2", !indexerThread2.failed);
---------------
-------------
@@ -142,7 +142,7 @@
     assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("lazy")));
     assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("dog")));
     assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("the")));
-    DocsEnum de = te.docs(null, null);
+DocsEnum de = te.docs(null, null, true);
     assertTrue(de.advance(0) != DocIdSetIterator.NO_MORE_DOCS);
     assertEquals(2, de.freq());
     assertTrue(de.advance(1) != DocIdSetIterator.NO_MORE_DOCS);
---------------
-------------
@@ -27,7 +27,7 @@
 import org.apache.derby.iapi.error.StandardException;
 import org.apache.derby.iapi.reference.SQLState;
 import org.apache.derby.iapi.services.i18n.MessageService;
-import org.apache.derby.shared.common.error.ExceptionUtil;
+import org.apache.derby.iapi.error.ExceptionUtil;
 
 /**
  * This is an output stream built on top of LOBStreamControl.
---------------
-------------
@@ -70,7 +70,7 @@
                                       numElements, bucketsPerElement, targetBucketsPerElem));
         }
         BloomCalculations.BloomSpecification spec = BloomCalculations.computeBloomSpec(bucketsPerElement);
-        logger.debug("Creating bloom filter for {} elements and spec {}", numElements, spec);
+logger.trace("Creating bloom filter for {} elements and spec {}", numElements, spec);
         return new BloomFilter(spec.K, bucketsFor(numElements, spec.bucketsPerElement));
     }
 
---------------
-------------
@@ -239,7 +239,7 @@
             "Could not find config name for collection:" + collection);
       }
       solrLoader = new ZkSolrResourceLoader(instanceDir, zkConfigName,
-          loader.getClassLoader(), dcore.getCoreProperties(), zkController);
+loader.getClassLoader(), dcore.getSubstitutableProperties(), zkController);
       config = getSolrConfigFromZk(zkConfigName, dcore.getConfigName(),
           solrLoader);
       schema = IndexSchemaFactory.buildIndexSchema(dcore.getSchemaName(),
---------------
-------------
@@ -51,6 +51,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new ArmenianAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new ArmenianAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -96,7 +96,7 @@
 
     public static String getColumnFamily(Tree astNode, List<CfDef> cfDefs)
     {
-        return getColumnFamily(astNode.getChild(0).getText(), cfDefs);
+return getColumnFamily(CliUtils.unescapeSQLString(astNode.getChild(0).getText()), cfDefs);
     }
 
     public static String getColumnFamily(String cfName, List<CfDef> cfDefs)
---------------
-------------
@@ -559,7 +559,7 @@
     
     @Override
     public int docID() {
-      return docIt >= upto ? NO_MORE_DOCS : docs[docIt];
+return docIt < 0 ? -1 : docIt >= upto ? NO_MORE_DOCS : docs[docIt];
     }
     
     @Override
---------------
-------------
@@ -227,7 +227,7 @@
         {
             throw new InvalidRequestException("get_column requires non-empty columnfamily");
         }
-        if (DatabaseDescriptor.getColumnFamilyType(null, values[0]).equals("Standard"))
+if (DatabaseDescriptor.getColumnFamilyType(tablename, values[0]).equals("Standard"))
         {
             if (values.length != 2)
             {
---------------
-------------
@@ -51,7 +51,7 @@
   }
 
   public UAX29URLEmailTokenizer create(Reader input) {
-    UAX29URLEmailTokenizer tokenizer = new UAX29URLEmailTokenizer(input); 
+UAX29URLEmailTokenizer tokenizer = new UAX29URLEmailTokenizer(luceneMatchVersion, input);
     tokenizer.setMaxTokenLength(maxTokenLength);
     return tokenizer;
   }
---------------
-------------
@@ -509,7 +509,7 @@
 
 		long shutdownTime = System.currentTimeMillis();
 		boolean logBootTrace = PropertyUtil.getSystemBoolean(Property.LOG_BOOT_TRACE);
-		istream.println(LINE);
+logMsg(LINE);
 		logMsg("\n" + new Date() +
                 MessageService.getTextMessage(
                     MessageId.STORE_SHUTDOWN_MSG,
---------------
-------------
@@ -41,7 +41,7 @@
   private String name;
   private int depth = 0;
   private int maxDepthLogStart = 0;
-  private String params = null;
+protected String params = null;
   
   protected static final String NEW_LINE = System.getProperty("line.separator");
 
---------------
-------------
@@ -52,7 +52,7 @@
       bufferedCh = -1;
       currentOffset++;
       
-      addOffCorrectMap(currentOffset+delta, delta-1);
+addOffCorrectMap(currentOffset, delta-1);
       delta--;
       return ch;
     }
---------------
-------------
@@ -26,7 +26,7 @@
 
   @Override
   protected double doComputeResult(int rowA, int rowB, Iterable<Cooccurrence> cooccurrences, double weightOfVectorA,
-      double weightOfVectorB, int numberOfColumns) {
+double weightOfVectorB, long numberOfColumns) {
 
     int count = 0;
     double sumX = 0.0;
---------------
-------------
@@ -38,7 +38,7 @@
     protected UpdateColumnFamily() { }
     
     /** assumes validation has already happened. That is, replacing oldCfm with newCfm is neither illegal or totally whackass. */
-    public UpdateColumnFamily(org.apache.cassandra.avro.CfDef cf_def) throws ConfigurationException, IOException
+public UpdateColumnFamily(org.apache.cassandra.db.migration.avro.CfDef cf_def) throws ConfigurationException, IOException
     {
         super(UUIDGen.makeType1UUIDFromHost(FBUtilities.getLocalAddress()), DatabaseDescriptor.getDefsVersion());
         
---------------
-------------
@@ -114,7 +114,7 @@
         try
         {
             Message message = rowMutationMessage.makeRowMutationMessage(StorageService.readRepairVerbHandler_);
-    		String key = target + ":" + message.getMessageId();
+String key = target.getHostAddress() + ":" + message.getMessageId();
     		readRepairTable_.put(key, message);
         }
         catch ( IOException ex )
---------------
-------------
@@ -214,7 +214,7 @@
       return binaryValue;
     }
     
-    public final boolean incrementToken() throws IOException {
+public final boolean incrementToken() {
       // lazy init the iterator
       if (it == null) {
         it = cachedStates.iterator();
---------------
-------------
@@ -479,7 +479,7 @@
       if (segmentInfos.size() > 0 || newSegment != null) {
         final FrozenBufferedDeletes packet = new FrozenBufferedDeletes(pendingDeletes, delGen);
         if (infoStream != null) {
-          message("flush: push buffered deletes");
+message("flush: push buffered deletes startSize=" + pendingDeletes.bytesUsed.get() + " frozenSize=" + packet.bytesUsed);
         }
         bufferedDeletesStream.push(packet);
         if (infoStream != null) {
---------------
-------------
@@ -186,7 +186,7 @@
       File file = new File(workDir, fileName);
       Document doc = new Document();
       InputStreamReader is = new InputStreamReader(new FileInputStream(file), "UTF-8");
-      doc.add(new TextField("contents", is, Field.Store.NO));
+doc.add(new TextField("contents", is));
       writer.addDocument(doc);
       writer.commit();
       is.close();
---------------
-------------
@@ -256,7 +256,7 @@
 
         dir.allIndexInputs.clear();
 
-        IndexReader reader = IndexReader.open(dir);
+IndexReader reader = IndexReader.open(dir, false);
         Term aaa = new Term("content", "aaa");
         Term bbb = new Term("content", "bbb");
         Term ccc = new Term("content", "ccc");
---------------
-------------
@@ -100,7 +100,7 @@
     }
 
     writer.forceMerge(1);
-    writer.close();
+writer.shutdown();
   }
 
 
---------------
-------------
@@ -10,7 +10,7 @@
     public void testMissingSubcolumn() {
     	byte[] val = "sample value".getBytes();
     	SuperColumn sc = new SuperColumn("sc1");
-    	sc.addColumn("col1", new Column("col1",val,1L));
+sc.addColumn(new Column("col1",val,1L));
     	assertNotNull(sc.getSubColumn("col1"));
     	assertNull(sc.getSubColumn("col2"));
     }
---------------
-------------
@@ -188,7 +188,7 @@
           boolean continueRegen = regenerator.regenerateItem(searcher, this, old, keys[i], vals[i]);
           if (!continueRegen) break;
         }
-        catch (Throwable e) {
+catch (Exception e) {
           SolrException.log(log,"Error during auto-warming of key:" + keys[i], e);
         }
       }
---------------
-------------
@@ -148,7 +148,7 @@
       }
 
       assertTrue(last.compareTo(term) < 0);
-      last.copy(term);
+last.copyBytes(term);
 
       final String s = term.utf8ToString();
       assertTrue("term " + termDesc(s) + " was not added to index (count=" + allTerms.size() + ")", allTerms.contains(s));
---------------
-------------
@@ -37,7 +37,7 @@
       Directory dir = newDirectory();
       IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random())));
       writer.commit();
-      writer.close();
+writer.shutdown();
       DirectoryReader open = DirectoryReader.open(dir);
       final boolean throwOnClose = !rarely();
       AtomicReader wrap = SlowCompositeReaderWrapper.wrap(open);
---------------
-------------
@@ -46,7 +46,7 @@
   
   private HTable table;
   
-  private ThreadLocal<HBaseConfiguration> hBconf;
+private ThreadLocal<HBaseConfiguration> hBconf = new ThreadLocal<HBaseConfiguration>();
   
   private boolean useHbase;
   
---------------
-------------
@@ -197,7 +197,7 @@
     }
     
     @Override
-    public int getUniqueFieldCount() throws IOException {
+public int size() throws IOException {
       return fields.size();
     }
   }
---------------
-------------
@@ -35,7 +35,7 @@
   @Test
   public void testRollingUpdates() throws Exception {
     Random random = new Random(random().nextLong());
-    final MockDirectoryWrapper dir = newDirectory();
+final BaseDirectoryWrapper dir = newDirectory();
     dir.setCheckIndexOnClose(false); // we use a custom codec provider
     final LineFileDocs docs = new LineFileDocs(random, true);
 
---------------
-------------
@@ -322,7 +322,7 @@
      * @param onDiskType The object read that represents the type.
      * @return A type descriptor.
      */
-    private static TypeDescriptor getStoredType(Object onDiskType)
+public static TypeDescriptor getStoredType(Object onDiskType)
     {
         if (onDiskType instanceof OldRoutineType)
             return ((OldRoutineType) onDiskType).getCatalogType();
---------------
-------------
@@ -73,7 +73,7 @@
  */
 public class ColumnFamilyInputFormat extends InputFormat<ByteBuffer, SortedMap<ByteBuffer, IColumn>>
 {
-    private static final Logger logger = LoggerFactory.getLogger(StorageService.class);
+private static final Logger logger = LoggerFactory.getLogger(ColumnFamilyInputFormat.class);
     
     private String keyspace;
     private String cfName;
---------------
-------------
@@ -20,7 +20,7 @@
  * Exception thrown when there is a cardinality mismatch in matrix operations
  * 
  */
-public class CardinalityException extends Exception {
+public class CardinalityException extends RuntimeException {
 
   private static final long serialVersionUID = 1L;
 
---------------
-------------
@@ -351,7 +351,7 @@
       }
       result.append(userIDs[i]);
     }
-    if (result.length() > 3) {
+if (userIDs.length > 3) {
       result.append("...");
     }
     result.append(']');
---------------
-------------
@@ -244,7 +244,7 @@
 
       @Override
       public int nextDoc() throws IOException {
-        if (pos < valueCount) {
+if (pos >= valueCount) {
           return pos = NO_MORE_DOCS;
         }
         return advance(pos + 1);
---------------
-------------
@@ -169,7 +169,7 @@
     public long totalBytesSize() throws IOException {
       long total = 0;
       for (SegmentInfo info : segments) {
-        total += info.sizeInBytes();
+total += info.sizeInBytes(true);
       }
       return total;
     }
---------------
-------------
@@ -91,7 +91,7 @@
 
         for (int nRows = minRow; nRows < maxRow; nRows++)
         {
-            ByteBuffer row = ByteBuffer.wrap((rowPrefix + nRows).getBytes());
+ByteBuffer row = ByteBufferUtil.bytes((rowPrefix + nRows));
             ColumnPath col = new ColumnPath("Standard1").setSuper_column((ByteBuffer)null).setColumn("col1".getBytes());
             ColumnParent parent = new ColumnParent("Standard1").setSuper_column((ByteBuffer)null);
 
---------------
-------------
@@ -122,7 +122,7 @@
     }
   
     @Override
-    public void setNextEnum(TermsEnum termsEnum) throws IOException {
+public void setNextEnum(TermsEnum termsEnum) {
       this.termsEnum = termsEnum;
     }
       
---------------
-------------
@@ -173,7 +173,7 @@
 		for (int i = 0; i < hits.length(); i++)
 		{
     		String text = hits.doc(i).get(FIELD_NAME);
-    		highlighter.getBestFragment(analyzer, text);
+highlighter.getBestFragment(analyzer, FIELD_NAME,text);
 		}
 		assertTrue("Failed to find correct number of highlights " + numHighlights + " found", numHighlights == 4);
 
---------------
-------------
@@ -30,7 +30,7 @@
 
 public class OutputDriver {
 
-  public static void main(String[] args) throws Exception {
+public static void main(String[] args) throws IOException {
     runJob(args[0], args[1]);
   }
 
---------------
-------------
@@ -52,7 +52,7 @@
       doc.add(new TextField("repetitiveField", repetitiveFieldValue, Field.Store.YES));
       writer.addDocument(doc);
     }
-    writer.close();
+writer.shutdown();
     reader = DirectoryReader.open(dir);
   }
 
---------------
-------------
@@ -45,7 +45,7 @@
 import org.apache.derby.impl.sql.catalog.XPLAINStatementTimingsDescriptor;
 import org.apache.derby.iapi.sql.execute.RunTimeStatistics;
 import org.apache.derby.iapi.sql.execute.xplain.XPLAINVisitor;
-import org.apache.derby.impl.sql.execute.rts.ResultSetStatistics;
+import org.apache.derby.iapi.sql.execute.ResultSetStatistics;
 
 /**
  * This is the Visitor, which explains the information and stores the statistics in 
---------------
-------------
@@ -486,7 +486,7 @@
 
             if (readPositions) {
               final int code = prox.readVInt();
-              position += code >> 1;
+position += code >>> 1;
 
               if ((code & 1) != 0) {
 
---------------
-------------
@@ -227,7 +227,7 @@
     }
     doc.add(newTextField(rnd, "different_field", content, Field.Store.YES));
     writer.addDocument(doc);
-    writer.close();
+writer.shutdown();
     
   }
   
---------------
-------------
@@ -102,7 +102,7 @@
         return new TokenStreamComponents(tokenizer, new ReverseStringFilter(TEST_VERSION_CURRENT, tokenizer));
       }
     };
-    checkRandomData(random(), a, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -494,7 +494,7 @@
                 StreamOutSession outsession = StreamOutSession.create(request.cf.left, request.endpoint, callback);
                 StreamOut.transferSSTables(outsession, sstables, differences, OperationType.AES);
                 // request ranges from the remote node
-                StreamIn.requestRanges(request.endpoint, request.cf.left, Collections.singletonList(cfstore), differences, callback, OperationType.AES);
+StreamIn.requestRanges(request.endpoint, request.cf.left, differences, callback, OperationType.AES);
             }
             catch(Exception e)
             {
---------------
-------------
@@ -29,7 +29,7 @@
 
 public class InputDriver {
 
-  public static void main(String[] args) throws Exception {
+public static void main(String[] args) throws IOException {
     runJob(args[0], args[1]);
   }
 
---------------
-------------
@@ -73,7 +73,7 @@
 
     Path outPath = new Path(output);
     client.setConf(conf);
-    FileSystem dfs = FileSystem.get(conf);
+FileSystem dfs = FileSystem.get(outPath.toUri(), conf);
     if (dfs.exists(outPath))
       dfs.delete(outPath, true);
     InputDriver.runJob(input, output + "/data");
---------------
-------------
@@ -420,7 +420,7 @@
       }
       };
 
-    Scorer spanScorer = snq.weight(searcher).scorer(searcher.getIndexReader(), true, false);
+Scorer spanScorer = searcher.createNormalizedWeight(snq).scorer(searcher.getIndexReader(), true, false);
 
     assertTrue("first doc", spanScorer.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
     assertEquals("first doc number", spanScorer.docID(), 11);
---------------
-------------
@@ -79,7 +79,7 @@
     try {
       log.info("Deleting document: " + id);
       DeleteUpdateCommand delCmd = new DeleteUpdateCommand(req);
-      delCmd.id = id.toString();
+delCmd.setId(id.toString());
       processor.processDelete(delCmd);
     } catch (IOException e) {
       log.error("Exception while deleteing: " + id, e);
---------------
-------------
@@ -140,7 +140,7 @@
     final Analyzer a = new JapaneseAnalyzer(TEST_VERSION_CURRENT, null, Mode.SEARCH,
         JapaneseAnalyzer.getDefaultStopSet(),
         JapaneseAnalyzer.getDefaultStopTags());
-    checkRandomData(random, a, 200*RANDOM_MULTIPLIER, 8192);
+checkRandomData(random, a, 100*RANDOM_MULTIPLIER, 8192);
   }
 
   // Copied from TestJapaneseTokenizer, to make sure passing
---------------
-------------
@@ -54,7 +54,7 @@
    * 
    * @param environment
    */
-  public ServiceRegistryContext(@SuppressWarnings("unused") Hashtable<?, ?> environment)
+public ServiceRegistryContext(Hashtable<?, ?> environment)
   {
     env = new HashMap<Object, Object>();
     env.putAll(environment);
---------------
-------------
@@ -17,7 +17,7 @@
 
 package org.apache.mahout.classifier.sgd;
 
-import org.apache.commons.math.special.Gamma;
+import org.apache.commons.math3.special.Gamma;
 
 import java.io.DataInput;
 import java.io.DataOutput;
---------------
-------------
@@ -20,7 +20,7 @@
 
 import org.apache.cassandra.CleanupHelper;
 import org.apache.cassandra.config.CFMetaData;
-import static org.apache.cassandra.config.DatabaseDescriptor.ConfigurationException;
+import org.apache.cassandra.config.ConfigurationException;
 import org.apache.cassandra.config.DatabaseDescriptor;
 import org.apache.cassandra.config.KSMetaData;
 import org.apache.cassandra.db.commitlog.CommitLog;
---------------
-------------
@@ -119,7 +119,7 @@
         put(Verb.RANGE_SLICE, Stage.READ);
         put(Verb.BOOTSTRAP_TOKEN, Stage.MISC);
         put(Verb.TREE_REQUEST, Stage.AE_SERVICE);
-        put(Verb.TREE_RESPONSE, Stage.RESPONSE);
+put(Verb.TREE_RESPONSE, Stage.AE_SERVICE);
         put(Verb.GOSSIP_DIGEST_ACK, Stage.GOSSIP);
         put(Verb.GOSSIP_DIGEST_ACK2, Stage.GOSSIP);
         put(Verb.GOSSIP_DIGEST_SYN, Stage.GOSSIP);
---------------
-------------
@@ -72,7 +72,7 @@
         return new TokenStreamComponents(tokenizer, new HunspellStemFilter(tokenizer, DICTIONARY));
       }  
     };
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -48,7 +48,7 @@
     }
     w.close();
 
-    IndexReader reader = IndexReader.open(rd);
+IndexReader reader = IndexReader.open(rd, true);
     IndexSearcher searcher = new IndexSearcher(reader);
     int numDocs = reader.numDocs();
     ScoreDoc[] results;
---------------
-------------
@@ -142,7 +142,7 @@
     // unnecessary vectors later
     int overshoot = (int) ((double) clusters * OVERSHOOT_MULTIPLIER);
     DistributedLanczosSolver solver = new DistributedLanczosSolver();
-    LanczosState state = new LanczosState(L, clusters, DistributedLanczosSolver.getInitialVector(L));
+LanczosState state = new LanczosState(L, overshoot, DistributedLanczosSolver.getInitialVector(L));
     Path lanczosSeqFiles = new Path(outputCalc, "eigenvectors-" + (System.nanoTime() & 0xFF));
     solver.runJob(conf,
                   state,
---------------
-------------
@@ -70,7 +70,7 @@
   @Override
   protected void setup(Context context) throws IOException, InterruptedException {
     super.setup(context);
-    Parameters params = Parameters.fromString(context.getConfiguration().get(PFPGrowth.PFP_PARAMETERS, ""));
+Parameters params = new Parameters(context.getConfiguration().get(PFPGrowth.PFP_PARAMETERS, ""));
     
     int i = 0;
     for (Pair<String,Long> e : PFPGrowth.deserializeList(params, PFPGrowth.F_LIST, context.getConfiguration())) {
---------------
-------------
@@ -192,7 +192,7 @@
         //assertEquals(pb.getProcessedBy(Phase.AFTER_DESTROY).get(0),bp);
         
         
-        Object objOther = beanContainer.getComponentInstance("PlaceHolerTestBean");
+Object objOther = beanContainer.getComponentInstance("PlaceHolderTestBean");
         assertTrue(objOther instanceof OtherBean);
         assertEquals("test1value", ((OtherBean)objOther).getTestValue());
     }
---------------
-------------
@@ -84,7 +84,7 @@
         {
             long timestampOfLastDelete = dis.readLong();
             ByteBuffer pc = FBUtilities.readShortByteArray(dis);
-            byte[] partitionedCounter = Arrays.copyOfRange(pc.array(), pc.position() + pc.arrayOffset(), pc.limit());
+byte[] partitionedCounter = ByteBufferUtil.getArray(pc);
             long timestamp = dis.readLong();
             ByteBuffer value = FBUtilities.readByteArray(dis);
             return new CounterColumn(name, value, timestamp, partitionedCounter, timestampOfLastDelete);
---------------
-------------
@@ -497,7 +497,7 @@
       
       SolrCore core = coreContainer.create(dcore);
       
-      coreContainer.register(dcore.getName(), core, false);
+coreContainer.register(core, false);
       
       if (coreContainer.getCoresLocator() instanceof SolrXMLCoresLocator) {
         // hack - in this case we persist once more because a core create race might
---------------
-------------
@@ -64,7 +64,7 @@
       ts.reset();
       while (ts.incrementToken()) {
         termAtt.fillBytesRef();
-        term = new Term(fieldName, new BytesRef(bytes));
+term = new Term(fieldName, BytesRef.deepCopyOf(bytes));
         tf.addTerm(term);
       }
       ts.end();
---------------
-------------
@@ -787,7 +787,7 @@
           SchemaField schemaField = searcher.getSchema().getField(groupBy);
           FieldType fieldType = schemaField.getType();
           String readableValue = fieldType.indexedToReadable(group.groupValue.utf8ToString());
-          IndexableField field = schemaField.createField(readableValue, 0.0f);
+IndexableField field = schemaField.createField(readableValue, 1.0f);
           nl.add("groupValue", fieldType.toObject(field));
         } else {
           nl.add("groupValue", null);
---------------
-------------
@@ -293,7 +293,7 @@
      */
     public IValidator getValidator(String table, String cf, InetAddress initiator, boolean major)
     {
-        if (!major || table.equals(Table.SYSTEM_TABLE))
+if (!major || table.equals(Table.SYSTEM_TABLE) || table.equals(Table.DEFINITIONS))
             return new NoopValidator();
         if (StorageService.instance.getTokenMetadata().sortedTokens().size()  < 1)
             // gossiper isn't started
---------------
-------------
@@ -48,7 +48,7 @@
       w.addDocument(doc);
       w.forceMerge(1);
       r = w.getReader();
-      w.close();
+w.shutdown();
 
       subR = r.leaves().get(0).reader();
     }
---------------
-------------
@@ -137,7 +137,7 @@
     private SystemTable.StorageMetadata storageMetadata_;
 
     /* This thread pool does consistency checks when the client doesn't care about consistency */
-    private ExecutorService consistencyManager_ = new DebuggableThreadPoolExecutor(DatabaseDescriptor.getConsistencyThreads(),
+private ExecutorService consistencyManager_ = new JMXEnabledThreadPoolExecutor(DatabaseDescriptor.getConsistencyThreads(),
                                                                                    DatabaseDescriptor.getConsistencyThreads(),
                                                                                    Integer.MAX_VALUE,
                                                                                    TimeUnit.SECONDS,
---------------
-------------
@@ -199,7 +199,7 @@
       // we expect full throttle fails, but cloud client should not easily fail
       for (StopableThread indexThread : threads) {
         if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {
-          assertFalse("There were too many update fails - we expect it can happen, but shouldn't easily", ((StopableIndexingThread) indexThread).getFails() > 1);
+assertFalse("There were too many update fails - we expect it can happen, but shouldn't easily", ((StopableIndexingThread) indexThread).getFails() > 10);
         }
       }
       
---------------
-------------
@@ -1236,7 +1236,7 @@
 		String entityName = (schemaName == null ? tableName : schemaName + "." + tableName); 
 		String binsertSql = 
 			"insert into " + entityName +
-			" PROPERTIES insertMode=bulkInsert " +
+" --DERBY-PROPERTIES insertMode=bulkInsert \n" +
 			"select * from new " + vtiName + 
 			"(" + 
 			"'" + schemaName + "'" + ", " + 
---------------
-------------
@@ -84,7 +84,7 @@
 
 		}catch(Exception e)
 		{
-			throw LoadError.unexpectedError(e);
+throw importError(e);
 		}
 	}
 
---------------
-------------
@@ -84,7 +84,7 @@
         d.add(new IntField(ID_FIELD, j, Field.Store.YES));
         writer.addDocument(d);
       }
-      writer.close();
+writer.shutdown();
 
       // try a search without OR
       IndexReader reader = DirectoryReader.open(directory);
---------------
-------------
@@ -106,7 +106,7 @@
             
             if (previousCount == null)
                 previousCount = 0L;           
-            if (count == previousCount)
+if (count.equals(previousCount))
                 continue;
             
             gccounts.put(gc.getName(), count);
---------------
-------------
@@ -131,6 +131,6 @@
 
   /** blast some random strings through the analyzer */
   public void testRandom() throws Exception {
-    checkRandomData(random(), getTestAnalyzer(), 10000 * RANDOM_MULTIPLIER); 
+checkRandomData(random(), getTestAnalyzer(), 1000 * RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -957,7 +957,7 @@
               int docId = 12;
               for(int i=0;i<13;i++) {
                 reader.deleteDocument(docId);
-                reader.setNorm(docId, "contents", (float) 2.0);
+reader.setNorm(docId, "content", (float) 2.0);
                 docId += 12;
               }
             }
---------------
-------------
@@ -61,7 +61,7 @@
     {
         // Response is been managed by the map so make it 1 for the superclass.
         super(writeEndpoints, hintedEndpoints, consistencyLevel);
-        assert consistencyLevel == ConsistencyLevel.LOCAL_QUORUM;
+assert consistencyLevel == ConsistencyLevel.EACH_QUORUM;
 
         strategy = (NetworkTopologyStrategy) Table.open(table).getReplicationStrategy();
 
---------------
-------------
@@ -416,7 +416,7 @@
     
     @Override
     public void setNextReader(AtomicReaderContext context) {
-      this.docBase = docBase;
+docBase = context.docBase;
     }
     @Override
     public boolean acceptsDocsOutOfOrder() {
---------------
-------------
@@ -225,7 +225,7 @@
     if (clauses.size() == 1) {                    // optimize 1-clause queries
       BooleanClause c = (BooleanClause)clauses.elementAt(0);
       if (!c.prohibited) {			  // just return clause
-        Query query = c.query;
+Query query = c.query.rewrite(reader);    // rewrite first
         if (getBoost() != 1.0f) {                 // have to clone to boost
           query = (Query)query.clone();
           query.setBoost(getBoost() * query.getBoost());
---------------
-------------
@@ -807,7 +807,7 @@
 	 *
 	 * @see CompilerContext#addRequiredSchemaPriv
 	 */
-	public void addRequiredSchemaPriv(String schemaName, String aid, boolean privType)
+public void addRequiredSchemaPriv(String schemaName, String aid, int privType)
 	{
 		if( requiredSchemaPrivileges == null || schemaName == null)
 			return;
---------------
-------------
@@ -85,7 +85,7 @@
 
 			if (firstTime) {
 				Logs.reportString("----------------------------------------------");
-				Logs.reportMessage("CSLOOK_Header", "tables");
+Logs.reportMessage("CSLOOK_TablesHeader");
 				Logs.reportString("----------------------------------------------\n");
 			}
 
---------------
-------------
@@ -549,7 +549,7 @@
         T rval = null;
         try
         {
-            rval = (T) cls.getDeclaredMethod("getInstance").invoke(null, (Object) null);
+rval = (T) cls.getDeclaredMethod("getInstance").invoke(new Object[] {null, null});
 
         }
         catch (NoSuchMethodException e)
---------------
-------------
@@ -50,7 +50,7 @@
       mergeState.fieldInfo = fieldInfo; // set the field we are merging
       if (canMerge(fieldInfo)) {
         for (int i = 0; i < docValues.length; i++) {
-          docValues[i] = getDocValuesForMerge(mergeState.readers.get(i).reader, fieldInfo);
+docValues[i] = getDocValuesForMerge(mergeState.readers.get(i), fieldInfo);
         }
         Type docValuesType = getDocValuesType(fieldInfo);
         assert docValuesType != null;
---------------
-------------
@@ -309,7 +309,7 @@
         return segments.getLast();
     }
     
-    public CommitLogSegment.CommitLogContext getContext() throws IOException
+public CommitLogSegment.CommitLogContext getContext()
     {
         Callable<CommitLogSegment.CommitLogContext> task = new Callable<CommitLogSegment.CommitLogContext>()
         {
---------------
-------------
@@ -50,7 +50,7 @@
 		Obtain a Container shared or exclusive lock	until
 		the end of the nested transaction.
 
-		@exception StandardException Standard Cloudscape error policy
+@exception StandardException Standard Derby error policy
 	*/
 	public boolean lockContainer(
     Transaction     t, 
---------------
-------------
@@ -82,7 +82,7 @@
     assertResults(((StringWriter) w).getBuffer().toString().getBytes("UTF-8"));
     
     // again with default file
-    File tmpFile = _TestUtil.getTempDir("solr.xml");
+File tmpFile = _TestUtil.createTempFile("solr.xml", null, TEMP_DIR);
     
     serializer.persistFile(tmpFile, solrXMLDef);
 
---------------
-------------
@@ -422,7 +422,7 @@
 		if (! resultColumnList.columnTypesAndLengthsMatch(
 												resultSet.getResultColumns()))
 		{
-			resultSet = resultSet.genNormalizeResultSetNode(resultSet, false);
+resultSet = resultSet.genNormalizeResultSetNode(false);
 			resultColumnList.copyTypesAndLengthsToSource(resultSet.getResultColumns());
 		}
 
---------------
-------------
@@ -33,7 +33,7 @@
   String PRODUCE_SUMMARY = CARROT_PREFIX + "produceSummary";
   String NUM_DESCRIPTIONS = CARROT_PREFIX + "numDescriptions";
   String OUTPUT_SUB_CLUSTERS = CARROT_PREFIX + "outputSubClusters";
-  String SUMMARY_FRAGSIZE = CARROT_PREFIX + "fragzise";
+String SUMMARY_FRAGSIZE = CARROT_PREFIX + "fragSize";
 
   String LEXICAL_RESOURCES_DIR = CARROT_PREFIX + "lexicalResourcesDir";
 
---------------
-------------
@@ -22,7 +22,7 @@
 package org.apache.derby.catalog;
 
 /**
- <p>An interface for describing a default for a column or parameter in Cloudscape systems.</p>
+<p>An interface for describing a default for a column or parameter in Derby systems.</p>
  */
 public interface DefaultInfo
 {
---------------
-------------
@@ -141,7 +141,7 @@
         array.reset(term.buffer(), 0, term.length());
       }
 
-      public void reset(Reader input) throws IOException {
+public void reset(Reader input) {
         try {
           sentenceTokenizer.reset(input);
           wordTokenFilter = (TokenStream) tokenFilterClass.getConstructor(
---------------
-------------
@@ -975,7 +975,7 @@
     private void sendReplicationNotification(InetAddress local, InetAddress remote)
     {
         // notify the remote token
-        Message msg = new Message(local, StorageService.Verb.REPLICATION_FINISHED, new byte[0]);
+Message msg = new Message(local, StorageService.Verb.REPLICATION_FINISHED, new byte[0], Gossiper.instance.getVersion(remote));
         IFailureDetector failureDetector = FailureDetector.instance;
         while (failureDetector.isAlive(remote))
         {
---------------
-------------
@@ -617,7 +617,7 @@
               break;
             }
           }
-          TokenStream ts = analyzer.tokenStream("ignore", new StringReader(term));
+TokenStream ts = analyzer.tokenStream("ignore", term);
           CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);
           ts.reset();
           while(ts.incrementToken()) {
---------------
-------------
@@ -505,7 +505,7 @@
     subDocUpto = 0;
     docBase = readerContext.docBase;
     //System.out.println("setNextReader base=" + docBase + " r=" + readerContext.reader);
-    lastDocPerGroupBits = lastDocPerGroup.getDocIdSet(readerContext).iterator();
+lastDocPerGroupBits = lastDocPerGroup.getDocIdSet(readerContext, readerContext.reader.getLiveDocs()).iterator();
     groupEndDocID = -1;
 
     currentReaderContext = readerContext;
---------------
-------------
@@ -67,7 +67,7 @@
     writer.optimize();
     writer.close();
 
-    Searcher searcher = new IndexSearcher(store);
+Searcher searcher = new IndexSearcher(store, true);
     searcher.setSimilarity(new SimpleSimilarity());
 
     Term a = new Term("field", "a");
---------------
-------------
@@ -405,7 +405,7 @@
 
     TokenStream source;
     try {
-      source = analyzer.tokenStream(field, new StringReader(queryText));
+source = analyzer.tokenStream(field, queryText);
       source.reset();
     } catch (IOException e) {
       throw new SyntaxError("Unable to initialize TokenStream to analyze query text", e);
---------------
-------------
@@ -262,7 +262,7 @@
                   DataImportHandlerException.SEVERE,
                   "<function> must have a 'name' and 'class' attributes");
         try {
-          evaluators.put(func, (Evaluator) DocBuilder.loadClass(clz)
+evaluators.put(func, (Evaluator) DocBuilder.loadClass(clz, null)
                   .newInstance());
         } catch (Exception exp) {
           throw new DataImportHandlerException(
---------------
-------------
@@ -153,7 +153,7 @@
 		
         public DataRepairHandler() throws IOException
         {
-            readResponseResolver_ = new ReadResponseResolver(readCommand_.table, replicas_.size());
+readResponseResolver_ = new ReadResponseResolver(readCommand_.table, readCommand_.key, replicas_.size());
             majority_ = (replicas_.size() / 2) + 1;
             // wrap original data Row in a response Message so it doesn't need to be special-cased in the resolver
             ReadResponse readResponse = new ReadResponse(row_);
---------------
-------------
@@ -52,7 +52,7 @@
       for (int m=0, c=random.nextInt(10); m<=c; m++) {
         int value = random.nextInt(Integer.MAX_VALUE);
         doc.add(newField("asc", format.format(value), StringField.TYPE_UNSTORED));
-        doc.add(new NumericField("trie").setIntValue(value));
+doc.add(new NumericField("trie", value));
       }
       writer.addDocument(doc);
     }
---------------
-------------
@@ -1,6 +1,6 @@
 /*
 
-   Derby - Class org.apache.derbyTesting.functionTests.tests.tools.sysinfo_api
+Derby - Class org.apache.derbyTesting.functionTests.tests.tools.SysinfoAPITest
 
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -194,7 +194,7 @@
     // filter out all directories starting with . (e.g. .svn)
     Collection<File> sourceFiles = FileUtils.listFiles(sourceConfDir, TrueFileFilter.INSTANCE, new RegexFileFilter("[^\\.].*"));
     for (File sourceFile :sourceFiles){
-        int indexOfRelativePath = sourceFile.getAbsolutePath().lastIndexOf("collection1/conf");
+int indexOfRelativePath = sourceFile.getAbsolutePath().lastIndexOf("collection1" + File.separator + "conf");
         String relativePathofFile = sourceFile.getAbsolutePath().substring(indexOfRelativePath + 17, sourceFile.getAbsolutePath().length());
         File downloadedFile = new File(confDir,relativePathofFile);
         assertTrue(downloadedFile.getAbsolutePath() + " does not exist source:" + sourceFile.getAbsolutePath(), downloadedFile.exists());
---------------
-------------
@@ -1,6 +1,6 @@
 /*
 
-   Derby - Class com.ihost.cs.ReuseFactory
+Derby - Class org.apache.derby.iapi.util.ReuseFactory
 
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -81,7 +81,7 @@
         endOfSameIdChainData();
 
         NetXACallInfo callInfo = conn.xares_.callInfoArray_[conn.currXACallInfoOffset_];
-        if (synctype == NetXAResource.XARETVAL_XARDONLY) { // xaretval of read-only, make sure flag agrees
+if (synctype == XAResource.XA_RDONLY) { // xaretval of read-only, make sure flag agrees
             callInfo.setReadOnlyTransactionFlag(true);
         } else { // xaretval NOT read-only, make sure flag agrees
             callInfo.setReadOnlyTransactionFlag(false);
---------------
-------------
@@ -60,7 +60,7 @@
         assertEquals("expected doc Id " + j + " found " + sd[j].doc, j, sd[j].doc);
       }
     }
-    writer.close();
+writer.shutdown();
     reader.close();
     dir.close();
   }
---------------
-------------
@@ -50,7 +50,7 @@
     }
     writer.close();
 
-    IndexReader reader = IndexReader.open(dir);
+IndexReader reader = IndexReader.open(dir, true);
 
     SpanTermQuery query = new SpanTermQuery(new Term("field", English.intToEnglish(10).trim()));
     SpanQueryFilter filter = new SpanQueryFilter(query);
---------------
-------------
@@ -168,7 +168,7 @@
 
   /** @see #setFloorSegmentMB */
   public double getFloorSegmentMB() {
-    return floorSegmentBytes/1024*1024.;
+return floorSegmentBytes/(1024*1024.);
   }
 
   /** When forceMergeDeletes is called, we only merge away a
---------------
-------------
@@ -22,7 +22,7 @@
 import java.io.IOException;
 import java.util.zip.ZipFile;
 
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 
 public class ZipCloseableDirectory extends CloseableDirectory {
 	private final ZipFile zip;
---------------
-------------
@@ -793,7 +793,7 @@
 		try {
 
             nested_tc = 
-                tc.startNestedUserTransaction(false);
+tc.startNestedUserTransaction(false, true);
 
             switch (td.getTableType())
             {
---------------
-------------
@@ -240,7 +240,7 @@
         ByteArrayInputStream bufIn = new ByteArrayInputStream(body);
         try
         {
-            ReadResponse result = ReadResponse.serializer().deserialize(new DataInputStream(bufIn));
+ReadResponse result = ReadResponse.serializer().deserialize(new DataInputStream(bufIn), message.getVersion());
             if (logger_.isDebugEnabled())
                 logger_.debug("Preprocessed {} response", result.isDigestQuery() ? "digest" : "data");
             results.put(message, result);
---------------
-------------
@@ -30,7 +30,7 @@
   TERM_VECTOR_OFFSET('o', "Store Offset With TermVector"),
   TERM_VECTOR_POSITION('p', "Store Position With TermVector"),
   OMIT_NORMS('O', "Omit Norms"), 
-  OMIT_TF('F', "Omit Tf"), 
+OMIT_TF('F', "Omit Term Frequencies & Positions"),
   OMIT_POSITIONS('P', "Omit Positions"),
   LAZY('L', "Lazy"), 
   BINARY('B', "Binary"), 
---------------
-------------
@@ -146,7 +146,7 @@
                   Thread.currentThread().interrupt();
                   // we must have been asked to stop
                   throw new RuntimeException(e);
-                } catch (Throwable t) {
+} catch (Exception t) {
                   closeKeeper(keeper);
                   throw new RuntimeException(t);
                 }
---------------
-------------
@@ -965,7 +965,7 @@
   private long seed;
   
   private static final Random seedRand = new Random();
-  protected static final Random random = new Random();
+protected static final Random random = new Random(0);
 
   private String name = "<unknown>";
   
---------------
-------------
@@ -37,7 +37,7 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.cache.DoubleBarrelLRUCache;
+import org.apache.lucene.util.DoubleBarrelLRUCache;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CodecUtil;
 
---------------
-------------
@@ -196,7 +196,7 @@
     return res;
   }
 
-  protected boolean requiresDocScores() {
+public boolean requiresDocScores() {
     return getAggregator().requiresDocScores();
   }
 }
---------------
-------------
@@ -114,7 +114,7 @@
 
 				// requalify the current row
 				if (cursorRow == null) {
-				     throw StandardException.newException(SQLState.LANG_NO_CURRENT_ROW, cursorName);
+throw StandardException.newException(SQLState.NO_CURRENT_ROW);
 				}
 				// we know it will be requested, may as well get it now.
 				rowLocation = cursor.getRowLocation();
---------------
-------------
@@ -107,7 +107,7 @@
     addOption("filterFile", "f", "File containing comma-separated userID,itemID pairs. Used to exclude the item from "
         + "the recommendations for that user (optional)", null);
     addOption("booleanData", "b", "Treat input as without pref values", Boolean.FALSE.toString());
-    addOption("maxPrefsPerUser", "mp",
+addOption("maxPrefsPerUser", "mxp",
         "Maximum number of preferences considered per user in final recommendation phase",
         String.valueOf(UserVectorSplitterMapper.DEFAULT_MAX_PREFS_PER_USER_CONSIDERED));
     addOption("minPrefsPerUser", "mp", "ignore users with less preferences than this in the similarity computation "
---------------
-------------
@@ -32,7 +32,7 @@
 	to one that just contains the application's data.
 	Simply read and save the length information.
 */
-final class BinaryToRawStream
+class BinaryToRawStream
 extends java.io.FilterInputStream
 {
     /**
---------------
-------------
@@ -121,7 +121,7 @@
 
         writer.close();
 
-        r = IndexReader.open(index);
+r = IndexReader.open(index, true);
         s = new IndexSearcher(r);
         s.setSimilarity(sim);
     }
---------------
-------------
@@ -477,7 +477,7 @@
     }
 
     @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs /* ignored */) throws IOException {
+public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags /* ignored */) throws IOException {
       TVDocsEnum docsEnum;
       if (reuse != null && reuse instanceof TVDocsEnum) {
         docsEnum = (TVDocsEnum) reuse;
---------------
-------------
@@ -542,7 +542,7 @@
     
     ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(2);
 
-    for (int i = 0; i < 10; i++) {
+for (int i = 0; i < 100*_TestUtil.getRandomMultiplier(); i++) {
       writer.addDocument(createDocument(i, "test", 4));
     }
     ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).sync();
---------------
-------------
@@ -45,7 +45,7 @@
 
   @Override
   public String getDescription() {
-    return "GapFragmenter";
+return "HtmlFormatter";
   }
 
   @Override
---------------
-------------
@@ -500,7 +500,7 @@
       TermsEnum termsEnum = terms.iterator(null);
       DocsEnum docs = null;
       while(termsEnum.next() != null) {
-        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, true);
+docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(reader), docs, DocsEnum.FLAG_FREQS);
         while(docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
           totalTokenCount2 += docs.freq();
         }
---------------
-------------
@@ -80,7 +80,7 @@
         cf = ColumnFamily.serializer().deserialize(new DataInputStream(bufIn));
         for (String cName : map.navigableKeySet())
         {
-            ByteBuffer val = cf.getColumn(ByteBuffer.wrap(cName.getBytes())).value();
+ByteBuffer val = cf.getColumn(ByteBufferUtil.bytes(cName)).value();
             assert new String(val.array(),val.position(),val.remaining()).equals(map.get(cName));
         }
         assert cf.getColumnNames().size() == map.size();
---------------
-------------
@@ -67,7 +67,7 @@
   }
   
   public void testRandomData() throws IOException {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -94,7 +94,7 @@
 	 */
 	public QueryTreeNode bind() throws StandardException
 	{
-		privileges = (PrivilegeNode) privileges.bind( new HashMap());
+privileges = (PrivilegeNode) privileges.bind( new HashMap(), grantees);
 		return this;
 	} // end of bind
 
---------------
-------------
@@ -59,7 +59,7 @@
 import java.io.IOException;
 
 /** Implements the wildcard search query */
-final public class WildcardQuery extends MultiTermQuery {
+public class WildcardQuery extends MultiTermQuery {
     private Term wildcardTerm;
 
     public WildcardQuery(Term term) {
---------------
-------------
@@ -59,7 +59,7 @@
         if (ft != null) {
           ft.readableToIndexed(val, term);
         } else {
-          term.copy(val);
+term.copyChars(val);
         }
         return new TermQuery(new Term(fname, term));
       }
---------------
-------------
@@ -202,7 +202,7 @@
             ReadResponse result = results.get(message);
             if (result == null)
                 continue; // arrived concurrently
-            if (result.isDigestQuery())
+if (!result.isDigestQuery())
                 return true;
         }
         return false;
---------------
-------------
@@ -54,7 +54,7 @@
     rootDir = rootFile.getAbsolutePath();
     
     if (f.equals(rootFile)) name = "";
-    else name = file.getAbsolutePath().substring(rootDir.length() + 1);
+else name = file.getAbsolutePath().substring(rootDir.length() + 1).replace('\\', '/');
   }
   
   @Override
---------------
-------------
@@ -694,7 +694,7 @@
 
 		@param	action	The action causing the invalidation
 
-	 	@exception StandardException Standard Cloudscape error policy.
+@exception StandardException Standard Derby error policy.
 	 */
 	public void makeInvalid(int action, LanguageConnectionContext lcc)
 		 throws StandardException
---------------
-------------
@@ -36,7 +36,7 @@
     return new IndicNormalizationFilter(input);
   }
   
-  @Override
+//@Override
   public Object getMultiTermComponent() {
     return this;
   }
---------------
-------------
@@ -142,7 +142,7 @@
         true, false);
     // run ClusterDumper
     ClusterDumper clusterDumper = new ClusterDumper(new Path(output,
-        "clusters-" + maxIterations), new Path(output, "clusteredPoints"));
+"clusters-*-final"), new Path(output, "clusteredPoints"));
     clusterDumper.printClusters(null);
   }
   
---------------
-------------
@@ -267,7 +267,7 @@
 	*/
 	String DEADLOCK = "40001";
 	String LOCK_TIMEOUT = "40XL1";
-    String LOCK_TIMEOUT_LOG = "40XL2";
+String LOCK_TIMEOUT_LOG = "40XL1.T.1";
 
 	/*
 	** Store - access.protocol.Interface statement exceptions
---------------
-------------
@@ -85,7 +85,7 @@
     public static SSTableReader writeRawSSTable(String tablename, String cfname, Map<byte[], byte[]> entries) throws IOException
     {
         File datafile = tempSSTableFile(tablename, cfname);
-        SSTableWriter writer = new SSTableWriter(datafile.getAbsolutePath(), entries.size(), StorageService.getPartitioner());
+SSTableWriter writer = new SSTableWriter(datafile.getAbsolutePath(), entries.size());
         SortedMap<DecoratedKey, byte[]> sortedEntries = new TreeMap<DecoratedKey, byte[]>();
         for (Map.Entry<byte[], byte[]> entry : entries.entrySet())
             sortedEntries.put(writer.partitioner.decorateKey(entry.getKey()), entry.getValue());
---------------
-------------
@@ -150,7 +150,7 @@
       w.addDocument(getDocumentFromString(docFieldValue));
     }
     w.forceMerge(1);
-    w.close();
+w.shutdown();
     reader = DirectoryReader.open(dir);
     searcher = newSearcher(reader);
 
---------------
-------------
@@ -242,7 +242,7 @@
     private void nonDbaTest()
         throws Exception
     {
-        String          reservedToDBO = "2850A";
+String          reservedToDBO = "42504";
         Connection  conn = openUserConnection( NON_DBO_USER );
 
         assertTrue( "Initially, should be able to read property.", canReadProperty() );
---------------
-------------
@@ -184,7 +184,7 @@
   }
   
   protected long sizeBytes(SegmentInfo info) throws IOException {
-    long byteSize = info.sizeInBytes();
+long byteSize = info.sizeInBytes(true);
     if (calibrateSizeByDeletes) {
       int delCount = writer.get().numDeletedDocs(info);
       double delRatio = (info.docCount <= 0 ? 0.0f : ((float)delCount / (float)info.docCount));
---------------
-------------
@@ -67,7 +67,7 @@
 
     public BoostedWeight(IndexSearcher searcher) throws IOException {
       this.searcher = searcher;
-      this.qWeight = q.weight(searcher);
+this.qWeight = q.createWeight(searcher);
       this.fcontext = boostVal.newContext(searcher);
       boostVal.createWeight(fcontext,searcher);
     }
---------------
-------------
@@ -71,7 +71,7 @@
       case None:
         TermsCollector termsCollector = TermsCollector.create(fromField, multipleValuesPerDocument);
         fromSearcher.search(fromQuery, termsCollector);
-        return new TermsQuery(toField, termsCollector.getCollectorTerms());
+return new TermsQuery(toField, fromQuery, termsCollector.getCollectorTerms());
       case Total:
       case Max:
       case Avg:
---------------
-------------
@@ -58,7 +58,7 @@
       writer.addDocument(doc);
     }
     reader = writer.getReader();
-    writer.close();
+writer.shutdown();
     searcher = newSearcher(reader);
   }
 
---------------
-------------
@@ -542,7 +542,7 @@
     
     ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(2);
 
-    for (int i = 0; i < 10; i++) {
+for (int i = 0; i < 100*_TestUtil.getRandomMultiplier(); i++) {
       writer.addDocument(createDocument(i, "test", 4));
     }
     ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).sync();
---------------
-------------
@@ -42,7 +42,7 @@
   public static class SimpleSimilarity extends DefaultSimilarity {
     public float queryNorm(float sumOfSquaredWeights) { return 1.0f; }
     public float coord(int overlap, int maxOverlap) { return 1.0f; }
-    @Override public void computeNorm(FieldInvertState state, Norm norm) { norm.setByte(encodeNormValue(state.getBoost())); }
+@Override public float lengthNorm(FieldInvertState state) { return state.getBoost(); }
     @Override public float tf(float freq) { return freq; }
     @Override public float sloppyFreq(int distance) { return 2.0f; }
     @Override public float idf(long docFreq, long numDocs) { return 1.0f; }
---------------
-------------
@@ -51,6 +51,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new GalicianAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new GalicianAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -53,7 +53,7 @@
 				for (Object o : collationInfo) {
 					if (o instanceof String) {
 						collations.add(new Collation()
-								.setCollationQueryString((String) sugg.getVal(i)));
+.setCollationQueryString((String) o));
 					} else if (o instanceof NamedList) {
 						NamedList expandedCollation = (NamedList) o;
 						String collationQuery = (String) expandedCollation
---------------
-------------
@@ -447,7 +447,7 @@
 
     public double getMergeShardsChance()
     {
-        return readRepairChance;
+return mergeShardsChance;
     }
 
     public boolean getReplicateOnWrite()
---------------
-------------
@@ -71,7 +71,7 @@
   }
 
   @Override
-  public void release() {
+public void close() {
     synchronized(locks) {
       locks.remove(lockName);
     }
---------------
-------------
@@ -146,7 +146,7 @@
 	{
 		if (SanityManager.DEBUG)
 		{
-			return objectName.toString() + super.toString();
+return ((objectName==null)?"":objectName.toString()) + super.toString();
 		}
 		else
 		{
---------------
-------------
@@ -92,7 +92,7 @@
         		{
 	    			if (  subColumn.timestamp()  >=  timeLimit_ )
 	    			{
-			            filteredSuperColumn.addColumn(subColumn.name(), subColumn);
+filteredSuperColumn.addColumn(subColumn);
 	    				++i;
 	    			}
 	    			else
---------------
-------------
@@ -78,7 +78,7 @@
         activator.log(LogService.LOG_DEBUG, "Found bundles providing " + requestedClass + ": " + bundles);
                 
         Map<Pair<Integer, String>, String> args = new HashMap<Pair<Integer,String>, String>();
-        args.put(new Pair<Integer, String>(1, Class.class.getName()), clsArg.getName());
+args.put(new Pair<Integer, String>(0, Class.class.getName()), clsArg.getName());
         Collection<Bundle> allowedBundles = activator.findConsumerRestrictions(consumerBundle, className, methodName, args);
 
         if (allowedBundles != null) {
---------------
-------------
@@ -94,7 +94,7 @@
 
     public void write(DataOutput out) throws IOException
     {
-        if (rows.size() == 1 && !shouldPurge)
+if (rows.size() == 1 && !shouldPurge && rows.get(0).sstable.descriptor.isLatestVersion)
         {
             SSTableIdentityIterator row = rows.get(0);
             out.writeLong(row.dataSize);
---------------
-------------
@@ -42,6 +42,6 @@
 
 public class Constants {
 
-  public static final String VERSION = "8.5.0";
+public static final String VERSION = "9.0.0";
 
 }
---------------
-------------
@@ -250,7 +250,7 @@
       if (!hasNext()) {
         throw new NoSuchElementException();
       }
-      if (currentUpto == currentLength) {
+while (currentUpto == currentLength) {
         // refill next doc, and sort remapped ords within the doc.
         currentUpto = 0;
         currentLength = (int) counts.next();
---------------
-------------
@@ -49,7 +49,7 @@
  *
  * <P>See ClientDataSource for DataSource properties.</p>
  */
-public class ClientXADataSource extends ClientBaseDataSource implements XADataSource {
+public class ClientXADataSource extends ClientDataSource implements XADataSource {
     public static final String className__ = "org.apache.derby.jdbc.ClientXADataSource";
 
     // following serialVersionUID was generated by the JDK's serialver program
---------------
-------------
@@ -171,7 +171,7 @@
                 addPosition(fieldName, t.termText(), position++, null);
               
               lastToken = t;
-              if (++length > maxFieldLength) {
+if (++length >= maxFieldLength) {
                 if (infoStream != null)
                   infoStream.println("maxFieldLength " +maxFieldLength+ " reached, ignoring following tokens");
                 break;
---------------
-------------
@@ -94,7 +94,7 @@
         {
             try
             {
-                selector.select(100);
+selector.select(1);
                 doProcess();
                 synchronized(gate) {}
             }
---------------
-------------
@@ -99,7 +99,7 @@
     }
   }
   
-  private int parseIntAt(BytesRef bytes, int offset, CharsRef scratch) throws IOException {
+private int parseIntAt(BytesRef bytes, int offset, CharsRef scratch) {
     UnicodeUtil.UTF8toUTF16(bytes.bytes, bytes.offset+offset, bytes.length-offset, scratch);
     return ArrayUtil.parseInt(scratch.chars, 0, scratch.length);
   }
---------------
-------------
@@ -189,7 +189,7 @@
 
   /** verify if a node contains a tag */
   public boolean containsTag(String tagName) {
-    return this.tags.containsKey(tagName);
+return this.tags.containsKey(tagName.toLowerCase());
   }
 
   /**
---------------
-------------
@@ -314,7 +314,7 @@
         }
 
         @Override
-        protected Reader initReader(Reader reader) {
+protected Reader initReader(String fieldName, Reader reader) {
           return new PatternReplaceCharFilter(p, replacement, CharReader.get(reader));
         }
       };
---------------
-------------
@@ -196,7 +196,7 @@
         		{"XSDB5","Log has change record on page {0}, which is beyond the end of the container.","45000"},
         		{"XSDB6","Another instance of Derby may have already booted the database {0}.","45000"},
         		{"XSDB7","WARNING: Derby (instance {0}) is attempting to boot the database {1} even though Derby (instance {2}) may still be active.  Only one instance of Derby should boot a database at a time. Severe and non-recoverable corruption can result and may have already occurred.","45000"},
-        		{"XSDB8","WARNING: Derby (instance {0}) is attempting to boot the database {1} even though Derby (instance {2}) may still be active.  Only one instance of Derby should boot a database at a time. Severe and non-recoverable corruption can result if 2 instances of Derby boot on the same database at the same time.  The db2j.database.forceDatabaseLock=true property has been set, so the database will not boot until the db.lck is no longer present.  Normally this file is removed when the first instance of Derby to boot on the database exits, but it may be left behind in some shutdowns.  It will be necessary to remove the file by hand in that case.  It is important to verify that no other VM is accessing the database before deleting the db.lck file by hand.","45000"},
+{"XSDB8","WARNING: Derby (instance {0}) is attempting to boot the database {1} even though Derby (instance {2}) may still be active.  Only one instance of Derby should boot a database at a time. Severe and non-recoverable corruption can result if 2 instances of Derby boot on the same database at the same time.  The derby.database.forceDatabaseLock=true property has been set, so the database will not boot until the db.lck is no longer present.  Normally this file is removed when the first instance of Derby to boot on the database exits, but it may be left behind in some shutdowns.  It will be necessary to remove the file by hand in that case.  It is important to verify that no other VM is accessing the database before deleting the db.lck file by hand.","45000"},
         		{"XSDB9","Stream container {0} is corrupt.","45000"},
         		{"XSDBA","Attempt to allocate object {0} failed.","45000"},
         		{"XSDBB", "Unknown page format at page {0}, page dump follows: {1} ", "45000"},
---------------
-------------
@@ -92,7 +92,7 @@
 		// the application is looking for a connection from some other
 		// driver.
 		//
-		return ( isBooted() && InternalDriver.embeddedDriverAcceptsURL(url) );
+return InternalDriver.embeddedDriverAcceptsURL(url);
 	}
 
    
---------------
-------------
@@ -649,7 +649,7 @@
           
           if (gen == -1) {
             // Neither approach found a generation
-            throw new FileNotFoundException("no segments* file found in " + directory + ": files: " + Arrays.toString(files));
+throw new IndexNotFoundException("no segments* file found in " + directory + ": files: " + Arrays.toString(files));
           }
         }
 
---------------
-------------
@@ -594,7 +594,7 @@
      */
     public String getFlushPath()
     {
-        long guessedSize = 2 * memsize.value() * 1024*1024; // 2* adds room for keys, column indexes
+long guessedSize = 2L * memsize.value() * 1024*1024; // 2* adds room for keys, column indexes
         String location = DatabaseDescriptor.getDataFileLocationForTable(table.name, guessedSize);
         if (location == null)
             throw new RuntimeException("Insufficient disk space to flush");
---------------
-------------
@@ -137,7 +137,7 @@
       }
       
       this.liveDocs = bits;
-      this.numDocs = (int) bits.cardinality();
+this.numDocs = bits.cardinality();
     }
     
     @Override
---------------
-------------
@@ -157,7 +157,7 @@
         return size();
     }
 
-    public void addColumn(String name, IColumn column)
+public void addColumn(IColumn column)
     {
         throw new UnsupportedOperationException("This operation is not supported for simple columns.");
     }
---------------
-------------
@@ -51,6 +51,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new IndonesianAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new IndonesianAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -101,7 +101,7 @@
         {
             List<AbstractBounds> unwrapped = bound.unwrap();
             assert previous == null || previous.right.compareTo(unwrapped.get(0).left) <= 0 :
-                "Overlapping ranges passed to normalize: see CASSANDRA-2461: " + previous + " and " + unwrapped;
+"Overlapping ranges passed to normalize: see CASSANDRA-2641: " + previous + " and " + unwrapped;
             output.addAll(unwrapped);
             previous = unwrapped.get(unwrapped.size() - 1);
         }
---------------
-------------
@@ -2284,7 +2284,7 @@
             // DERBY-4564. Make replication tests use derby.tests.networkServerTimeout proeprty
             String userStartTimeout = getSystemProperty("derby.tests.networkServerStartTimeout");
             long startTimeout = (userStartTimeout != null )? 
-            		Long.parseLong(userStartTimeout): DEFAULT_SERVER_START_TIMEOUT;
+(Long.parseLong(userStartTimeout) * 1000): DEFAULT_SERVER_START_TIMEOUT;
             long iterations = startTimeout / PINGSERVER_SLEEP_TIME_MILLIS;		
             util.DEBUG(debugId+"************** Do .start().");
             serverThread.start();
---------------
-------------
@@ -122,7 +122,7 @@
     {
         logger.info("Writing " + this);
         String path = cfs.getFlushPath();
-        SSTableWriter writer = new SSTableWriter(path, sortedKeys.size(), cfs.metadata, cfs.partitioner_);
+SSTableWriter writer = new SSTableWriter(path, sortedKeys.size(), cfs.metadata, cfs.partitioner);
 
         for (DecoratedKey key : sortedKeys)
         {
---------------
-------------
@@ -324,7 +324,7 @@
       reversed[i] = sortField.getReverse() ? -1 : 1;
       comparators[i] = sortField.getComparator(size, i);
       if (scorer != null) comparators[i].setScorer(scorer);
-      if (reader != null) comparators[i].setNextReader(reader, docBase);
+if (reader != null) comparators[i] = comparators[i].setNextReader(reader, docBase);
     }
   }
 
---------------
-------------
@@ -1663,7 +1663,7 @@
     private static final HashMap storageFactories = new HashMap();
     static {
 		String dirStorageFactoryClass;
-		if( JVMInfo.JDK_ID >= JVMInfo.J2SE_14)
+if( !JVMInfo.J2ME && (JVMInfo.JDK_ID >= JVMInfo.J2SE_14) )
             dirStorageFactoryClass = "org.apache.derby.impl.io.DirStorageFactory4";
         else
             dirStorageFactoryClass = "org.apache.derby.impl.io.DirStorageFactory";
---------------
-------------
@@ -170,7 +170,7 @@
 
   public void testBufferOverflow() throws Exception {
     StringBuilder testBuilder = new StringBuilder(HTMLStripCharFilter.DEFAULT_READ_AHEAD + 50);
-    testBuilder.append("ah<?> ");
+testBuilder.append("ah<?> ??????");
     appendChars(testBuilder, HTMLStripCharFilter.DEFAULT_READ_AHEAD + 500);
     processBuffer(testBuilder.toString(), "Failed on pseudo proc. instr.");//processing instructions
 
---------------
-------------
@@ -313,7 +313,7 @@
 		FileOutputStream os = new FileOutputStream(outputfile);
 		try
 		{
-			outputProp.save(os, 
+outputProp.store(os,
 							header.
 							concat("# config is ").concat(config).
 							concat(footer));
---------------
-------------
@@ -4587,7 +4587,7 @@
 
   // utility routines for tests
   SegmentInfo newestSegment() {
-    return segmentInfos.info(segmentInfos.size()-1);
+return segmentInfos.size() > 0 ? segmentInfos.info(segmentInfos.size()-1) : null;
   }
 
   public synchronized String segString() {
---------------
-------------
@@ -30,7 +30,7 @@
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.*;
+import static org.apache.lucene.analysis.VocabularyAssert.*;
 
 /**
  * Test the PorterStemFilter with Martin Porter's test data.
---------------
-------------
@@ -77,7 +77,7 @@
   private final static int TERM_DOC_FREQ_RAND = 20;
 
   @BeforeClass
-  public static void beforeClass() throws Exception {
+public static void beforeClass() {
     NUM_TEST_ITER = atLeast(20);
   }
 
---------------
-------------
@@ -844,7 +844,7 @@
         else
         {
 			// arg 2 generate code to evaluate generation clauses
-			generateGenerationClauses( resultColumnList, resultSet.getResultSetNumber(), acb, mb );
+generateGenerationClauses( resultColumnList, resultSet.getResultSetNumber(), true, acb, mb );
 
             // generate code to evaluate CHECK CONSTRAINTS
             generateCheckConstraints( checkConstraints, acb, mb ); // arg 3
---------------
-------------
@@ -65,7 +65,7 @@
     writer.addDocument(doc);
     reader = writer.getReader();
     searcher = newSearcher(reader);
-    writer.close();
+writer.shutdown();
   }
   
   @Override
---------------
-------------
@@ -306,7 +306,7 @@
                 // from previous loop iterations
                 message.removeHeader(RowMutation.FORWARD_HEADER);
 
-                if (dataCenter.equals(localDataCenter) || StorageService.instance.useEfficientCrossDCWrites())
+if (dataCenter.equals(localDataCenter))
                 {
                     // direct writes to local DC or old Cassadra versions
                     for (InetAddress destination : messages.getValue())
---------------
-------------
@@ -340,7 +340,7 @@
     public static ColumnFamily getDroppedCFs() throws IOException
     {
         ColumnFamilyStore cfs = Table.open(Table.SYSTEM_TABLE).getColumnFamilyStore(SystemTable.STATUS_CF);
-        return cfs.getColumnFamily(QueryFilter.getSliceFilter(decorate(GRAVEYARD_KEY), new QueryPath(STATUS_CF), "".getBytes(), "".getBytes(), null, false, 100));
+return cfs.getColumnFamily(QueryFilter.getSliceFilter(decorate(GRAVEYARD_KEY), new QueryPath(STATUS_CF), "".getBytes(), "".getBytes(), false, 100));
     }
     
     public static void deleteDroppedCfMarkers(Collection<IColumn> cols) throws IOException
---------------
-------------
@@ -965,7 +965,7 @@
     public static double getRowsCachedFraction(String tableName, String columnFamilyName)
     {
         Double v = tableRowsCachedFractions_.get(Pair.create(tableName, columnFamilyName));
-        return v == null ? 0.01 : v;
+return v == null ? 0 : v;
     }
 
     private static class ConfigurationException extends Exception
---------------
-------------
@@ -44,7 +44,7 @@
 
 public class TestDeletionPolicy extends LuceneTestCase {
   
-  private void verifyCommitOrder(List<? extends IndexCommit> commits) throws IOException {
+private void verifyCommitOrder(List<? extends IndexCommit> commits) {
     final IndexCommit firstCommit =  commits.get(0);
     long last = SegmentInfos.generationFromSegmentsFileName(firstCommit.getSegmentsFileName());
     assertEquals(last, firstCommit.getGeneration());
---------------
-------------
@@ -71,7 +71,7 @@
 				java.sql.Connection.class,
 				new MD[]
 				{
-						new MD( "createArray", new Class[] { String.class, Object[].class } ),
+new MD( "createArrayOf", new Class[] { String.class, Object[].class } ),
 						new MD( "createNClob", new Class[] { } ),
 						new MD( "createSQLXML", new Class[] { } ),
 						new MD( "createStruct", new Class[] { String.class, Object[].class } ),
---------------
-------------
@@ -26,7 +26,7 @@
 import org.apache.derby.client.am.Agent;
 import org.apache.derby.client.am.ClientMessageId;
 import org.apache.derby.client.am.SqlException;
-import org.apache.derby.iapi.services.sanity.SanityManager;
+import org.apache.derby.shared.common.sanity.SanityManager;
 import org.apache.derby.shared.common.reference.SQLState;
 
 public class Utf8CcsidManager extends CcsidManager {
---------------
-------------
@@ -169,7 +169,7 @@
     /* This abstraction maintains the token/endpoint metadata information */
     private TokenMetadata tokenMetadata_ = new TokenMetadata();
 
-    private Set<InetAddress> replicatingNodes = new Collections.synchronizedSet(new HashSet<InetAddress>());
+private Set<InetAddress> replicatingNodes = Collections.synchronizedSet(new HashSet<InetAddress>());
     private InetAddress removingNode;
 
     /* Are we starting this node in bootstrap mode? */
---------------
-------------
@@ -43,7 +43,7 @@
 /** 
 	
 
-	EmbeddedDataSource is Derby's DataSource implementation.
+EmbeddedDataSource is Derby's DataSource implementation for JDBC3.0 and JDBC2.0.
 	
 
 	<P>A DataSource  is a factory for Connection objects. An object that
---------------
-------------
@@ -34,7 +34,7 @@
 import org.apache.lucene.analysis.TokenStreamToAutomaton;
 import org.apache.lucene.search.spell.TermFreqIterator;
 import org.apache.lucene.search.suggest.Lookup;
-import org.apache.lucene.search.suggest.fst.Sort;
+import org.apache.lucene.search.suggest.Sort;
 import org.apache.lucene.store.ByteArrayDataInput;
 import org.apache.lucene.store.ByteArrayDataOutput;
 import org.apache.lucene.store.DataInput;
---------------
-------------
@@ -101,7 +101,7 @@
   
   public ConcurrentUpdateSolrServer(String solrServerUrl,
       HttpClient client, int queueSize, int threadCount) {
-    this(solrServerUrl, null, queueSize, threadCount, Executors.newCachedThreadPool(
+this(solrServerUrl, client, queueSize, threadCount, Executors.newCachedThreadPool(
         new SolrjNamedThreadFactory("concurrentUpdateScheduler")));
     shutdownExecutor = true;
   }
---------------
-------------
@@ -500,7 +500,7 @@
     assertEquals(info, leftField.binaryValue(), rightField.binaryValue());
     assertEquals(info, leftField.stringValue(), rightField.stringValue());
     assertEquals(info, leftField.numericValue(), rightField.numericValue());
-    assertEquals(info, leftField.numericDataType(), rightField.numericDataType());
+assertEquals(info, leftField.fieldType().numericType(), rightField.fieldType().numericType());
     // TODO: should we check the FT at all?
   }
   
---------------
-------------
@@ -66,6 +66,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new GermanAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new GermanAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -3040,7 +3040,7 @@
      *      in soft upgrade mode, {@code false} if not, and {@code null} if
      *      unknown
      */
-    public void setSoftUpgradeMode(Boolean inSoftUpgradeMode) {
+public void setStreamHeaderFormat(Boolean inSoftUpgradeMode) {
         // Ignore this for CHAR, VARCHAR and LONG VARCHAR.
     }
     
---------------
-------------
@@ -71,7 +71,7 @@
 
   public void runSSVDSolver(int q) throws IOException {
 
-    Configuration conf = new Configuration();
+Configuration conf = getConfiguration();
     conf.set("mapred.job.tracker", "local");
     conf.set("fs.default.name", "file:///");
 
---------------
-------------
@@ -59,7 +59,7 @@
     
     SegmentInfos sis = new SegmentInfos();
     sis.read(dir);
-    double min = sis.info(0).sizeInBytes();
+double min = sis.info(0).sizeInBytes(true);
     
     conf = newWriterConfig();
     LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();
---------------
-------------
@@ -776,7 +776,7 @@
     }
     // final check
     IndexReader r2 = IndexReader.openIfChanged(r);
-    if (r2 != r) {
+if (r2 != null) {
       r.close();
       r = r2;
     }
---------------
-------------
@@ -521,7 +521,7 @@
   
           if (!isEmpty) {
             // we have a non-empty index, check if the term exists
-            currentTerm.copy(word);
+currentTerm.copyChars(word);
             for (TermsEnum te : termsEnums) {
               if (te.seekExact(currentTerm, false)) {
                 continue terms;
---------------
-------------
@@ -63,7 +63,7 @@
 
   static final String project = "solr";
   static final String base = "org.apache" + "." + project;
-  static final String[] packages = {"","analysis.","schema.","handler.","search.","update.","core.","request.","update.processor.","util.", "spelling.", "handler.component.", "handler.dataimport"};
+static final String[] packages = {"","analysis.","schema.","handler.","search.","update.","core.","request.","update.processor.","util.", "spelling.", "handler.component.", "handler.dataimport."};
 
   private URLClassLoader classLoader;
   private final String instanceDir;
---------------
-------------
@@ -26,7 +26,7 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 
-import static org.apache.lucene.analysis.util.VocabularyAssert.*;
+import static org.apache.lucene.analysis.VocabularyAssert.*;
 
 /**
  * Simple tests for {@link SpanishLightStemFilter}
---------------
-------------
@@ -54,7 +54,7 @@
 		outfile = new File(outfileName);
 	}
 	
-	public void setUp() throws Exception{
+public void setUp() {
 	    super.setUp();
 		setSystemProperty("ij.outfile", outfileName);
 		setSystemProperty("ij.defaultResourcePackage",
---------------
-------------
@@ -109,7 +109,7 @@
   protected void setup(Context context) throws IOException, InterruptedException {
     
     super.setup(context);
-    Parameters params = Parameters.fromString(context.getConfiguration().get(PFPGrowth.PFP_PARAMETERS, ""));
+Parameters params = new Parameters(context.getConfiguration().get(PFPGrowth.PFP_PARAMETERS, ""));
     
     int i = 0;
     for (Pair<String,Long> e : PFPGrowth.deserializeList(params, PFPGrowth.F_LIST, context.getConfiguration())) {
---------------
-------------
@@ -23,7 +23,7 @@
 
     public void doVerb(Message message)
     {
-        byte[] body = (byte[])message.getMessageBody()[0];
+byte[] body = message.getMessageBody();
         /* Obtain a Read Context from TLS */
         ReadContext readCtx = tls_.get();
         if ( readCtx == null )
---------------
-------------
@@ -94,7 +94,7 @@
     doc.add(field);
     
     for (int i = 0; i < terms; i++) {
-      field.setValue(mapInt(codePointTable, i));
+field.setStringValue(mapInt(codePointTable, i));
       writer.addDocument(doc);
     }   
     
---------------
-------------
@@ -46,7 +46,7 @@
 	 * transaction object). Might be <code>null</code>.
 	 * @return an object which represents a compatibility space
 	 */
-	public CompatibilitySpace createCompatibilitySpace(Object owner);
+public CompatibilitySpace createCompatibilitySpace(LockOwner owner);
 
 	/**
 		Lock an object within a compatibility space
---------------
-------------
@@ -586,7 +586,7 @@
     public TermHistogram histogram;
     
     TopTermQueue(int size) {
-      initialize(size);
+super(size);
       histogram = new TermHistogram();
     }
     
---------------
-------------
@@ -308,7 +308,7 @@
     
     private String cachePath(Bundle bundle, String filePath)
     {
-      return bundle.getSymbolicName() + "/" + bundle.getVersion() + "/" + filePath;
+return Integer.toHexString(bundle.hashCode()) + "/" + filePath;
     }    
     
     private URL getOverrideURLForCachePath(String privatePath){
---------------
-------------
@@ -461,7 +461,7 @@
         }
         else if (propertyName.equals("version"))
         {
-            return "1";
+return "0.3.0";
         }
         else
         {
---------------
-------------
@@ -116,7 +116,7 @@
         "item similarity computation phase, users with more preferences will be sampled down (default: " +
         DEFAULT_MAX_PREFS_PER_USER + ')', String.valueOf(DEFAULT_MAX_PREFS_PER_USER));
     addOption("similarityClassname", "s", "Name of distributed similarity measures class to instantiate, " +
-        "alternatively use one of the predefined similarities (" + VectorSimilarityMeasures.list() + ')');
+"alternatively use one of the predefined similarities (" + VectorSimilarityMeasures.list() + ')', true);
     addOption("threshold", "tr", "discard item pairs with a similarity value below this", false);
 
     Map<String,String> parsedArgs = parseArguments(args);
---------------
-------------
@@ -1,6 +1,6 @@
 package org.apache.cassandra.cql.jdbc;
 
-public class JdbcCounterColumn extends LongTerm
+public class JdbcCounterColumn extends JdbcLong
 {
     public static final JdbcCounterColumn instance = new JdbcCounterColumn();
     
---------------
-------------
@@ -285,7 +285,7 @@
     Dictionary dict = new Hashtable();
     dict.put(ApplicationRepository.REPOSITORY_SCOPE, app.getApplicationMetadata().getApplicationScope());
     _bundleContext.registerService(BundleRepository.class.getName(), 
-        new ApplicationRepository(_resolver), 
+new ApplicationRepository(app),
         dict);
     
     AriesApplicationContext result = _applicationContextManager.getApplicationContext(app);
---------------
-------------
@@ -228,7 +228,7 @@
         MessagingService.instance.registerVerbHandlers(Verb.GOSSIP_DIGEST_ACK2, new GossipDigestAck2VerbHandler());
 
         replicationStrategies = new HashMap<String, AbstractReplicationStrategy>();
-        for (String table : DatabaseDescriptor.getNonSystemTables())
+for (String table : DatabaseDescriptor.getTables())
         {
             AbstractReplicationStrategy strat = getReplicationStrategy(tokenMetadata_, table);
             replicationStrategies.put(table, strat);
---------------
-------------
@@ -86,7 +86,7 @@
     public java.io.InputStream getAsciiStream(String columnName) throws SQLException { throw notImplemented( "io.InputStream getAsciiStream" ); }
     public java.io.InputStream getUnicodeStream(String columnName) throws SQLException { throw notImplemented( "io.InputStream getUnicodeStream" ); }
     public java.io.InputStream getBinaryStream(String columnName) throws SQLException { throw notImplemented( "io.InputStream getBinaryStream" ); }
-    public SQLWarning getWarnings() throws SQLException { throw notImplemented( "getWarnings" ); }
+public SQLWarning getWarnings() throws SQLException { return null; }
     public void clearWarnings() throws SQLException { throw notImplemented( "clearWarnings" ); }
     public String getCursorName() throws SQLException { throw notImplemented( "getCursorName" ); }
     public Object getObject(int columnIndex) throws SQLException { throw notImplemented( "getObject" ); }
---------------
-------------
@@ -144,7 +144,7 @@
             assert bytes.length > 0;
             writer.append(key, bytes);
         }
-        SSTableReader sstable = writer.closeAndOpenReader();
+SSTableReader sstable = writer.closeAndOpenReader(DatabaseDescriptor.getKeysCachedFraction(table_));
         logger_.info("Completed flushing " + writer.getFilename());
         return sstable;
     }
---------------
-------------
@@ -24,7 +24,7 @@
 public class IndexException extends IllegalArgumentException {
 
   public IndexException(int index, int cardinality) {
-    super("Index " + index + " is outside allowable range of [0," + cardinality + ']');
+super("Index " + index + " is outside allowable range of [0," + cardinality + ')');
   }
 
 }
---------------
-------------
@@ -147,7 +147,7 @@
    * NOTE: add() and refresh() CANNOT be used together. If you call add(),
    * this changes the arrays and refresh() can no longer be used.
    */
-  void add(int ordinal, int parentOrdinal) throws IOException {
+void add(int ordinal, int parentOrdinal) {
     if (ordinal >= prefetchParentOrdinal.length) {
       // grow the array, if necessary.
       // In Java 6, we could just do Arrays.copyOf()...
---------------
-------------
@@ -103,7 +103,7 @@
     assert field != null;
     Similarity sim = previousMappings.get(field);
     if (sim == null) {
-      sim = knownSims.get(Math.abs(perFieldSeed ^ field.hashCode()) % knownSims.size());
+sim = knownSims.get(Math.max(0, Math.abs(perFieldSeed ^ field.hashCode())) % knownSims.size());
       previousMappings.put(field, sim);
     }
     return sim;
---------------
-------------
@@ -108,7 +108,7 @@
     Bits liveDocs = MultiFields.getLiveDocs(indexReader);
     int updatedCount = countIntersection(MultiFields.getTermDocsEnum(indexReader, liveDocs,
                                                                      drillDownTerm.field(), drillDownTerm.bytes(),
-                                                                     0),
+false),
                                          docIds.iterator());
 
     fresNode.setValue(updatedCount);
---------------
-------------
@@ -96,7 +96,7 @@
     };
   }
   
-  URI resolveRelativeURI(String baseURI, String systemId) throws IOException,URISyntaxException {
+URI resolveRelativeURI(String baseURI, String systemId) throws URISyntaxException {
     URI uri;
     
     // special case for backwards compatibility: if relative systemId starts with "/" (we convert that to an absolute solrres:-URI)
---------------
-------------
@@ -482,7 +482,7 @@
 
   void addFlushableState(ThreadState perThread) {
     if (documentsWriter.infoStream.isEnabled("DWFC")) {
-      documentsWriter.infoStream.message("DWFC", Thread.currentThread().getName() + ": addFlushableState " + perThread.dwpt);
+documentsWriter.infoStream.message("DWFC", "addFlushableState " + perThread.dwpt);
     }
     final DocumentsWriterPerThread dwpt = perThread.dwpt;
     assert perThread.isHeldByCurrentThread();
---------------
-------------
@@ -587,7 +587,7 @@
     {
         return metadata.cfType == ColumnFamilyType.Standard
                ? Column.serializer()
-               : SuperColumn.serializer(getColumnComparator());
+: SuperColumn.serializer(metadata.subcolumnComparator);
     }
 
     /**
---------------
-------------
@@ -47,6 +47,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new HindiAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new HindiAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -43,7 +43,7 @@
    *  @param pageSize         the size of a single page */
   public MonotonicAppendingLongBuffer(int initialPageCount, int pageSize) {
     super(initialPageCount, pageSize);
-    averages = new float[pageSize];
+averages = new float[initialPageCount];
   }
 
   /** Create an {@link MonotonicAppendingLongBuffer} with initialPageCount=16
---------------
-------------
@@ -60,7 +60,7 @@
    */
   private static String normalize( String p )
   {
-    if( p != null && p.endsWith( "/" ) )
+if( p != null && p.endsWith( "/" ) && p.length() > 1 )
       return p.substring( 0, p.length()-1 );
     
     return p;
---------------
-------------
@@ -226,7 +226,7 @@
     return null;
   } 
 
-  public ApplicationContext install(AriesApplication app) {
+public ApplicationContext install(AriesApplication app) throws BundleException, ManagementException {
     ApplicationContext result = _applicationContextManager.getApplicationContext(app);
     return result;
   }
---------------
-------------
@@ -330,7 +330,7 @@
               if (positionDeltas.length == numPositions) {
                 positionDeltas = ArrayUtil.grow(positionDeltas, 1+numPositions);
               }
-              final int pos = positions[i].pos;
+final int pos = positions[posIndex].pos;
               positionDeltas[numPositions++] = pos - lastPos;
               lastPos = pos;
             }
---------------
-------------
@@ -196,7 +196,7 @@
         lastSkipDoc = skipDoc;
         lastFreqPointer = freqPointer;
         lastProxPointer = proxPointer;
-        if (skipDoc >= doc)
+if (skipDoc != 0 && skipDoc >= doc)
           numSkipped += skipInterval;
         
         if ((count + numSkipped + skipInterval) > df)
---------------
-------------
@@ -59,7 +59,7 @@
 
     public ByteBuffer fromString(String source)
     {
-        return ByteBuffer.wrap(source.getBytes(Charsets.US_ASCII));
+return ByteBufferUtil.bytes(source, Charsets.US_ASCII);
     }
 
     public void validate(ByteBuffer bytes) throws MarshalException
---------------
-------------
@@ -22,7 +22,7 @@
 import org.apache.commons.httpclient.methods.GetMethod;
 import org.apache.commons.httpclient.methods.HeadMethod;
 import org.apache.commons.httpclient.methods.PostMethod;
-import org.apache.solr.client.solrj.SolrJettyTestBase;
+import org.apache.solr.SolrJettyTestBase;
 import org.apache.solr.client.solrj.impl.CommonsHttpSolrServer;
 import org.junit.Test;
 
---------------
-------------
@@ -118,7 +118,7 @@
       //
       // writer.forceMerge(1);
 
-      writer.close();
+writer.shutdown();
 
       Date end = new Date();
       System.out.println(end.getTime() - start.getTime() + " total milliseconds");
---------------
-------------
@@ -124,7 +124,7 @@
                 assert subColumns.size() == 4;
                 for (IColumn subColumn : subColumns)
                 {
-                    long k = subColumn.name().getLong(subColumn.name().position() + subColumn.name().arrayOffset());
+long k = subColumn.name().getLong(subColumn.name().position());
                     byte[] bytes = (j + k) % 2 == 0 ? "a".getBytes() : "b".getBytes();
                     assertEquals(new String(bytes), ByteBufferUtil.string(subColumn.value()));
                 }
---------------
-------------
@@ -34,7 +34,7 @@
  * <br>Example: <code>{!lucene q.op=AND df=text sort='price asc'}myfield:foo +bar -baz</code>
  */
 public class LuceneQParserPlugin extends QParserPlugin {
-  public static String NAME = "lucene";
+public static final String NAME = "lucene";
 
   @Override
   public void init(NamedList args) {
---------------
-------------
@@ -66,7 +66,7 @@
  */
 public abstract class Migration
 {
-    private static final Logger logger = LoggerFactory.getLogger(Migration.class);
+protected static final Logger logger = LoggerFactory.getLogger(Migration.class);
     
     public static final String NAME_VALIDATOR_REGEX = "\\w+";
     public static final String MIGRATIONS_CF = "Migrations";
---------------
-------------
@@ -8245,7 +8245,7 @@
 				//have to add 1 to the month value returned because the method give 0-January, 1-February and so on and so forth
 				generatedSystemSQLName.append(twoDigits(calendarForLastSystemSQLName.get(Calendar.MONTH)+1));
 				generatedSystemSQLName.append(twoDigits(calendarForLastSystemSQLName.get(Calendar.DAY_OF_MONTH)));
-				generatedSystemSQLName.append(twoDigits(calendarForLastSystemSQLName.get(Calendar.HOUR)));
+generatedSystemSQLName.append(twoDigits(calendarForLastSystemSQLName.get(Calendar.HOUR_OF_DAY)));
 				generatedSystemSQLName.append(twoDigits(calendarForLastSystemSQLName.get(Calendar.MINUTE)));
 				generatedSystemSQLName.append(twoDigits(calendarForLastSystemSQLName.get(Calendar.SECOND)));
 				//because we don't have enough space to store the entire millisec value, just store the higher 2 digits.
---------------
-------------
@@ -48,7 +48,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -33,7 +33,7 @@
 /**
  * Appending postings impl.
  */
-class AppendingPostingsFormat extends PostingsFormat {
+final class AppendingPostingsFormat extends PostingsFormat {
   public static String CODEC_NAME = "Appending";
   
   public AppendingPostingsFormat() {
---------------
-------------
@@ -44,7 +44,7 @@
   final static BytesRef PAYLOAD = new BytesRef("        payload ");
 
   public SimpleTextFieldsWriter(SegmentWriteState state) throws IOException {
-    final String fileName = SimpleTextCodec.getPostingsFileName(state.segmentName);
+final String fileName = SimpleTextCodec.getPostingsFileName(state.segmentName, state.codecId);
     out = state.directory.createOutput(fileName);
     state.flushedFiles.add(fileName);
   }
---------------
-------------
@@ -339,7 +339,7 @@
             throws IOException, UnavailableException, TimeoutException, InvalidRequestException
     {
         if (StorageService.instance.isBootstrapMode())
-            throw new InvalidRequestException("This node cannot accept reads until it has bootstrapped");
+throw new UnavailableException();
         long startTime = System.nanoTime();
 
         List<Row> rows;
---------------
-------------
@@ -133,7 +133,7 @@
   private void openExampleIndex() throws IOException {
     //Create a RAM-based index from our test data file
     RAMDirectory rd = new RAMDirectory();
-    IndexWriterConfig iwConfig = new IndexWriterConfig(Version.LUCENE_40, analyzer);
+IndexWriterConfig iwConfig = new IndexWriterConfig(Version.LUCENE_CURRENT, analyzer);
     IndexWriter writer = new IndexWriter(rd, iwConfig);
     InputStream dataIn = getServletContext().getResourceAsStream("/WEB-INF/data.tsv");
     BufferedReader br = new BufferedReader(new InputStreamReader(dataIn, IOUtils.CHARSET_UTF_8));
---------------
-------------
@@ -89,7 +89,7 @@
 	private boolean	calledOnNullInput;
 
 	// What type of alias is this: PROCEDURE or FUNCTION?
-	private char aliasType;
+private transient char aliasType;
 
 	public RoutineAliasInfo() {
 	}
---------------
-------------
@@ -134,7 +134,7 @@
         DirectoryReader open = null;
         for (int i = 0; i < num; i++) {
           Document doc = new Document();// docs.nextDoc();
-          doc.add(newField("id", "test", StringField.TYPE_UNSTORED));
+doc.add(newStringField("id", "test", Field.Store.NO));
           writer.updateDocument(new Term("id", "test"), doc);
           if (random().nextInt(3) == 0) {
             if (open == null) {
---------------
-------------
@@ -82,7 +82,7 @@
         br.copyBytes(term.bytes());
         assert termsEnum != null;
         if (termsEnum.seekCeil(br) == TermsEnum.SeekStatus.FOUND) {
-          docs = termsEnum.docs(acceptDocs, docs, false);
+docs = termsEnum.docs(acceptDocs, docs, 0);
           while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
             result.set(docs.docID());
           }
---------------
-------------
@@ -530,7 +530,7 @@
     protected abstract void modifyIndex(int i) throws IOException;
   }
   
-  static class KeepAllCommits implements IndexDeletionPolicy {
+static class KeepAllCommits extends IndexDeletionPolicy {
     @Override
     public void onInit(List<? extends IndexCommit> commits) {
     }
---------------
-------------
@@ -346,7 +346,7 @@
   }
 
 
-  @Test @Ignore("Please fix me!")
+@Test
   public void testClientErrorOnMalformedNumbers() throws Exception {
 
     final String BAD_VALUE = "NOT_A_NUMBER";
---------------
-------------
@@ -66,6 +66,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new IrishAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new IrishAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -318,7 +318,7 @@
     {
         if (daemon == null)
         {
-            throw new IllegalStateException("No configured RPC daemon");
+return false;
         }
         return daemon.isRPCServerRunning();
     }
---------------
-------------
@@ -106,7 +106,7 @@
     for (int i = 0; i < open.maxDoc(); i++) {
       StoredDocument document = open.document(i);
       int expected = Integer.parseInt(document.get(byteTestField));
-      assertEquals((byte)expected, normValues.get(i));
+assertEquals((byte)expected, (byte)normValues.get(i));
     }
     open.close();
     dir.close();
---------------
-------------
@@ -304,7 +304,7 @@
 		// current plans using "this" node as the key.  If needed, we'll
 		// then make the call to revert the plans in OptimizerImpl's
 		// getNextDecoratedPermutation() method.
-		addOrLoadBestPlanMapping(true, this);
+updateBestPlanMap(ADD_PLAN, this);
 
 		/* If the childResult is instanceof Optimizable, then we optimizeIt.
 		 * Otherwise, we are going into a new query block.  If the new query
---------------
-------------
@@ -417,7 +417,7 @@
             {
                 isBootstrapMode = false;
                 SystemTable.setBootstrapped(true);
-                tokenMetadata_.updateNormalToken(token, FBUtilities.getLocalAddress());
+setToken(token);
                 Gossiper.instance.addLocalApplicationState(ApplicationState.STATUS, valueFactory.normal(token));
                 setMode("Normal", false);
             }
---------------
-------------
@@ -242,7 +242,7 @@
       }
       //System.out.println("  skipFP=" + termState.skipFP);
     } else if (isFirstTerm) {
-      termState.skipFP = termState.bytesReader.readVLong();
+termState.skipFP = 0;
     }
   }
 
---------------
-------------
@@ -64,7 +64,7 @@
   private static class PayloadAnalyzer extends Analyzer {
 
     private PayloadAnalyzer() {
-      super(new PerFieldReuseStrategy());
+super(PER_FIELD_REUSE_STRATEGY);
     }
 
     @Override
---------------
-------------
@@ -187,7 +187,7 @@
     }
     
     // make sure we got all the keys/values
-    if (index != keys.length) {
+if (keys != null && index != keys.length) {
       throw new IllegalStateException();
     }
   }
---------------
-------------
@@ -263,7 +263,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new FrenchAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new FrenchAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
   
   /** test accent-insensitive */
---------------
-------------
@@ -34,7 +34,7 @@
  * APPLICATION This is not a realistic application of Bloom Filters as they
  * ordinarily are larger and operate on only primary key type fields.
  */
-public class TestBloomFilteredLucene40Postings extends PostingsFormat {
+public final class TestBloomFilteredLucene40Postings extends PostingsFormat {
   
   private BloomFilteringPostingsFormat delegate;
   
---------------
-------------
@@ -86,7 +86,7 @@
 
 
   /** make sure to close req after you are done using the response */
-  public SolrQueryResponse getResponse(SolrQueryRequest req) throws IOException, Exception {
+public SolrQueryResponse getResponse(SolrQueryRequest req) throws Exception {
     SolrQueryResponse rsp = new SolrQueryResponse();
     h.getCore().execute(h.getCore().getRequestHandler(null),req,rsp);
     if (rsp.getException() != null) {
---------------
-------------
@@ -156,7 +156,7 @@
     protected PackedIntsReader(Directory dir, String id, int numDocs,
         IOContext context) throws IOException {
       datIn = dir.openInput(
-          IndexFileNames.segmentFileName(id, "", Writer.DATA_EXTENSION),
+IndexFileNames.segmentFileName(id, Bytes.DV_SEGMENT_SUFFIX, Writer.DATA_EXTENSION),
           context);
       this.numDocs = numDocs;
       boolean success = false;
---------------
-------------
@@ -67,7 +67,7 @@
   }
   
   private void initialize(IndexReader[] subReaders, boolean closeSubReaders) {
-    this.subReaders = subReaders;
+this.subReaders = (IndexReader[]) subReaders.clone();
     starts = new int[subReaders.length + 1];    // build starts array
     decrefOnClose = new boolean[subReaders.length];
     for (int i = 0; i < subReaders.length; i++) {
---------------
-------------
@@ -830,7 +830,7 @@
       }
     }
 
-    if (totalCharsSoFar != -1) {
+if (totalCharsSoFar > -1) {
 
       /* This is a hack to fix a problem: When there is missing data in columns
       and hasDelimiterAtEnd==true, then the last delimiter was read as the last column data.
---------------
-------------
@@ -78,7 +78,7 @@
       }
 
       if (i%2 == 0) {
-        doc.add(new NumericField("numInt").setIntValue(i));
+doc.add(new NumericField("numInt", i));
       }
 
       // sometimes skip the field:
---------------
-------------
@@ -227,7 +227,7 @@
     Terms cterms = fields.terms(term.field);
     TermsEnum ctermsEnum = cterms.iterator(null);
     if (ctermsEnum.seekExact(new BytesRef(term.text()), false)) {
-      DocsEnum docsEnum = _TestUtil.docs(random(), ctermsEnum, bits, null, 0);
+DocsEnum docsEnum = _TestUtil.docs(random(), ctermsEnum, bits, null, false);
       return toArray(docsEnum);
     }
     return null;
---------------
-------------
@@ -74,7 +74,7 @@
   /**
    * {@inheritDoc}
    */
-  public Map<String, Pair<Integer, Collection<SearchGroup<BytesRef>>>> transformToNative(NamedList<NamedList> shardResponse, Sort groupSort, Sort sortWithinGroup, String shard) throws IOException {
+public Map<String, Pair<Integer, Collection<SearchGroup<BytesRef>>>> transformToNative(NamedList<NamedList> shardResponse, Sort groupSort, Sort sortWithinGroup, String shard) {
     Map<String, Pair<Integer, Collection<SearchGroup<BytesRef>>>> result = new HashMap<String, Pair<Integer, Collection<SearchGroup<BytesRef>>>>();
     for (Map.Entry<String, NamedList> command : shardResponse) {
       List<SearchGroup<BytesRef>> searchGroups = new ArrayList<SearchGroup<BytesRef>>();
---------------
-------------
@@ -121,7 +121,7 @@
 
         // TODO just use a raw RandomAccessFile since we're managing our own buffer here
         RandomAccessReader file = (header.file.sstable.compression) // try to skip kernel page cache if possible
-                                ? CompressedRandomAccessReader.open(header.file.getFilename(), true)
+? CompressedRandomAccessReader.open(header.file.getFilename(), header.file.sstable.getCompressionMetadata(), true)
                                 : RandomAccessReader.open(new File(header.file.getFilename()), true);
 
         // setting up data compression stream
---------------
-------------
@@ -399,7 +399,7 @@
     if (container != null) {
       for (SolrCore c : container.getCores()) {
         if (c.getOpenCount() > 1)
-          throw new RuntimeException("SolrCore.getOpenCount()=="+core.getOpenCount());
+throw new RuntimeException("SolrCore.getOpenCount()=="+c.getOpenCount());
       }      
     }
 
---------------
-------------
@@ -1628,7 +1628,7 @@
 
 		switch (jdbcType) {
 		case Types.BIT:
-		case org.apache.derby.iapi.reference.JDBC30Translation.SQL_TYPES_BOOLEAN:
+case Types.BOOLEAN:
 		case Types.TINYINT:
 		case Types.SMALLINT:
 		case Types.INTEGER:
---------------
-------------
@@ -70,7 +70,7 @@
         // update bloom filter and create a list of IndexInfo objects marking the first and last column
         // in each block of ColumnIndexSize
         List<IndexHelper.IndexInfo> indexList = new ArrayList<IndexHelper.IndexInfo>();
-        int endPosition = 0, startPosition = -1;
+long endPosition = 0, startPosition = -1;
         int indexSizeInBytes = 0;
         IColumn lastColumn = null, firstColumn = null;
         for (IColumn column : columns)
---------------
-------------
@@ -507,7 +507,7 @@
                                     new BytesRef(t.text()),
                                     MultiFields.getLiveDocs(reader),
                                     null,
-                                    false);
+0);
 
     int count = 0;
     while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
---------------
-------------
@@ -39,7 +39,7 @@
     
     public void doVerb(Message message)
     { 
-        byte[] bytes = (byte[])message.getMessageBody()[0];
+byte[] bytes = message.getMessageBody();
         /* Obtain a Row Mutation Context from TLS */
         RowMutationContext rowMutationCtx = tls_.get();
         if ( rowMutationCtx == null )
---------------
-------------
@@ -85,7 +85,7 @@
       }
       // Only one .fdt and one .fdx files must have been found
       assertEquals(2, counter);
-      iw.close();
+iw.shutdown();
       dir.close();
     }
   }
---------------
-------------
@@ -98,7 +98,7 @@
       }
       writer.close();
 
-      Searcher searcher = new IndexSearcher(directory);
+Searcher searcher = new IndexSearcher(directory, true);
 
       String[] queries = {
         "a b",
---------------
-------------
@@ -2755,7 +2755,7 @@
     // Tests that if FSDir is opened w/ a NoLockFactory (or SingleInstanceLF),
     // then IndexWriter ctor succeeds. Previously (LUCENE-2386) it failed 
     // when listAll() was called in IndexFileDeleter.
-    Directory dir = newFSDirectory(new File(TEMP_DIR, "emptyFSDirNoLock"), NoLockFactory.getNoLockFactory());
+Directory dir = newFSDirectory(_TestUtil.getTempDir("emptyFSDirNoLock"), NoLockFactory.getNoLockFactory());
     new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random))).close();
     dir.close();
   }
---------------
-------------
@@ -609,7 +609,7 @@
     }
 
     @Override
-    protected Reader initReader(Reader reader) {
+protected Reader initReader(String fieldName, Reader reader) {
       Random random = new Random(seed);
       CharFilterSpec charfilterspec = newCharFilterChain(random, reader);
       return charfilterspec.reader;
---------------
-------------
@@ -67,7 +67,7 @@
     
     assertEquals(1, writer.numDocs());
 
-    writer.close();
+writer.shutdown();
     directory.close();
   }
 
---------------
-------------
@@ -52,7 +52,7 @@
       + " */" + NL + NL;
 
 
-  public static void main(String args[]) throws Exception {
+public static void main(String args[]) {
     outputHeader();
     outputMacro("ID_Start_Supp", "[:ID_Start:]");
     outputMacro("ID_Continue_Supp", "[:ID_Continue:]");
---------------
-------------
@@ -752,7 +752,7 @@
     IndexReader reader = IndexReader.open(dir);
     Fields tfv = reader.getTermVectors(0);
     assertNotNull(tfv);
-    assertTrue(tfv.getUniqueFieldCount() > 0);
+assertTrue(tfv.size() > 0);
     reader.close();
   }
 
---------------
-------------
@@ -67,7 +67,7 @@
       try {
         ZkController zk = new ZkController(zkHost, zkConnectTimeout, zkClientTimeout, null, null, null);
         zk.addShardZkNodeWatches();
-        zk.updateCloudState(true);
+zk.getZkStateReader().updateCloudState(true);
         zkController = zk;
       } catch (InterruptedException e) {
         Thread.currentThread().interrupt();
---------------
-------------
@@ -1301,7 +1301,7 @@
 	
 	/** Returns a list of statistics for this table.
 	 */
-	private synchronized List getStatistics() throws StandardException
+public synchronized List getStatistics() throws StandardException
 	{
 		// if table already has the statistics descriptors initialized
 		// no need to do anything
---------------
-------------
@@ -115,7 +115,7 @@
 		occurs then ...
 
 		@see org.apache.derby.iapi.services.cache.Cacheable#clean
-		@exception StandardException Standard Cloudscape error policy
+@exception StandardException Standard Derby error policy
 	*/
 	public final void clean(boolean forRemove) throws StandardException {
 
---------------
-------------
@@ -26,7 +26,7 @@
     Directory directory = newDirectory();
     IndexWriter writer = new IndexWriter(directory, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));
     assertNull(writer.newestSegment());
-    writer.close();
+writer.shutdown();
     directory.close();
   }
 }
---------------
-------------
@@ -85,7 +85,7 @@
       int hi = docMaps.length - 1;                  // for first element less
 
       while (hi >= lo) {
-        int mid = (lo + hi) >> 1;
+int mid = (lo + hi) >>> 1;
         int midValue = starts[mid];
         if (oldDocID < midValue)
           hi = mid - 1;
---------------
-------------
@@ -1431,7 +1431,7 @@
     }
 
     // TRUNCATE <columnFamily>
-    private void executeTruncate(String columnFamily) throws TException, InvalidRequestException, UnavailableException
+private void executeTruncate(String columnFamily) throws TException, InvalidRequestException, UnavailableException, TimedOutException
     {
         if (!CliMain.isConnected() || !hasKeySpace())
             return;
---------------
-------------
@@ -58,7 +58,7 @@
 
     /** Call this sequentially for each term encoutered,
      *  after calling {@link #getIndexOffset}. */
-    public abstract boolean isIndexTerm(long ord, int docFreq) throws IOException;
+public abstract boolean isIndexTerm(long ord, int docFreq, boolean onlyLoaded) throws IOException;
 
     /** Finds the next index term, after the specified
      *  ord.  Returns true if one exists.  */
---------------
-------------
@@ -167,7 +167,7 @@
       SequenceFile.Reader classifiedVectors = new SequenceFile.Reader(fs,
           partFile.getPath(), conf);
       Writable clusterIdAsKey = new IntWritable();
-      WeightedVectorWritable point = new WeightedVectorWritable();
+WeightedPropertyVectorWritable point = new WeightedPropertyVectorWritable();
       while (classifiedVectors.next(clusterIdAsKey, point)) {
         collectVector(clusterIdAsKey.toString(), point.getVector());
       }
---------------
-------------
@@ -1388,7 +1388,7 @@
     }
     
     DocTermOrds dto = (DocTermOrds) caches.get(DocTermOrds.class).get(reader, new CacheKey(field, null), false);
-    return dto.iterator(dto.getOrdTermsEnum(reader));
+return dto.iterator(reader);
   }
 
   static final class DocTermOrdsCache extends Cache {
---------------
-------------
@@ -17,7 +17,7 @@
  */
 
 import junit.framework.Assert;
-import org.apache.solr.client.solrj.SolrJettyTestBase;
+import org.apache.solr.SolrJettyTestBase;
 import org.apache.solr.client.solrj.SolrQuery;
 import org.apache.solr.client.solrj.request.QueryRequest;
 import org.apache.solr.client.solrj.response.SpellCheckResponse.Collation;
---------------
-------------
@@ -262,6 +262,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new UAX29URLEmailAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new UAX29URLEmailAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -324,7 +324,7 @@
 
     //final TermsEnum te = subR.fields().terms("field").iterator();
     final TermsEnum te = dto.getOrdTermsEnum(r);
-    if (te == null) {
+if (dto.numTerms() == 0) {
       if (prefixRef == null) {
         assertNull(MultiFields.getTerms(r, "field"));
       } else {
---------------
-------------
@@ -106,7 +106,7 @@
   private Reader getReader(Blob blob)
           throws SQLException, UnsupportedEncodingException {
     if (encoding == null) {
-      return (new InputStreamReader(blob.getBinaryStream()));
+return (new InputStreamReader(blob.getBinaryStream(), "UTF-8"));
     } else {
       return (new InputStreamReader(blob.getBinaryStream(), encoding));
     }
---------------
-------------
@@ -84,7 +84,7 @@
    * @param in the name of a stemmer
    */
   public SnowballFilter(TokenStream in, String name) {
-    this.input = in;
+super(in);
     try {
       Class stemClass =
         Class.forName("net.sf.snowball.ext." + name + "Stemmer");
---------------
-------------
@@ -78,7 +78,7 @@
       b.append(" z");
       doc.add(newTextField("field", b.toString(), Field.Store.NO));
       writer.addDocument(doc);
-      writer.close();
+writer.shutdown();
       
       IndexReader reader = DirectoryReader.open(dir);
       Term t = new Term("field", "x");
---------------
-------------
@@ -181,7 +181,7 @@
 					.append(')');
 			String filter = service.getFilter();
 			if (filter != null)
-				builder.append('(').append(filter).append(')');
+builder.append(filter);
 			builder.append(')');
 			requirements.add(new BasicRequirement.Builder()
 					.namespace(ServiceNamespace.SERVICE_NAMESPACE)
---------------
-------------
@@ -113,7 +113,7 @@
     double distErrPct = ((PrefixTreeStrategy) strategy).getDistErrPct();
     double distErr = SpatialArgs.calcDistanceFromErrPct(snapMe, distErrPct, ctx);
     int detailLevel = grid.getLevelForDistance(distErr);
-    List<Node> cells = grid.getNodes(snapMe, detailLevel, false);
+List<Node> cells = grid.getNodes(snapMe, detailLevel, false, true);
 
     //calc bounding box of cells.
     double minX = Double.POSITIVE_INFINITY, maxX = Double.NEGATIVE_INFINITY;
---------------
-------------
@@ -121,7 +121,7 @@
     // test merge factor picked up
     SolrCore core = h.getCore();
 
-    IndexWriter writer = ((DirectUpdateHandler2)core.getUpdateHandler()).getIndexWriterProvider().getIndexWriter(core);
+IndexWriter writer = ((DirectUpdateHandler2)core.getUpdateHandler()).getSolrCoreState().getIndexWriter(core);
     assertEquals("Mergefactor was not picked up", ((LogMergePolicy)writer.getConfig().getMergePolicy()).getMergeFactor(), 8);
 
     lrf.args.put(CommonParams.VERSION,"2.2");
---------------
-------------
@@ -638,7 +638,7 @@
           
           if (gen == -1) {
             // Neither approach found a generation
-            throw new FileNotFoundException("no segments* file found in " + directory + ": files: " + Arrays.toString(files));
+throw new IndexNotFoundException("no segments* file found in " + directory + ": files: " + Arrays.toString(files));
           }
         }
 
---------------
-------------
@@ -101,7 +101,7 @@
     Text value = new Text();
     boolean converged = true;
     while (converged && reader.next(key, value))
-      converged = converged && value.toString().startsWith("V");
+converged = value.toString().startsWith("V");
     return converged;
   }
 
---------------
-------------
@@ -468,7 +468,7 @@
             return;
 
         IndexClause clause = new IndexClause();
-        String columnFamily = statement.getChild(0).getText();
+String columnFamily = CliCompiler.getColumnFamily(statement, keyspacesMap.get(keySpace).cf_defs);
         // ^(CONDITIONS ^(CONDITION $column $value) ...)
         Tree conditions = statement.getChild(1);
         
---------------
-------------
@@ -44,7 +44,7 @@
 import org.apache.derby.iapi.store.access.xa.XAXactId;
 import org.apache.derby.impl.jdbc.EmbedConnection;
 import org.apache.derby.impl.jdbc.TransactionResourceImpl;
-import org.apache.derby.shared.common.sanity.SanityManager;
+import org.apache.derby.iapi.services.sanity.SanityManager;
 import org.apache.derby.iapi.services.property.PropertyUtil;
 import org.apache.derby.iapi.reference.Property;
 
---------------
-------------
@@ -484,7 +484,7 @@
     try {
       final String path = URLDecoder.decode(nodeName.substring(1+_offset),
                                             "UTF-8");
-      return "http://" + hostAndPort + (path.isEmpty() ? "" : ("/" + path));
+return "http://" + hostAndPort + "/" + path;
     } catch (UnsupportedEncodingException e) {
       throw new IllegalStateException("JVM Does not seem to support UTF-8", e);
     }
---------------
-------------
@@ -371,7 +371,7 @@
                                      : 16;
             if (conf.rpc_max_threads == null)
                 conf.rpc_max_threads = conf.rpc_server_type.toLowerCase().equals("hsha")
-                                     ? Runtime.getRuntime().availableProcessors()
+? Runtime.getRuntime().availableProcessors() * 4
                                      : Integer.MAX_VALUE;
 
             /* data file and commit log directories. they get created later, when they're needed. */
---------------
-------------
@@ -187,7 +187,7 @@
 		// current plans using "this" node as the key.  If needed, we'll
 		// then make the call to revert the plans in OptimizerImpl's
 		// getNextDecoratedPermutation() method.
-		addOrLoadBestPlanMapping(true, this);
+updateBestPlanMap(ADD_PLAN, this);
 
 		/*
 		** RESOLVE: Most types of Optimizables only implement estimateCost(),
---------------
-------------
@@ -33,7 +33,7 @@
 
   public void testBasic() throws Exception {
     Directory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true,
+IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true,
         IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     doc.add(new Field("field", "value", Store.NO, Index.ANALYZED));
---------------
-------------
@@ -70,7 +70,7 @@
       indexWriter.commit();
     }
     indexReader = DirectoryReader.open(indexWriter, random().nextBoolean());
-    indexWriter.close();
+indexWriter.shutdown();
     indexSearcher = new IndexSearcher(indexReader);
     parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new WildcardQuery(new Term("parent", "*"))));
   }
---------------
-------------
@@ -63,7 +63,7 @@
         DataOutputStream dos = new DataOutputStream(bos);
         MembershipCleanerMessage.serializer().serialize(mcMessage, dos);
         /* Construct the token update message to be sent */
-        Message mbrshipCleanerMessage = new Message( new EndPoint(FBUtilities.getHostAddress(), port_), "", StorageService.mbrshipCleanerVerbHandler_, new Object[]{bos.toByteArray()} );
+Message mbrshipCleanerMessage = new Message( new EndPoint(FBUtilities.getHostAddress(), port_), "", StorageService.mbrshipCleanerVerbHandler_, bos.toByteArray() );
         
         BufferedReader bufReader = new BufferedReader( new InputStreamReader( new FileInputStream(file) ) );
         String line = null;
---------------
-------------
@@ -664,7 +664,7 @@
             {
             	try
             	{
-            		fs = new FileStruct(SequenceFile.bufferedReader(file, bufferSize));
+fs = new FileStruct(SequenceFile.bufferedReader(file, bufferSize), StorageService.getPartitioner());
 	                fs.advance();
 	                if(fs.isExhausted())
 	                	continue;
---------------
-------------
@@ -34,7 +34,7 @@
     protected abstract boolean termCompare(Term term);
     
     /** Equality measure on the term */
-    protected abstract float difference();
+public abstract float difference();
 
     /** Indiciates the end of the enumeration has been reached */
     protected abstract boolean endEnum();
---------------
-------------
@@ -204,7 +204,7 @@
     try {
       sreq.ursp = server.request(ureq);
     } catch (Exception e) {
-      throw new SolrException(ErrorCode.SERVER_ERROR, "Failed synchronous update on shard " + sreq.node, sreq.exception);
+throw new SolrException(ErrorCode.SERVER_ERROR, "Failed synchronous update on shard " + sreq.node + " update: " + ureq , e);
     }
   }
 
---------------
-------------
@@ -181,7 +181,7 @@
 		}
 		long shutdownTime = System.currentTimeMillis();
 		//Make a note of Engine shutdown in the log file
-		Monitor.getStream().printlnWithHeader("\n" + CheapDateFormatter.formatDate(shutdownTime) +
+Monitor.getStream().printlnWithHeader("\n" +
 				COLON +
                 MessageService.getTextMessage(
                     MessageId.CONN_SHUT_DOWN_ENGINE));
---------------
-------------
@@ -25,7 +25,7 @@
     public void testDiffSuperColumn()
     {
         SuperColumn sc1 = new SuperColumn("one");
-        sc1.addColumn("subcolumn", new Column("subcolumn", "A".getBytes(), 0));
+sc1.addColumn(new Column("subcolumn", "A".getBytes(), 0));
 
         SuperColumn sc2 = new SuperColumn("one");
         sc2.markForDeleteAt(0, 0);
---------------
-------------
@@ -37,7 +37,7 @@
     return new ArabicNormalizationFilter(input);
   }
 
-  @Override
+//@Override
   public Object getMultiTermComponent() {
     return this;
   }
---------------
-------------
@@ -28,7 +28,7 @@
 
   @Override
   public void testSize() {
-    assertEquals("size", 3, getTestVector().getNumNondefaultElements());
+assertEquals("size", 3, getTestVector().getNumNonZeroElements());
   }
 
   @Override
---------------
-------------
@@ -1,6 +1,6 @@
 /*
 
-   Derby - Class com.ihost.cs.DoubleProperties
+Derby - Class org.apache.derby.iapi.util.DoubleProperties
 
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -323,7 +323,7 @@
     }
     
     @Override
-    public void reset() throws IOException {
+public void reset() {
       startTerm = 0;
       nextStartOffset = 0;
       snippet = null;
---------------
-------------
@@ -485,7 +485,7 @@
     /** 
      * Do a cleanup of keys that do not belong locally.
      */
-    public void doGC()
+public void forceCleanup()
     {
         Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
         for ( String columnFamily : columnFamilies )
---------------
-------------
@@ -88,5 +88,5 @@
                                  Object[] inputs) throws SqlException;
 
 
-    public void writeSetSpecialRegister(java.util.ArrayList sqlsttList) throws SqlException;
+public void writeSetSpecialRegister(Section section, java.util.ArrayList sqlsttList) throws SqlException;
 }
---------------
-------------
@@ -132,7 +132,7 @@
     System.out.println("  encode...");
 
     PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);
-    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true);
+Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true, true);
     IntsRef scratch = new IntsRef();
     long ord = -1; // first ord will be 0
     String lastValue = null;
---------------
-------------
@@ -442,7 +442,7 @@
             if (bothDeleted || obsoleteRowTombstone || obsoleteColumn)
             {
                 if (logger.isDebugEnabled())
-                    logger.debug("skipping index update for obsolete mutation of " + cf.getComparator().getString(oldColumn.name()));
+logger.debug("skipping index update for obsolete mutation of " + cf.getComparator().getString(name));
                 iter.remove();
                 oldIndexedColumns.remove(name);
             }
---------------
-------------
@@ -51,7 +51,7 @@
       for (int i = 0; i < 10; i++) {
         writer.addDocument(new Document());
       }
-      writer.close();
+writer.shutdown();
     } finally {
       tmpDir.close();
     }
---------------
-------------
@@ -75,7 +75,7 @@
     conf.setReducerClass(IdentityReducer.class);
 
     client.setConf(conf);
-    FileSystem dfs = FileSystem.get(conf);
+FileSystem dfs = FileSystem.get(outPath.toUri(), conf);
     if (dfs.exists(outPath))
       dfs.delete(outPath, true);
     JobClient.runJob(conf);
---------------
-------------
@@ -212,7 +212,7 @@
             //maxPos the length requested.
             actualLength = len;
         }
-        int retValue = super.read(b, off, actualLength);
+int retValue = stream.read(b, off, actualLength);
         if (retValue > 0)
             pos += retValue;
         return retValue;
---------------
-------------
@@ -64,7 +64,7 @@
 
             try
             {
-                SSTableReader sstable = SSTableWriter.renameAndOpen(pendingFile.getFilename());
+SSTableReader sstable = SSTableWriter.renameAndOpen(pendingFile.getDescriptor());
                 Table.open(tableName).getColumnFamilyStore(temp[0]).addSSTable(sstable);
                 logger.info("Streaming added " + sstable.getFilename());
             }
---------------
-------------
@@ -51,6 +51,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new SwedishAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new SwedishAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -53,7 +53,7 @@
     /**
      * 
      */
-    public void doGC();
+public void forceTableCleanup();
 
     /**
      * Stream the files in the bootstrap directory over to the
---------------
-------------
@@ -54,7 +54,7 @@
     }
 
     s = newSearcher(w.getReader());
-    w.close();
+w.shutdown();
   }
 
   @AfterClass
---------------
-------------
@@ -78,7 +78,7 @@
     requirement.setAttribute("multiple", "false");
     requirement.setAttribute("optional", "false");
     
-    requirement.setAttribute("filter", ManifestHeaderProcessor.generateFilter("bundle", p.getContentName(), p.getAttributes()));
+requirement.setAttribute("filter", ManifestHeaderProcessor.generateFilter("symbolicname", p.getContentName(), p.getAttributes()));
     
     resource.appendChild(requirement);
   }
---------------
-------------
@@ -69,7 +69,7 @@
     r = new Random(random().nextLong());
   }
 
-  protected int shardCount = 4;
+protected int shardCount = 4;      // the actual number of solr cores that will be created in the cluster
 
   /**
    * Sub classes can set this flag in their constructor to true if they
---------------
-------------
@@ -3662,7 +3662,7 @@
           // This merge (and, generally, any change to the
           // segments) may now enable new merges, so we call
           // merge policy & update pending merges.
-          if (success && !merge.isAborted() && (merge.optimize || (!closed && !closing))) {
+if (success && !merge.isAborted() && !closed && !closing) {
             updatePendingMerges(merge.maxNumSegmentsOptimize, merge.optimize);
           }
         }
---------------
-------------
@@ -68,7 +68,7 @@
 		}
                 // need to make a guess so we copy text files to local encoding
                 // on non-ascii systems...
-		        if ((fileName.indexOf("sql") > 0) || (fileName.indexOf("txt") > 0) || (fileName.indexOf(".view") > 0) || (fileName.indexOf(".policy") > 0))
+if ((fileName.indexOf("sql") > 0) || (fileName.indexOf("txt") > 0) || (fileName.indexOf(".view") > 0) || (fileName.indexOf(".policy") > 0) || (fileName.indexOf(".multi") > 0) || (fileName.indexOf(".properties") > 0))
                 {
                     BufferedReader inFile = new BufferedReader(new InputStreamReader(is, "UTF-8"));
                     PrintWriter pw = new PrintWriter
---------------
-------------
@@ -45,7 +45,7 @@
   }
   
   static int idToIndex(long itemID) {
-    return 0x7FFFFFFF & (int) itemID ^ (int) (itemID >>> 32);
+return 0x7FFFFFFF & ((int) itemID ^ (int) (itemID >>> 32));
   }
   
 }
---------------
-------------
@@ -49,7 +49,7 @@
   private IndexOutput termsOut;
 
   public FixedGapTermsIndexWriter(SegmentWriteState state) throws IOException {
-    final String indexFileName = IndexFileNames.segmentFileName(state.segmentName, "", TERMS_INDEX_EXTENSION);
+final String indexFileName = IndexFileNames.segmentFileName(state.segmentName, state.codecId, TERMS_INDEX_EXTENSION);
     state.flushedFiles.add(indexFileName);
     termIndexInterval = state.termIndexInterval;
     out = state.directory.createOutput(indexFileName);
---------------
-------------
@@ -120,7 +120,7 @@
 
   public double dot(Vector x) {
     if (size() != x.size()) {
-      throw new CardinalityException();
+throw new CardinalityException(size(), x.size());
     }
     double result = 0;
     Iterator<Element> iter = iterateNonZero();
---------------
-------------
@@ -1533,7 +1533,7 @@
      *
      * @return connection to default database.
      */
-    Connection openDefaultConnection()
+public Connection openDefaultConnection()
         throws SQLException {
         return connector.openConnection();
     }
---------------
-------------
@@ -37,7 +37,7 @@
     @Test
     public void testWithFlush() throws IOException, ExecutionException, InterruptedException
     {
-        CompactionManager.instance.disableCompactions();
+CompactionManager.instance.disableAutoCompaction();
 
         for (int i = 0; i < 100; i++)
         {
---------------
-------------
@@ -107,7 +107,7 @@
 
     @Override
     public long encodeNormValue(float f) {
-      return (long) f;
+return (byte) f;
     }
 
     @Override
---------------
-------------
@@ -652,7 +652,7 @@
                 if (!FailureDetector.instance.isAlive(endpoint))
                 {
                     differencingDone.signalAll();
-                    logger.info("[repair #%s] Could not proceed on repair because a neighbor (%s) is dead: session failed", getName(), endpoint);
+logger.info(String.format("[repair #%s] Cannot proceed on repair because a neighbor (%s) is dead: session failed", getName(), endpoint));
                     return;
                 }
             }
---------------
-------------
@@ -204,7 +204,7 @@
     if (this.field != other.field  // interned comparison
         || this.includeLower != other.includeLower
         || this.includeUpper != other.includeUpper
-        || (this.collator != null && ! this.collator.equals(other.collator))
+|| (this.collator != null && ! this.collator.equals(other.collator) || (this.collator == null && other.collator != null))
        ) { return false; }
     String lowerVal = this.lowerTerm == null ? null : lowerTerm.text();
     String upperVal = this.upperTerm == null ? null : upperTerm.text();
---------------
-------------
@@ -33,7 +33,7 @@
 
 import org.apache.aries.util.filesystem.IDirectory;
 import org.apache.aries.util.filesystem.IFile;
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 
 /**
  * This class contains utilities for parsing manifests. It provides methods to
---------------
-------------
@@ -55,7 +55,7 @@
         super.bytesPerChar = BYTES_PER_CHAR;
         EmbedStatement embStmt = (EmbedStatement)createStatement();
         EmbedConnection embCon =(EmbedConnection)getConnection();
-        iClob = new TemporaryClob(embCon.getDBName(), embStmt);
+iClob = new TemporaryClob(embStmt);
         transferData(
             new LoopingAlphabetReader(CLOBLENGTH, CharAlphabet.tamil()),
             iClob.getWriter(1L),
---------------
-------------
@@ -51,7 +51,7 @@
 
     public String toString()
     {
-        return "Token(" + token + ")";
+return token.toString();
     }
 
     public boolean equals(Object obj)
---------------
-------------
@@ -2937,7 +2937,7 @@
 
   /** Returns true if there are changes that have not been committed */
   public final boolean hasUncommittedChanges() {
-    return changeCount != lastCommitChangeCount;
+return changeCount != lastCommitChangeCount || docWriter.anyChanges() || bufferedDeletesStream.any();
   }
 
   private final void commitInternal() throws IOException {
---------------
-------------
@@ -132,7 +132,7 @@
   
   public static void purgeFieldCache(IndexReader r) throws IOException {
     // this is just a hack, to get an atomic reader that contains all subreaders for insanity checks
-    FieldCache.DEFAULT.purge(SlowCompositeReaderWrapper.wrap(r));
+FieldCache.DEFAULT.purgeByCacheKey(SlowCompositeReaderWrapper.wrap(r).getCoreCacheKey());
   }
   
   /** This is a MultiReader that can be used for randomly wrapping other readers
---------------
-------------
@@ -70,7 +70,7 @@
             return endpoints;
         }
         startIndex = (index + 1)%totalNodes;
-        EndPointSnitch endPointSnitch = (EndPointSnitch) StorageService.instance().getEndPointSnitch();
+EndPointSnitch endPointSnitch = (EndPointSnitch) StorageService.instance.getEndPointSnitch();
 
         for (int i = startIndex, count = 1; count < totalNodes && foundCount < replicas_; ++count, i = (i + 1) % totalNodes)
         {
---------------
-------------
@@ -60,7 +60,7 @@
     
     NumberFormat df = new DecimalFormat("000", new DecimalFormatSymbols(Locale.ENGLISH));
     for (int i = 0; i < 1000; i++) {
-      field.setValue(df.format(i));
+field.setStringValue(df.format(i));
       writer.addDocument(doc);
     }
     
---------------
-------------
@@ -48,7 +48,7 @@
 
   @Before
   public void before() throws Exception {
-    conf = new Configuration();
+conf = getConfiguration();
     conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization,"
       + "org.apache.hadoop.io.serializer.WritableSerialization");
 
---------------
-------------
@@ -89,7 +89,7 @@
             }
 
             cf = table.get(key, "Super1");
-            assert cf != null;
+assert cf != null : "key " + key + " is missing!";
             Collection<IColumn> superColumns = cf.getAllColumns();
             assert superColumns.size() == 8;
             for (IColumn superColumn : superColumns)
---------------
-------------
@@ -272,7 +272,7 @@
         case 0: queryShape = randomPoint(); break;
         case 1:case 2:case 3:
           if (!indexedAtLeastOneShapePair) { // avoids ShapePair.relate(ShapePair), which isn't reliable
-            queryShape = randomShapePairRect(biasContains);
+queryShape = randomShapePairRect(!biasContains);//invert biasContains for query side
             break;
           }
         default: queryShape = randomRectangle();
---------------
-------------
@@ -58,7 +58,7 @@
         this(name, ArrayUtils.EMPTY_BYTE_ARRAY);
     }
 
-    Column(byte[] name, byte[] value)
+public Column(byte[] name, byte[] value)
     {
         this(name, value, 0);
     }
---------------
-------------
@@ -121,7 +121,7 @@
 				}
 				catch (SQLException expectedException)
 				{
-					if (! "0AX01".equals(expectedException.getSQLState()))
+if (! "0A000".equals(expectedException.getSQLState()))
 					{
 						System.out.println("DERBY-1184: Caught UNexpected: " +
 							expectedException.getMessage());
---------------
-------------
@@ -558,7 +558,7 @@
                     modifiedRowCount > 1 ? (order >= lastOrder) :
                         (order > lastOrder);
                 assertTrue("matching triggers need to be fired in order creation:"
-                        +info, orderOk);
++info+". Triggers got fired in this order:"+TRIGGER_INFO.get().toString(), orderOk);
                 lastOrder = order;
                 continue;
             }
---------------
-------------
@@ -116,7 +116,7 @@
       
     // assure that we deal with a single segment  
     writer.forceMerge(1);
-    writer.close();
+writer.shutdown();
     
     IndexReader reader = DirectoryReader.open(dir);
 
---------------
-------------
@@ -389,7 +389,7 @@
                 // Destroy the process if a failed shutdown
                 // to avoid hangs running tests as the complete()
                 // waits for the process to complete.
-                spawnedServer.complete(failedShutdown != null);
+spawnedServer.complete(failedShutdown != null, getWaitTime());
                 spawnedServer = null;
             }
             
---------------
-------------
@@ -159,7 +159,7 @@
         original.close();
         Sed hostSed = new Sed();
         InputStream sedIs = new ByteArrayInputStream(("substitute=localhost;" + hostName).getBytes("UTF-8"));
-        hostSed.exec(tmpFile, orgFile, sedIs, false, false);		
+hostSed.exec(tmpFile, orgFile, sedIs, false, false, false);
     }
 
 	public static void main (String args[]) throws Exception
---------------
-------------
@@ -43,7 +43,7 @@
   }
 
   public synchronized AEProvider getAEProvider(String core, String aePath,
-          Map<String, String> runtimeParameters) {
+Map<String, Object> runtimeParameters) {
     String key = new StringBuilder(core).append(aePath).toString();
     if (providerCache.get(key) == null) {
       providerCache.put(key, new OverridingParamsAEProvider(aePath, runtimeParameters));
---------------
-------------
@@ -66,7 +66,7 @@
   
   public LeaderElector(SolrZkClient zkClient) {
     this.zkClient = zkClient;
-    zkCmdExecutor = new ZkCmdExecutor((int) (zkClient.getZkClientTimeout()/1000.0 + 3000));
+zkCmdExecutor = new ZkCmdExecutor(zkClient.getZkClientTimeout());
   }
   
   /**
---------------
-------------
@@ -60,7 +60,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    Configuration conf = new Configuration();
+Configuration conf = getConfiguration();
     fs = FileSystem.get(conf);
   }
   
---------------
-------------
@@ -221,7 +221,7 @@
         // For now allow users to override the new behavior through a debug
         // property. Will be removed or renamed in a future release.
         boolean keepDisposableStats = PropertyUtil.getSystemBoolean(
-              Property.STORAGE_AUTO_INDEX_STATS_DEBUG_FORCE_OLD_BEHAVIOR);
+Property.STORAGE_AUTO_INDEX_STATS_DEBUG_KEEP_DISPOSABLE_STATS);
         this.skipDisposableStats = dbAtLeast10_9(db) && !keepDisposableStats;
 
         this.db = db;
---------------
-------------
@@ -278,7 +278,7 @@
       return binarySearch(value, spare, 0, getValueCount() - 1);
     }    
 
-    protected int binarySearch(BytesRef b, BytesRef bytesRef, int low,
+private int binarySearch(BytesRef b, BytesRef bytesRef, int low,
         int high) {
       int mid = 0;
       while (low <= high) {
---------------
-------------
@@ -858,7 +858,7 @@
 
       for (Entry<String, String> entry : entrySet) {
         int docId = docId(slowR, new Term("id", entry.getKey()));
-        expected.copyChars(entry.getValue());
+expected = new BytesRef(entry.getValue());
         assertEquals(expected, asSortedSource.getBytes(docId, actual));
       }
 
---------------
-------------
@@ -143,7 +143,7 @@
     
     //Test Expanded Collation Results
     query.set(SpellingParams.SPELLCHECK_COLLATE_EXTENDED_RESULTS, true);
-    query.set(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, 5);
+query.set(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, 10);
     query.set(SpellingParams.SPELLCHECK_MAX_COLLATIONS, 2); 
     request = new QueryRequest(query);
     response = request.process(server).getSpellCheckResponse();
---------------
-------------
@@ -58,7 +58,7 @@
         stages.put(RESPONSE_STAGE, multiThreadedStage(RESPONSE_STAGE, Math.max(2, Runtime.getRuntime().availableProcessors())));
         // the rest are all single-threaded
         stages.put(STREAM_STAGE, new JMXEnabledThreadPoolExecutor(STREAM_STAGE));
-        stages.put(GOSSIP_STAGE, new JMXEnabledThreadPoolExecutor("GMFD"));
+stages.put(GOSSIP_STAGE, new JMXEnabledThreadPoolExecutor("GOSSIP_STAGE"));
         stages.put(AE_SERVICE_STAGE, new JMXEnabledThreadPoolExecutor(AE_SERVICE_STAGE));
         stages.put(LOADBALANCE_STAGE, new JMXEnabledThreadPoolExecutor(LOADBALANCE_STAGE));
         stages.put(MIGRATION_STAGE, new JMXEnabledThreadPoolExecutor(MIGRATION_STAGE));
---------------
-------------
@@ -62,7 +62,7 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), analyzer, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -311,7 +311,7 @@
     MockAnalyzer stdAnalyzer = new MockAnalyzer(random());
 
     public AnalyzerReturningNull() {
-      super(new PerFieldReuseStrategy());
+super(PER_FIELD_REUSE_STRATEGY);
     }
 
     @Override
---------------
-------------
@@ -1328,7 +1328,7 @@
       List<String> createNodeList = ((createNodeSetStr = message.getStr(CREATE_NODE_SET)) == null)?null:StrUtils.splitSmart(createNodeSetStr, ",", true);
       
       if (repFactor <= 0) {
-        throw new SolrException(ErrorCode.BAD_REQUEST, REPLICATION_FACTOR + " must be greater than or equal to 0");
+throw new SolrException(ErrorCode.BAD_REQUEST, REPLICATION_FACTOR + " must be greater than 0");
       }
       
       if (numSlices <= 0) {
---------------
-------------
@@ -114,7 +114,7 @@
     assertJQ(req("json.nl","map", "qt",rh, SpellCheckComponent.COMPONENT_NAME, "true", "q","documemtsss broens", SpellCheckComponent.SPELLCHECK_COLLATE, "true")
        ,"/spellcheck/suggestions/collation=='document brown'"
     );
-    assertJQ(req("json.nl","map", "qt",rh, SpellCheckComponent.COMPONENT_NAME, "true", "q","pixma-a-b-c-d-e-f-g", SpellCheckComponent.SPELLCHECK_COLLATE, "true")
+assertJQ(req("json.nl","map", "qt",rh, SpellCheckComponent.COMPONENT_NAME, "true", "q","pixma", SpellCheckComponent.SPELLCHECK_COLLATE, "true")
        ,"/spellcheck/suggestions/collation=='pixmaa'"
     );
   }
---------------
-------------
@@ -165,7 +165,7 @@
 
     String result = null;
     if (rawSymName != null) {
-      NameValuePair<String, NameValueMap<String, String>> info = ManifestHeaderProcessor.parseBundleSymbolicName(rawSymName);
+NameValuePair info = ManifestHeaderProcessor.parseBundleSymbolicName(rawSymName);
       result = info.getName();
     }
     
---------------
-------------
@@ -196,7 +196,7 @@
   static class BoostingSimilarity extends DefaultSimilarity {
 
     // TODO: Remove warning after API has been finalized
-    public float scorePayload(byte[] payload, int offset, int length) {
+public float scorePayload(String fieldName, byte[] payload, int offset, int length) {
       //we know it is size 4 here, so ignore the offset/length
       return payload[0];
     }
---------------
-------------
@@ -180,7 +180,7 @@
     synchronized (this) {
       RAMFile existing = fileMap.get(name);
       if (existing!=null) {
-        sizeInBytes.addAndGet(existing.sizeInBytes);
+sizeInBytes.addAndGet(-existing.sizeInBytes);
         existing.directory = null;
       }
       fileMap.put(name, file);
---------------
-------------
@@ -36,7 +36,7 @@
   }
 
   @Override
-  protected void setUp() throws Exception {
+protected void setUp() {
     super.setUp();
     test = generateTestVector(2 * values.length + 1);
     for (int i = 0; i < values.length; i++) {
---------------
-------------
@@ -225,7 +225,7 @@
         //System.out.println("Term: " + term);
         assertEquals(testTerms[i], term);
         
-        docsEnum = _TestUtil.docs(random(), termsEnum, null, docsEnum, false);
+docsEnum = _TestUtil.docs(random(), termsEnum, null, docsEnum, 0);
         assertNotNull(docsEnum);
         int doc = docsEnum.docID();
         assertTrue(doc == -1 || doc == DocIdSetIterator.NO_MORE_DOCS);
---------------
-------------
@@ -356,7 +356,7 @@
                     StorageFile oldFile = getFile(files[i]);
                     if (!privDelete(oldFile)) {
                         throw StandardException.newException(
-                                      SQLState.FILE_CANNOT_REMOVE_FILE,
+SQLState.FILE_CANNOT_REMOVE_ENCRYPT_FILE,
                                       oldFile);
                     }
                 }
---------------
-------------
@@ -603,7 +603,7 @@
         ValueSource source = parseValSource(sp,schema);
         return new SimpleFloatFunction(source) {
           protected String name() {
-            return "log";
+return "abs";
           }
           protected float func(int doc, DocValues vals) {
             return (float)Math.abs(vals.floatVal(doc));
---------------
-------------
@@ -60,7 +60,7 @@
   private static final long WEEK = 7 * 24 * 3600;
   
   private final Random rand = RandomUtils.getRandom();  
-  private final Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_41);
+private final Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_42);
   private final FeatureVectorEncoder encoder = new StaticWordValueEncoder("body");
   private final FeatureVectorEncoder bias = new ConstantValueEncoder("Intercept");
   
---------------
-------------
@@ -335,7 +335,7 @@
     assertEquals(1 + endOffset + offsetGap, dpe.endOffset());
     assertEquals(null, te.next());
     reader.close();
-    writer.close();
+writer.shutdown();
     writer.w.getDirectory().close();
   }
 
---------------
-------------
@@ -4404,7 +4404,7 @@
 		// the ResultSet or the Statement. So we only need
 		// to convert the exception to a SQLException.
 
-		return TransactionResourceImpl.wrapInSQLException((SQLException) null, thrownException);
+return TransactionResourceImpl.wrapInSQLException(thrownException);
 
 	}
 
---------------
-------------
@@ -30,7 +30,7 @@
 import org.apache.derby.iapi.services.i18n.MessageService;
 import org.apache.derby.iapi.services.sanity.SanityManager;
 import org.apache.derby.iapi.types.PositionedStream;
-import org.apache.derby.shared.common.error.ExceptionUtil;
+import org.apache.derby.iapi.error.ExceptionUtil;
 
 /**
  * This input stream is built on top of {@link LOBStreamControl}.
---------------
-------------
@@ -348,7 +348,7 @@
       return map;
     }
 
-      void add( Query query, IndexReader reader ) throws IOException {
+void add( Query query, IndexReader reader ) {
       if( query instanceof TermQuery ){
         addTerm( ((TermQuery)query).getTerm(), query.getBoost() );
       }
---------------
-------------
@@ -90,7 +90,7 @@
         List<Range> ranges = (size == 0) ? null : new ArrayList<Range>();
         for( int i = 0; i < size; ++i )
         {
-            ranges.add(Range.serializer().deserialize(dis));
+ranges.add((Range) Range.serializer().deserialize(dis));
         }            
         return new StreamRequestMetadata( target, ranges );
     }
---------------
-------------
@@ -555,7 +555,7 @@
 
     // Load our ID:
     final String idFileName = IndexFileNames.segmentFileName(readState.segmentInfo.name, readState.segmentSuffix, ID_EXTENSION);
-    IndexInput in = readState.dir.openInput(idFileName, readState.context);
+IndexInput in = readState.directory.openInput(idFileName, readState.context);
     boolean success = false;
     final int id;
     try {
---------------
-------------
@@ -61,7 +61,7 @@
 
     public abstract void markClosedOnServer_();
 
-    public abstract void writeSetSpecialRegister_(java.util.ArrayList sqlsttList) throws SqlException;
+public abstract void writeSetSpecialRegister_(Section section, java.util.ArrayList sqlsttList) throws SqlException;
 
     public abstract void readSetSpecialRegister_() throws SqlException;
 
---------------
-------------
@@ -852,7 +852,7 @@
                                                                   generation);
     success = false;
     try {
-      dir.sync(fileName);
+dir.sync(Collections.singleton(fileName));
       success = true;
     } finally {
       if (!success) {
---------------
-------------
@@ -575,7 +575,7 @@
         String keyspace = state().getKeyspace();
         state().hasColumnFamilyAccess(column_parent.column_family, Permission.READ);
 
-        CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_parent.column_family, false);
+CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_parent.column_family);
         ThriftValidation.validateColumnParent(metadata, column_parent);
         ThriftValidation.validatePredicate(metadata, column_parent, predicate);
         ThriftValidation.validateKeyRange(range);
---------------
-------------
@@ -113,7 +113,7 @@
 
   // inherit javadocs
   public boolean validateData(QualityQuery[] qq, PrintWriter logger) {
-    HashMap<String,QRelJudgement> missingQueries = (HashMap<String, QRelJudgement>) judgements.clone();
+HashMap<String,QRelJudgement> missingQueries = new HashMap<String, QRelJudgement>(judgements);
     ArrayList<String> missingJudgements = new ArrayList<String>();
     for (int i=0; i<qq.length; i++) {
       String id = qq[i].getQueryID();
---------------
-------------
@@ -209,7 +209,7 @@
   }
 
   private String readFully(Reader stream) throws IOException {
-    StringBuffer buffer = new StringBuffer();
+StringBuilder buffer = new StringBuilder();
     int ch;
     while ((ch = stream.read()) != -1) {
       buffer.append((char) ch);
---------------
-------------
@@ -107,7 +107,7 @@
 
     @Override
     public long encodeNormValue(float f) {
-      return (long) f;
+return (byte) f;
     }
 
     @Override
---------------
-------------
@@ -112,6 +112,6 @@
     start = newText.getBeginIndex();
     end = newText.getEndIndex();
     text = newText;
-    current = newText.getIndex();
+current = start;
   }
 }
---------------
-------------
@@ -210,7 +210,7 @@
 					else
 					{
 						sb.append(sqle.getMessage());
-						sqle.printStackTrace();
+sqle.printStackTrace(System.out);
 					}
 				} else {
 					sb.append(sqle.getMessage());
---------------
-------------
@@ -120,7 +120,7 @@
 
         try
         {
-            AbstractCompactedRow compactedRow = controller.getCompactedRow(rows);
+AbstractCompactedRow compactedRow = controller.getCompactedRow(new ArrayList<SSTableIdentityIterator>(rows));
             if (compactedRow.isEmpty())
             {
                 controller.invalidateCachedRow(compactedRow.key);
---------------
-------------
@@ -56,7 +56,7 @@
 
     Shape shape = args.getShape();
 
-    int detailLevel = grid.getMaxLevelForPrecision(shape,args.getDistPrecision());
+int detailLevel = grid.getLevelForDistance(args.resolveDistErr(ctx, distErrPct));
 
     return new RecursivePrefixTreeFilter(
         getFieldName(), grid,shape, prefixGridScanLevel, detailLevel);
---------------
-------------
@@ -332,7 +332,7 @@
    * @param key the key
    * @param cmd the patch command
    */
-  public void add(CharSequence key, CharSequence cmd) {
+void add(CharSequence key, CharSequence cmd) {
     if (key == null || cmd == null) {
       return;
     }
---------------
-------------
@@ -604,7 +604,7 @@
     field.setTokenStream(ts);
     writer.addDocument(doc);
     DirectoryReader reader = writer.getReader();
-    AtomicReader sr = reader.getSequentialSubReaders().get(0);
+AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);
     DocsAndPositionsEnum de = sr.termPositionsEnum(null, "field", new BytesRef("withPayload"));
     de.nextDoc();
     de.nextPosition();
---------------
-------------
@@ -376,7 +376,7 @@
         else if (a instanceof double[]) {
           double ad[] = (double[]) a;
           int n = roundNum % ad.length;
-          sb.append(Format.format(2, (float) ad[n],template));
+sb.append(Format.format(2, ad[n],template));
         }
         else {
           boolean ab[] = (boolean[]) a;
---------------
-------------
@@ -60,7 +60,7 @@
 	}
 
 	/**
-		@exception StandardException Standard Cloudscape error policy
+@exception StandardException Standard Derby error policy
 	*/
 	public void open() throws StandardException
 	{
---------------
-------------
@@ -145,7 +145,7 @@
         ColumnFamilyStore cfStore = defs.getColumnFamilyStore(SCHEMA_CF);
         QueryFilter filter = QueryFilter.getNamesFilter(LAST_MIGRATION_KEY, new QueryPath(SCHEMA_CF), LAST_MIGRATION_KEY.getBytes());
         ColumnFamily cf = cfStore.getColumnFamily(filter);
-        if (cf.getColumnNames().size() == 0)
+if (cf == null || cf.getColumnNames().size() == 0)
             return null;
         else
             return UUIDGen.makeType1UUID(cf.getColumn(LAST_MIGRATION_KEY.getBytes()).value());
---------------
-------------
@@ -222,7 +222,7 @@
         		{"XSLA1","Log Record has been sent to the stream, but it cannot be applied to the store (Object {0}).  This may cause recovery problems also.","45000"},
         		{"XSLA2","System will shutdown, got I/O Exception while accessing log file.","45000"},
         		{"XSLA3","Log Corrupted, has invalid data in the log stream.","45000"},
-        		{"XSLA4","Cannot write to the log, most likely the log is full.  Please delete unnecessary files.  It is also possible that the file system is read only, or the disk has failed, or some other problems with the media.  ","45000"},
+{"XSLA4","Error encountered when attempting to write the transaction recovery log. Most likely the disk holding the recovery log is full. If the disk is full, the only way to proceed is to free up space on the disk by either expanding it or deleting files not related to Derby. It is also possible that the file system and/or disk where the Derby transaction log resides is read-only. The error can also be encountered if the disk or file system has failed.","45000"},
         		{"XSLA5","Cannot read log stream for some reason to rollback transaction {0}.","45000"},
         		{"XSLA6","Cannot recover the database.","45000"},
         		{"XSLA7","Cannot redo operation {0} in the log.","45000"},
---------------
-------------
@@ -1,6 +1,6 @@
 /*
  
-Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun
+Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun_local_1
  
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -97,7 +97,7 @@
       r.step[i] = lambda * step[i] + r.omni * gen.nextGaussian();
       r.params[i] += r.step[i];
     }
-    if (r.payload != null) {
+if (this.payload != null) {
       r.payload.update(r.getMappedParams());
     }
     return r;
---------------
-------------
@@ -328,7 +328,7 @@
     cps.add(cp);
     Document d = new Document();
     new CategoryDocumentBuilder(tw, iParams).setCategoryPaths(cps).build(d);
-    d.add(new Field("content", "alpha", TextField.TYPE_STORED));
+d.add(new TextField("content", "alpha", Field.Store.YES));
     iw.addDocument(d);
   }
 
---------------
-------------
@@ -99,7 +99,7 @@
 
     /** get a byte representation of the given string.
      *  defaults to unsupportedoperation so people deploying custom Types can update at their leisure. */
-    public ByteBuffer fromString(String source)
+public ByteBuffer fromString(String source) throws MarshalException
     {
         throw new UnsupportedOperationException();
     }
---------------
-------------
@@ -157,7 +157,7 @@
         }
       }
     } else {
-      Varint.writeUnsignedVarInt(vector.getNumNondefaultElements(), out);
+Varint.writeUnsignedVarInt(vector.getNumNonZeroElements(), out);
       Iterator<Element> iter = vector.nonZeroes().iterator();
       if (sequential) {
         int lastIndex = 0;
---------------
-------------
@@ -213,7 +213,7 @@
     logger.warn("Test set = " + test);
 
     // now train many times and collect information on accuracy each time
-    int[] correct = new int[test.size()];
+int[] correct = new int[test.size() + 1];
     for (int run = 0; run < 200; run++) {
       OnlineLogisticRegression lr = new OnlineLogisticRegression(3, 5, new L2(1));
       // 30 training passes should converge to > 95% accuracy nearly always but never to 100%
---------------
-------------
@@ -42,6 +42,6 @@
 
 public class Constants {
 
-  public static final String VERSION = "6.0.0";
+public static final String VERSION = "6.1.0";
 
 }
---------------
-------------
@@ -26,7 +26,7 @@
 /**
  *
  */
-public class FakeDeletionPolicy implements IndexDeletionPolicy, NamedListInitializedPlugin {
+public class FakeDeletionPolicy extends IndexDeletionPolicy implements NamedListInitializedPlugin {
 
   private String var1;
   private String var2;
---------------
-------------
@@ -65,7 +65,7 @@
     }
     reader = writer.getReader();
     searcher = newSearcher(reader);
-    writer.close();
+writer.shutdown();
   }
 
   @Override
---------------
-------------
@@ -26,7 +26,7 @@
  *
  **/
 public class BlockJoinChildQParserPlugin extends BlockJoinParentQParserPlugin {
-  public static String NAME = "child";
+public static final String NAME = "child";
 
   @Override
   protected QParser createBJQParser(String qstr, SolrParams localParams, SolrParams params, SolrQueryRequest req) {
---------------
-------------
@@ -444,7 +444,7 @@
       st.close();
     }
     if (result == null) { 
-      throw new ServiceUnavailableException ("The BlueprintContainer service could not be located");
+throw new ServiceUnavailableException ("The BlueprintContainer service for bundle: " + b.getSymbolicName() + '_' + b.getVersion() + " not be located");
     }
     return result;
   }
---------------
-------------
@@ -27,7 +27,7 @@
 
   @Override
   public void setUp() throws Exception {
-    LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE = true;
+LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true;
     super.setUp();
   }
 
---------------
-------------
@@ -156,7 +156,7 @@
 	/**
 		Start this module. We need a read/write version of the class utilities
 
-		@exception StandardException standard cloudscape policy
+@exception StandardException standard Derby policy
 	 */
 	public void boot(boolean create, Properties properties) throws StandardException {
 
---------------
-------------
@@ -122,7 +122,7 @@
         break;
       count++;
     }
-    Assert.assertEquals(2, local.get());
+Assert.assertEquals(2, (int) local.get());
     Assert.assertEquals(4, count);
   }
 
---------------
-------------
@@ -82,7 +82,7 @@
     }
     List<RecommendedItem> result = new ArrayList<RecommendedItem>(size);
     result.addAll(topItems);
-    Collections.sort(result);
+Collections.sort(result, ByValueRecommendedItemComparator.getInstance());
     return result;
   }
   
---------------
-------------
@@ -46,7 +46,7 @@
     public static Option[] configuration() {
         Option[] options = CoreOptions.options(CoreOptions.equinox(), mavenBundle("org.ops4j.pax.logging",
                 "pax-logging-api"), mavenBundle("org.ops4j.pax.logging", "pax-logging-service"), mavenBundle(
-                "org.apache.aries.jmx", "org.apache.aries.jmx"));
+"org.apache.aries.jmx", "org.apache.aries.jmx"),mavenBundle("org.apache.aries", "org.apache.aries.util"));
         options = updateOptions(options);
         return options;
     }
---------------
-------------
@@ -72,7 +72,7 @@
         {
             ByteBuffer bname = ByteBuffer.wrap(name.getBytes());
             ColumnPath cpath = new ColumnPath(cf).setColumn(bname);
-            CounterColumn col = client.get_counter(key, cpath, cl).column;
+CounterColumn col = client.get(key, cpath, cl).counter_column;
             assertEquals(bname, col.name);
             assertEquals(value.longValue(), col.value);
         }
---------------
-------------
@@ -53,7 +53,7 @@
      * java org.apache.derbyTesting.functionTests.tests.i18n.LocalizedSuite
      * </code>
      */
-    public static void main()
+public static void main(String[] args)
     {
         junit.textui.TestRunner.run(getSuite());
     }
---------------
-------------
@@ -118,7 +118,7 @@
 
       doHashingTest();
       doTestNumRequests();
-      // doAtomicUpdate();  TODO: this currently fails!
+doAtomicUpdate();
 
       testFinished = true;
     } finally {
---------------
-------------
@@ -48,7 +48,7 @@
     {
         TestSuite suite = new TestSuite("errorcode Test");
         
-        suite.addTest(TestConfiguration.embeddedSuite(ErrorCodeTest.class));
+suite.addTest(TestConfiguration.defaultSuite(ErrorCodeTest.class));
         
         return new LocaleTestSetup(suite, Locale.ENGLISH);
     }
---------------
-------------
@@ -1025,7 +1025,7 @@
     pq.add(new Term("field3", "text"));
     TopDocs td = is.search(pq, 10);
     assertEquals(1, td.totalHits);
-    SlowCompositeReaderWrapper wrapper = new SlowCompositeReaderWrapper(ir);
+AtomicReader wrapper = SlowCompositeReaderWrapper.wrap(ir);
     DocsAndPositionsEnum de = wrapper.termPositionsEnum(new Term("field3", "broken"));
     assert de != null;
     assertEquals(0, de.nextDoc());
---------------
-------------
@@ -66,7 +66,7 @@
 
     Path outPath = new Path(output);
     client.setConf(conf);
-    FileSystem dfs = FileSystem.get(conf);
+FileSystem dfs = FileSystem.get(outPath.toUri(), conf);
     if (dfs.exists(outPath))
       dfs.delete(outPath, true);
     InputDriver.runJob(input, output + "/data");
---------------
-------------
@@ -256,7 +256,7 @@
   /** Prints a user-readable version of this query. */
   public String toString(String f) {
     StringBuffer buffer = new StringBuffer();
-    if (!field.equals(f)) {
+if (field != null && !field.equals(f)) {
       buffer.append(field);
       buffer.append(":");
     }
---------------
-------------
@@ -74,7 +74,7 @@
   @After
   public void tearDown() throws Exception {
     super.tearDown();
-    indexWriter.close();
+indexWriter.shutdown();
     dir.close();
   }
 
---------------
-------------
@@ -658,7 +658,7 @@
    * status code that may have been returned by the remote server or a 
    * proxy along the way.
    */
-  protected static class RemoteSolrException extends SolrException {
+public static class RemoteSolrException extends SolrException {
     /**
      * @param code Arbitrary HTTP status code
      * @param msg Exception Message
---------------
-------------
@@ -252,6 +252,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new UAX29URLEmailAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new UAX29URLEmailAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -69,7 +69,7 @@
             return 1;
         }
 
-        return -baseType.compare(o1, o2);
+return baseType.compare(o2, o1);
     }
 
     public String getString(ByteBuffer bytes)
---------------
-------------
@@ -63,7 +63,7 @@
     // delete the output directory
     JobConf conf = new JobConf(MeanShiftCanopyDriver.class);
     Path outPath = new Path(output);
-    FileSystem fs = FileSystem.get(conf);
+FileSystem fs = FileSystem.get(outPath.toUri(), conf);
     if (fs.exists(outPath)) {
       fs.delete(outPath, true);
     }
---------------
-------------
@@ -525,7 +525,7 @@
       field.add("positionIncrementGap", ft.getAnalyzer().getPositionIncrementGap(f.getName()));
     }
     field.add("copyDests", toListOfStringDests(schema.getCopyFieldsList(f.getName())));
-    field.add("copySources", toListOfStrings(schema.getCopySources(f.getName())));
+field.add("copySources", schema.getCopySources(f.getName()));
 
 
     fields.put( f.getName(), field );
---------------
-------------
@@ -559,7 +559,7 @@
     return "DirectUpdateHandler2" + getStatistics();
   }
   
-  public SolrCoreState getIndexWriterProvider() {
+public SolrCoreState getSolrCoreState() {
     return solrCoreState;
   }
 
---------------
-------------
@@ -1,6 +1,6 @@
 /*
 
-   Derby - Class org.apache.derbyTesting.functionTests.harness.ibm15
+Derby - Class org.apache.derbyTesting.functionTests.harness.ibm16
 
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -34,7 +34,7 @@
       doc.add(newStringField("pk", Integer.toString(i), Field.Store.YES));
       rw.addDocument(doc);
     }
-    rw.close();
+rw.shutdown();
 
     // If buffer size is small enough to cause a flush, errors ensue...
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setOpenMode(IndexWriterConfig.OpenMode.APPEND));
---------------
-------------
@@ -104,7 +104,7 @@
 	}
 
 	public DataValueDescriptor getNewNull() {
-		return null;
+return new HeapRowLocation();
 	}
 	public void setValue(Object o) {
 	}
---------------
-------------
@@ -68,7 +68,7 @@
      */
     private synchronized void loadEndPoints(TokenMetadata metadata) throws IOException
     {
-        this.tokens = new ArrayList<Token>(tokens);
+this.tokens = new ArrayList<Token>(metadata.sortedTokens());
         String localDC = ((DatacenterEndPointSnitch)snitch_).getLocation(InetAddress.getLocalHost());
         dcMap = new HashMap<String, List<Token>>();
         for (Token token : this.tokens)
---------------
-------------
@@ -52,7 +52,7 @@
 
       @Override
       public int numOrd() {
-        return termsIndex.numOrd();
+return termsIndex.getValueCount();
       }
 
       @Override
---------------
-------------
@@ -101,7 +101,7 @@
                     "Invalid ObjectName? Please report this as a bug.", e);
         }
 
-        Map<Range, List<String>> rangeMap = ssProxy.getRangeToEndPointMap(null);
+Map<Range, List<String>> rangeMap = ssProxy.getRangeToEndpointMap(null);
         List<Range> ranges = new ArrayList<Range>(rangeMap.keySet());
         Collections.sort(ranges);
         
---------------
-------------
@@ -68,7 +68,7 @@
   public void testAutomaticDeprecationSupport()
   {
     // make sure the "admin/file" handler is registered
-    ShowFileRequestHandler handler = (ShowFileRequestHandler) h.getCore().getRequestHandler( "admin/file" );
+ShowFileRequestHandler handler = (ShowFileRequestHandler) h.getCore().getRequestHandler( "/admin/file" );
     assertTrue( "file handler should have been automatically registered", handler!=null );
 
     //System.out.println( handler.getHiddenFiles() );
---------------
-------------
@@ -148,7 +148,7 @@
   static final class JustCompileSpanScorer extends SpanScorer {
 
     protected JustCompileSpanScorer(Spans spans, Weight weight,
-        Similarity.SloppySimScorer docScorer) throws IOException {
+Similarity.SimScorer docScorer) throws IOException {
       super(spans, weight, docScorer);
     }
 
---------------
-------------
@@ -147,7 +147,7 @@
 
     int getReplayPosition()
     {
-        return cfDirtiedAt.isEmpty() ? 0 : Collections.min(cfDirtiedAt.values());
+return cfDirtiedAt.isEmpty() ? -1 : Collections.min(cfDirtiedAt.values());
     }
 
     static class CommitLogHeaderSerializer implements ICompactSerializer2<CommitLogHeader>
---------------
-------------
@@ -1055,7 +1055,7 @@
     if (XML.classpathMeetsXMLReqs()) {
         checkLangBasedQuery(s, "SELECT ID, XMLSERIALIZE(V AS CLOB) " +
         		" FROM DERBY_2961 ORDER BY 1",
-        		null);
+new String[][] {{"1",null}});
     }
     s.close();
  
---------------
-------------
@@ -37,7 +37,7 @@
       writer.addDocument(doc);
     }
     IndexReader reader = writer.getReader();
-    writer.close();
+writer.shutdown();
 
     IndexSearcher searcher = newSearcher(reader);
     TotalHitCountCollector c = new TotalHitCountCollector();
---------------
-------------
@@ -20,7 +20,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldSelector;
 import org.apache.lucene.index.values.DocValues;
-import org.apache.lucene.index.values.ValuesEnum;
+import org.apache.lucene.index.values.DocValuesEnum;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.search.FieldCache; // not great (circular); used only to purge FieldCache entry on close
---------------
-------------
@@ -214,7 +214,7 @@
                             }
                         }
                     }
-                }).start();
+}, "PERIODIC-COMMIT-LOG-SYNCER").start();
             }
         }
     }
---------------
-------------
@@ -90,7 +90,7 @@
 		int	operandType;
 		TypeId opTypeId;
 
-		super.bindExpression(fromList, subqueryList,
+bindOperand(fromList, subqueryList,
 				aggregateVector);
 
 		opTypeId = operand.getTypeId();
---------------
-------------
@@ -139,7 +139,7 @@
           }
         }
       }
-      if (!sawLiveRecovering || cnt == 120) {
+if (!sawLiveRecovering || cnt == 520) {
         if (!sawLiveRecovering) {
           if (verbose) System.out.println("no one is recoverying");
         } else {
---------------
-------------
@@ -85,7 +85,7 @@
     SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
     String[] preTags = { "[" };
     String[] postTags = { "]" };
-    assertEquals( "&lt;h1&gt; [a] &lt;/h1&gt;",
+assertEquals( "&lt;h1&gt; [a] &lt;&#x2F;h1&gt;",
         sfb.createFragment( reader, 0, F, ffl, preTags, postTags, new SimpleHTMLEncoder() ) );
   }
 
---------------
-------------
@@ -79,7 +79,7 @@
 // TODO: Maybe name this 'Cached' or something to reflect
 // the reality that it is actually written to disk, but
 // loads itself in ram?
-public class MemoryPostingsFormat extends PostingsFormat {
+public final class MemoryPostingsFormat extends PostingsFormat {
 
   private final boolean doPackFST;
   private final float acceptableOverheadRatio;
---------------
-------------
@@ -169,7 +169,7 @@
     protected PackedIntsReader(Directory dir, String id, int numDocs,
         IOContext context) throws IOException {
       datIn = dir.openInput(
-                IndexFileNames.segmentFileName(id, Bytes.DV_SEGMENT_SUFFIX, Writer.DATA_EXTENSION),
+IndexFileNames.segmentFileName(id, Bytes.DV_SEGMENT_SUFFIX, DocValuesWriterBase.DATA_EXTENSION),
           context);
       this.numDocs = numDocs;
       boolean success = false;
---------------
-------------
@@ -1321,7 +1321,7 @@
     assertTrue(q instanceof MultiPhraseQuery);
     assertEquals(1, s.search(q, 10).totalHits);
     r.close();
-    w.close();
+w.shutdown();
     dir.close();
   }
 
---------------
-------------
@@ -69,7 +69,7 @@
 
   public void runSSVDSolver(int q) throws IOException {
 
-    Configuration conf = new Configuration();
+Configuration conf = getConfiguration();
     conf.set("mapred.job.tracker", "local");
     conf.set("fs.default.name", "file:///");
 
---------------
-------------
@@ -39,7 +39,7 @@
 
     public void write(int c) {
         StringBuffer sb = new StringBuffer(clob_.string_.substring(0, (int) offset_ - 1));
-        sb.append(c);
+sb.append((char)c);
         clob_.string_ = sb.toString();
         clob_.asciiStream_ = new java.io.StringBufferInputStream(clob_.string_);
         clob_.unicodeStream_ = new java.io.StringBufferInputStream(clob_.string_);
---------------
-------------
@@ -91,7 +91,7 @@
             }
             catch (ExecutionException e)
             {
-                Thread.getDefaultUncaughtExceptionHandler().uncaughtException(Thread.currentThread(), e);
+Thread.getDefaultUncaughtExceptionHandler().uncaughtException(Thread.currentThread(), e.getCause());
             }
         }
 
---------------
-------------
@@ -34,7 +34,7 @@
  *  any text editor, and even edit it to alter your index.
  *
  *  @lucene.experimental */
-public class SimpleTextPostingsFormat extends PostingsFormat {
+public final class SimpleTextPostingsFormat extends PostingsFormat {
   
   public SimpleTextPostingsFormat() {
     super("SimpleText");
---------------
-------------
@@ -144,7 +144,7 @@
   }
 
   public void testBoostsSimple() throws Exception {
-    Map<CharSequence,Float> boosts = new HashMap<CharSequence,Float>();
+Map<String,Float> boosts = new HashMap<String,Float>();
     boosts.put("b", Float.valueOf(5));
     boosts.put("t", Float.valueOf(10));
     String[] fields = { "b", "t" };
---------------
-------------
@@ -719,7 +719,7 @@
         SolrCore newCore = core.reload(solrLoader, core);
         // keep core to orig name link
         solrCores.removeCoreToOrigName(newCore, core);
-        registerCore(false, name, newCore, false);
+registerCore(false, name, newCore, false, false);
       } finally {
         solrCores.removeFromPendingOps(name);
       }
---------------
-------------
@@ -34,7 +34,7 @@
     public void doVerb(Message message)
     {
         logger.debug("Received schema check request.");
-        Message response = message.getInternalReply(DatabaseDescriptor.getDefsVersion().toString().getBytes());
+Message response = message.getInternalReply(DatabaseDescriptor.getDefsVersion().toString().getBytes(), message.getVersion());
         MessagingService.instance().sendOneWay(response, message.getFrom());
     }
 }
---------------
-------------
@@ -1,6 +1,6 @@
 /*
  
-Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun
+Derby - Class org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun_Local_StateTest_part1_3
  
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
---------------
-------------
@@ -40,7 +40,7 @@
       doc.add(f);
       final Field idField = newField("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS);
       doc.add(idField);
-      int num = atLeast(5000);
+int num = atLeast(4097);
       for(int id=0;id<num;id++) {
         if (random.nextInt(4) == 3) {
           f.setValue("a");
---------------
-------------
@@ -1261,7 +1261,7 @@
         if ((database.securityMechanism == CodePoint.SECMEC_USRSSBPWD) &&
             (database.dbName.indexOf(Attribute.PASSWORD_ATTR) == -1))
         {
-            p.put(Attribute.CLIENT_SECURITY_MECHANISM,
+p.put(Attribute.DRDA_SECMEC,
                   String.valueOf(database.securityMechanism));
             p.put(Attribute.DRDA_SECTKN_IN,
                   DecryptionManager.toHexString(database.secTokenIn, 0,
---------------
-------------
@@ -294,7 +294,7 @@
       new MultiFieldQueryParser(new String[] {"body"}, analyzer);
     mfqp.setDefaultOperator(QueryParser.Operator.AND);
     Query q = mfqp.parse("the footest");
-    IndexSearcher is = new IndexSearcher(ramDir);
+IndexSearcher is = new IndexSearcher(ramDir, true);
     ScoreDoc[] hits = is.search(q, null, 1000).scoreDocs;
     assertEquals(1, hits.length);
     is.close();
---------------
-------------
@@ -66,7 +66,7 @@
     );
 
     assertJQ(req("qt",rh, SpellCheckComponent.COMPONENT_NAME, "true", "q","bluo", SpellCheckComponent.SPELLCHECK_COUNT,"3", SpellCheckComponent.SPELLCHECK_EXTENDED_RESULTS,"true")
-       ,"/spellcheck/suggestions/[1]/suggestion==[{'word':'blue','freq':1}, {'word':'blud','freq':1}, {'word':'boue','freq':1}]"
+,"/spellcheck/suggestions/[1]/suggestion==[{'word':'blud','freq':1}, {'word':'blue','freq':1}, {'word':'blee','freq':1}]"
     );
   }
 
---------------
-------------
@@ -678,7 +678,7 @@
 		try {
 			drdaParamState_.drainStreamedParameter();
 		} catch (IOException e) { 
-			Util.javaException(e);
+throw Util.javaException(e);
 		}
 		// java.sql.Statement says any result sets that are opened
 		// when the statement is re-executed must be closed; this
---------------
-------------
@@ -294,7 +294,7 @@
 
   public void norms(String field, byte[] bytes, int offset) throws IOException {
     byte[] norms = getIndex().getNormsByFieldNameAndDocumentNumber().get(field);
-    System.arraycopy(norms, offset, bytes, 0, norms.length);
+System.arraycopy(norms, 0, bytes, offset, norms.length);
   }
 
   protected void doSetNorm(int doc, String field, byte value) throws IOException {
---------------
-------------
@@ -120,7 +120,7 @@
         int i = 0;
         for (SSTableReader sstable : sstables)
         {
-            for (String component : sstable.getAllComponents())
+for (String component : SSTable.components)
             {
                 SSTable.Descriptor desc = sstable.getDescriptor();
                 long filelen = new File(desc.filenameFor(component)).length();
---------------
-------------
@@ -709,7 +709,7 @@
     Directory dir = newDirectory();
     IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
     cfg.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
+RandomIndexWriter iw = new RandomIndexWriter(random(), dir, cfg);
     Document doc = new Document();
     LongField field = new LongField("f", 0L, Store.YES);
     doc.add(field);
---------------
-------------
@@ -164,7 +164,7 @@
             normsOut.writeByte(defaultNorm);
         }
 
-        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : ".nrm file size mismatch: expected=" + (4+normCount*state.numDocs) + " actual=" + normsOut.getFilePointer();
+assert 4+normCount*(long)state.numDocs == normsOut.getFilePointer() : ".nrm file size mismatch: expected=" + (4+normCount*(long)state.numDocs) + " actual=" + normsOut.getFilePointer();
       }
       success = true;
     } finally {
---------------
-------------
@@ -62,7 +62,7 @@
      *
      * @return mapping of ranges to end points
      */
-    public Map<Range, List<String>> getRangeToEndPointMap(String keyspace);
+public Map<Range, List<String>> getRangeToEndpointMap(String keyspace);
 
     /**
      * Numeric load value.
---------------
-------------
@@ -264,7 +264,7 @@
 	*/
 	String DEADLOCK = "40001";
 	String LOCK_TIMEOUT = "40XL1";
-    String LOCK_TIMEOUT_LOG = "40XL2";
+String LOCK_TIMEOUT_LOG = "40XL1.T.1";
 
 	/*
 	** Store - access.protocol.Interface statement exceptions
---------------
-------------
@@ -423,7 +423,7 @@
 
             String  connectionURL = Attribute.PROTOCOL + _credentialsDB;
 
-            Connection  conn = InternalDriver.activeDriver().connect( connectionURL, properties );
+Connection  conn = InternalDriver.activeDriver().connect( connectionURL, properties, 0 );
             
             warnings = conn.getWarnings();
             conn.close();
---------------
-------------
@@ -57,7 +57,7 @@
     private boolean exhausted = false;
     private CharTermAttribute term = addAttribute(CharTermAttribute.class);
 
-    public DataTokenStream(String text, IntEncoder encoder) throws IOException {
+public DataTokenStream(String text, IntEncoder encoder) {
       this.encoder = encoder;
       term.setEmpty().append(text);
     }
---------------
-------------
@@ -149,7 +149,7 @@
              * should be pretty close to `start_key`. */
             if (logger.isDebugEnabled())
                 logger.debug(String.format("Scanning index %s starting with %s",
-                                           expressionString(primary), index.getUnderlyingCfs().getComparator().getString(startKey)));
+expressionString(primary), index.getBaseCFStore().metadata.getKeyValidator().getString(startKey)));
 
             // We shouldn't fetch only 1 row as this provides buggy paging in case the first row doesn't satisfy all clauses
             int count = Math.max(clause.count, 2);
---------------
-------------
@@ -42,6 +42,6 @@
 
 public class Constants {
 
-  public static final String VERSION = "12.0.0";
+public static final String VERSION = "12.1.0";
 
 }
---------------
-------------
@@ -523,7 +523,7 @@
 
         Table table = cfs.table;
         if (DatabaseDescriptor.isSnapshotBeforeCompaction())
-            table.snapshot("compact-" + cfs.columnFamily);
+table.snapshot(System.currentTimeMillis() + "-" + "compact-" + cfs.columnFamily);
 
         // sanity check: all sstables must belong to the same cfs
         for (SSTableReader sstable : sstables)
---------------
-------------
@@ -1148,7 +1148,7 @@
     private Similarity cachedSimilarity;
     
     @Override
-    public NumericDocValues simpleNormValues(String field) {
+public NumericDocValues getNormValues(String field) {
       if (fieldInfos.get(field).omitsNorms())
         return null;
       NumericDocValues norms = cachedNormValues;
---------------
-------------
@@ -42,6 +42,6 @@
 
 public class Constants {
 
-  public static final String VERSION = "7.0.0";
+public static final String VERSION = "8.0.0";
 
 }
---------------
-------------
@@ -55,7 +55,7 @@
     @Override
     public int getLocalDeletionTime()
     {
-       return value.getInt(value.position()+value.arrayOffset()	);
+return value.getInt(value.position());
     }
     
     @Override
---------------
-------------
@@ -157,7 +157,7 @@
       String name = (String) entry.getName();
       String objectAttrs = entry.getValue();
 
-      String type = getType(name);
+String type = (entry.getType() == null) ? getType(name) : entry.getType();
 
       // remove the beginning " and tailing "
       if (objectAttrs.startsWith("\"") && objectAttrs.endsWith("\""))
---------------
-------------
@@ -560,7 +560,7 @@
 	{
 	    if ( ! isOpen ) 
 		{
-			throw StandardException.newException(SQLState.LANG_RESULT_SET_NOT_OPEN, "next");
+throw StandardException.newException(SQLState.LANG_RESULT_SET_NOT_OPEN, "previous");
 		}
 
 		if (SanityManager.DEBUG)
---------------
-------------
@@ -39,7 +39,7 @@
    *          The </code>Similarity.ExactSimScorer</code> implementation 
    *          to be used for score computations.
    */
-  TermScorer(Weight weight, DocsEnum td, Similarity.ExactSimScorer docScorer) throws IOException {
+TermScorer(Weight weight, DocsEnum td, Similarity.ExactSimScorer docScorer) {
     super(weight);
     this.docScorer = docScorer;
     this.docsEnum = td;
---------------
-------------
@@ -597,7 +597,7 @@
         }
       }
 
-      int len = docsGathered - offset;
+int len = docsGathered > offset ? docsGathered - offset : 0;
       int[] docs = ArrayUtils.toPrimitive(ids.toArray(new Integer[ids.size()]));
       float[] docScores = ArrayUtils.toPrimitive(scores.toArray(new Float[scores.size()]));
       DocSlice docSlice = new DocSlice(offset, len, docs, docScores, getMatches(), maxScore);
---------------
-------------
@@ -64,7 +64,7 @@
 
 	</UL>
 	<P>
-	The internal identifier of a cached statement matches the toString() method of a PreparedStatement object for a Cloudscape database.
+The internal identifier of a cached statement matches the toString() method of a PreparedStatement object for a Derby database.
 
 	<P>
 	This class also provides a static method to empty the statement cache, StatementCache.emptyCache()
---------------
-------------
@@ -65,7 +65,7 @@
     try {
       IndexReader reader = new SlowMultiReaderWrapper(IndexReader.open(ramDir, true));
       try {
-        assertNull(filter.getDocIdSet((AtomicReaderContext) reader.getTopReaderContext()));
+assertNull(filter.getDocIdSet((AtomicReaderContext) reader.getTopReaderContext(), reader.getLiveDocs()));
       }
       finally {
         reader.close();
---------------
-------------
@@ -227,7 +227,7 @@
     Terms cterms = fields.terms(term.field);
     TermsEnum ctermsEnum = cterms.iterator(null);
     if (ctermsEnum.seekExact(new BytesRef(term.text()), false)) {
-      DocsEnum docsEnum = _TestUtil.docs(random(), ctermsEnum, bits, null, false);
+DocsEnum docsEnum = _TestUtil.docs(random(), ctermsEnum, bits, null, 0);
       return toArray(docsEnum);
     }
     return null;
---------------
-------------
@@ -86,7 +86,7 @@
     public java.io.InputStream getAsciiStream(String columnName) throws SQLException { throw notImplemented( "io.InputStream getAsciiStream" ); }
     public java.io.InputStream getUnicodeStream(String columnName) throws SQLException { throw notImplemented( "io.InputStream getUnicodeStream" ); }
     public java.io.InputStream getBinaryStream(String columnName) throws SQLException { throw notImplemented( "io.InputStream getBinaryStream" ); }
-    public SQLWarning getWarnings() throws SQLException { throw notImplemented( "getWarnings" ); }
+public SQLWarning getWarnings() throws SQLException { return null; }
     public void clearWarnings() throws SQLException { throw notImplemented( "clearWarnings" ); }
     public String getCursorName() throws SQLException { throw notImplemented( "getCursorName" ); }
     public Object getObject(int columnIndex) throws SQLException { throw notImplemented( "getObject" ); }
---------------
-------------
@@ -180,7 +180,7 @@
   private static final class HyperbolicSweetSpotSimilarity 
     extends SweetSpotSimilarity {
     @Override
-    public float tf(int freq) {
+public float tf(float freq) {
       return hyperbolicTf(freq);
     }
   };
---------------
-------------
@@ -86,7 +86,7 @@
       add(docText[i%docText.length], iw);
     }
     reader = iw.getReader();
-    iw.close();
+iw.shutdown();
     searcher = newSearcher(reader);
 
     BooleanQuery booleanQuery = new BooleanQuery();
---------------
-------------
@@ -229,7 +229,7 @@
         if (postingsEnum == null) {
           assert (reader.termDocsEnum(liveDocs, t.field(), t.bytes(), state) != null) : "termstate found but no term exists in reader";
           // term does exist, but has no positions
-          throw new IllegalStateException("field \"" + t.field() + "\" was indexed with Field.omitTermFreqAndPositions=true; cannot run PhraseQuery (term=" + t.text() + ")");
+throw new IllegalStateException("field \"" + t.field() + "\" was indexed without position data; cannot run PhraseQuery (term=" + t.text() + ")");
         }
         // get the docFreq without seeking
         TermsEnum te = reader.fields().terms(field).getThreadTermsEnum();
---------------
-------------
@@ -365,7 +365,7 @@
         w.deleteDocuments(new Term("id", ""+random().nextInt(i+1)));
       }
     }
-    assertTrue(((TrackingCMS) iwc.getMergeScheduler()).totMergedBytes != 0);
+assertTrue(((TrackingCMS) w.w.getConfig().getMergeScheduler()).totMergedBytes != 0);
     w.close();
     d.close();
   }
---------------
-------------
@@ -135,7 +135,7 @@
     {
         super(new File(dataFilePath), metadata.chunkLength, skipIOCache);
         this.metadata = metadata;
-        compressed = new byte[metadata.chunkLength];
+compressed = new byte[Snappy.maxCompressedLength(metadata.chunkLength)];
         // can't use super.read(...) methods
         // that is why we are allocating special InputStream to read data from disk
         // from already open file descriptor
---------------
-------------
@@ -25,7 +25,7 @@
 
     protected void testSerializeDeserialize(ColumnDefinition cd) throws Exception
     {
-        ColumnDefinition newCd = ColumnDefinition.deserialize(ColumnDefinition.serialize(cd));
+ColumnDefinition newCd = ColumnDefinition.inflate(cd.deflate());
         assert cd != newCd;
         assert cd.hashCode() == newCd.hashCode();
         assert cd.equals(newCd);
---------------
-------------
@@ -111,7 +111,7 @@
   }
 
   public static final String SNAP_DIR = "snapDir";
-  public static final String DATE_FMT = "yyyyMMddhhmmss";
+public static final String DATE_FMT = "yyyyMMddHHmmss";
   
 
   private class FileCopier {
---------------
-------------
@@ -40,7 +40,7 @@
 
   /* @override constructor */
   public TestOrdValues(String name) {
-    super(name);
+super(name, false);
   }
 
   /** Test OrdFieldSource */
---------------
-------------
@@ -486,7 +486,7 @@
       fail("fake disk full IOExceptions not hit");
     } catch (IOException ioe) {
       // expected
-      assertTrue(ftdm.didFail1);
+assertTrue(ftdm.didFail1 || ftdm.didFail2);
     }
     _TestUtil.checkIndex(dir);
     ftdm.clearDoFail();
---------------
-------------
@@ -118,7 +118,7 @@
         BundleTrackerCustomizer customizer = Skeleton.newMock(BundleTrackerCustomizer.class);
 
         sut = new InternalRecursiveBundleTracker(context, 
-                Bundle.INSTALLED | Bundle.STARTING | Bundle.ACTIVE | Bundle.STOPPING, customizer);
+Bundle.INSTALLED | Bundle.STARTING | Bundle.ACTIVE | Bundle.STOPPING, customizer, true);
         
         sut.open();
     }
---------------
-------------
@@ -4047,7 +4047,7 @@
         message("merge store matchedCount=" + merger.getMatchedSubReaderCount() + " vs " + merge.readers.size());
       }
 
-      anyNonBulkMerges |= merger.getMatchedSubReaderCount() != merge.readers.size();
+anyNonBulkMerges |= merger.getAnyNonBulkMerges();
       
       assert mergedDocCount == totDocCount: "mergedDocCount=" + mergedDocCount + " vs " + totDocCount;
 
---------------
-------------
@@ -172,7 +172,7 @@
         final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(
               "OutOfMemoryError likely caused by the Sun VM Bug described in "
               + "https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize "
-              + "with a a value smaller than the current chunk size (" + chunkSize + ")");
++ "with a value smaller than the current chunk size (" + chunkSize + ")");
         outOfMemoryError.initCause(e);
         throw outOfMemoryError;
       }
---------------
-------------
@@ -197,7 +197,7 @@
 	/**
 	 * The key PERSISTENTLY_STARTED, used in {@link #PERSISTENTLY_STARTED_ITEM}.
 	 */
-	String PERSISTENTLY_STARTED = "PeristentlyStarted";
+String PERSISTENTLY_STARTED = "PersistentlyStarted";
 
 	/**
 	 * The item containing the indication of persistently started in
---------------
-------------
@@ -135,7 +135,7 @@
     int maxPrefsPerUserInItemSimilarity = Integer.parseInt(getOption("maxPrefsPerUserInItemSimilarity"));
     int maxSimilaritiesPerItem = Integer.parseInt(getOption("maxSimilaritiesPerItem"));
     String similarityClassname = getOption("similarityClassname");
-    double threshold = parsedArgs.containsKey("threshold") ?
+double threshold = hasOption("threshold") ?
             Double.parseDouble(getOption("threshold")) : RowSimilarityJob.NO_THRESHOLD;
 
 
---------------
-------------
@@ -492,7 +492,7 @@
     ToolRunner.run(conf, new MeanShiftCanopyDriver(), args);
     Path outPart = new Path(output, "clusters-3-final/part-r-00000");
     long count = HadoopUtil.countRecords(outPart, conf);
-    assertEquals("count", 4, count);
+assertEquals("count", 3, count);
     Iterator<?> iterator = new SequenceFileValueIterator<Writable>(outPart,
         true, conf);
     while (iterator.hasNext()) {
---------------
-------------
@@ -333,7 +333,7 @@
               result.put(colName, resultSet.getDouble(colName));
               break;
             case Types.DATE:
-              result.put(colName, resultSet.getDate(colName));
+result.put(colName, resultSet.getTimestamp(colName));
               break;
             case Types.BOOLEAN:
               result.put(colName, resultSet.getBoolean(colName));
---------------
-------------
@@ -86,7 +86,7 @@
                 addStreamContext(message.getFrom(), localFile, streamStatus);
             }
 
-            StreamInManager.registerStreamCompletionHandler(message.getFrom(), new StreamCompletionHandler());
+StreamInManager.registerFileStatusHandler(message.getFrom(), new FileStatusHandler());
             if (logger.isDebugEnabled())
               logger.debug("Sending a stream initiate done message ...");
             Message doneMessage = new Message(FBUtilities.getLocalAddress(), "", StorageService.Verb.STREAM_INITIATE_DONE, new byte[0] );
---------------
-------------
@@ -202,7 +202,7 @@
       //then be sure all of the new words have the same optional/required/prohibited status in the query.
       while(indexOfSpace>-1 && indexOfSpace<corr.length()-1) {
         addParenthesis = true;
-        char previousChar = tok.startOffset()>0 ? collation.charAt(tok.startOffset()-1) : ' ';
+char previousChar = tok.startOffset()>0 ? origQuery.charAt(tok.startOffset()-1) : ' ';
         if(previousChar=='-' || previousChar=='+') {
           corrSb.insert(indexOfSpace + bump, previousChar);
           if(requiredOrProhibited==null) {
---------------
-------------
@@ -67,7 +67,7 @@
       return ((TermPositions) this.in).nextPosition();
     }
     
-    public int getPayloadLength() {
+public int getPayloadLength() throws IOException {
       return ((TermPositions) this.in).getPayloadLength();
     }
 
---------------
-------------
@@ -49,7 +49,7 @@
  * to the field, making it useful as a component of the main query or a boosting query.
  */
 public class SpatialFilterQParserPlugin extends QParserPlugin {
-  public static String NAME = "geofilt";
+public static final String NAME = "geofilt";
 
   @Override
   public QParser createParser(String qstr, SolrParams localParams,
---------------
-------------
@@ -55,7 +55,7 @@
  */
 
 /** A clause in a BooleanQuery. */
-public class BooleanClause {
+public class BooleanClause implements java.io.Serializable {
   /** The query whose matching documents are combined by the boolean query. */
   public Query query;
   /** If true, documents documents which <i>do not</i>
---------------
-------------
@@ -595,7 +595,7 @@
         }
 
         // derby-5490. workaround problem if executable name contains spaces
-        if ( vmname.contains( " " ) )
+if ( vmname.indexOf( " " ) >= 0 )
         {
             if ( getSystemProperty( "os.name" ).equals( "Mac OS X" ) )
             {
---------------
-------------
@@ -1280,7 +1280,7 @@
 	
 	/** Returns a list of statistics for this table.
 	 */
-	private synchronized List getStatistics() throws StandardException
+public synchronized List getStatistics() throws StandardException
 	{
 		// if table already has the statistics descriptors initialized
 		// no need to do anything
---------------
-------------
@@ -90,7 +90,7 @@
         testOne(de, bDocIDs);
       }
 
-      w.close();
+w.shutdown();
       r.close();
       dir.close();
     }
---------------
-------------
@@ -114,7 +114,7 @@
 			String s = stemmer.stem( token.termText() );
 			// If not stemmed, dont waste the time creating a new token
 			if ( !s.equals( token.termText() ) ) {
-				return new Token( s, 0, s.length(), token.type() );
+return new Token( s, token.startOffset(), token.endOffset(), token.type());
 			}
 			return token;
 		}
---------------
-------------
@@ -131,7 +131,7 @@
       writer.addDocument(doc);
     }
     reader = DirectoryReader.open(writer, true);
-    writer.close();
+writer.shutdown();
 
     IndexSearcher searcher = LuceneTestCase.newSearcher(reader);
     searcher.setSimilarity(similarity);
---------------
-------------
@@ -125,7 +125,7 @@
 
             protected Row getReduced()
             {
-                Comparator<IColumn> colComparator = QueryFilter.getColumnComparator(comparator);
+Comparator<IColumn> colComparator = filter.filter.getColumnComparator(comparator);
                 Iterator<IColumn> colCollated = IteratorUtils.collatedIterator(colComparator, colIters);
 
                 ColumnFamily returnCF = null;
---------------
-------------
@@ -57,7 +57,7 @@
     
     NumberFormat df = new DecimalFormat("000", new DecimalFormatSymbols(Locale.ENGLISH));
     for (int i = 0; i < 1000; i++) {
-      field.setValue(df.format(i));
+field.setStringValue(df.format(i));
       writer.addDocument(doc);
     }
     
---------------
-------------
@@ -164,7 +164,7 @@
         BufferedRandomAccessFile input = new BufferedRandomAccessFile(SSTable.indexFilename(ssTableFile), "r");
         while (!input.isEOF())
         {
-            DecoratedKey decoratedKey = partitioner.convertFromDiskFormat(input.readUTF());
+DecoratedKey decoratedKey = partitioner.convertFromDiskFormat(FBUtilities.readShortByteArray(input));
             long dataPosition = input.readLong();
             outs.println(asStr(decoratedKey.key));
         }
---------------
-------------
@@ -552,7 +552,7 @@
 			if (leftRC != null)
 			{
 				throw StandardException.newException(SQLState.LANG_AMBIGUOUS_COLUMN_NAME, 
-						 columnReference.getFullColumnName());
+columnReference.getSQLColumnName());
 			}
 			resultColumn = rightRC;
 		}
---------------
-------------
@@ -1137,7 +1137,7 @@
       final Class<? extends Directory> clazz = Class.forName(clazzName).asSubclass(Directory.class);
       // If it is a FSDirectory type, try its ctor(File)
       if (FSDirectory.class.isAssignableFrom(clazz)) {
-        final File tmpFile = File.createTempFile("test", "tmp", TEMP_DIR);
+final File tmpFile = _TestUtil.createTempFile("test", "tmp", TEMP_DIR);
         tmpFile.delete();
         tmpFile.mkdir();
         registerTempFile(tmpFile);
---------------
-------------
@@ -722,7 +722,7 @@
       writer.close();
       dir.close();
       if (exception.get() != null) {
-        throw new AssertionError("One thread threw an exception", exception.get());
+throw new RuntimeException("One thread threw an exception", exception.get());
       }
     }
   }
---------------
-------------
@@ -113,7 +113,7 @@
 	{
         try
         {
-            Message message = RowMutationMessage.makeRowMutationMessage(rowMutationMessage, StorageService.readRepairVerbHandler_);
+Message message = rowMutationMessage.makeRowMutationMessage(StorageService.readRepairVerbHandler_);
     		String key = target + ":" + message.getMessageId();
     		readRepairTable_.put(key, message);
         }
---------------
-------------
@@ -36,7 +36,7 @@
 class LoadError {
 	
 	/**
-	 Raised if, the Cloudscape database connection is null.
+Raised if, the Derby database connection is null.
 	*/
 
 	static SQLException connectionNull() {
---------------
-------------
@@ -432,7 +432,7 @@
 
             InternalDriver driver = InternalDriver.activeDriver();
             if (driver != null) {
-                driver.connect(conStr, (Properties) null);
+driver.connect( conStr, (Properties) null, 0 );
             }
         } catch (Exception e) {
             // Todo: report error to derby.log if exception is not
---------------
-------------
@@ -118,7 +118,7 @@
    */
   @Deprecated
   public StopFilter(boolean enablePositionIncrements, TokenStream in, Set<?> stopWords) {
-    this(Version.LUCENE_CURRENT, enablePositionIncrements, in, stopWords, false);
+this(Version.LUCENE_30, enablePositionIncrements, in, stopWords, false);
   }
   
   /**
---------------
-------------
@@ -156,7 +156,7 @@
         SpawnedProcess spawned = new SpawnedProcess( process, "UnbootedTest" );
         
         // Ensure it completes without failures.
-        assertEquals(0, spawned.complete(false));
+assertEquals(0, spawned.complete());
 
         assertEquals( SUCCESS, spawned.getFullServerOutput() );
     }
---------------
-------------
@@ -51,6 +51,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new PortugueseAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), new PortugueseAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
 }
---------------
-------------
@@ -42,7 +42,7 @@
     subDocsEnum = new DocsEnum[subReaderCount];
   }
 
-  MultiDocsEnum reset(final EnumWithSlice[] subs, final int numSubs) throws IOException {
+MultiDocsEnum reset(final EnumWithSlice[] subs, final int numSubs) {
     this.numSubs = numSubs;
 
     this.subs = new EnumWithSlice[subs.length];
---------------
-------------
@@ -90,7 +90,7 @@
       }
       
     }};
-    BooleanScorer bs = new BooleanScorer(sim, 1, Arrays.asList(scorers), null);
+BooleanScorer bs = new BooleanScorer(sim, 1, Arrays.asList(scorers), null, scorers.length);
     
     assertEquals("should have received 3000", 3000, bs.nextDoc());
     assertEquals("should have received NO_MORE_DOCS", DocIdSetIterator.NO_MORE_DOCS, bs.nextDoc());
---------------
-------------
@@ -52,7 +52,7 @@
     writer.addDocument(adoc(new String[] {"id", "z", "title", "boosted boosted boosted","str_s", "z"}));
 
     IndexReader r = DirectoryReader.open(writer, true);
-    writer.close();
+writer.shutdown();
 
     IndexSearcher searcher = newSearcher(r);
     searcher.setSimilarity(new DefaultSimilarity());
---------------
-------------
@@ -56,7 +56,7 @@
     public void testDummy() throws Exception {
       makeIndex();
       assertNotNull(FieldCache.DEFAULT.getTermsIndex(subR, "ints"));
-      assertNotNull(FieldCache.DEFAULT.getTerms(subR, "ints"));
+assertNotNull(FieldCache.DEFAULT.getTerms(subR, "ints", false));
       // NOTE: do not close reader/directory, else it
       // purges FC entries
     }
---------------
-------------
@@ -349,7 +349,7 @@
     // test ensureCapacityWords
     int numWords = random().nextInt(10) + 2; // make sure we grow the array (at least 128 bits)
     bits.ensureCapacityWords(numWords);
-    bit = _TestUtil.nextInt(random(), 128, numWords << 6); // pick a higher bit than 128, but still within range
+bit = _TestUtil.nextInt(random(), 127, (numWords << 6)-1); // pick a bit >= to 128, but still within range
     bits.fastSet(bit);
     assertTrue(bits.fastGet(bit));
     bits.fastClear(bit);
---------------
-------------
@@ -75,7 +75,7 @@
 		Fully create the bytecode and load the
 		class using the ClassBuilder's ClassFactory.
 
-		@exception StandardException Standard Cloudscape policy
+@exception StandardException Standard Derby policy
 	*/
 	GeneratedClass getGeneratedClass() throws StandardException;
 
---------------
-------------
@@ -79,7 +79,7 @@
     // delete the output directory
     JobConf conf = new JobConf(DirichletJob.class);
     Path outPath = new Path(output);
-    FileSystem fs = FileSystem.get(conf);
+FileSystem fs = FileSystem.get(outPath.toUri(), conf);
     if (fs.exists(outPath)) {
       fs.delete(outPath, true);
     }
---------------
-------------
@@ -64,7 +64,7 @@
     DirectoryReader iwReader = iw.getReader();
     assertEquals(3, iwReader.leaves().size());
     iwReader.close();
-    iw.close();
+iw.shutdown();
     // we should have 2 segments now
     IndexSplitter is = new IndexSplitter(dir);
     String splitSegName = is.infos.info(1).info.name;
---------------
-------------
@@ -42,7 +42,7 @@
    *          The </code>Similarity.ExactSimScorer</code> implementation 
    *          to be used for score computations.
    */
-  MatchOnlyTermScorer(Weight weight, DocsEnum td, Similarity.ExactSimScorer docScorer) throws IOException {
+MatchOnlyTermScorer(Weight weight, DocsEnum td, Similarity.ExactSimScorer docScorer) {
     super(weight);
     this.docScorer = docScorer;
     this.docsEnum = td;
---------------
-------------
@@ -52,7 +52,7 @@
       
   /** Creates a new HMMChineseTokenizer, supplying the AttributeFactory */
   public HMMChineseTokenizer(AttributeFactory factory) {
-    super((BreakIterator)sentenceProto.clone());
+super(factory, (BreakIterator)sentenceProto.clone());
   }
 
   @Override
---------------
-------------
@@ -1051,7 +1051,7 @@
     dir1.close();
 
     try {
-      dir1.list();
+dir1.listAll();
       fail("did not hit AlreadyClosedException");
     } catch (AlreadyClosedException ace) {
       // expected
---------------
-------------
@@ -49,7 +49,7 @@
       @Override
       public void evaluate() throws Throwable {
         if (NestedTestSuite.class.isAssignableFrom(d.getTestClass())) {
-          LuceneTestCase.assumeTrue("Nested suite class ignored (started as stand-along).",
+LuceneTestCase.assumeTrue("Nested suite class ignored (started as stand-alone).",
               isRunningNested());
         }
         s.evaluate();
---------------
-------------
@@ -521,7 +521,7 @@
   protected abstract void doClose() throws IOException;
 
   /** Release the write lock, if needed. */
-  protected final void finalize() throws IOException {
+protected final void finalize() {
     if (writeLock != null) {
       writeLock.release();                        // release write lock
       writeLock = null;
---------------
-------------
@@ -39,7 +39,7 @@
     return new CJKWidthFilter(input);
   }
   
-  @Override
+//@Override
   public Object getMultiTermComponent() {
     return this;
   }
---------------
-------------
@@ -225,7 +225,7 @@
         //System.out.println("Term: " + term);
         assertEquals(testTerms[i], term);
         
-        docsEnum = _TestUtil.docs(random(), termsEnum, null, docsEnum, 0);
+docsEnum = _TestUtil.docs(random(), termsEnum, null, docsEnum, false);
         assertNotNull(docsEnum);
         int doc = docsEnum.docID();
         assertTrue(doc == -1 || doc == DocIdSetIterator.NO_MORE_DOCS);
---------------
-------------
@@ -56,7 +56,7 @@
     /** creates a type 1 uuid from raw bytes. */
     public static UUID getUUID(ByteBuffer raw)
     {
-        return new UUID(raw.getLong(raw.position() + raw.arrayOffset()), raw.getLong(raw.position() + raw.arrayOffset() + 8));
+return new UUID(raw.getLong(raw.position()), raw.getLong(raw.position() + 8));
     }
 
     /** decomposes a uuid into raw bytes. */
---------------
-------------
@@ -36,7 +36,7 @@
   /** Returns the number of fields or -1 if the number of
    * distinct field names is unknown. If &gt;= 0,
    * {@link #iterator} will return as many field names. */
-  public abstract int size() throws IOException;
+public abstract int size();
   
   /** Returns the number of terms for all fields, or -1 if this 
    *  measure isn't stored by the codec. Note that, just like 
---------------
-------------
@@ -418,7 +418,7 @@
                     modifiedRowCount > 1 ? (order >= lastOrder) :
                         (order > lastOrder);
                 assertTrue("matching triggers need to be fired in order creation:"
-                        +info, orderOk);
++info+". Triggers got fired in this order:"+TRIGGER_INFO.get().toString(), orderOk);
                 lastOrder = order;
                 continue;
             }
---------------
-------------
@@ -159,7 +159,7 @@
         {
             JSONObject json = (JSONObject)JSONValue.parseWithException(new FileReader(jsonFile));
             
-            SSTableWriter writer = new SSTableWriter(ssTablePath, json.size(), partitioner);
+SSTableWriter writer = new SSTableWriter(ssTablePath, json.size());
             SortedMap<DecoratedKey,String> decoratedKeys = new TreeMap<DecoratedKey,String>();
             
             // sort by dk representation, but hold onto the hex version
---------------
-------------
@@ -32,7 +32,7 @@
  * This is generally equivalent to the Lucene query parser expression <code>myfield:"Foo Bar"</code>
  */
 public class FieldQParserPlugin extends QParserPlugin {
-  public static String NAME = "field";
+public static final String NAME = "field";
 
   @Override
   public void init(NamedList args) {
---------------
-------------
@@ -47,7 +47,7 @@
 
   @BeforeClass
   public static void beforeClass() throws Exception {
-    initCore("solrConfig.xml", "schema.xml");
+initCore("solrconfig.xml", "schema.xml");
     parser = new SolrRequestParsers( h.getCore().getSolrConfig() );
   }
   
---------------
-------------
@@ -91,7 +91,7 @@
     writer.close(true);
 
     DirectoryReader reader = DirectoryReader.open(dir, 1);
-    assertEquals(1, reader.getSequentialSubReaders().size());
+assertEquals(1, reader.leaves().size());
 
     IndexSearcher searcher = new IndexSearcher(reader);
 
---------------
-------------
@@ -228,7 +228,7 @@
     // nanoseconds precision: yyyy-mm-dd-hh.mm.ss.ffffff
     // In contrast, JDBC supports full nanoseconds precision: yyyy-mm-dd-hh.mm.ss.fffffffff
     //
-    public   static final int DRDA_TIMESTAMP_LENGTH = 26;
+public   static final int DRDA_TIMESTAMP_LENGTH = 29;
     public   static final int JDBC_TIMESTAMP_LENGTH = 29;
 
     // Values for the EXTDTA stream status byte.
---------------
-------------
@@ -44,7 +44,7 @@
         for (int j = 0; j < insertsPerTable; j++) {
             DecoratedKey key = Util.dk(String.valueOf(j));
             RowMutation rm = new RowMutation("Keyspace1", key.key);
-            rm.add(new QueryPath(columnFamilyName, null, "0".getBytes()), new byte[0], j);
+rm.add(new QueryPath(columnFamilyName, null, "0".getBytes()), new byte[0], new TimestampClock(j));
             rm.apply();
             inserted.add(key);
             store.forceBlockingFlush();
---------------
-------------
@@ -71,7 +71,7 @@
     public PayloadFilter(TokenStream input, String fieldName) {
       super(input);
       this.fieldName = fieldName;
-      payAtt = (PayloadAttribute) addAttribute(PayloadAttribute.class);
+payAtt = addAttribute(PayloadAttribute.class);
     }
 
     @Override
---------------
-------------
@@ -85,7 +85,7 @@
             Gossiper.instance.examineGossiper(gDigestList, deltaGossipDigestList, deltaEpStateMap);
 
             GossipDigestAckMessage gDigestAck = new GossipDigestAckMessage(deltaGossipDigestList, deltaEpStateMap);
-            Message gDigestAckMessage = Gossiper.instance.makeGossipDigestAckMessage(gDigestAck);
+Message gDigestAckMessage = Gossiper.instance.makeGossipDigestAckMessage(gDigestAck, message.getVersion());
             if (logger_.isTraceEnabled())
                 logger_.trace("Sending a GossipDigestAckMessage to {}", from);
             MessagingService.instance().sendOneWay(gDigestAckMessage, from);
---------------
-------------
@@ -743,7 +743,7 @@
     // builder's buffer.
 
     StringBuilder sb = new StringBuilder();
-    if (cal==null) cal = Calendar.getInstance(TimeZone.getTimeZone("GMT"));
+if (cal==null) cal = Calendar.getInstance(TimeZone.getTimeZone("GMT"), Locale.US);
     cal.setTime(val);
 
     int i = cal.get(Calendar.YEAR);
---------------
-------------
@@ -315,7 +315,7 @@
     IndexReader r = writer.getReader();
 
     // Test each sub-segment
-    for (AtomicReaderContext ctx : r.getTopReaderContext().leaves()) {
+for (AtomicReaderContext ctx : r.leaves()) {
       checkTermsOrder(ctx.reader(), allTerms, false);
     }
     checkTermsOrder(r, allTerms, true);
---------------
-------------
@@ -73,7 +73,7 @@
 
 
   @Override
-  protected FieldType create( ResourceLoader loader, 
+protected FieldType create( SolrResourceLoader loader,
                               String name, 
                               String className, 
                               Node node ) throws Exception {
---------------
-------------
@@ -352,7 +352,7 @@
                     int seenTermCount = 0;
                     int shift;
                     int trigger; 
-                    if (totTermCount.get() < 10) {
+if (totTermCount.get() < 30) {
                       shift = 0;
                       trigger = 1;
                     } else {
---------------
-------------
@@ -70,6 +70,6 @@
           }
     sb.setLength(sb.length() - 2); // discard last ", "
     sb.append("]");
-    return super.toString();
+return sb.toString();
   }
 }
---------------
-------------
@@ -228,7 +228,7 @@
         MessagingService.instance.registerVerbHandlers(Verb.GOSSIP_DIGEST_ACK2, new GossipDigestAck2VerbHandler());
 
         replicationStrategies = new HashMap<String, AbstractReplicationStrategy>();
-        for (String table : DatabaseDescriptor.getTables())
+for (String table : DatabaseDescriptor.getNonSystemTables())
         {
             AbstractReplicationStrategy strat = getReplicationStrategy(tokenMetadata_, table);
             replicationStrategies.put(table, strat);
---------------
-------------
@@ -263,7 +263,7 @@
 	*/
 	String DEADLOCK = "40001";
 	String LOCK_TIMEOUT = "40XL1";
-    String LOCK_TIMEOUT_LOG = "40XL2";
+String LOCK_TIMEOUT_LOG = "40XL1.T.1";
 
 	/*
 	** Store - access.protocol.Interface statement exceptions
---------------
-------------
@@ -60,7 +60,7 @@
 
   @Override
   public void processDelete(DeleteUpdateCommand cmd) throws IOException {
-    if( cmd.id != null ) {
+if( cmd.isDeleteById()) {
       updateHandler.delete(cmd);
     }
     else {
---------------
-------------
@@ -751,7 +751,7 @@
   }
   
   
-  @Test @Ignore("https://issues.apache.org/jira/browse/SOLR-5343")
+@Test
   public void doTestStressReplication() throws Exception {
     // change solrconfig on slave
     // this has no entry for pollinginterval
---------------
-------------
@@ -54,7 +54,7 @@
     	ByteArrayOutputStream bos = new ByteArrayOutputStream();
         DataOutputStream dos = new DataOutputStream(bos);
         TruncateResponse.serializer().serialize(truncateResponseMessage, dos);
-        return original.getReply(FBUtilities.getLocalAddress(), bos.toByteArray());
+return original.getReply(FBUtilities.getLocalAddress(), bos.toByteArray(), original.getVersion());
     }
 
     public TruncateResponse(String keyspace, String columnFamily, boolean success) {
---------------
-------------
@@ -125,7 +125,7 @@
         }
         
         writer.close();
-        IndexReader reader = IndexReader.open(directory);
+IndexReader reader = IndexReader.open(directory, true);
         TermPositions tp = reader.termPositions();
         tp.seek(new Term(this.field, "b"));
         for (int i = 0; i < 10; i++) {
---------------
-------------
@@ -890,7 +890,7 @@
 
     public void onJoin(InetAddress endpoint, EndpointState epState)
     {
-        for (Map.Entry<String,ApplicationState> entry : epState.getSortedApplicationStates())
+for (Map.Entry<String,ApplicationState> entry : epState.getApplicationStateMap().entrySet())
         {
             onChange(endpoint, entry.getKey(), entry.getValue());
         }
---------------
-------------
@@ -128,7 +128,7 @@
 
     public Row getRow(Table table) throws IOException, ColumnFamilyNotDefinedException
     {
-        if (columnNames != EMPTY_COLUMNS)
+if (!columnNames.isEmpty())
         {
             return table.getRow(key, columnFamilyColumn, columnNames);
         }
---------------
-------------
@@ -391,7 +391,7 @@
 	/**
 		Check that there are not output parameters defined
 		by the parameter set. If there are unknown parameter
-		types they are forced to input types. i.e. Cloudscape static method
+types they are forced to input types. i.e. Derby static method
 		calls with parameters that are array.
 
 		@return true if a declared Java Procedure INOUT or OUT parameter is in the set, false otherwise.
---------------
-------------
@@ -44,7 +44,7 @@
             return false;
         }
         @Override
-        public void release() {
+public void close() {
             // do nothing
         }
         @Override
---------------
-------------
@@ -62,7 +62,7 @@
         Range range3 = ss.getPrimaryRangeForEndPoint(three);
         Token fakeToken = ((IPartitioner)StorageService.getPartitioner()).midpoint(range3.left(), range3.right());
         assert range3.contains(fakeToken);
-        ss.onChange(myEndpoint, StorageService.STATE_BOOTSTRAPPING, new ApplicationState(ss.getPartitioner().getTokenFactory().toString(fakeToken)));
+ss.onChange(myEndpoint, StorageService.MOVE_STATE, new ApplicationState(StorageService.STATE_BOOTSTRAPPING + StorageService.Delimiter + ss.getPartitioner().getTokenFactory().toString(fakeToken)));
         tmd = ss.getTokenMetadata();
 
         InetAddress source2 = BootStrapper.getBootstrapSource(tmd, load);
---------------
-------------
@@ -38,7 +38,7 @@
 
     public void reset(FileMark mark) throws IOException;
 
-    public int bytesPastMark(FileMark mark);
+public long bytesPastMark(FileMark mark);
 
     /**
      * Read length bytes from current file position
---------------
-------------
@@ -51,7 +51,7 @@
             "resultsetJdbc20",           
             
             // from old jdbcapi.runall
-            "derbyStress",
+// "derbyStress",       TODO: Need a way to control heap size from Junit tests
             // "nullSQLText",  TODO: investigate failure/convert
             // "prepStmtMetaData",  TODO: convert - different canon for client
             // "resultsetStream", TODO: investigate failure/convert needs ext files
---------------
-------------
@@ -164,7 +164,7 @@
 
   private void addDoc(RandomIndexWriter w, Collection<String> terms, Map<BytesRef,Integer> termToID, int id) throws IOException {
     Document doc = new Document();
-    doc.add(new NumericField("id").setIntValue(id));
+doc.add(new NumericField("id", id));
     if (VERBOSE) {
       System.out.println("TEST: addDoc id:" + id + " terms=" + terms);
     }
---------------
-------------
@@ -803,7 +803,7 @@
       // These characters are part of the query syntax and must be escaped
       if (c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':'
         || c == '^' || c == '[' || c == ']' || c == '\"' || c == '{' || c == '}' || c == '~'
-        || c == '*' || c == '?') {
+|| c == '*' || c == '?' || c == '|' || c == '&') {
         sb.append('\\');
       }
       sb.append(c);
---------------
-------------
@@ -60,7 +60,7 @@
     PerFieldAnalyzerWrapper analyzer = new PerFieldAnalyzerWrapper(new SimpleAnalyzer());
     analyzer.addAnalyzer("partnum", new KeywordAnalyzer());
 
-    QueryParser queryParser = new QueryParser(Version.LUCENE_CURRENT, "description", analyzer);
+QueryParser queryParser = new QueryParser(TEST_VERSION_CURRENT, "description", analyzer);
     Query query = queryParser.parse("partnum:Q36 AND SPACE");
 
     ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
---------------
-------------
@@ -880,7 +880,7 @@
         return dataFileDirectory;
     }
 
-    public static String getLogFileLocation()
+public static String getCommitLogLocation()
     {
         return conf.commitlog_directory;
     }
---------------
-------------
@@ -87,7 +87,7 @@
    * @param expected expected JSON Object
    * @param delta tollerance allowed in comparing float/double values
    */
-  public static String matchObj(String path, Object input, Object expected, double delta) throws Exception {
+public static String matchObj(String path, Object input, Object expected, double delta) {
     CollectionTester tester = new CollectionTester(input,delta);
     boolean reversed = path.startsWith("!");
     String positivePath = reversed ? path.substring(1) : path;
---------------
-------------
@@ -412,7 +412,7 @@
 
 		try {
 			xaResource.end(xid,xaflags);
-            xid = null;
+this.xid = null;
 			if (SanityManager.DEBUG)
 			{
 				connThread.trace("ended XA transaction. xid =  " + xid +
---------------
-------------
@@ -116,7 +116,7 @@
             byte[] startColumn = ArrayUtils.EMPTY_BYTE_ARRAY;
             while (true)
             {
-                QueryFilter filter = new SliceQueryFilter(tableName, new QueryPath(cfs.getColumnFamilyName()), startColumn, ArrayUtils.EMPTY_BYTE_ARRAY, false, PAGE_SIZE);
+QueryFilter filter = new SliceQueryFilter(key, new QueryPath(cfs.getColumnFamilyName()), startColumn, ArrayUtils.EMPTY_BYTE_ARRAY, false, PAGE_SIZE);
                 ColumnFamily cf = cfs.getColumnFamily(filter);
                 if (pagingFinished(cf, startColumn))
                     break;
---------------
-------------
@@ -204,7 +204,7 @@
       final SegmentInfo si = new SegmentInfo(si1.info.dir, Constants.LUCENE_MAIN_VERSION, merged, -1, false, codec, null, null);
 
       SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), trackingDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,
-                                               MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), context);
+MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), context);
 
       merger.add(r1);
       merger.add(r2);
---------------
-------------
@@ -518,7 +518,7 @@
     @Override
     public void run() {
       try {
-        final LineFileDocs docs = new LineFileDocs(random);
+final LineFileDocs docs = new LineFileDocs(random, defaultCodecSupportsDocValues());
         int numDocs = 0;
         while (System.nanoTime() < endTimeNanos) {
           final int what = random.nextInt(3);
---------------
-------------
@@ -160,7 +160,7 @@
 
         /* get the various column ranges we have to read */
         AbstractType comparator = metadata.comparator;
-        SortedSet<IndexHelper.IndexInfo> ranges = new TreeSet<IndexHelper.IndexInfo>(IndexHelper.getComparator(comparator));
+SortedSet<IndexHelper.IndexInfo> ranges = new TreeSet<IndexHelper.IndexInfo>(IndexHelper.getComparator(comparator, false));
         for (ByteBuffer name : filteredColumnNames)
         {
             int index = IndexHelper.indexFor(name, indexList, comparator, false);
---------------
-------------
@@ -52,6 +52,6 @@
   public void testPropsDefaults() throws Exception {
     IndexWriter writer = new ExposeWriterHandler().getWriter();
     ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler)writer.getMergeScheduler();
-    assertEquals(10, cms.getMaxThreadCount());
+assertEquals(4, cms.getMaxThreadCount());
   }
 }
---------------
-------------
@@ -161,7 +161,7 @@
       }
     }
 
-    addVmOpt(b, "randomized.file.encoding", System.getProperty("file.encoding"));
+addVmOpt(b, "tests.file.encoding", System.getProperty("file.encoding"));
 
     System.err.println(b.toString());
   }
---------------
-------------
@@ -179,7 +179,7 @@
   
   public static long getTotalTermFreq(IndexReader reader, final String field, final BytesRef termText) throws Exception {   
     long totalTF = 0L;
-    for (final AtomicReaderContext ctx : reader.getTopReaderContext().leaves()) {
+for (final AtomicReaderContext ctx : reader.leaves()) {
       AtomicReader r = ctx.reader();
       Bits liveDocs = r.getLiveDocs();
       if (liveDocs == null) {
---------------
-------------
@@ -97,7 +97,7 @@
    * Tests {@link KMeansClusterer#runKMeansIteration) single run convergence with a given distance threshold.
    */
   @Test
-  public void testRunKMeansIteration_convergesInOneRunWithGivenDistanceThreshold() {
+public void testRunKMeansIterationConvergesInOneRunWithGivenDistanceThreshold() {
     double[][] rawPoints = { {0,0}, {0,0.25}, {0,0.75}, {0, 1}};
     List<Vector> points = getPoints(rawPoints);
 
---------------
-------------
@@ -116,7 +116,7 @@
     int i = 0;
     try {
       for (Object label : labels) {
-        String theLabel = ((Pair<?,?>) label).getFirst().toString();
+String theLabel = ((Pair<?,?>) label).getFirst().toString().split("/")[1];
         if (!seen.contains(theLabel)) {
           writer.append(new Text(theLabel), new IntWritable(i++));
           seen.add(theLabel);
---------------
-------------
@@ -138,7 +138,7 @@
           iwRef.decref();
         } else {
           if (success) {
-            IOUtils.close(iw);
+iw.shutdown();
           } else {
             IOUtils.closeWhileHandlingException(iw);
           }
---------------
-------------
@@ -184,7 +184,7 @@
 			realConnection.setApplicationConnection(currentConnectionHandle);
 	}
 
-	final Connection getNewCurrentConnectionHandle() {
+final Connection getNewCurrentConnectionHandle() throws SQLException {
 		Connection applicationConnection = currentConnectionHandle =
 			((org.apache.derby.jdbc.Driver20) (realConnection.getLocalDriver())).newBrokeredConnection(this);
 		realConnection.setApplicationConnection(applicationConnection);
---------------
-------------
@@ -239,7 +239,7 @@
                     if (url == null) {
                         LOGGER.warn("No URL is defined for schema " + ns + ". This schema will not be validated");
                     } else {
-                        schemaSources.add(new StreamSource(url.openStream()));
+schemaSources.add(new StreamSource(url.openStream(), url.toExternalForm()));
                     }
                 }
                 schema = getSchemaFactory().newSchema(schemaSources.toArray(new Source[schemaSources.size()]));
---------------
-------------
@@ -37,7 +37,7 @@
   protected int count;
   protected int position;
 
-  public TermSpans(DocsAndPositionsEnum postings, Term term) throws IOException {
+public TermSpans(DocsAndPositionsEnum postings, Term term) {
     this.postings = postings;
     this.term = term;
     doc = -1;
---------------
-------------
@@ -96,7 +96,7 @@
     final static String GOSSIP_DIGEST_ACK2_VERB = "GA2V";
     public final static int intervalInMillis_ = 1000;
     private static Logger logger_ = Logger.getLogger(Gossiper.class);
-    static Gossiper gossiper_;
+private static volatile Gossiper gossiper_;
 
     public synchronized static Gossiper instance()
     {
---------------
-------------
@@ -70,6 +70,6 @@
           }
     sb.setLength(sb.length() - 2); // discard last ", "
     sb.append("]");
-    return super.toString();
+return sb.toString();
   }
 }
---------------
-------------
@@ -89,7 +89,7 @@
      * @param key - key for which we need to find the endpoint return value -
      * the endpoint responsible for this key
      */
-    public List<InetAddress> getNaturalEndpoints(String key, String table);
+public List<InetAddress> getNaturalEndpoints(String table, byte[] key);
 
     /**
      * Forces major compaction (all sstable files compacted)
---------------
-------------
@@ -54,7 +54,7 @@
     }
 
     @Override
-    public final boolean incrementToken() throws IOException {
+public final boolean incrementToken() {
       clearAttributes();
       if (index < testToken.length) {
         Token t = testToken[index++];
---------------
-------------
@@ -39,7 +39,7 @@
     public byte[] value(String key);
     public Collection<IColumn> getSubColumns();
     public IColumn getSubColumn(String columnName);
-    public void addColumn(String name, IColumn column);
+public void addColumn(IColumn column);
     public IColumn diff(IColumn column);
     public int getObjectCount();
     public byte[] digest();
---------------
-------------
@@ -143,7 +143,7 @@
     SimpleFragListBuilder sflb = new SimpleFragListBuilder();
     FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
     assertEquals( 1, ffl.fragInfos.size() );
-    assertEquals( "subInfos=(d((6,7)))/1.0(0,100)", ffl.fragInfos.get( 0 ).toString() );
+assertEquals( "subInfos=(d((9,10)))/1.0(3,103)", ffl.fragInfos.get( 0 ).toString() );
   }
   
   public void test1PhraseLongMV() throws Exception {
---------------
-------------
@@ -553,7 +553,7 @@
       expected = new String[] {"_0.cfs",
                                "_0_1.del",
                                "_0_1.s" + contentFieldIndex,
-                               "segments_3",
+"segments_2",
                                "segments.gen"};
 
       String[] actual = dir.listAll();
---------------
-------------
@@ -349,7 +349,7 @@
 
   /** Given an indexed term, append the human readable representation*/
   public CharsRef indexedToReadable(BytesRef input, CharsRef output) {
-    input.utf8ToChars(output);
+UnicodeUtil.UTF8toUTF16(input, output);
     return output;
   }
 
---------------
-------------
@@ -275,7 +275,7 @@
     ff.inform( new ResourceLoader() {
 
       @Override
-      public <T> T newInstance(String cname, Class<T> expectedType, String... subpackages) {
+public <T> T newInstance(String cname, Class<T> expectedType) {
         throw new RuntimeException("stub");
       }
 
---------------
-------------
@@ -167,7 +167,7 @@
             writer.write(prefix + Path.SEPARATOR + name, file.toString());
           }
         }
-      } catch (Exception e) {
+} catch (IOException e) {
         throw new IllegalStateException(e);
       }
       return false;
---------------
-------------
@@ -44,7 +44,7 @@
 import org.apache.derby.iapi.store.access.xa.XAXactId;
 import org.apache.derby.impl.jdbc.EmbedConnection;
 import org.apache.derby.impl.jdbc.TransactionResourceImpl;
-import org.apache.derby.shared.common.sanity.SanityManager;
+import org.apache.derby.iapi.services.sanity.SanityManager;
 import org.apache.derby.iapi.services.property.PropertyUtil;
 import org.apache.derby.iapi.reference.Property;
 
---------------
-------------
@@ -1610,7 +1610,7 @@
             String apppropsjvmflags = ap.getProperty("jvmflags");
             if (apppropsjvmflags != null)
             {
-                if (jvmflags != null)
+if (jvmflags != null && jvmflags.length() > 0)
                     jvmflags = apppropsjvmflags + "^" + jvmflags;
                 else
                     jvmflags = apppropsjvmflags;
---------------
-------------
@@ -100,7 +100,7 @@
     numTerms.addAndGet(packet.numTermDeletes);
     bytesUsed.addAndGet(packet.bytesUsed);
     if (infoStream != null) {
-      message("push deletes " + packet + " delGen=" + packet.delGen() + " packetCount=" + deletes.size());
+message("push deletes " + packet + " delGen=" + packet.delGen() + " packetCount=" + deletes.size() + " totBytesUsed=" + bytesUsed.get());
     }
     assert checkDeleteStats();
     return packet.delGen();
---------------
-------------
@@ -447,7 +447,7 @@
   }
 
   static class MyFieldComparatorSource extends FieldComparatorSource {
-    FieldComparator newComparator(String fieldname, IndexReader[] subReaders, int numHits, int sortPos, boolean reversed) {
+public FieldComparator newComparator(String fieldname, IndexReader[] subReaders, int numHits, int sortPos, boolean reversed) {
       return new MyFieldComparator(numHits);
     }
   }
---------------
-------------
@@ -493,7 +493,7 @@
 	 *
 	 * @exception StandardException		Thrown on error
 	 */
-	public ResultColumnList getAllResultColumns(String allTableName)
+public ResultColumnList getAllResultColumns(TableName allTableName)
 					throws StandardException
 	{
 		if (SanityManager.DEBUG)
---------------
-------------
@@ -116,7 +116,7 @@
 
   @Test
   public void testTermIndexInterval() throws Exception {
-    IndexWriter writer = ((DirectUpdateHandler2)h.getCore().getUpdateHandler()).getIndexWriterProvider().getIndexWriter(h.getCore());
+IndexWriter writer = ((DirectUpdateHandler2)h.getCore().getUpdateHandler()).getSolrCoreState().getIndexWriter(h.getCore());
     int interval = writer.getConfig().getTermIndexInterval();
     assertEquals(256, interval);
   }
---------------
-------------
@@ -65,7 +65,7 @@
       CodecProvider.getDefault().setDefaultFieldCodec("Standard");
     }
 
-    final LineFileDocs docs = new LineFileDocs(true);
+final LineFileDocs docs = new LineFileDocs(random);
     final File tempDir = _TestUtil.getTempDir("nrtopenfiles");
     final MockDirectoryWrapper dir = new MockDirectoryWrapper(random, FSDirectory.open(tempDir));
     final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer());
---------------
-------------
@@ -1018,7 +1018,7 @@
 	 * @return	The DataTypeServices from this Node.  This
 	 *		may be null if the node isn't bound yet.
 	 */
-	public DataTypeDescriptor getTypeServices()
+public DataTypeDescriptor getTypeServices() throws StandardException
 	{
         DataTypeDescriptor dtd = super.getTypeServices();
         if( dtd == null && source != null)
---------------
-------------
@@ -126,7 +126,7 @@
     assert docWriter.writer.testPoint("StoredFieldsWriter.finishDocument end");
   }
 
-  public void addField(IndexableField field, FieldInfo fieldInfo) throws IOException {
+public void addField(IndexableField field, FieldInfo fieldInfo) {
     if (numStoredFields == storedFields.length) {
       int newSize = ArrayUtil.oversize(numStoredFields + 1, RamUsageEstimator.NUM_BYTES_OBJECT_REF);
       IndexableField[] newArray = new IndexableField[newSize];
---------------
-------------
@@ -706,7 +706,7 @@
     /***  REMOVED -YCS
     if (defaultFieldType != null) return new SchemaField(fieldName,defaultFieldType);
     ***/
-    throw new SolrException(1,"undefined field "+fieldName);
+throw new SolrException(400,"undefined field "+fieldName);
   }
 
   /**
---------------
-------------
@@ -724,7 +724,7 @@
           // Write term stats, to separate byte[] blob:
           bytesWriter2.writeVInt(term.stats.docFreq);
           if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-            assert term.stats.totalTermFreq >= term.stats.docFreq;
+assert term.stats.totalTermFreq >= term.stats.docFreq: term.stats.totalTermFreq + " vs " + term.stats.docFreq;
             bytesWriter2.writeVLong(term.stats.totalTermFreq - term.stats.docFreq);
           }
         }
---------------
-------------
@@ -56,7 +56,7 @@
       if (cnt++ == 15) {
         // get out of line
         lock.unlock();
-        throw new RuntimeException("Coulnd't aquire the shard lock");
+throw new RuntimeException("Couldn't acquire the shard lock");
       }
       Thread.sleep(1000);
     }
---------------
-------------
@@ -345,7 +345,7 @@
     private Map<Method, List<Object>> findMatchingMethods(Class type, String name, boolean instance, List<Object> args, List<ReifiedType> types) {
         Map<Method, List<Object>> matches = new HashMap<Method, List<Object>>();
         // Get constructors
-        List<Method> methods = getPublicMethods(type);
+List<Method> methods = new ArrayList<Method>(Arrays.asList(getPublicMethods(type)));
         // Discard any signature with wrong cardinality
         for (Iterator<Method> it = methods.iterator(); it.hasNext();) {
             Method mth = it.next();
---------------
-------------
@@ -59,7 +59,7 @@
             for (int i = 0; i < session.getSuperColumns(); i++)
             {
                 String superColumnName = "S" + Integer.toString(i);
-                superColumns.add(new SuperColumn(ByteBuffer.wrap(superColumnName.getBytes()), columns));
+superColumns.add(new SuperColumn(ByteBufferUtil.bytes(superColumnName), columns));
             }
         }
 
---------------
-------------
@@ -65,7 +65,7 @@
 
   public SolrDispatchFilter() {
     try {
-      adminRequestParser = new SolrRequestParsers(new Config(null,"solr",new ByteArrayInputStream("<root/>".getBytes()),"") );
+adminRequestParser = new SolrRequestParsers(new Config(null,"solr",new ByteArrayInputStream("<root/>".getBytes("UTF-8")),"") );
     } catch (Exception e) {
       //unlikely
       throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,e);
---------------
-------------
@@ -1357,7 +1357,7 @@
     String TYPE_MISMATCH = "XJ020.S";
     String INVALID_JDBCTYPE = "XJ021.S";
     String SET_STREAM_FAILURE = "XJ022.S";
-    String SET_STREAM_INSUFFICIENT_DATA = "XJ023.S";
+String SET_STREAM_INEXACT_LENGTH_DATA = "XJ023.S";
     String SET_UNICODE_INVALID_LENGTH = "XJ024.S";
     String NEGATIVE_STREAM_LENGTH = "XJ025.S";
     String NO_AUTO_COMMIT_ON = "XJ030.S";
---------------
-------------
@@ -4157,7 +4157,7 @@
     public void listenToUnitOfWork() {
         if (!listenToUnitOfWork_) {
             listenToUnitOfWork_ = true;
-            connection_.CommitAndRollbackListeners_.add(this);
+connection_.CommitAndRollbackListeners_.put(this, null);
         }
     }
 
---------------
-------------
@@ -226,7 +226,7 @@
                                       new BytesRef("aaa"),
                                       MultiFields.getLiveDocs(reader),
                                       null,
-                                      0);
+false);
       int count = 0;
       while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
         count++;
---------------
-------------
@@ -149,7 +149,7 @@
     //IndexReader reader = new TestReader(IndexReader.open(directory, true));
     Directory target = newDirectory();
     writer = new IndexWriter(target, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));
-    IndexReader reader = new TestReader(IndexReader.open(directory, true));
+IndexReader reader = new TestReader(SlowMultiReaderWrapper.wrap(IndexReader.open(directory, true)));
     writer.addIndexes(reader);
     writer.close();
     reader.close();
---------------
-------------
@@ -433,7 +433,7 @@
     IndexReader reader = writer.getReader(applyAllDeletes);
 
     // If in fact no changes took place, return null:
-    if (reader.getVersion() == getVersion()) {
+if (reader.getVersion() == segmentInfos.getVersion()) {
       reader.decRef();
       return null;
     }
---------------
-------------
@@ -963,7 +963,7 @@
 
   @Nightly
   public void testBigSet() throws IOException {
-    testRandomWords(atLeast(50000), atLeast(1));
+testRandomWords(_TestUtil.nextInt(random, 50000, 60000), atLeast(1));
   }
 
   private static String inputToString(int inputMode, IntsRef term) {
---------------
-------------
@@ -45,7 +45,7 @@
     float totalBoost = 0;
     List<SubInfo> subInfos = new ArrayList<SubInfo>();
     for( WeightedPhraseInfo phraseInfo : phraseInfoList ){
-      subInfos.add( new SubInfo( phraseInfo.getText(), phraseInfo.getTermsOffsets(), phraseInfo.getSeqnum() ) );
+subInfos.add( new SubInfo( phraseInfo.getText(), phraseInfo.getTermsOffsets(), phraseInfo.getSeqnum(), phraseInfo.getBoost() ) );
       totalBoost += phraseInfo.getBoost();
     }
     getFragInfos().add( new WeightedFragInfo( startOffset, endOffset, subInfos, totalBoost ) );
---------------
-------------
@@ -86,7 +86,7 @@
 
 			if (firstTime) {
 				Logs.reportString("----------------------------------------------");
-				Logs.reportMessage("CSLOOK_TablesHeader");
+Logs.reportMessage("DBLOOK_TablesHeader");
 				Logs.reportString("----------------------------------------------\n");
 			}
 
---------------
-------------
@@ -47,7 +47,7 @@
     writer.optimize();
     writer.close();
 
-    Searcher searcher = new IndexSearcher(store);
+Searcher searcher = new IndexSearcher(store, true);
       QueryParser parser = new QueryParser("field", new SimpleAnalyzer());
     Query query = parser.parse("a NOT b");
     //System.out.println(query);
---------------
-------------
@@ -262,7 +262,7 @@
         LOG.info("running multithreaded full-import");
         new EntityRunner(root,null).run(null,Context.FULL_DUMP,null);
       } catch (Exception e) {
-        LOG.error("error in import", e);
+throw new RuntimeException("Error in multi-threaded import", e);
       }
     } else {
       buildDocument(getVariableResolver(), null, null, root, true, null);
---------------
-------------
@@ -65,7 +65,7 @@
   protected void setUp() throws Exception {
     super.setUp();
     similarityOne = new SimilarityOne();
-    anlzr = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+anlzr = new StandardAnalyzer(TEST_VERSION_CURRENT);
   }
 
   /**
---------------
-------------
@@ -604,7 +604,7 @@
   /**
    * Tell all {@link ResourceLoaderAware} instances about the loader
    */
-  public void inform( ResourceLoader loader ) 
+public void inform( ResourceLoader loader ) throws IOException
   {
 
      // make a copy to avoid potential deadlock of a callback adding to the list
---------------
-------------
@@ -150,7 +150,7 @@
   public void testStressIndexAndSearching() throws Exception {
 
     // First in a RAM directory:
-    Directory directory = new RAMDirectory();
+Directory directory = new MockRAMDirectory();
     runStressTest(directory);
     directory.close();
 
---------------
-------------
@@ -56,7 +56,7 @@
       dir = new File(str);
     }
 
-    if ("false".equals(args.get("wait"))) wait=false;
+if ("false".equals(args.get("wait")) || Boolean.FALSE.equals(args.get("wait"))) wait=false;
   }
 
   protected int exec(String callback) {
---------------
-------------
@@ -68,7 +68,7 @@
      * the memtable. This version will respect the threshold and flush
      * the memtable to disk when the size exceeds the threshold.
     */
-    void put(String key, byte[] buffer) throws IOException
+void put(String key, byte[] buffer)
     {
         if (isThresholdViolated())
         {
---------------
-------------
@@ -478,7 +478,7 @@
     }
 
     @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs /* ignored */) throws IOException {
+public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags /* ignored */) throws IOException {
       TVDocsEnum docsEnum;
       if (reuse != null && reuse instanceof TVDocsEnum) {
         docsEnum = (TVDocsEnum) reuse;
---------------
-------------
@@ -67,7 +67,7 @@
 
 
 	/**
-		@exception StandardException Standard Cloudscape error policy
+@exception StandardException Standard Derby error policy
 	*/
 	public void cleanupOnError(Throwable error) throws StandardException {
 
---------------
-------------
@@ -186,7 +186,7 @@
    * will return a refreshed instance. Otherwise, consider using the
    * non-blocking {@link #maybeRefresh()}.
    */
-  public final void maybeRefreshBlocking() throws IOException, InterruptedException {
+public final void maybeRefreshBlocking() throws IOException {
     ensureOpen();
 
     // Ensure only 1 thread does refresh at once
---------------
-------------
@@ -697,7 +697,7 @@
       
       start = pos;      
       
-      if (ch=='+' || ch=='-') {
+if ((ch=='+' || ch=='-') && (pos+1)<end) {
         clause.must = ch;
         pos++;
       }
---------------
-------------
@@ -27,7 +27,7 @@
 
 
 public class ZkCmdExecutor {
-  private long retryDelay = 1300L; // 300 ms over for padding
+private long retryDelay = 1500L; // 500 ms over for padding
   private int retryCount;
   private List<ACL> acl = ZooDefs.Ids.OPEN_ACL_UNSAFE;
   
---------------
-------------
@@ -69,7 +69,7 @@
             {            
                 if (logger_.isDebugEnabled())
                   logger_.debug("Processing node " + node);
-                byte[] bytes = headers.remove(node);
+headers.remove(node);
                 /* Send a message to this node to alter its membership state. */
                 EndPoint targetNode = new EndPoint(node, DatabaseDescriptor.getStoragePort());                
                 
---------------
-------------
@@ -1505,7 +1505,7 @@
         		}
         		// if the first fixVersion does not match the current release, or the beta,
         		// check the next one
-        		if ( (!_releaseID.equals(fixVersion)) && (!"10.3.0.0".equals(fixVersion))) {
+if ( (!_releaseID.equals(fixVersion)) && (!"10.3.0.0".equals(fixVersion)) && (!"10.3.1.1".equals(fixVersion))) {
         			try {
         				fixVersion = squeezeText(getNextChild( itemElement, JIRA_FIXIN, i+1));
         				continue;
---------------
-------------
@@ -53,7 +53,7 @@
         }
         
         // Create a BufferedReader to read the list of tests to skip
-        BufferedReader listFile = new BufferedReader(new InputStreamReader(is));
+BufferedReader listFile = new BufferedReader(new InputStreamReader(is, "UTF-8"));
         String str = "";
         // Read the list of tests to skip, compare to testName
         while ( (str = listFile.readLine()) != null )
---------------
-------------
@@ -550,7 +550,7 @@
   private void verifyTermDocs(Directory dir, Term term, int numDocs)
       throws IOException {
     IndexReader reader = DirectoryReader.open(dir);
-    DocsEnum docsEnum = _TestUtil.docs(random(), reader, term.field, term.bytes, null, null, false);
+DocsEnum docsEnum = _TestUtil.docs(random(), reader, term.field, term.bytes, null, null, 0);
     int count = 0;
     while (docsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS)
       count++;
---------------
-------------
@@ -175,7 +175,7 @@
                 writer.append(key, bytes);
             }
         }
-        cfStore.storeLocation(writer.closeAndOpenReader(DatabaseDescriptor.getKeysCachedFraction(table_)));
+cfStore.storeLocation(writer.closeAndOpenReader());
         columnFamilies_.clear();       
     }
 }
---------------
-------------
@@ -50,7 +50,7 @@
 
     public static final BigInteger TWO = new BigInteger("2");
 
-    private static InetAddress localInetAddress_;
+private static volatile InetAddress localInetAddress_;
 
     public static String[] strip(String string, String token)
     {
---------------
-------------
@@ -39,7 +39,7 @@
   public static class SimpleSimilarity extends TFIDFSimilarity {
     public float queryNorm(float sumOfSquaredWeights) { return 1.0f; }
     public float coord(int overlap, int maxOverlap) { return 1.0f; }
-    @Override public void computeNorm(FieldInvertState state, Norm norm) { norm.setByte(encodeNormValue(state.getBoost())); }
+@Override public float lengthNorm(FieldInvertState state) { return state.getBoost(); }
     @Override public float tf(float freq) { return freq; }
     @Override public float sloppyFreq(int distance) { return 2.0f; }
     @Override public float idf(long docFreq, long numDocs) { return 1.0f; }
---------------
-------------
@@ -168,7 +168,7 @@
     int hi = searchables.length - 1;		  // for first element less
 						  // than n, return its index
     while (hi >= lo) {
-      int mid = (lo + hi) >> 1;
+int mid = (lo + hi) >>> 1;
       int midValue = starts[mid];
       if (n < midValue)
 	hi = mid - 1;
---------------
-------------
@@ -361,7 +361,7 @@
 			(FromBaseTable)
 				(getNodeFactory().getNode(
 										C_NodeTypes.FROM_BASE_TABLE,
-										targetTableName,
+synonymTableName != null ? synonymTableName : targetTableName,
 										null,
 										null,
 										null,
---------------
-------------
@@ -79,7 +79,7 @@
      */
     private void validateRoleType(Role role, int roleType) throws IOException {
         if (role.getType() != roleType) {
-            throw new IllegalArgumentException("Unexpected role type. Expected " + roleType + " but got " + role.getType());
+throw new IOException("Unexpected role type. Expected " + roleType + " but got " + role.getType());
         }
     }
 
---------------
-------------
@@ -842,7 +842,7 @@
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
     final String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, EXTENSION);
-    final IndexInput in = state.dir.openInput(fileName, IOContext.READONCE);
+final IndexInput in = state.directory.openInput(fileName, IOContext.READONCE);
 
     final SortedMap<String,TermsReader> fields = new TreeMap<String,TermsReader>();
 
---------------
-------------
@@ -1367,7 +1367,7 @@
           r = (r instanceof AtomicReader) ?
             new ParallelAtomicReader((AtomicReader) r) :
             new ParallelCompositeReader((CompositeReader) r);
-        } else {
+} else if (r instanceof CompositeReader) { // only wrap if not already atomic (some tests may fail)
           r = new MultiReader(r);
         }
       }
---------------
-------------
@@ -49,7 +49,7 @@
       bits.flip(idx, idx + 1);
     }
     
-    FixedBitSet verify = new FixedBitSet(bits);
+FixedBitSet verify = bits.clone();
 
     ScoredDocIDs scoredDocIDs = ScoredDocIdsUtils.createScoredDocIds(bits, n); 
 
---------------
-------------
@@ -369,7 +369,7 @@
             catch (DigestMismatchException ex)
             {
                 AbstractReplicationStrategy rs = Table.open(command.table).getReplicationStrategy();
-                QuorumResponseHandler<Row> handler = rs.getQuorumResponseHandler(new ReadResponseResolver(command.table), ConsistencyLevel.QUORUM);
+QuorumResponseHandler<Row> handler = rs.getQuorumResponseHandler(new ReadResponseResolver(command.table), consistency_level);
                 if (logger.isDebugEnabled())
                     logger.debug("Digest mismatch:", ex);
                 Message messageRepair = command.makeReadMessage();
---------------
-------------
@@ -86,7 +86,7 @@
    * Shift the bias of the model.
    * @param factor factor to multiply the bias by.
    */
-  public synchronized void shiftBias(double factor) {
+public void shiftBias(double factor) {
     this.bias += factor;
   }
   
---------------
-------------
@@ -17,7 +17,7 @@
  */
 
 import com.spatial4j.core.distance.DistanceUtils;
-import com.spatial4j.core.util.GeohashUtils;
+import com.spatial4j.core.io.GeohashUtils;
 import org.apache.solr.SolrTestCaseJ4;
 import org.apache.solr.common.SolrException;
 import org.junit.BeforeClass;
---------------
-------------
@@ -389,7 +389,7 @@
       gotExpectedException = true;
     }
     assertTrue("Should have gotten NoMoreDataException!", gotExpectedException);
-    assertEquals("Wrong numbre of documents created by osurce!",5,n);
+assertEquals("Wrong number of documents created by osurce!",5,n);
     assertTrue("Did not see all types!",unseenTypes.isEmpty());
   }
 
---------------
-------------
@@ -79,7 +79,7 @@
             }
 
             GossipDigestAck2Message gDigestAck2 = new GossipDigestAck2Message(deltaEpStateMap);
-            Message gDigestAck2Message = Gossiper.instance.makeGossipDigestAck2Message(gDigestAck2);
+Message gDigestAck2Message = Gossiper.instance.makeGossipDigestAck2Message(gDigestAck2, message.getVersion());
             if (logger_.isTraceEnabled())
                 logger_.trace("Sending a GossipDigestAck2Message to {}", from);
             MessagingService.instance().sendOneWay(gDigestAck2Message, from);
---------------
-------------
@@ -1700,7 +1700,7 @@
 			ResultColumn resultColumn = (ResultColumn) elementAt(index);
 
 			/* Skip over generated columns */
-			if (resultColumn.isGenerated())
+if (resultColumn.isGenerated() || resultColumn.isGeneratedForUnmatchedColumnInInsert())
 			{
 				continue;
 			}
---------------
-------------
@@ -92,7 +92,7 @@
     return analyze(analyzer, text, reuse);
   }
   
-  private void addInternal(CharsRef synset[], int size) throws IOException {
+private void addInternal(CharsRef synset[], int size) {
     if (size <= 1) {
       return; // nothing to do
     }
---------------
-------------
@@ -478,7 +478,7 @@
     }
 
     @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags /* ignored */) throws IOException {
+public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs /* ignored */) throws IOException {
       TVDocsEnum docsEnum;
       if (reuse != null && reuse instanceof TVDocsEnum) {
         docsEnum = (TVDocsEnum) reuse;
---------------
-------------
@@ -48,7 +48,7 @@
       throw new UnsupportedSpatialOperation(op);
 
     Shape shape = args.getShape();
-    int detailLevel = grid.getMaxLevelForPrecision(shape, args.getDistPrecision());
+int detailLevel = grid.getLevelForDistance(args.resolveDistErr(ctx, distErrPct));
     List<Node> cells = grid.getNodes(shape, detailLevel, false);
     TermsFilter filter = new TermsFilter();
     for (Node cell : cells) {
---------------
-------------
@@ -218,7 +218,7 @@
     if (start < 0 || end > length || start > end) {
       throw new IndexOutOfBoundsException();
     }
-    return new CharsRef(chars, offset + start, offset + end);
+return new CharsRef(chars, offset + start, end - start);
   }
   
   /** @deprecated This comparator is only a transition mechanism */
---------------
-------------
@@ -1265,7 +1265,7 @@
       reader = (AtomicReader) readerIn;
     } else {
       // Composite reader: lookup sub-reader and re-base docID:
-      List<AtomicReaderContext> leaves = readerIn.getTopReaderContext().leaves();
+List<AtomicReaderContext> leaves = readerIn.leaves();
       int subIndex = ReaderUtil.subIndex(docID, leaves);
       reader = leaves.get(subIndex).reader();
       docID -= leaves.get(subIndex).docBase;
---------------
-------------
@@ -586,7 +586,7 @@
 
       // Finally tell anyone who wants to know
       resourceLoader.inform( resourceLoader );
-      resourceLoader.inform( this );
+resourceLoader.inform( this );  // last call before the latch is released.
       instance = this;   // set singleton for backwards compatibility
     } catch (IOException e) {
       throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);
---------------
-------------
@@ -43,7 +43,7 @@
    *  @param pageSize         the size of a single page */
   public MonotonicAppendingLongBuffer(int initialPageCount, int pageSize) {
     super(initialPageCount, pageSize);
-    averages = new float[pageSize];
+averages = new float[initialPageCount];
   }
 
   /** Create an {@link MonotonicAppendingLongBuffer} with initialPageCount=16
---------------
-------------
@@ -327,7 +327,7 @@
         final CFMetaData indexedCfMetadata = CFMetaData.newIndexMetadata(metadata, info, columnComparator);
         ColumnFamilyStore indexedCfs = ColumnFamilyStore.createColumnFamilyStore(table,
                                                                                  indexedCfMetadata.cfName,
-                                                                                 new LocalPartitioner(metadata.getColumn_metadata().get(info.name).validator),
+new LocalPartitioner(metadata.getColumn_metadata().get(info.name).getValidator()),
                                                                                  indexedCfMetadata);
 
         // link in indexedColumns.  this means that writes will add new data to the index immediately,
---------------
-------------
@@ -135,7 +135,7 @@
   @SuppressWarnings("unchecked")
   public TopGroups<BytesRef> result() {
     if (firstPhaseGroups.isEmpty()) {
-      return new TopGroups<BytesRef>(groupSort.getSort(), sortWithinGroup.getSort(), 0, 0, new GroupDocs[0]);
+return new TopGroups<BytesRef>(groupSort.getSort(), sortWithinGroup.getSort(), 0, 0, new GroupDocs[0], Float.NaN);
     }
 
     return secondPassCollector.getTopGroups(0);
---------------
-------------
@@ -90,7 +90,7 @@
     /**
      * The Derby network client.
      */
-    static final JDBCClient DERBYNETCLIENT= new JDBCClient(
+public static final JDBCClient DERBYNETCLIENT= new JDBCClient(
             "DerbyNetClient",
             "org.apache.derby.jdbc.ClientDriver",
 
---------------
-------------
@@ -127,7 +127,7 @@
             logger.debug(String.format("Binding avro service to %s:%s", listenAddr, listenPort));
         SpecificResponder responder = new SpecificResponder(Cassandra.class, new CassandraServer());
         
-        logger.info("Cassandra starting up...");
+logger.info("Listening for avro clients...");
         Mx4jTool.maybeLoad();
         // FIXME: This isn't actually binding to listenAddr (it should).
         server = new HttpServer(responder, listenPort);
---------------
-------------
@@ -321,7 +321,7 @@
   }
 
   @Override
-  protected void visitSubScorers(Query parent, Occur relationship, ScorerVisitor<Query, Query, Scorer> visitor) {
+public void visitSubScorers(Query parent, Occur relationship, ScorerVisitor<Query, Query, Scorer> visitor) {
     super.visitSubScorers(parent, relationship, visitor);
     final Query q = weight.getQuery();
     for (Scorer s : optionalScorers) {
---------------
-------------
@@ -90,7 +90,7 @@
                     is = url.openStream();
                 } catch (IOException e) {
                     if (ignoreMissingLocations) {
-                        LOGGER.info("Unable to load properties from url " + url, e);
+LOGGER.debug("Unable to load properties from url " + url + " while ignoreMissingLocations is set to true");
                     } else {
                         throw e;
                     }
---------------
-------------
@@ -177,7 +177,7 @@
     // Second in an FSDirectory:
     String tempDir = System.getProperty("java.io.tmpdir");
     File dirPath = new File(tempDir, "lucene.test.atomic");
-    directory = FSDirectory.getDirectory(dirPath, null, false);
+directory = FSDirectory.getDirectory(dirPath);
     runTest(directory);
     directory.close();
     _TestUtil.rmDir(dirPath);
---------------
-------------
@@ -188,7 +188,7 @@
   private void extractFacetInfo( NamedList<Object> info )
   {
     // Parse the queries
-    _facetQuery = new HashMap<String, Integer>();
+_facetQuery = new LinkedHashMap<String, Integer>();
     NamedList<Integer> fq = (NamedList<Integer>) info.get( "facet_queries" );
     if (fq != null) {
       for( Map.Entry<String, Integer> entry : fq ) {
---------------
-------------
@@ -71,7 +71,7 @@
     int num = atLeast(200);
     for (int i = 0; i < num; i++) {
       String s = _TestUtil.randomUnicodeString(random);
-      field.setValue(s);
+field.setStringValue(s);
       terms.add(new BytesRef(s));
       writer.addDocument(doc);
     }
---------------
-------------
@@ -725,7 +725,7 @@
         // keyspace names must be unique case-insensitively because the keyspace name becomes the directory
         // where we store CF sstables.  Names that differ only in case would thus cause problems on
         // case-insensitive filesystems (NTFS, most installations of HFS+).
-        for (String ksName : DatabaseDescriptor.getTables())
+for (String ksName : Schema.instance.getTables())
         {
             if (ksName.equalsIgnoreCase(newKsName))
                 throw new InvalidRequestException("Keyspace names must be case-insensitively unique");
---------------
-------------
@@ -43,7 +43,7 @@
 		@param lockList the list of Lockable's in the group
 		@param lockCount the number of locks in the group
 
-        @exception StandardException Standard Cloudscape error policy.
+@exception StandardException Standard Derby error policy.
 	*/
 	public void reached(CompatibilitySpace compatibilitySpace, Object group,
 						int limit, Enumeration lockList, int lockCount)
---------------
-------------
@@ -44,7 +44,7 @@
 import org.apache.aries.util.filesystem.ICloseableDirectory;
 import org.apache.aries.util.filesystem.IDirectory;
 import org.apache.aries.util.filesystem.IFile;
-import org.apache.aries.util.filesystem.IOUtils;
+import org.apache.aries.util.io.IOUtils;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
---------------
-------------
@@ -62,7 +62,7 @@
     }
     
     w.forceMerge(1);
-    w.close();
+w.shutdown();
     
     System.out.println("verifying...");
     System.out.flush();
---------------
-------------
@@ -617,7 +617,7 @@
     int NUM_DOCS = atLeast(10);
     for (int i = 0; i < NUM_DOCS; i++) {
       // must be > 4096 so it spans multiple chunks
-      int termCount = atLeast(5000);
+int termCount = _TestUtil.nextInt(random, 4097, 8200);
 
       List<String> doc = new ArrayList<String>();
 
---------------
-------------
@@ -193,7 +193,7 @@
             c2.close();
             fail();
         } catch (SQLException e) {
-            assertSQLState("58009", e);
+assertSQLState("08006", e);
         }
     }
 }
---------------
-------------
@@ -132,7 +132,7 @@
       }
     };
     
-    checkRandomData(random(), a, 10000*RANDOM_MULTIPLIER);
+checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
   }
   
   public void testEmptyTerm() throws IOException {
---------------
-------------
@@ -393,7 +393,7 @@
             {
                 public void run()
                 {
-                    MemtableManager.instance().submit(cfName_, Memtable.this, cLogCtx);
+ColumnFamilyStore.submitFlush(Memtable.this, cLogCtx);
                 }
             };
             flushQueuer = new FutureTask(runnable, null);
---------------
-------------
@@ -175,7 +175,7 @@
 
         // register the mbean
         String type = this.partitioner instanceof LocalPartitioner ? "IndexColumnFamilies" : "ColumnFamilies";
-        mbeanName = "org.apache.cassandra.db:type=" + type + ",keyspace=" + this.table + ",columnfamily=" + columnFamily;
+mbeanName = "org.apache.cassandra.db:type=" + type + ",keyspace=" + this.table.name + ",columnfamily=" + columnFamily;
         try
         {
             MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
---------------
-------------
@@ -85,7 +85,7 @@
         return executorService_.isShutdown();
     }
     
-    public long getTaskCount(){
+public long getPendingTasks(){
         return (executorService_.getTaskCount() - executorService_.getCompletedTaskCount());
     }
 }
---------------
-------------
@@ -61,7 +61,7 @@
     //execute queries for each prefix grid scan level
     for(int i = 0; i <= maxLength; i++) {
       ((RecursivePrefixTreeStrategy)strategy).setPrefixGridScanLevel(i);
-      executeQueries(SpatialMatchConcern.FILTER, QTEST_Cities_IsWithin_BBox);
+executeQueries(SpatialMatchConcern.FILTER, QTEST_Cities_Intersects_BBox);
     }
   }
 
---------------
-------------
@@ -110,7 +110,7 @@
 
 
 	/**
-		@exception StandardException Standard Cloudscape error policy
+@exception StandardException Standard Derby error policy
 	*/
 	public void open() throws StandardException
 	{
---------------
